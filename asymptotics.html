<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Advanced Econometrics with Examples - 2&nbsp; Asymptotic Properties of Estimators</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./testing.html" rel="next">
<link href="./estimators.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./estimators.html">Statistical Theory</a></li><li class="breadcrumb-item"><a href="./asymptotics.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Asymptotic Properties of Estimators</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Advanced Econometrics with Examples</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preliminaries</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Statistical Theory</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimators.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Finite Sample Properties of Estimators</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotics.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Asymptotic Properties of Estimators</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exp_fam.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exponential Families</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stochastic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./adv_asymptotics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Advanced Asymptotics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Linear Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./endog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Endogeniety I: IV and 2SLS</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gls.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Generalized Least Squares</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./simul.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Endogeniety II: Simultaneity and Multiple Regressions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Estimation Frameworks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./extremum.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Extremum Estimators</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gmm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">The Generalized Method of Moments</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mle.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Basic Microeconometrics and Time Series</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./binary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Binary Choice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./panel.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Panel Data I: The Basics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./time_series.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Basic Time Series</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nonpar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Nonparametrics I: Distribution and Density Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nonpar_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Nonparametrics II: Functional Forms</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./semipar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Semiparametrics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sieve.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Sieve Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./partial_id.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Partial Identification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./endog_nonlinear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Endogeniety III: Nonlinear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./weak_inst.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Endogeniety IV: Weak Instruments</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Microeconometrics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./discrete_choice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Discrete Choice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tobit.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Tobit</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./survival.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Survival Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./count.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Count Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mixed.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Mixed and Multilevel Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./GLM.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./panel2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Panel Methods II</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
 <span class="menu-text">Time Series</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./time_math.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Math Review</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
 <span class="menu-text">Causal Inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./causal1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Potential Outcomes Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./matching.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Matching Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rdd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Regression Discontinuity Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./did.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Difference in Differences</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./LATE.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Non-Compliance and LATE</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hetero.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Heterogeneous Treatment Effects</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true">
 <span class="menu-text">More Statistical Theory</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayes1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Bayesian Estimation I: Theory</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayes2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Bayesian Estimation II: Computation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayes3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Bayesian Estimation III: Applications</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./modelselection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./robust.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Robust Statistics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="true">
 <span class="menu-text">Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./learning_theory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Statistical Learning Theory</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ML1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Supervised Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ML2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Dimmension Reduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ML3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Unsupervised Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ML4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Text Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ML5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">ML meets Causal Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#convergence" id="toc-convergence" class="nav-link active" data-scroll-target="#convergence"><span class="header-section-number">2.1</span> Convergence</a>
  <ul class="collapse">
  <li><a href="#convergence-in-mse" id="toc-convergence-in-mse" class="nav-link" data-scroll-target="#convergence-in-mse"><span class="header-section-number">2.1.1</span> Convergence in MSE</a></li>
  <li><a href="#convergence-in-probability" id="toc-convergence-in-probability" class="nav-link" data-scroll-target="#convergence-in-probability"><span class="header-section-number">2.1.2</span> Convergence in Probability</a></li>
  <li><a href="#almost-sure-convergence" id="toc-almost-sure-convergence" class="nav-link" data-scroll-target="#almost-sure-convergence"><span class="header-section-number">2.1.3</span> Almost Sure Convergence</a></li>
  <li><a href="#convergence-in-distribution" id="toc-convergence-in-distribution" class="nav-link" data-scroll-target="#convergence-in-distribution"><span class="header-section-number">2.1.4</span> Convergence in Distribution</a></li>
  <li><a href="#putting-the-pieces-together" id="toc-putting-the-pieces-together" class="nav-link" data-scroll-target="#putting-the-pieces-together"><span class="header-section-number">2.1.5</span> Putting the Pieces Together</a></li>
  </ul></li>
  <li><a href="#consistency" id="toc-consistency" class="nav-link" data-scroll-target="#consistency"><span class="header-section-number">2.2</span> Consistency</a></li>
  <li><a href="#laws-of-large-numbers" id="toc-laws-of-large-numbers" class="nav-link" data-scroll-target="#laws-of-large-numbers"><span class="header-section-number">2.3</span> Laws of Large Numbers</a>
  <ul class="collapse">
  <li><a href="#khinchines-weak-law-of-large-numbers" id="toc-khinchines-weak-law-of-large-numbers" class="nav-link" data-scroll-target="#khinchines-weak-law-of-large-numbers"><span class="header-section-number">2.3.1</span> Khinchine’s Weak Law of Large Numbers</a></li>
  <li><a href="#kolmogorovs-strong-law-of-large-numbers" id="toc-kolmogorovs-strong-law-of-large-numbers" class="nav-link" data-scroll-target="#kolmogorovs-strong-law-of-large-numbers"><span class="header-section-number">2.3.2</span> Kolmogorov’s Strong Law of Large Numbers</a></li>
  <li><a href="#chebyshevs-weak-law-of-large-numbers" id="toc-chebyshevs-weak-law-of-large-numbers" class="nav-link" data-scroll-target="#chebyshevs-weak-law-of-large-numbers"><span class="header-section-number">2.3.3</span> Chebyshev’s Weak Law of Large Numbers</a></li>
  </ul></li>
  <li><a href="#the-continuous-mapping-theorem-and-slutskys-theorem" id="toc-the-continuous-mapping-theorem-and-slutskys-theorem" class="nav-link" data-scroll-target="#the-continuous-mapping-theorem-and-slutskys-theorem"><span class="header-section-number">2.4</span> The Continuous Mapping Theorem and Slutsky’s Theorem</a></li>
  <li><a href="#central-limit-theorems" id="toc-central-limit-theorems" class="nav-link" data-scroll-target="#central-limit-theorems"><span class="header-section-number">2.5</span> Central Limit Theorems</a>
  <ul class="collapse">
  <li><a href="#lindeberg-lévy-clt" id="toc-lindeberg-lévy-clt" class="nav-link" data-scroll-target="#lindeberg-lévy-clt"><span class="header-section-number">2.5.1</span> Lindeberg-Lévy CLT</a></li>
  <li><a href="#not-identically-distributed" id="toc-not-identically-distributed" class="nav-link" data-scroll-target="#not-identically-distributed"><span class="header-section-number">2.5.2</span> Not Identically Distributed</a></li>
  </ul></li>
  <li><a href="#delta-method" id="toc-delta-method" class="nav-link" data-scroll-target="#delta-method"><span class="header-section-number">2.6</span> Delta Method</a></li>
  <li><a href="#little-o_p-big-o_p-and-taylor-expansions" id="toc-little-o_p-big-o_p-and-taylor-expansions" class="nav-link" data-scroll-target="#little-o_p-big-o_p-and-taylor-expansions"><span class="header-section-number">2.7</span> Little <span class="math inline">\(o_p\)</span>, Big <span class="math inline">\(O_p\)</span>, and Taylor Expansions</a></li>
  <li><a href="#asymptotically-normal-estimators" id="toc-asymptotically-normal-estimators" class="nav-link" data-scroll-target="#asymptotically-normal-estimators"><span class="header-section-number">2.8</span> Asymptotically Normal Estimators</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">2.9</span> Further Reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./estimators.html">Statistical Theory</a></li><li class="breadcrumb-item"><a href="./asymptotics.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Asymptotic Properties of Estimators</span></a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-asy" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Asymptotic Properties of Estimators</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>When considering estimators in <a href="#sec-est" class="quarto-xref"><span class="quarto-unresolved-ref">sec-est</span></a>, we kept the sample size <span class="math inline">\(n\)</span> fixed when assessing estimators. We now consider how estimators behave as <span class="math inline">\(n\to\infty.\)</span> In practice, we will never have infinite data, asymptotics gives us an approximate idea of how estimators perform for large data sets. A comprehensive reference in asymptotic theory is due to <span class="citation" data-cites="van2000asymptotic">Van der Vaart (<a href="#ref-van2000asymptotic" role="doc-biblioref">2000</a>)</span>. For a treatment concerned purely with econometrics, <span class="citation" data-cites="newey1994large">Newey and McFadden (<a href="#ref-newey1994large" role="doc-biblioref">1994</a>)</span> provide a phenomenal survey, most of which we will touch on when discussing general classes of estimators.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;With loss of some generality, we will assume that all random variables have finite expectation and variances. Dispensing with this assumption is something for a probability course.</p>
<section id="convergence" class="level2 page-columns page-full" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="convergence"><span class="header-section-number">2.1</span> Convergence</h2>
<p>At some point in high school, most students encounter the concept of a numeric sequence, and how they can converge to a limit. Later on, perhaps when taking a real analysis course, sequences are generalized to spaces of functions. A sequence of functions may also converge to a limit, whether that be converging pointwise and/or converging uniformly (for details see <span class="citation" data-cites="rudin1976principles">Rudin (<a href="#ref-rudin1976principles" role="doc-biblioref">1976</a>)</span>). Random variables are functions from a sample space to <span class="math inline">\(\mathbb R\)</span>, so we can consider how these functions converge.</p>
<section id="convergence-in-mse" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="convergence-in-mse"><span class="header-section-number">2.1.1</span> Convergence in MSE</h3>
<p>The first type of convergence we’ll work with deals with MSE.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.1</strong></span> A sequence of random variables <span class="math inline">\(X_n\)</span> <span style="color:red"><strong><em>converges in mean square</em></strong></span> to a random variable <span class="math inline">\(X\)</span>, written as <span class="math inline">\(X_n\overset{ms}{\to} X\)</span>, if <span class="math display">\[\lim_{n\to\infty} \text{E}\left[(X_n - X)^2\right] = 0.\]</span> A sequence of random vectors <span class="math inline">\(\mathbf{X}_n = (X_{1,n},\ldots X_{k,n})\)</span> converges in mean square to <span class="math inline">\(\mathbf{X}\)</span> if <span class="math inline">\(X_{i,n}\overset{ms}{\to} X_i\)</span> for <span class="math inline">\(i=1,\ldots, k\)</span>.</p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(X_n \overset{ms}{\to}X\)</span> if the average distance between <span class="math inline">\(X_n\)</span> and <span class="math inline">\(X\)</span> shrinks as <span class="math inline">\(n\to\infty\)</span> where distance is measured as <span class="math inline">\((X_n - X)^2\)</span>. We can also have <span class="math inline">\(X_n \overset{ms}{\to}c\)</span> for some constant <span class="math inline">\(c\)</span>, as <span class="math inline">\(c\)</span> is a trivial random variable.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.1</strong></span> Suppose we draw a sample of <span class="math inline">\(n\)</span> iid random variables <span class="math inline">\(Z_i\)</span> and define <span class="math inline">\(X_n\)</span> to be the sample mean of our observations. <span class="math display">\[X_n = \frac{1}{n}\sum_{i=1}^n Z_i\]</span> If <span class="math inline">\(\text{E}\left[Z_i\right] = \mu\)</span> and <span class="math inline">\(\text{Var}\left(Z_i\right) = \sigma^2\)</span> for all <span class="math inline">\(i\)</span>, we have <span class="math inline">\(X_n\overset{ms}{\to}\mu\)</span>: <span class="math display">\[\begin{align*}
\lim_{n\to\infty}\text{E}\left[(X_n - \mu)^2\right] &amp; = \lim_{n\to\infty}\text{Var}\left(X_n\right) + \text{Bias}(X_n) \\
&amp;= \lim_{n\to\infty} \frac{\sigma^2}{n} + 0 &amp; (X_n \text{ unbiased}) \\
&amp; = \lim_{n\to\infty} \frac{\sigma^2}{n} \\
&amp; = 0.
\end{align*}\]</span></p>
<p>What does this convergence “look like”? If <span class="math inline">\(Z_i\overset{iid}{\sim}N(0,1)\)</span>, we know that <span class="math inline">\(X_n = \bar Z \sim N(\mu, \sigma^2/n)\)</span>. Let’s plot this distribution for increasing values of <span class="math inline">\(n\)</span>.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">expand_grid</span>(</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length =</span> <span class="dv">500</span>),</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="fu">dnorm</span>(x, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">/</span>n))</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,y)) <span class="sc">+</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Value of X_n"</span>, <span class="at">y =</span> <span class="st">"Density"</span>, <span class="at">color =</span> <span class="st">"Sample Size"</span>) <span class="sc">+</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transition_states</span>(n) <span class="sc">+</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'n = {closest_state}'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot21" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot21-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-plot21-1.gif" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot21-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.1: The distribution of X_n for increasing values of n
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This example betrays a useful property related to variables which converge in mean square.</p>
<div id="prp-mse3" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 2.1</strong></span> A sequence of random variables <span class="math inline">\(X_n\)</span> converges in mean square to a constant <span class="math inline">\(c\)</span> <em>if and only if</em> <span class="math inline">\(\text{E}\left[X_n\right]\to c\)</span> and <span class="math inline">\(\text{Var}\left(X_n\right)\to 0\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span style="color:white">space</span></p>
<p><span class="math inline">\((\Longrightarrow)\)</span> Suppose <span class="math inline">\(X_n \overset{ms}{\to}c\)</span>. Then <span class="math display">\[\begin{align*}
&amp; \lim_{n\to\infty}\text{E}\left[(X_n - c)^2\right] = 0\\
\implies &amp; \lim_{n\to\infty}\text{E}\left[(X_n - c)^2\right] = 0\\
\implies&amp; \lim_{n\to\infty}\left[\text{E}\left[X_n\right]^2 -2c \text{E}\left[X_n\right] + c^2\right]= 0\\
\implies&amp; \lim_{n\to\infty}\left[(\text{E}\left[X_n\right]^2 -\text{E}\left[X_n\right]^2) + \text{E}\left[X_n\right]^2 - 2c \text{E}\left[X_n\right] + c^2\right]= 0 \\
\implies &amp;\lim_{n\to\infty} \text{Var}\left(X_n\right) + \lim_{n\to\infty}\left[\text{E}\left[X_n\right] -c\right]^2 = 0
\end{align*}\]</span> This final equality gives the desired result.</p>
<p><span class="math inline">\((\Longleftarrow)\)</span> Suppose <span class="math inline">\(\text{E}\left[X_n\right]\to c\)</span> and <span class="math inline">\(\text{Var}\left(X_n\right)\to 0\)</span>. We have <span class="math display">\[\begin{align*}
&amp;\lim_{n\to\infty} \text{Var}\left(X_n\right) + \lim_{n\to\infty}\left[\text{E}\left[X_n\right] -c\right]^2 = 0 \\
\implies &amp; \lim_{n\to\infty}\text{E}\left[(X_n - c)^2\right] = 0\\
\implies &amp; X_n\overset{ms}{\to}c
\end{align*}\]</span></p>
</div>
<div id="cor-mseconv" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 2.1</strong></span> Suppose <span class="math inline">\(X_n\)</span> is a sequence of random variables such that <span class="math inline">\(\text{E}\left[X_n\right] = c\)</span> for all <span class="math inline">\(n\)</span>. Then <span class="math inline">\(X_n\overset{ms}{\to}c\)</span> <em>if and only if</em> <span class="math inline">\(\text{Var}\left(X_n\right)\to 0\)</span>.</p>
</div>
</section>
<section id="convergence-in-probability" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="convergence-in-probability"><span class="header-section-number">2.1.2</span> Convergence in Probability</h3>
<p>Convergence in mean square captures the idea that a random variable gets “closer” to some value <span class="math inline">\(c,\)</span> but it is hardly the only way to define this behavior. A more “traditional” approach would be defining convergence using an inequality involving an arbitrarily small <span class="math inline">\(\varepsilon &gt;0\)</span> (akin the to <span class="math inline">\(\varepsilon-\delta\)</span> definition of a limit).</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.2</strong></span> A sequence of random variables <span class="math inline">\(X_n\)</span> <span style="color:red"><strong><em>converges in probability</em></strong></span> to a random variable <span class="math inline">\(X\)</span>, written as <span class="math inline">\(X_n\overset{p}{\to}X\)</span> or <span class="math inline">\(\mathop{\mathrm{plim}}X_n = X\)</span>, if <span class="math display">\[\lim_{n\to\infty} \Pr (|X_n - X| &gt; \varepsilon)= 0\]</span> for all <span class="math inline">\(\varepsilon &gt; 0\)</span>. Equivalently, <span class="math inline">\(X_n\overset{p}{\to}X\)</span> if for all <span class="math inline">\(\varepsilon &gt; 0\)</span> and <span class="math inline">\(\delta &gt; 0\)</span>, there exists some <span class="math inline">\(N\)</span> such that for all <span class="math inline">\(n \ge N\)</span>, <span class="math display">\[ \Pr (|X_n - X| &gt; \varepsilon) &lt; \delta.\]</span> A sequence of random vectors <span class="math inline">\(\mathbf{X}_n = (X_{1,n},\ldots X_{k,n})\)</span> converges in probability to <span class="math inline">\(\mathbf{X}\)</span> if <span class="math inline">\(X_{i,n}\overset{p}{\to} X_i\)</span> for <span class="math inline">\(i=1,\ldots, k\)</span>.</p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Intuitively, <span class="math inline">\(X_n \overset{p}{\to}X\)</span> if the probability that the difference <span class="math inline">\(|X_n - X|\)</span> is not small (greater than some <span class="math inline">\(\varepsilon\)</span>) goes to zero as <span class="math inline">\(n\to\infty\)</span>. In other words, for large values of <span class="math inline">\(n\)</span>, there is a large probability that <span class="math inline">\(X_n\)</span> is close to <span class="math inline">\(X\)</span></p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.2</strong></span> Return to the previous example where <span class="math inline">\(X_n = \bar Z\)</span>, and assume <span class="math inline">\(Z_i \overset{iid}{\sim}N(\mu,\sigma^2)\)</span>. We will verify that <span class="math inline">\(X_n\overset{p}{\to}\mu\)</span> using the definition of convergence in probability using the fact that <span class="math inline">\(X_n \sim N(\mu, \sigma^2/n)\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-plot22" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot22-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/converge.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1330">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot22-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.2: For any arbitrary ε, we can find some n such that the probability X_n falls outside the set |μ-ε| is arbitrarily small
</figcaption>
</figure>
</div>
</div>
</div>
<p>For some <span class="math inline">\(\varepsilon &gt; 0\)</span>, <span class="math display">\[\begin{align*}
\Pr (|X_n - \mu| &gt; \varepsilon) &amp; = 1 - \Pr (\mu - \varepsilon &lt; X_n &lt; \mu + \varepsilon)\\
&amp; = 1 - (F_{X_n}(\mu + \varepsilon) + F_{X_n}(\mu - \varepsilon))\\
&amp; = 1 - 2\left[F_{X_n}(\mu + \varepsilon) - \frac{1}{2}\right] &amp; (F_{X_n} \text{symmetric about }\mu)\\
&amp; = 1 - 2\left[\Phi\left(\frac{(\mu + \varepsilon) - \mu}{\sigma/\sqrt{n}}\right) - \frac{1}{2}\right] &amp; (\Phi\text{ standard normal distribution})\\
&amp; = 1 - 2\left[\Phi\left(\frac{\varepsilon}{\sigma/\sqrt{n}}\right) - \frac{1}{2}\right].
\end{align*}\]</span> Given some <span class="math inline">\(\delta &gt;0\)</span>, we can solve for the lowest value of <span class="math inline">\(n\)</span> that satisfies <span class="math inline">\(\Pr (|X_n - c| &gt; \varepsilon) &lt; \delta\)</span>. <span class="math display">\[\begin{align*}
&amp;\Pr (|X_n - c| &gt; \varepsilon) &lt; \delta \\
\implies &amp; 1 - 2\left[\Phi\left(\frac{\varepsilon}{\sigma/\sqrt{n}}\right) - \frac{1}{2}\right] &lt; \delta \\
\implies&amp;  n &gt; \left(\frac{\sigma \Phi^{-1}(1-\delta/2)}{\varepsilon}\right)^2
\\\implies &amp; n &gt; \left\lceil \left(\frac{\sigma \Phi^{-1}(1-\delta/2)}{\varepsilon}\right)^2 \right\rceil
\end{align*}\]</span> Just to be excruciatingly pedantic, we rounded our solution up to the closest positive integer, as <span class="math inline">\(n\)</span> corresponds to a sample size. For fixed values of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> (say 3 and 2, respectively), we can define a function of <span class="math inline">\((\varepsilon, \delta)\)</span> which calculates the sample size required to satisfy <span class="math inline">\(\Pr(|X_n - c|&gt;\varepsilon)&lt;\delta\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>n_fun <span class="ot">&lt;-</span> <span class="cf">function</span>(delta, ep, mu, sigma){</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a> <span class="fu">ceiling</span>(((sigma<span class="sc">*</span><span class="fu">qnorm</span>(<span class="dv">1</span><span class="sc">-</span>delta <span class="sc">/</span><span class="dv">2</span>))<span class="sc">/</span>ep)<span class="sc">^</span><span class="dv">2</span>) </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s plot this function for various values of <span class="math inline">\((\varepsilon, \delta)\)</span>.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">expand_grid</span>(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">e =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">0.1</span>, <span class="fl">0.01</span>, <span class="fl">0.001</span>, <span class="fl">0.0001</span>),</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">d =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">9999</span><span class="sc">/</span><span class="dv">10000</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>)  <span class="sc">%&gt;%</span> </span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">sample =</span> <span class="fu">map2_dbl</span>(d, e, n_fun, <span class="at">mu =</span> <span class="dv">3</span>, <span class="at">sigma =</span> <span class="fu">sqrt</span>(<span class="dv">2</span>))) <span class="sc">%&gt;%</span> </span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(d, sample, <span class="at">color =</span> <span class="fu">as.factor</span>(e))) <span class="sc">+</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_reverse</span>() <span class="sc">+</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_log10</span>() <span class="sc">+</span> </span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"δ"</span>, <span class="at">y =</span> <span class="st">"Sample Size"</span>, <span class="at">color =</span> <span class="st">"ε"</span>)<span class="sc">+</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot23" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot23-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-plot23-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot23-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.3: The sample size required to satisfy the inequality in the definition of convergence of probability for various values (ε,δ)
</figcaption>
</figure>
</div>
</div>
</div>
<p>We can also verify that <span class="math inline">\(\lim_{n\to\infty}\Pr (|X_n - \mu| &gt; \varepsilon) = 0\)</span> for various values of <span class="math inline">\(\varepsilon.\)</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>prob_ep <span class="ot">&lt;-</span> <span class="cf">function</span>(n, ep, mu, sigma){</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="dv">1</span> <span class="sc">-</span> <span class="dv">2</span><span class="sc">*</span>(<span class="fu">pnorm</span>(ep <span class="sc">/</span> (sigma <span class="sc">/</span> <span class="fu">sqrt</span>(n))) <span class="sc">-</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">expand_grid</span>(</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">e =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">0.1</span>, <span class="fl">0.01</span>, <span class="fl">0.001</span>, <span class="fl">0.0001</span>),</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="dv">1</span><span class="sc">:</span><span class="fl">1e6</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">prob =</span> <span class="fu">map2_dbl</span>(e, n, \(e, n) <span class="fu">prob_ep</span>(n, e, <span class="at">mu =</span> <span class="dv">3</span>, <span class="at">sigma =</span> <span class="fu">sqrt</span>(<span class="dv">3</span>)))) <span class="sc">%&gt;%</span> </span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(n, prob, <span class="at">color =</span> <span class="fu">as.factor</span>(e))) <span class="sc">+</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>() <span class="sc">+</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Sample Size"</span>, <span class="at">y =</span> <span class="st">"Pr(|X_n - mu| &gt; ε)"</span>, <span class="at">color =</span> <span class="st">"ε"</span>) <span class="sc">+</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot24" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot24-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-plot24-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot24-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.4: The probability that X_n falls outside the interval |μ-ε| for various values of (ε,n)
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;How does convergence in mean square related to convergence in probability? As it turns out the latter is a weaker condition implied by the prior. Before stating and proving this result, we will need a lemma.</p>
<div id="lem-markovineq" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2.1 (Markov’s inequality)</strong></span> If <span class="math inline">\(X\)</span> is a nonnegative random variable, and <span class="math inline">\(a &gt; 0\)</span>, then <span class="math display">\[\Pr(X\ge a) \le \frac{\text{E}\left[X\right]}{a}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The expectation of <span class="math inline">\(X\)</span> can be written as <span class="math display">\[\begin{align*}
\text{E}\left[X\right] &amp; = \int_{-\infty}^\infty x\ dF_X(x) \\
&amp; = \int_{0}^\infty x\ dF_X(x) &amp; (X\text{ is nonnegative}) \\
&amp; = \int_{0}^a x\ dF_X(x) + \int_{a}^\infty x\ dF_X(x) \\
&amp; \ge \int_a^\infty x\ dF_X(x)\\
&amp; \ge \int_a^\infty a\ dF_X(x) &amp; (a \ge x \text{ on }(a,\infty))\\
&amp; = a \int_a^\infty\ dF_X(x) \\
&amp; = a\Pr(X \ge a).
\end{align*}\]</span> Dividing both sides of this inequality by <span class="math inline">\(a\)</span> gives <span class="math inline">\(\Pr(X\ge a) \le \text{E}\left[X\right]/a\)</span>.</p>
</div>
<div id="prp-conv" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 2.2 (Convergence in MSE –&gt; Convergence in Probability)</strong></span> Let <span class="math inline">\(X_n\)</span> be a sequence of random variables. If <span class="math inline">\(X_n\overset{ms}{\to}X\)</span>, then <span class="math inline">\(X_n\overset{p}{\to}X\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose <span class="math inline">\(X_n\overset{ms}{\to}X\)</span>. For all <span class="math inline">\(\varepsilon &gt; 0\)</span> <span class="math display">\[\begin{align*}
\lim_{n\to\infty} \Pr (|X_n - X| &gt; \varepsilon) &amp; = \lim_{n\to\infty} \Pr ((X_n - X)^2 &gt; \varepsilon^2) \\
&amp; \le \lim_{n\to\infty} \frac{\text{E}\left[(X_n - X)^2\right]}{\varepsilon^2} &amp; (\text{Markov's Inequality}) \\
&amp; = \frac{0}{\varepsilon^2} &amp; (X_n\overset{ms}{\to}X)\\
&amp; = 0.
\end{align*}\]</span> Therefore <span class="math inline">\(X_n\overset{p}{\to}c\)</span>.</p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The usefulness of <a href="#prp-conv" class="quarto-xref">Proposition&nbsp;<span class="quarto-unresolved-ref">prp-conv</span></a> cannot be emphasized enough. Proving convergence in probability using the definition is cumbersome, so we will almost show convergence in mean square and then appeal to <a href="#prp-conv" class="quarto-xref">Proposition&nbsp;<span class="quarto-unresolved-ref">prp-conv</span></a> to verify convergence in probability. Nevertheless, situations can arise where <span class="math inline">\(X_n\overset{p}{\to} X\)</span>, but <span class="math inline">\(X_n \not\overset{ms}{\to} X\)</span>.</p>
<div id="exm-pnotmse" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.3 (Convergence in Probability but not in Mean Square)</strong></span> Suppose <span class="math inline">\(X_n\)</span> is defined on the sample space <span class="math inline">\(\{1,n^2\}\)</span> such that: <span class="math display">\[\begin{align*}
\Pr(X_n = 0) &amp;= 1-1/n\\
\Pr(X_n = n^2) &amp;= 1/n
\end{align*}\]</span></p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">expand_grid</span>(<span class="at">n =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>, <span class="at">x =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> x<span class="sc">*</span>(n<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">p =</span> (<span class="dv">1-1</span><span class="sc">/</span>n)<span class="sc">*</span>(x <span class="sc">==</span> <span class="dv">0</span>) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">/</span>n)<span class="sc">*</span>(x <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> p)) <span class="sc">+</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">xend =</span> x, <span class="at">y =</span> <span class="dv">0</span>, <span class="at">yend =</span> p)) <span class="sc">+</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_sqrt</span>(<span class="at">breaks =</span> (<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>)<span class="sc">*</span><span class="dv">500</span>) <span class="sc">+</span> </span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">color =</span> <span class="st">"n"</span>, <span class="at">y =</span> <span class="st">"Probability Density"</span>, <span class="at">x =</span> <span class="st">"X_n"</span>) <span class="sc">+</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)  <span class="sc">+</span> </span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transition_states</span>(n) <span class="sc">+</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'n = {closest_state}'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot24.2" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot24.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-plot24.2-1.gif" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot24.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.5: The probability density function associated with X_n for increasing values of n.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The expected value of <span class="math inline">\(X_n\)</span> is <span class="math display">\[\text{E}\left[X_n\right] = 0(1-1/n) + n^2(1/n) = n,\]</span> so <span class="math inline">\(\text{E}\left[X_n\right]\to\infty\)</span> as <span class="math inline">\(n\to \infty\)</span>. This rules out <span class="math inline">\(X_n\)</span> converging in mean square to any value. Nevertheless, we have <span class="math inline">\(X_n\overset{p}{\to}0\)</span>. For all <span class="math inline">\(\varepsilon &gt; 0\)</span>, <span class="math display">\[\Pr(|X_n - 0| &gt; \varepsilon) = \Pr(X_n \neq 0) = \Pr(X_n = n^2) = 1/n \to 0.\]</span> This disagreement among definitions of convergence arises because the convergence in probability only takes into consideration the probability assigned to each value in the sample space, whereas convergence in MSE is based on an expectation which takes into consideration the probability assigned to each value in the sample space <em>weighted</em> by the value in the sample space. In this particular example, the probability that <span class="math inline">\(X_n = n^2\)</span> is <span class="math inline">\(1/n\)</span>, and the growth of <span class="math inline">\(n^2\)</span> as <span class="math inline">\(n\to \infty\)</span> outpaces the growth of <span class="math inline">\(1/n\)</span>, so the expected value of <span class="math inline">\(X_n\)</span> grows indefinitely.</p>
</div>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>We can think of the counterexample in <a href="#exm-pnotmse" class="quarto-xref">Example&nbsp;<span class="quarto-unresolved-ref">exm-pnotmse</span></a> arising from the “tail” of a density, where the “tail” happened to just be a single point because the random variable was discrete. This is a pattern that comes up often in asymptotics – the tails of distributions and densities can cause trouble. Many theorems and results depend on these tails behaving well.</p>
</div>
</section>
<section id="almost-sure-convergence" class="level3" data-number="2.1.3">
<h3 data-number="2.1.3" class="anchored" data-anchor-id="almost-sure-convergence"><span class="header-section-number">2.1.3</span> Almost Sure Convergence</h3>
<p>An third type of convergence is almost sure convergence. Recalling that a random variable is just a function, we can define a stochastic analog to pointwise convergence of a sequence of functions.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.3</strong></span> A sequence of random variables <span class="math inline">\(X_n\)</span> <span style="color:red"><strong><em>converges almost surely</em></strong></span> to a random variable <span class="math inline">\(X\)</span>, written as <span class="math inline">\(X_n\overset{a.s}{\to}X\)</span> if <span class="math display">\[ \Pr \left(\lim_{n\to\infty} X_n = X\right) = 1.\]</span> A sequence of random vectors <span class="math inline">\(\mathbf{X}_n = (X_{1,n},\ldots X_{k,n})\)</span> converges almost surely to <span class="math inline">\(\mathbf{X}\)</span> if <span class="math inline">\(X_{i,n}\overset{a.s}{\to} X_i\)</span> for <span class="math inline">\(i=1,\ldots, k\)</span>.</p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Almost sure convergence is the probabilistic equivalent of a sequence of functions converging pointwise almost everywhere. The difference between <span class="math inline">\(X_n\overset{a.s}{\to} X\)</span> and <span class="math inline">\(X_n\overset{p}{\to}X\)</span> is subtle, and arises from the limit being taken before or after we take the probability of the event. While convergence in probability tells us that the chance that <span class="math inline">\(X_n\)</span> and <span class="math inline">\(X\)</span> are far approaches zero, almost sure convergence says that the probability that <span class="math inline">\(X_n\to X\)</span> as <span class="math inline">\(n\to \infty\)</span> is 1. We’re certain that <span class="math inline">\(X_n\)</span> will eventually coincide with <span class="math inline">\(X\)</span>, although we don’t know when that will happen. The next example makes the difference between these two definitions a bit more concrete. Later on in <a href="#exm-strongvsweak" class="quarto-xref">Example&nbsp;<span class="quarto-unresolved-ref">exm-strongvsweak</span></a> an illustration will be presented to distinguish the two definitions.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.4 (Convergence in Probability but not in Almost Surely)</strong></span> Define <span class="math inline">\(X_n\)</span> on <span class="math inline">\(\{0,1\}\)</span> such that: <span class="math display">\[\begin{align*}
\Pr(X_n = 1) &amp; = 1/n\\
\Pr(X_n = 0) &amp; = 1-1/n
\end{align*}\]</span> We have <span class="math inline">\(X_n\overset{p}{\to}0\)</span> since <span class="math display">\[ \lim_{n\to\infty}\Pr(|X_n - 0| &gt; \varepsilon) =  \lim_{n\to\infty}\Pr(X_n &gt; \varepsilon) = \lim_{n\to\infty}\Pr(X_n = 1) = \lim_{n\to\infty} 1/n = 0\]</span> for all <span class="math inline">\(\varepsilon &gt; 0\)</span>. At the same time, we have <span class="math display">\[\sum_{n=1}^\infty \Pr(X_n = 1) =  \sum_{n=1}^\infty 1/n = \infty,\]</span> so by a form of the <a href="http://www.columbia.edu/~ks20/stochastic-I/stochastic-I-BC.pdf">Borel-Cantelli Lemma</a> then <span class="math inline">\(X_n = 1\)</span> occurs an infinite number of times. This rules out the possibility that <span class="math inline">\(\lim_{n\to\infty} X_n = 0\)</span> with probability 1.</p>
</div>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 2.3 (Convergence A.S -&gt; Convergence in Probability)</strong></span> Let <span class="math inline">\(X_n\)</span> be a sequence of random variables. If <span class="math inline">\(X_n\overset{a.s}\to X\)</span> then <span class="math inline">\(X_n\overset{p}{\to}X\)</span>.</p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The proof of this result can be found in <span class="citation" data-cites="van2000asymptotic">Van der Vaart (<a href="#ref-van2000asymptotic" role="doc-biblioref">2000</a>)</span>. Almost sure convergence is not nearly as important as convergence in probability when it comes to assessing estimators. In fact, <span class="citation" data-cites="lehmann1999elements">Lehmann (<a href="#ref-lehmann1999elements" role="doc-biblioref">1999</a>)</span> doesn’t even mention it in his treatment of asymptotic statistics. The math underlying almost sure convergence is also a bit complex, so the related proofs aren’t very informative and will be omitted.</p>
</section>
<section id="convergence-in-distribution" class="level3 page-columns page-full" data-number="2.1.4">
<h3 data-number="2.1.4" class="anchored" data-anchor-id="convergence-in-distribution"><span class="header-section-number">2.1.4</span> Convergence in Distribution</h3>
<p>The final notion of convergence we will use related to the probability distribution of random variables.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.4</strong></span> A sequence of random variables <span class="math inline">\(X_n\)</span> <span style="color:red"><strong><em>converges in distribution (converges weakly)</em></strong></span> to a random variable <span class="math inline">\(X\)</span>, written as <span class="math inline">\(X_n \overset{d}{\to}X\)</span>, if <span class="math display">\[\lim_{n\to\infty} F_{X_n}(x)= F_X(x)\]</span> for all points <span class="math inline">\(x\)</span> where <span class="math inline">\(F_{X}\)</span> is continuous. In this case, we refer to <span class="math inline">\(F_X\)</span> as the <span style="color:red"><strong><em>asymptotic distribution</em></strong></span> of <span class="math inline">\(X_n\)</span>, and write <span class="math inline">\(X_n \overset{a}{\sim}F_X\)</span>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> A sequence of random vectors <span class="math inline">\(\mathbf{X}_n = (X_{1,n},\ldots X_{k,n})\)</span> converges in distribution to <span class="math inline">\(\mathbf{X}\)</span> if <span class="math inline">\(\lim_{n\to\infty} F_{\mathbf{X}_n}(x)= F_{\mathbf{X}}(x)\)</span>.</p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For our purposes, <span class="math inline">\(X_n\overset{d}{\to}X\)</span> means the distribution of <span class="math inline">\(X_n\)</span> can be approximated by <span class="math inline">\(F_X\)</span>, and this approximation becomes increasingly better as <span class="math inline">\(n\to\infty\)</span>.</p>
<div id="exm-tdist" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.5</strong></span> One example of convergence in distribution you may be familiar with deals with the student’s <span class="math inline">\(t-\)</span>distribution where the degrees of freedom <span class="math inline">\(n\to\infty\)</span>. If <span class="math inline">\(X_n\sim t_n\)</span>, then <span class="math inline">\(X_n \overset{d}{\to}X\)</span> where <span class="math inline">\(X\sim N(0,1)\)</span>.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">expand_grid</span>(</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="at">length =</span> <span class="fl">1e3</span>),</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist =</span> <span class="st">"Student's t"</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">val =</span> <span class="fu">dt</span>(x, n)) <span class="sc">%&gt;%</span> </span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, val)) <span class="sc">+</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">fun =</span> dnorm, </span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>), </span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">"red"</span>, </span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">linetype=</span><span class="st">"dashed"</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">color =</span> <span class="st">"t-distribution degrees of freedom, n"</span>, <span class="at">y =</span> <span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transition_states</span>(n) <span class="sc">+</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'n = {closest_state}'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot25" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot25-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-plot25-1.gif" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot25-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.6: The t-distribution converges to the standard normal distribution (represented by the dashed red line) as the degrees of freedom increase.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 2.4 (Convergence in Probability –&gt; Convergence in Distribution)</strong></span> Let <span class="math inline">\(X_n\)</span> be a sequence of random variables. If <span class="math inline">\(X_n\overset{p}{\to}X\)</span>, then <span class="math inline">\(X_n\overset{d}{\to}X\)</span>.</p>
</div>
<div class="proof page-columns page-full">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose <span class="math inline">\(X_n\overset{p}{\to}X\)</span> and let <span class="math inline">\(\varepsilon &gt; 0\)</span>. We have, <span class="math display">\[\begin{align*}
\Pr(X_n \le x) &amp; = \Pr(X_n\le x \text{ and } X \le x + \varepsilon) + \Pr(X_n\le x \text{ and } X &gt; x + \varepsilon) \\
  &amp; = \Pr(X \le x + \varepsilon) + \Pr(X_n - X\le x - X \text{ and } x - X &lt; -\varepsilon)\\
  &amp; \le \Pr(X \le x + \varepsilon) + \Pr(X_n - X &lt; -\varepsilon)\\
  &amp; \le \Pr(X \le x + \varepsilon) + \Pr(X_n - X &lt; -\varepsilon) + \Pr(X - X_n &gt; \varepsilon) \\
  &amp; = \Pr(X \le x + \varepsilon) + \Pr(|X_n - X| &gt; \varepsilon)
\end{align*}\]</span> Similarly, <span class="math display">\[ \Pr(X \le x-\varepsilon) \le \Pr(X_n \le x) + \Pr(|X_n - X| &gt; \varepsilon).\]</span> We can use these inequalities to find an upper and lower bound of <span class="math inline">\(\Pr(X_n \le x)\)</span>:</p>
<div class="column-screen-inset-right">
<p><span class="math display">\[\begin{align*}
&amp; \Pr(X \le x-\varepsilon) - \Pr(|X_n - X| &gt; \varepsilon)\le \Pr(X_n \le x) \le \Pr(X \le x + \varepsilon) + \Pr(|X_n - X| &gt; \varepsilon) \\
\implies &amp; \lim_{n\to\infty}[\Pr(X \le x-\varepsilon) - \Pr(|X_n - X| &gt; \varepsilon)]\le \lim_{n\to\infty}\Pr(X_n \le x) \le \lim_{n\to\infty}[\Pr(X \le x + \varepsilon) + \Pr(|X_n - X| &gt; \varepsilon) ]\\
\implies &amp; \Pr(X \le x-\varepsilon) - \underbrace{\lim_{n\to\infty}\Pr(|X_n - X| &gt; \varepsilon) }_0\le \lim_{n\to\infty}\Pr(X_n \le x) \le \Pr(X \le x + \varepsilon) + \underbrace{\lim_{n\to\infty}\Pr(|X_n - X| &gt; \varepsilon)}_0 \\
\implies &amp; \Pr(X \le x-\varepsilon)\le \lim_{n\to\infty} \Pr(X_n \le x) \le  \Pr(X \le x + \varepsilon) &amp; (X_n\overset{p}{\to}X)\\
\implies &amp; F_X(x-\varepsilon)\le \lim_{n\to\infty} F_{X_n}(x) \le  F_X(x-\varepsilon)
\end{align*}\]</span></p>
</div>
<p>This holds for all <span class="math inline">\(\varepsilon &gt; 0\)</span>, so it must be the case that <span class="math inline">\(\lim_{n\to\infty} F_{X_n}(x) = F_X(x)\)</span></p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Of the different concepts of stochastic convergence, convergence in distribution is the weakest (as the alternate name weak convergence implies). If <span class="math inline">\(X_n \overset{d}{\to} X\)</span>, we’re not saying that <span class="math inline">\(X_n\)</span> and <span class="math inline">\(X\)</span> become close, or that the probability that <span class="math inline">\(X_n\)</span> and <span class="math inline">\(X\)</span> are close approaches 1. We’re saying that, given a common probability space <span class="math inline">\((\mathcal X, \mathcal F, P)\)</span>,<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> that the distribution function <span class="math inline">\(F_{X_n}\)</span> defined via the distribution <span class="math inline">\(P(X^{-1}(I))\)</span><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> happens to converge to the same function corresponding to the random variable <span class="math inline">\(X\)</span>. Furthermore, the convergence of <span class="math inline">\(F_{X_n}\)</span> to <span class="math inline">\(F_X\)</span> only needs to occur at the points which <span class="math inline">\(F_X\)</span> is continuous, <em>and</em> even then the convergence only needs to be pointwise (opposed to uniform).</p>
<p>:::</p>
</section>
<section id="putting-the-pieces-together" class="level3" data-number="2.1.5">
<h3 data-number="2.1.5" class="anchored" data-anchor-id="putting-the-pieces-together"><span class="header-section-number">2.1.5</span> Putting the Pieces Together</h3>
<p>The biggest takeaway from this should be the following relations: <span class="math display">\[\begin{align*}
&amp;X_n \overset{ms}{\to}X \implies X_n \overset{p}{\to}X \implies X_n \overset{d}{\to}X\\
&amp;X_n \overset{a.s}{\to}X \implies X_n \overset{p}{\to}X
\end{align*}\]</span></p>
</section>
</section>
<section id="consistency" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="consistency"><span class="header-section-number">2.2</span> Consistency</h2>
<p>Our three modes of convergence were defined for any sequence of random variables. It should come as no surprise, considering the previous examples considering whether the sample mean converged, that we are interested in the convergence of estimators <span class="math inline">\(\hat\theta(\mathbf{X})\)</span> as sample size increases. In particular we are interested in whether <span class="math inline">\(\hat\theta(\mathbf{X})\)</span> converges to the constant <span class="math inline">\(\theta\in\Theta\)</span> it is estimating.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.5</strong></span> An estimator <span class="math inline">\(\hat\theta\)</span> is <span style="color:red"><strong><em>consistent (for estimand</em></strong> <span class="math inline">\(\theta\)</span>)</span> if <span class="math inline">\(\hat\theta \overset{p}{\to}\theta\)</span>.</p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; We’ve already seen that <span class="math inline">\(\bar X\)</span> is a consistent estimator for <span class="math inline">\(\mu\)</span> when we take an iid sample from a normal distribution. Let’s investigate it’s variance-counterpart <span class="math inline">\(S^2\)</span>.</p>
<div id="exm-consvarnorm" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.6</strong></span> For <span class="math inline">\(X_i \overset{iid}{\sim}N(\mu,\sigma^2)\)</span>, <span class="math inline">\(S^2 = \sum_{i=1}^n (X_i - \bar X)/(n-1)\)</span> is an unbiased estimator for <span class="math inline">\(\sigma^2\)</span>. This estimator’s MSE (which is just its variance as it is unbiased) is <span class="math inline">\(2\sigma^4/(n-1)\)</span> which converges to <span class="math inline">\(0\)</span> as <span class="math inline">\(n\to\infty\)</span>, so <span class="math inline">\(S^2\)</span> is consistent by <a href="#prp-conv" class="quarto-xref">Proposition&nbsp;<span class="quarto-unresolved-ref">prp-conv</span></a>.</p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This example highlights the fact that proving an unbiased estimator is consistent is a matter of showing its variance converges to 0.</p>
<div id="cor-unbcon" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 2.2</strong></span> Suppose <span class="math inline">\(\hat\theta\)</span> is an unbiased estimator for <span class="math inline">\(\theta\)</span>. Then <span class="math inline">\(\hat\theta\)</span> is consistent <em>if and only if</em> <span class="math inline">\(\text{Var}\left(\hat\theta\right)\to 0\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Apply <a href="#cor-mseconv" class="quarto-xref">Corollary&nbsp;<span class="quarto-unresolved-ref">cor-mseconv</span></a> to an unbiased estimator.</p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A second type of convergence related to estimators pertains to the bias of an estimator. In <a href="#sec-est" class="quarto-xref"><span class="quarto-unresolved-ref">sec-est</span></a> we saw a few estimators that were biased, but this bias was such that it diminished as <span class="math inline">\(n\to\infty\)</span>. In effect, they were unbiased in an asymptotic sense.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.6</strong></span> An estimator <span class="math inline">\(\hat\theta\)</span> is <span style="color:red"><strong><em>asymptotically unbiased</em></strong></span> if <span class="math inline">\(\lim_{n\to\infty}\text{Bias}(\hat\theta, \theta) = 0\)</span>.</p>
</div>
<div id="exm">
<p>For <span class="math inline">\(X_i \overset{iid}{\sim}N(\mu,\sigma^2)\)</span>, <span class="math inline">\(\hat\theta = \sum_{i=1}^n (X_i - \bar X)/n\)</span> is a biased estimator for <span class="math inline">\(\sigma^2\)</span>. Its bias is <span class="math display">\[\text{Bias}(\hat\theta, \sigma^2) = \frac{n-1}{n}\sigma^2 - \sigma^2.\]</span> As <span class="math inline">\(n\to\infty\)</span>, this bias vanishes. To illustrate this, we can simulate estimates for various sample sizes, taking <span class="math inline">\(X_i \sim N(0,1)\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>draw_estimate <span class="ot">&lt;-</span> <span class="cf">function</span>(theta, n, dist, dist_params, s){</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">do.call</span>(dist, <span class="fu">append</span>(n, dist_params))</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">sample_size =</span> n,</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">iter_num =</span> s,</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">estimate =</span> <span class="fu">theta</span>(X)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>draw_N_estimates <span class="ot">&lt;-</span> <span class="cf">function</span>(theta, N, n, dist, dist_params){</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>N <span class="sc">%&gt;%</span> </span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">map_df</span>(\(s) <span class="fu">draw_estimate</span>(theta, n, dist, dist_params, s)) </span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>draw_N_estimates_over_n <span class="ot">&lt;-</span> <span class="cf">function</span>(theta, N, n_vals, dist, dist_params){</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> n_vals <span class="sc">%&gt;%</span> </span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">map_df</span>(\(n) <span class="fu">draw_N_estimates</span>(theta, N, n, dist, dist_params)) </span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">draw_N_estimates_over_n</span>(</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">theta =</span> <span class="cf">function</span>(X){<span class="fu">sum</span>((X <span class="sc">-</span> <span class="fu">mean</span>(X))<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="fu">length</span>(X)},</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>  <span class="at">N =</span> <span class="fl">1e6</span>,</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>  <span class="at">n_vals =</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">25</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>),</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist =</span> rnorm,</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist_params =</span> <span class="fu">list</span>(</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    <span class="at">mean =</span> <span class="dv">0</span>,</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    <span class="at">sd =</span> <span class="dv">1</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="co">#Print bias calculated over 1,000,000 simulations</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(sample_size) <span class="sc">%&gt;%</span> </span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">bias =</span> <span class="fu">mean</span>(estimate) <span class="sc">-</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 2
  sample_size     bias
        &lt;dbl&gt;    &lt;dbl&gt;
1          10 -0.0998 
2          25 -0.0406 
3          50 -0.0202 
4         100 -0.0102 
5         500 -0.00197</code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(estimate, <span class="at">color =</span> <span class="fu">as.factor</span>(sample_size))) <span class="sc">+</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>() <span class="sc">+</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>,<span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Estimates of Variance, True Value = 1"</span>, <span class="at">y =</span> <span class="st">"Density"</span>, <span class="at">color =</span> <span class="st">"Sample Size"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot26" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot26-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-plot26-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot26-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.7: As the sample size increases, the bias of our estimator converges to zero.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The estimator <span class="math inline">\(\hat\theta\)</span> is also consistent, as it converges in mean square.</p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Asymptotic unbiasedness does not imply consistency, and consistency does not imply asymptotic unbiasedness.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.7 (Consistent, Not Asymptotically Unbiased)</strong></span> Recall that the sequence of discrete random variables <span class="math inline">\(X_n\)</span> with denisty <span class="math display">\[ f_{X_n}(x) = \begin{cases}1-1/n&amp; x=0\\ 1/n &amp; x = n^2 \end{cases}.\]</span> We established that <span class="math inline">\(X_n\overset{p}{\to}0\)</span>, so an estimator with this distribution would be a consistent estimator for <span class="math inline">\(0\)</span>. Despite this, the estimator would not be asymptotically unbiased, as <span class="math inline">\(\text{E}\left[X_n\right] = n\)</span>, which tends to infinity as <span class="math inline">\(n\)</span> grows.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.8 (Asymptotically Unbiased, Not Consistent)</strong></span> For <span class="math inline">\(X_i\overset{iid}{\sim}N(\mu,\sigma^2)\)</span>, define the estimator <span class="math inline">\(\hat\mu(\mathbf{X}) = X_1\)</span>. We simply take the first observation to be our estimate of <span class="math inline">\(\mu\)</span>. This estimator is unbiased, <span class="math display">\[\text{E}\left[\hat\mu\right] = \text{E}\left[X_1\right] = \mu,\]</span> so it is asymptotically unbiased. Nevertheless, the estimator fails to be consistent, as <span class="math display">\[\lim_{n\to\infty} \Pr (|\hat\mu - \mu| &gt; \varepsilon)= \lim_{n\to\infty} \left\{1 - 2\left[\Phi\left(\frac{\varepsilon}{\sigma}\right) - \frac{1}{2}\right]\right\} \neq 0 .\]</span></p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The incompatibility of asymptotic unbiasedness and consistency is due to the behavior of <span class="math inline">\(\text{Var}\left(X_n\right)\)</span> as <span class="math inline">\(n\to\infty\)</span>.</p>
<div id="prp-consbias" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 2.5 (Relating Consistency and Asymptotic Unbiasedness)</strong></span> Suppose <span class="math inline">\(\hat\theta\)</span> is an estimator for <span class="math inline">\(\theta\)</span>.</p>
<ol type="1">
<li>If <span class="math inline">\(\hat\theta\)</span> is consistent and there exists some <span class="math inline">\(M\)</span> such that <span class="math inline">\(\text{Var}\left(\hat\theta\right) \le M\)</span> for all <span class="math inline">\(n\)</span> (bounded variance), then <span class="math inline">\(\hat\theta\)</span> is asymptotically unbiased.</li>
<li>If <span class="math inline">\(\hat\theta\)</span> is asymptotically unbiased and <span class="math inline">\(\lim_{n\to\infty}\text{Var}\left(\hat\theta\right) = 0\)</span> (vanishing variance), then <span class="math inline">\(\hat\theta\)</span> is consistent</li>
</ol>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>test</p>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-plot27" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot27-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/relating_convergence.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="639">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot27-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.8: Relationship between various concepts of convergence in the context of estimators
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="laws-of-large-numbers" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="laws-of-large-numbers"><span class="header-section-number">2.3</span> Laws of Large Numbers</h2>
<p>In most examples until now, the properties of estimators were implicitly a function of the underlying model the data is generated from. We established that <span class="math inline">\(\bar X\)</span> and <span class="math inline">\(S^2\)</span> are consistent estimators for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>, respectively, <em>when</em> <span class="math inline">\(X_i \overset{iid}{\sim}N(\mu,\sigma^2)\)</span>. We know the distribution of <span class="math inline">\(\bar X\)</span>, <em>when</em> <span class="math inline">\(X_i \overset{iid}{\sim}N(\mu,\sigma^2)\)</span>. In an ideal world, we would be able to establish desirable properties of estimators under more robust settings where our specified model may include a wide array of distributions. Our first step in doing this will be introducing variants of one of the most important results in all of probability – the law of large numbers (LLN). In the context of estimation, LLNs give sufficient conditions for our favorite estimator, <span class="math inline">\(\bar X\)</span>, to be consistent.</p>
<section id="khinchines-weak-law-of-large-numbers" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="khinchines-weak-law-of-large-numbers"><span class="header-section-number">2.3.1</span> Khinchine’s Weak Law of Large Numbers</h3>
<p>The version of the LLN we’ll use the most often deals with convergence in probability. To prove that <span class="math inline">\(\bar X \overset{p}{\to}\mu\)</span> we’ll rely on an inequality similar to <a href="#lem-markovineq" class="quarto-xref">Lemma&nbsp;<span class="quarto-unresolved-ref">lem-markovineq</span></a>.</p>
<div id="lem-" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2.2 (Chebyshev’s Inequality)</strong></span> If <span class="math inline">\(X\)</span> is a random variable with an expected value <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then for all <span class="math inline">\(a &gt; 0\)</span> <span class="math display">\[\Pr(|X - \mu| \ge k) \le \frac{\sigma^2}{k^2}.\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[\begin{align*}
\Pr(|X - \mu| \ge k) &amp;= \Pr((X - \mu)^2 \ge k^2)\\
&amp; \le \frac{\text{E}\left[(X-\mu)^2\right]}{k^2} &amp; (\text{Markov's Inequality})\\
&amp; = \frac{\sigma^2}{k^2}
\end{align*}\]</span></p>
</div>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.1 (LLN I)</strong></span> If <span class="math inline">\((X_1,\ldots, X_n)\)</span> are a set of iid random variables where <span class="math inline">\(\text{E}\left[X_i\right] = \mu\)</span>, then <span class="math inline">\(\bar X \overset{p}{\to}\mu\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Recall that <span class="math inline">\(\text{Var}\left(\bar X\right) = \sigma^2/n\)</span>. By Chebyshev’s Inequality, <span class="math display">\[\begin{align*}
\lim_{n\to\infty}\Pr(|X_n - \mu| \ge \varepsilon) \le \lim_{n\to\infty}\frac{(\sigma^2/n)}{\varepsilon^2} =   \lim_{n\to\infty} \frac{\sigma^2}{n\varepsilon} = 0.
\end{align*}\]</span> Therefore, <span class="math inline">\(\bar X\overset{p}{\to}\mu\)</span>.</p>
</div>
<div id="exm-lln1" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.9</strong></span> To illustrate the LLN, let’s simulate realizations of iid random variables from a series of different distributions and show that regardless of the distribution, <span class="math inline">\(\bar X \to \mu\)</span>. We will use the following distributions: <span class="math display">\[\begin{align*}
X_i &amp; \overset{iid}{\sim}\text{Exp}(1/\mu)\\
X_i &amp; \overset{iid}{\sim}\chi_\mu^2\\
X_i &amp; \overset{iid}{\sim}\text{Uni}(0, 2\mu)\\
X_i &amp; \overset{iid}{\sim}\text{Gamma}(2\mu, 2)\\
X_i &amp; \overset{iid}{\sim}\text{HyperGeo}(10, 20, 15\mu)
\end{align*}\]</span> All these distributions have been selected such that <span class="math inline">\(\text{E}\left[X_i\right] = \mu\)</span>. For our simulations, we will take <span class="math inline">\(\mu = 5\)</span>. If we plot the value of the sample mean versus the sample size <span class="math inline">\(n\)</span>, we see that the values converge to the true value <span class="math inline">\(\mu = 5\)</span> regardless of the distribution of <span class="math inline">\(X_i\)</span>.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>mean_n_increases <span class="ot">&lt;-</span> <span class="cf">function</span>(n, dist, dist_label, dist_params, t){</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">sample_size =</span> <span class="dv">1</span><span class="sc">:</span>n,</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">prob_dist =</span> dist_label,</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">random_values =</span> <span class="fu">do.call</span>(dist, <span class="fu">append</span>(n, dist_params)),</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">iter_num =</span> t</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">%&gt;%</span> </span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">estimate =</span> <span class="fu">cummean</span>(random_values))</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="fu">list</span>(</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(rexp,  rchisq, runif, rgamma, rhyper),</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="st">"rexp"</span>, <span class="st">"rchisq"</span>, <span class="st">"runif"</span>, <span class="st">"rgamma"</span>, <span class="st">"rhyper"</span>),</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="fu">list</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">5</span>), <span class="fu">list</span>(<span class="dv">5</span>), <span class="fu">list</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="fu">list</span>(<span class="dv">10</span>, <span class="dv">2</span>), <span class="fu">list</span>(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">15</span>))</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pmap_df</span>(\(dist, dist_label, dist_params) <span class="fu">mean_n_increases</span>(<span class="at">n =</span> <span class="fl">1e5</span>, dist, dist_label, dist_params, <span class="at">t =</span> <span class="dv">1</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(sample_size, estimate, <span class="at">color =</span> prob_dist)) <span class="sc">+</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">alpha=</span><span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Sample Size, n"</span>, <span class="at">y =</span> <span class="st">"Sample Mean"</span>, <span class="at">color =</span> <span class="st">"Distribution of iid Random Sample"</span>) <span class="sc">+</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="fl">4.5</span>, <span class="fl">5.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot28" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot28-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-plot28-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot28-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.9: The sample mean of all samples converges to the population mean by the LLN
</figcaption>
</figure>
</div>
</div>
</div>
<p>For the sake of an even better illustration, let’s focus on the case where <span class="math inline">\(X_i \overset{iid}{\sim}\text{Exp}(1/\mu)\)</span> where <span class="math inline">\(\mu = 5\)</span>. We’ll draw 10,000 samples of <span class="math inline">\(X_i\)</span>, and calculate <span class="math inline">\(\bar X\)</span> for each sample as the sample size ranges from 1 all the way to 1,000.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>mean_n_increases_N_draws <span class="ot">&lt;-</span> <span class="cf">function</span>(N, n, dist, dist_label, dist_params){</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>N <span class="sc">%&gt;%</span> </span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">map_df</span>(\(t) <span class="fu">mean_n_increases</span>(n, dist, dist_label, dist_params, t))</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">mean_n_increases_N_draws</span>(</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">N =</span> <span class="fl">1e4</span>, </span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="fl">1e3</span>, </span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist =</span> rexp, </span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist_label =</span> <span class="st">"rexp"</span>, </span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist_params =</span> <span class="fu">list</span>(</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">rate =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">5</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>) </span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> sample_size, <span class="at">y =</span> estimate)) <span class="sc">+</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">group =</span> iter_num), <span class="at">alpha =</span> <span class="fl">0.1</span>, <span class="at">size =</span> <span class="fl">0.05</span>) <span class="sc">+</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="dv">15</span>) <span class="sc">+</span> </span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">5</span> <span class="sc">-</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linetype=</span><span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">5</span> <span class="sc">+</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linetype=</span><span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot28.2" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot28.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-plot28.2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot28.2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.10: 10,000 lines, each of which corresponds to a simulated realization of the sample mean over a range of sample sizes. The area within the red dashed lines is the set |μ - Sample Mean| &lt; 0.5
</figcaption>
</figure>
</div>
</div>
</div>
<p>The weak LLN tells us that since <span class="math inline">\(\bar X\overset{p}{\to}5\)</span>, then by the definition of convergence in probability, the probability one of the lines falls in <a href="#fig-plot28.2" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-plot28.2</span></a> outside the interval within the red dashed lines at a particular sample size <span class="math inline">\(n\)</span> approaches zero as <span class="math inline">\(n\to \infty\)</span>. Furthermore, this will hold regardless of how close we make the red lines to the true value <span class="math inline">\(\mu = 5\)</span>. For the sake of illustration we took <span class="math inline">\(\varepsilon = 0.5\)</span>, but it will hold <em>for all</em> <span class="math inline">\(\varepsilon &gt; 0\)</span>. In fact, we can go one step further and illustrate <span class="math inline">\(\Pr(|\bar X-\mu| &gt; \varepsilon)\)</span> by looking at the proportion of times that <span class="math inline">\(|\bar X-\mu| &gt; \varepsilon\)</span> holds across out 10,000 simulations. We’ll do this for <span class="math inline">\(\varepsilon=0.5,0.6,\ldots,1\)</span>.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">expand_grid</span>(<span class="at">epsilon =</span> <span class="dv">5</span><span class="sc">:</span><span class="dv">10</span><span class="sc">/</span><span class="dv">10</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">outside =</span> estimate <span class="sc">&gt;=</span> <span class="dv">5</span> <span class="sc">+</span> epsilon <span class="sc">|</span> estimate <span class="sc">&lt;=</span> <span class="dv">5</span> <span class="sc">-</span> epsilon) <span class="sc">%&gt;%</span> </span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(sample_size, epsilon) <span class="sc">%&gt;%</span> </span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">prob =</span> <span class="fu">sum</span>(outside) <span class="sc">/</span> <span class="fu">n</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> sample_size, prob, <span class="at">color =</span> <span class="fu">factor</span>(epsilon))) <span class="sc">+</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">"Observed Pr(|Estimate-μ|&gt;ε)"</span>, <span class="at">x =</span> <span class="st">"Sample Size n"</span>, <span class="at">color =</span> <span class="st">"ε"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot28.3" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot28.3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-plot28.3-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot28.3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.11: For each ε, the observed probability in question approaches 0 as the sample size grows.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.10 (Monte Carlo Simulations)</strong></span> In <a href="#exm-var" class="quarto-xref">Example&nbsp;<span class="quarto-unresolved-ref">exm-var</span></a> we performed a Monte Carlo simulation to illustrate the bias of <span class="math inline">\(\hat\theta(\mathbf{X}) = \sum_{i=1}^n (X_i - \bar X)/n\)</span> and unbiasedness of <span class="math inline">\(S^2\)</span>. We did this by fixing <span class="math inline">\(n=20\)</span>, drawing a random sample, recording estimates <span class="math inline">\(\hat\theta(\mathbf{x})\)</span> and <span class="math inline">\(S^2(\mathbf{x})\)</span>, and repeating this <span class="math inline">\(k\)</span> times. This is nothing more than drawing <span class="math inline">\(j=1,\ldots,k\)</span> observations from the random variables <span class="math inline">\(\hat\theta(\mathbf{X})\)</span> and <span class="math inline">\(S^2(\mathbf{X})\)</span>. By the LLN, <span class="math display">\[\begin{align*}
\frac{1}{k}\sum_{i=1}^k \hat\theta_j(\mathbf{x}) &amp;\overset{p}{\to}\text{E}\left[\hat\theta(\mathbf{X})\right],\\
\frac{1}{k}\sum_{i=1}^k S_j^2(\mathbf{x}) &amp;\overset{p}{\to}\text{E}\left[S^2(\mathbf{X})\right],
\end{align*}\]</span> so for a large enough <span class="math inline">\(k\)</span>, we can approximate the expected value with its sample counterpart.</p>
</div>
</section>
<section id="kolmogorovs-strong-law-of-large-numbers" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="kolmogorovs-strong-law-of-large-numbers"><span class="header-section-number">2.3.2</span> Kolmogorov’s Strong Law of Large Numbers</h3>
<p>A stronger version of the LLN is stated in terms of almost sure convergence. Since almost sure convergence is stronger than convergence in probability (which is all that is needed for an estimator to be consistent), this version is referred to as the “strong” LLN.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.2 (LLN II)</strong></span> If <span class="math inline">\((X_1,\ldots, X_n)\)</span> are a set of iid random variables where <span class="math inline">\(\text{E}\left[X_i\right] = \mu\)</span>, then <span class="math inline">\(\bar X \overset{as}{\to}\mu\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>See the proof of Theorem 6.1 in <span class="citation" data-cites="billingsley2008probability">Billingsley (<a href="#ref-billingsley2008probability" role="doc-biblioref">2008</a>)</span>.</p>
</div>
<div id="exm-strongvsweak" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.11 (Strong vs Weak LLN)</strong></span> We can actually visualize the difference between the strong and weak LLNs, and in doing so highlight the difference between almost sure convergence and convergence in probability. Like <a href="#exm-lln1" class="quarto-xref">Example&nbsp;<span class="quarto-unresolved-ref">exm-lln1</span></a>, assume <span class="math inline">\(X_i \sim\text{Exp}(1/\mu)\)</span> where <span class="math inline">\(\mu = 5\)</span>. We’ll perform a similar simulation to that which gave <a href="#fig-plot28.2" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-plot28.2</span></a>, but this time we’ll only look at one sample of <span class="math inline">\(X_i\)</span> (instead of 10,000).</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean_n_increases</span>(</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="fl">1e5</span>,</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist =</span> rexp,</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist_label =</span> <span class="st">"rexp"</span>,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist_params =</span> <span class="fu">list</span>(</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">rate =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">5</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>  ),</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">t =</span> <span class="dv">1</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(sample_size, estimate)) <span class="sc">+</span> </span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span> </span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">5</span> <span class="sc">-</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linetype=</span><span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">5</span> <span class="sc">+</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linetype=</span><span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot28.4" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot28.4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-plot28.4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot28.4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.12: A single simulation realization of the sample mean over a range of sample sizes. The area within the red dashed lines is the set |μ - Sample Mean| &lt; 0.5
</figcaption>
</figure>
</div>
</div>
</div>
<p>The strong LLN tells us that for some finite sample size <span class="math inline">\(N\)</span> the line in <a href="#fig-plot28.4" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-plot28.4</span></a> will fall within the red lines for all <span class="math inline">\(n &gt; N\)</span>.</p>
</div>
</section>
<section id="chebyshevs-weak-law-of-large-numbers" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="chebyshevs-weak-law-of-large-numbers"><span class="header-section-number">2.3.3</span> Chebyshev’s Weak Law of Large Numbers</h3>
<p>The crucial assumption made by both LLNs up to this point is that <span class="math inline">\(\bar X\)</span> is calculated with an iid random sample. If we drop this assumption, then <span class="math inline">\(\bar X\)</span> needn’t estimate <span class="math inline">\(\mu\)</span> consistently.</p>
<div id="exm-noiid" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.12 (LLN Failing with Non-IID Data)</strong></span> Suppose <span class="math inline">\(\mathbf{X}= (X_1, \ldots, X_n)\)</span> where <span class="math inline">\(X_i \sim N(-1, i)\)</span> if <span class="math inline">\(i\)</span> is odd, and <span class="math inline">\(X_i \sim N(1,i)\)</span> is <span class="math inline">\(i\)</span> is even. The data is independent, but not identically distributed. If some LLN would hold here, we would suspect that <span class="math inline">\(\bar X\)</span> would converge <span class="math inline">\(0\)</span> since the average of the underlying population means <span class="math inline">\(1\)</span> and <span class="math inline">\(-1\)</span>. Let’s simulate <span class="math inline">\(\bar X\)</span> for <span class="math inline">\(n\)</span> ranging from 1 to 100,000.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>draw_X_i <span class="ot">&lt;-</span> <span class="cf">function</span>(i){</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>  mu_i <span class="ot">&lt;-</span> (<span class="sc">-</span><span class="dv">1</span>)<span class="sc">^</span>(i <span class="sc">%%</span> <span class="dv">2</span>) <span class="sc">*</span> (<span class="dv">1</span>)<span class="sc">^</span>(<span class="dv">1</span> <span class="sc">-</span> i <span class="sc">%%</span> <span class="dv">2</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  sigma_i <span class="ot">&lt;-</span> i </span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, mu_i, sigma_i)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>draw_X <span class="ot">&lt;-</span> <span class="cf">function</span>(n){</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">i =</span> <span class="dv">1</span><span class="sc">:</span>n,</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">value =</span> <span class="fu">map_dbl</span>(<span class="dv">1</span><span class="sc">:</span>n, draw_X_i)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">estimate =</span> <span class="fu">cummean</span>(value))</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">draw_X</span>(<span class="fl">1e5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We see that our estimates very much do not converge to any particular value.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(i, estimate)) <span class="sc">+</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Sample Size"</span>, <span class="at">y =</span> <span class="st">"Sample Mean"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot29" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot29-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-plot29-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot29-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.13: The sample mean of non-IID data does not satisfy the LLN
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The proof of the LLN relied on Chebyshev’s equality and the fact that <span class="math inline">\(\text{Var}\left(\bar X\right) = \sigma^2/n \to 0\)</span> when <span class="math inline">\(\text{Var}\left(X_i\right) = \sigma^2\)</span>. Perhaps if we added an assumption regarding the variance of a non-iid sample, then we could salvage a result similar to the LLN. This is precisely what Chebyshev’s LLN does.</p>
<div id="prp-chebylln" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 2.6 (Chebyshev’s (Weak) Law of Large Numbers)</strong></span> Suppose <span class="math inline">\((X_1,\ldots, X_n)\)</span> are a sample such that <span class="math inline">\(\text{E}\left[X_i\right] = \mu_i\)</span>, <span class="math inline">\(\text{Cov}\left(X_i, X_j\right) = \sigma_{ij}^2\)</span>, and <span class="math inline">\(\lim_{n\to\infty}\frac{1}{n^2}\sum_{i=1}^n \sigma_{ij}^2 =0\)</span>. If <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n\mu_i \overset{p}{\to}\mu\)</span>, then <span class="math inline">\(\bar X \overset{p}{\to}\mu\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The expected value of <span class="math inline">\(\bar X\)</span> is <span class="math display">\[\text{E}\left[\bar X\right] = \frac{1}{n}\sum_{i=1}^n\text{E}\left[X_i\right]= \frac{1}{n}\sum_{i=1}^n\mu_i \overset{p}{\to}\mu.\]</span> The variance is <span class="math display">\[ \text{Var}\left(\bar X\right) = \text{Var}\left(\frac{1}{n}\sum_{i=1}^nX_i\right) = \frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n\text{Cov}\left(X_i,X_j\right) = \frac{1}{n}\sum_{i=1}^n \sigma_{ij}^2\to 0.\]</span> By Proposition <a href="#prp-mse3" class="quarto-xref">Proposition&nbsp;<span class="quarto-unresolved-ref">prp-mse3</span></a>, <span class="math inline">\(\bar X\overset{ms}{\to}\mu\)</span>, so <span class="math inline">\(\bar X \overset{p}{\to}\mu\)</span>.</p>
</div>
<div id="cor-" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 2.3</strong></span> Suppose <span class="math inline">\((X_1,\ldots, X_n)\)</span> are an independent sample such that <span class="math inline">\(\text{E}\left[X_i\right] = \mu_i\)</span>, <span class="math inline">\(\text{Var}\left(X_i\right) = \sigma_{i}^2\)</span>, and <span class="math inline">\(\lim_{n\to\infty}\frac{1}{n^2}\sum_{i=1}^n \sigma_i^2 =0\)</span>. If <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n\mu_i \overset{p}{\to}\mu\)</span>, then <span class="math inline">\(\bar X \overset{p}{\to}\mu\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If the sample is independent, then <span class="math display">\[\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n\text{Cov}\left(X_i,X_j\right) =\frac{1}{n^2}\sum_{i=1}^n\text{Var}\left(X_i\right). \]</span></p>
</div>
<p>The reason our non-iid sample in Example <a href="#exm-noiid" class="quarto-xref">Example&nbsp;<span class="quarto-unresolved-ref">exm-noiid</span></a> did not converge was because <span class="math display">\[ \frac{1}{n^2}\sum_{i=1}^n \sigma_i^2 = \frac{1}{n^2}\sum_{i=1}^n i = \frac{1}{n^2}\frac{n(n+1)}{2} = \frac{n^2 + n}{2n^2} \to \frac{1}{2} \neq 0.\]</span> Let’s modify it slightly so the sum of the variances does converge to zero.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.13 (LLN with Non-IID Data)</strong></span> Suppose <span class="math inline">\(\mathbf{X}= (X_1, \ldots, X_n)\)</span> where <span class="math inline">\(X_i \sim N(-1,i^{-1})\)</span> if <span class="math inline">\(i\)</span> is odd, and <span class="math inline">\(X_i \sim N(1,i^{-1})\)</span> is <span class="math inline">\(i\)</span> is even. Now we have <span class="math display">\[\lim_{n\to\infty}\frac{1}{n^2}\sum_{i=1}^n \sigma_i^2 = \left[\lim_{n\to\infty}\frac{1}{n^2}\right]\sum_{i=1}^\infty i^{-1} \to 0 .\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>draw_X_i <span class="ot">&lt;-</span> <span class="cf">function</span>(i){</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>  mu_i <span class="ot">&lt;-</span> (<span class="sc">-</span><span class="dv">1</span>)<span class="sc">^</span>(i <span class="sc">%%</span> <span class="dv">2</span>) <span class="sc">*</span> (<span class="dv">1</span>)<span class="sc">^</span>(<span class="dv">1</span> <span class="sc">-</span> i <span class="sc">%%</span> <span class="dv">2</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>  sigma_i <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>i </span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, mu_i, sigma_i)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>draw_X <span class="ot">&lt;-</span> <span class="cf">function</span>(n){</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">i =</span> <span class="dv">1</span><span class="sc">:</span>n,</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">value =</span> <span class="fu">map_dbl</span>(<span class="dv">1</span><span class="sc">:</span>n, draw_X_i)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">estimate =</span> <span class="fu">cummean</span>(value))</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">draw_X</span>(<span class="dv">150</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we see that <span class="math inline">\(\bar X\)</span> is converging to <span class="math inline">\(0\)</span>, and doing so rather quickly.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(i, estimate)) <span class="sc">+</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Sample Size"</span>, <span class="at">y =</span> <span class="st">"Sample Mean"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot210" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot210-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-plot210-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot210-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.14: Despite the data not being IID, the sample mean still converges to the population mean because the variance shrinks in the correct way.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="the-continuous-mapping-theorem-and-slutskys-theorem" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="the-continuous-mapping-theorem-and-slutskys-theorem"><span class="header-section-number">2.4</span> The Continuous Mapping Theorem and Slutsky’s Theorem</h2>
<p>At first glance, the LLN may not seem especially useful as it only applies to the sample mean. However, when paired with two key results about convergence, the LLN becomes an indispensable tool to analyze the convergence of many random variables and estimators. The first of these is an extension of a key result in real analysis. A useful, and defining property, of continuous functions is that they preserve limits of numeric sequences. If <span class="math inline">\(\{a_n\}\)</span> is a numeric sequence, then <span class="math display">\[\lim_{n \to\infty} f(a_n) = f\left(\lim_{n\to\infty} a_n\right) \iff f\text{ continuous}.\]</span></p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.3 (Continuous Mapping Theorem I)</strong></span> Suppose <span class="math inline">\(X_n \overset{p}{\to}X\)</span>, and let <span class="math inline">\(g\)</span> be a continuous function. Then <span class="math display">\[g(X_n)\overset{p}{\to}g(X).\]</span> In other words we are able to interchange the <span class="math inline">\(\mathop{\mathrm{plim}}\)</span> operator with a continuous function: <span class="math display">\[\mathop{\mathrm{plim}}g(X_n) = g\left(\mathop{\mathrm{plim}}X_n\right).\]</span></p>
</div>
<p>The proof of this result can be found in <span class="citation" data-cites="van2000asymptotic">Van der Vaart (<a href="#ref-van2000asymptotic" role="doc-biblioref">2000</a>)</span>. An immediate corollary follows from the fact that convergence in probability implies convergence in distribution.</p>
<div id="cor-" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 2.4 (Continuous Mapping Theorem II)</strong></span> Suppose <span class="math inline">\(X_n \overset{p}{\to}X\)</span>, and let <span class="math inline">\(g\)</span> be a continuous function. Then <span class="math display">\[g(X_n)\overset{d}{\to}g(X)\]</span></p>
</div>
<p>An equally useful result involves the limit of a sums and products of convergent random variables.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.4 (Slusky’s Theorem)</strong></span> Let <span class="math inline">\(X_n\)</span> and <span class="math inline">\(Y_n\)</span> be sequences of random variables. If <span class="math inline">\(X_n\overset{d}{\to}X\)</span> for some random variable <span class="math inline">\(X\)</span>, and <span class="math inline">\(Y_n\overset{p}{\to}c\)</span> for some constant <span class="math inline">\(c\)</span>, then <span class="math display">\[\begin{align*}
X_n + Y_n &amp;\overset{d}{\to}X + c\\
X_nY_n &amp; \overset{d}{\to}Xc.
\end{align*}\]</span> Furthermore, if <span class="math inline">\(c\neq 0\)</span>, <span class="math display">\[ X_n/Y_n \overset{d}{\to}X/c.\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Define a random vector to be <span class="math inline">\(\mathbf Z_n = (X_n,Y_n)\)</span>. We have <span class="math inline">\(\mathbf Z_n \overset{d}{\to}(X,c)\)</span> as <span class="math inline">\(X_n\overset{d}{\to}X\)</span> and <span class="math inline">\(Y_n \overset{d}{\to}c\)</span> (convergence in probability implies convergence in distribution). We can apply the continuous mapping theorem to <span class="math inline">\(g(x,y) = x + y\)</span>, <span class="math inline">\(g(x,y)=xy\)</span>, and <span class="math inline">\(g(x/y)\)</span> to establish the result.</p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Slutsky’s theorem can be a bit hard to remember because it involves a sequence of random variable which converges in distribution to a random variable, and a sequence of random variables which converges in probability to a constant. These asymmetries in mode of convergence and the type of limit are essential, otherwise the result will not hold. Fortunately, the result does hold if we replace all convergences in distribution with convergence in probability (as the later implies the prior).</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.14</strong></span> Suppose <span class="math inline">\(X_n \sim\text{Uni}(0,1)\)</span> and <span class="math inline">\(Y_n = - X_n\)</span>. We have <span class="math inline">\(X_n \overset{d}{\to}\text{Uni}(0,1)\)</span> and <span class="math inline">\(Y_n \overset{d}{\to}\text{Uni}(-1,0)\)</span>. Despite this <span class="math inline">\(X_n + Y_n = 0 \not\overset{d}{\to}\text{Uni}(0,1) + \text{Uni}(-1,0).\)</span></p>
</div>
<div id="exm-convar" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.15 (Consistency of Sample Variance)</strong></span> <a href="#exm-consvarnorm" class="quarto-xref">Example&nbsp;<span class="quarto-unresolved-ref">exm-consvarnorm</span></a> showed that <span class="math inline">\(S^2\)</span> is a consistent estimator for <span class="math inline">\(\sigma^2\)</span> when <span class="math inline">\(X_i\overset{iid}{\sim}N(\mu,\sigma^2)\)</span>. We can use the continuous mapping theorem, Slutsky’s theorem, and the LLN to show that <span class="math inline">\(S^2\)</span> is consistent regardless of the distribution of our iid sample. Suppose <span class="math inline">\(\text{E}\left[X_i\right] = \mu\)</span>, <span class="math inline">\(\text{E}\left[X_i^2\right]=\mu_2\)</span>, and <span class="math inline">\(\text{E}\left[X_i^4\right]=\mu_4\)</span> for all <span class="math inline">\(i\)</span>. If we define our continuous function to be <span class="math inline">\(g(x) = x^2\)</span>, then <span class="math display">\[\begin{align*}
S^2 &amp; = \frac{1}{n-1} \sum_{i=1}^nX_i - \frac{1}{n-1} \sum_{i=1}^n\bar X^2 \\
    &amp; = \frac{1}{n-1} \sum_{i=1}^nX_i^2 + \frac{n}{n-1}\bar X^2  \\
    &amp; = \frac{n}{n-1}\left[\frac{1}{n}\sum_{i=1}^{n}X_i^2 - \left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)^2\right]
\end{align*}\]</span> The first term in the brackets is an unbiased estimator of <span class="math inline">\(\mu_2\)</span> with vanishing variance, so by <a href="#prp-consbias" class="quarto-xref">Proposition&nbsp;<span class="quarto-unresolved-ref">prp-consbias</span></a> it is a consistent estimator for <span class="math inline">\(\text{E}\left[X^2\right]\)</span>: <span class="math display">\[\begin{align*}
\text{E}\left[\frac{1}{n}\sum_{i=1}^{n}X_i^2\right] &amp;= \frac{1}{n}\left(n \mu_2\right) = \mu^2\\
\lim_{n\to\infty}\text{Var}\left(\frac{1}{n}\sum_{i=1}^{n}X_i^2\right) &amp; = \lim_{n\to\infty}\frac{1}{n^2}n\left(\text{E}\left[X_i^4\right] - \text{E}\left[X_i^2\right]^2 \right) = \lim_{n\to\infty}\frac{\mu_4 + \mu_2^2}{n} = 0
\end{align*}\]</span> The second term in the brackets can be written as <span class="math inline">\(g(\bar X)\)</span>, so by the continuous mapping theorem and the LLN, <span class="math display">\[ g(\bar X) = \left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)^2 \overset{p}{\to}\mu^2 = g(\mu).\]</span> So <span class="math display">\[S^2= \underbrace{\frac{n}{n-1}}_{\to 1}\Bigg[\underbrace{\frac{1}{n}\sum_{i=1}^{n}X_i^2}_{\overset{p}{\to}\mu_2} - \underbrace{\left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)^2}_{\overset{p}{\to}\mu^2} \Bigg].\]</span> We can apply Slutsky’s theorem to the sum of sequences of random variables which converge in probability to constants, so <span class="math display">\[ S^2 \overset{p}{\to}\mu_2 - \mu^2 = \sigma^2,\]</span> making <span class="math inline">\(S^2\)</span> consistent.</p>
</div>
</section>
<section id="central-limit-theorems" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="central-limit-theorems"><span class="header-section-number">2.5</span> Central Limit Theorems</h2>
<p>The LLN told us that our favorite estimator for <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\bar X\)</span>, is consistent. We know turn to what is perhaps an even more important result regarding <span class="math inline">\(\bar X\)</span>, one that may in fact be the most important results in all of probability – the asymptotic distribution of <span class="math inline">\(\bar X\)</span> is a normal distribution.</p>
<section id="lindeberg-lévy-clt" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="lindeberg-lévy-clt"><span class="header-section-number">2.5.1</span> Lindeberg-Lévy CLT</h3>
<p>The <em>classic</em> version of the CLT is formally known as the Lindeberg-Lévy CLT, and is likely familiar.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.5 (CLT I)</strong></span> Suppose <span class="math inline">\(\mathbf{X}=(X_1,\ldots, X_n)\)</span> is a sequence of random variables with <span class="math inline">\(\text{E}\left[X_i\right]=\mu\)</span> and <span class="math inline">\(\text{Var}\left(X_i\right)=\sigma^2\)</span>. Then <span class="math display">\[\sqrt{n}(\bar X - \mu) \overset{d}{\to}N(0,\sigma^2),\]</span> which is also often written as <span class="math display">\[\bar X\overset{d}{\to}N(\mu, \sigma^2/n),\]</span> or <span class="math display">\[\sum_{i=1}^n X_i \overset{d}{\to}N(n\mu, \sqrt n \sigma^2) \]</span></p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The proof is a bit technical and requires some measure-theoretic based probability theory. It can be found in <span class="citation" data-cites="billingsley2008probability">Billingsley (<a href="#ref-billingsley2008probability" role="doc-biblioref">2008</a>)</span> or <span class="citation" data-cites="durrett2019probability">Durrett (<a href="#ref-durrett2019probability" role="doc-biblioref">2019</a>)</span>.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.16</strong></span> Suppose we have an iid sample from <span class="math inline">\(\text{Exp}(1)\)</span>. If we simulate 1000 realizations of <span class="math inline">\(\sqrt n(\bar X - \mu)\)</span> for various sample sizes, we should see that the distribution of our realizations becomes approximately normal as we increase the sample size.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">draw_N_estimates_over_n</span>(</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">theta =</span> <span class="cf">function</span>(X){<span class="fu">sqrt</span>(<span class="fu">length</span>(X))<span class="sc">*</span>(<span class="fu">mean</span>(X) <span class="sc">-</span> <span class="dv">1</span>)},</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">N =</span> <span class="fl">1e4</span>,</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">n_vals =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">25</span>, <span class="dv">50</span>, (<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>)<span class="sc">*</span><span class="dv">100</span>),</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist =</span> rexp,</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist_params =</span> <span class="fu">list</span>(</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">rate =</span> <span class="dv">1</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Even for modest values of <span class="math inline">\(n\)</span>, we can see that <span class="math inline">\(\sqrt n(\bar X - \mu) \overset{a}{\sim}N(0, \sigma^2)\)</span>.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(estimate)) <span class="sc">+</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> <span class="fu">after_stat</span>(density)), <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">fill =</span> <span class="st">"white"</span>, <span class="at">bins =</span> <span class="dv">50</span>) <span class="sc">+</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_function</span>(<span class="at">fun =</span> dnorm, <span class="at">colour =</span> <span class="st">"red"</span>) <span class="sc">+</span> </span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transition_states</span>(sample_size) <span class="sc">+</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'Sample Size: {closest_state}'</span>, <span class="at">x =</span> <span class="st">"Estimates"</span>, <span class="at">y =</span> <span class="st">""</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot211" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot211-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-plot211-1.gif" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot211-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.15: As the sample size increases, the distribution of the adjusted sample mean looks more and more like the standard normal.
</figcaption>
</figure>
</div>
</div>
</div>
<p>An alternate way to visually test whether our estimates are normally distributed is with a quantile-quantile plot (QQ-plot), which graphs the observed quantiles of our estimates against the theoretical quantiles of a normal distribution (or those of any distribution we suspect our data is drawn from). If our estimates are (approximately) normally distributed, then the observed quantiles should be approximately equal to the theoretical quantiles of a normal distribution, forming a 45-degree line.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">sample =</span> estimate)) <span class="sc">+</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq_line</span>(<span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_qq</span>(<span class="at">size=</span> <span class="fl">0.5</span>) <span class="sc">+</span> </span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transition_states</span>(sample_size) <span class="sc">+</span> </span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'Sample Size: {closest_state}'</span>, <span class="at">x =</span> <span class="st">"Quantiles of Simulated Estimates"</span>, <span class="at">y =</span> <span class="st">"Quantiles of Normal Distribution"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot212" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot212-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-plot212-1.gif" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot212-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.16: The QQ-plot for the simulated distribution of the adjusted sample mean
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The central limit theorem is similar to the LLN insofar that they only concern the estimator <span class="math inline">\(\bar X\)</span>, so how useful can they really be? Well with the continuous mapping theorem and Slutsky’s theorem, the answer is <em>very useful</em>!</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.17</strong></span> In <a href="#exm-tdist" class="quarto-xref">Example&nbsp;<span class="quarto-unresolved-ref">exm-tdist</span></a> we illustrated the fact that <span class="math inline">\(X_n \overset{d}{\to}N(0,1)\)</span> where <span class="math inline">\(X_n \sim t_n\)</span>, but we didn’t actually prove it. Directly proving this result is a matter of verify that <span class="math display">\[\lim_{n\to\infty} F_{X_n}(x) =\lim_{n\to\infty}\int_{-\infty}^x \frac{\Gamma\left(\frac{n+1}{2}\right)}{\Gamma\left(\frac{n}{2}\right)\sqrt{n\pi}}\left(1 + \frac{x^2}{n}\right)^{-\frac{n+1}{2}} = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^x e^{-x^2/2} = F_X(x)\]</span> where <span class="math inline">\(\Gamma\)</span> is the gamma function defined as <span class="math display">\[\Gamma(t) = \int_0^\infty s^{t-1}e^{-s}\ ds.\]</span> A much easier way to prove that <span class="math inline">\(X_n \overset{d}{\to}N(0,1)\)</span>, is by using Slutsky’s theorem and the continuous mapping theorem. First recall that <span class="math display">\[\frac{\bar X - \mu}{S/\sqrt n} \sim t_n,\]</span> so we can write <span class="math inline">\(X_n\)</span> as <span class="math inline">\(X_n = \frac{\bar X - \mu}{S/\sqrt n}\)</span> due to the fact that random variables are uniquely determined by their distributions. From <a href="#exm-consvarnorm" class="quarto-xref">Example&nbsp;<span class="quarto-unresolved-ref">exm-consvarnorm</span></a>, we have <span class="math inline">\(S^2 \overset{p}{\to}\sigma^2\)</span>. By the continuous mapping theorem <span class="math display">\[ \sqrt{S^2} = S \overset{p}{\to}\sigma = \sqrt{\sigma^2},\]</span> which gives <span class="math display">\[ X_n = \frac{\bar X - \mu}{S/\sqrt n}= \underbrace{\sqrt{n}(\bar X - \mu)}_{\overset{d}{\to}N(0, \sigma^2)}\underbrace{\frac{1}{s}}_{\overset{p}{\to}\sigma}.\]</span> Putting all the pieces together, Slutsky’s theorem yields <span class="math display">\[X_n \overset{d}{\to}\frac{N(0,\sigma^2)}{\sigma} = N(0,1).\]</span></p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The CLT can be generalized to samples of random vectors <span class="math inline">\(\mathbf{X}_i\)</span>.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.6</strong></span> Suppose <span class="math inline">\((\mathbf{X}_1,\ldots, \mathbf{X}_n)\)</span> is a sequence of iid random vectors with <span class="math inline">\(\text{E}\left[\mathbf{X}_i\right]=\boldsymbol\mu\)</span> and <span class="math inline">\(\text{Var}\left(\mathbf{X}_i\right)=\boldsymbol\Sigma\)</span>. Then <span class="math display">\[\sqrt{n}(\bar {\mathbf{X}}- \boldsymbol\mu) \overset{d}{\to}N(\mathbf{0},\boldsymbol\Sigma).\]</span></p>
</div>
</section>
<section id="not-identically-distributed" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="not-identically-distributed"><span class="header-section-number">2.5.2</span> Not Identically Distributed</h3>
<p><a href="#prp-chebylln" class="quarto-xref">Proposition&nbsp;<span class="quarto-unresolved-ref">prp-chebylln</span></a> allowed us to salvage a LLN when the iid assumption failed, so can we do the same with the CLT? Sort of. While we need an independent sample, we don’t necessarily need realizations to be drawn from an identical distribution. Instead, the theorem will rely on the tails of distributions meeting a certion criterion. This version of the CLT is known as the Lindeberg-Feller CLT.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.7 (Lindeberg-Feller CLT)</strong></span> Suppose <span class="math inline">\(\mathbf{X}=(X_1,\ldots, X_n)\)</span> is a sequence of independent random variables with <span class="math inline">\(\text{E}\left[X_i\right]=\mu_i\)</span> and <span class="math inline">\(\text{Var}\left(X_i\right)=\sigma_i^2\)</span>, and define <span class="math display">\[\begin{align*}
\bar \mu = \frac{1}{n}\sum_{i=1}^n\mu_i;\\
\bar \sigma_n^2 = \frac{1}{n}\sum_{i=1}^n\sigma_i^2.
\end{align*}\]</span> If the collection of variances <span class="math inline">\(\sigma_i^2\)</span> satisfies: <span class="math display">\[\begin{align*}
\lim_{n\to\infty} &amp;\frac{\max\{\sigma_i\}}{n\bar\sigma} = 0;\\
\lim_{n\to\infty} &amp;\bar\sigma_n^2 = \bar\sigma^2,
\end{align*}\]</span> then <span class="math inline">\(\sqrt n (\bar X-\mu)\overset{d}{\to}N(0, \bar\sigma^2).\)</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.18 (Lindeberg’s Condition)</strong></span> The Lindeberg-Feller conditions stipulates that <span class="math inline">\(\lim_{n\to\infty}\bar\sigma_n^2 = \bar\sigma^2\)</span> and <span class="math inline">\(\lim_{n\to\infty} \frac{\max\{\sigma_i\}}{n\bar\sigma} = 0\)</span>, a condition known as the <strong><em>Lindeberg’s condition</em></strong>. The condition is often presented in more general terms, but the intuition remains the same. For the CLT to hold for random variables that with different variances, we need to makes sure that no single term <span class="math inline">\(\sigma_i\)</span> dominates the standard deviation. We can think about the sample mean <span class="math inline">\(\bar X\)</span> as “mixing” many random variables <span class="math inline">\(X_i.\)</span> After mixing these random variables, we hope to have a normal distribution, but that only happens if tails of the various distributions of <span class="math inline">\(X_i\)</span> are negligible as <span class="math inline">\(n\to\infty\)</span>, giving us the trademark tails of a normal distribution which tapper off. Let’s consider a counterexample. Suppose <span class="math inline">\(X_i\)</span> is defined on the sample space <span class="math inline">\(\{-i,0,i\}\)</span> is distributed such that <span class="math display">\[\Pr(X_i = k) = \begin{cases} 1/2i^2 &amp; k = -i^2 \\ 1/2i^2 &amp; k = i^2 \\ 1 - 1/i^2&amp;k=0\end{cases}.\]</span> Graphing the density function for a few values of <span class="math inline">\(i\)</span> gives us a better sense of how <span class="math inline">\(X_i\)</span> behaves.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">expand_grid</span>(<span class="at">i =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">x =</span> <span class="sc">-</span><span class="dv">100</span><span class="sc">:</span><span class="dv">100</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(x <span class="sc">==</span> i<span class="sc">^</span><span class="dv">2</span> <span class="sc">|</span> x <span class="sc">==</span> <span class="sc">-</span>i<span class="sc">^</span><span class="dv">2</span><span class="sc">|</span> x<span class="sc">==</span> <span class="dv">0</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> <span class="fu">ifelse</span>(x <span class="sc">==</span> <span class="dv">0</span>, <span class="dv">1</span> <span class="sc">-</span> <span class="dv">1</span><span class="sc">/</span>(i<span class="sc">^</span><span class="dv">2</span>), <span class="dv">1</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>i<span class="sc">^</span><span class="dv">2</span>))) <span class="sc">%&gt;%</span> </span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"k"</span>) <span class="sc">+</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">xend =</span> x, <span class="at">y=</span> <span class="dv">0</span>, <span class="at">yend =</span> y)) <span class="sc">+</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span> </span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transition_states</span>(i) <span class="sc">+</span> </span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'i = {closest_state}'</span>, <span class="at">y =</span> <span class="st">"Pr(X_i = k)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot213" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot213-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-plot213-1.gif" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot213-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.17: Density of X_i for i = 1,…,5.
</figcaption>
</figure>
</div>
</div>
</div>
<p>As <span class="math inline">\(i\to\infty\)</span>, nearly all the probability is concentrated as <span class="math inline">\(k = 0\)</span>. The remaining probability is at the extreme tails of the distribution <span class="math inline">\(\pm i^2\)</span>, and these tails become more and more extreme (quadritically so) as <span class="math inline">\(i\to\infty\)</span>. This is the exact type of behavior Lindeberg’s condition rules out. The expectation, expectation squared, and variance of <span class="math inline">\(X_i\)</span> are: <span class="math display">\[\begin{align*}
\text{E}\left[X_i\right] &amp; = (1/2i^2)(-i^2) + (1/2i^2)(i^2) +  (1 - 1/i^2)(0) = 0\\
\text{E}\left[X_i^2\right] &amp; = (1/2i^2)(i^4) + (1/2i^2)(i^4) +  (1 - 1/i^2)(0) = i^2\\
\text{Var}\left(X_i\right) &amp; = i^2 - 0^2 = i^2
\end{align*}\]</span></p>
<p>We can verify that Lindeberg’s condition does not hold. <span class="math display">\[\begin{align*}
\lim_{n\to\infty}\bar \sigma_n = \lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^n i^2 = \lim_{n\to\infty}\frac{(n+1)(2n+1)}{6} \to\infty
\end{align*}\]</span></p>
<p>To simulate realizations of this random variable, we can define <span class="math inline">\(X_i\)</span> using <span class="math inline">\(U_i\sim\text{Uni}(0,1)\)</span>.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p><span class="math display">\[X_i = \begin{cases}-i^2 &amp; U_i\in[0, 1/2i^2)\\ i^2 &amp; U_i\in[1/2i^2, 1/i^2) \\ 0 &amp; U_i\in [1/i^2,1]\end{cases}\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>draw_X <span class="ot">&lt;-</span> <span class="cf">function</span>(n,...){</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>  U <span class="ot">&lt;-</span> <span class="fu">runif</span>(n)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>  prob_pos_i <span class="ot">&lt;-</span> (U <span class="sc">&lt;</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">:</span>n)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>  prob_neg_i <span class="ot">&lt;-</span> (U <span class="sc">&gt;</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">:</span>n)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">&amp;</span> U <span class="sc">&lt;</span> <span class="dv">1</span><span class="sc">/</span>((<span class="dv">1</span><span class="sc">:</span>n)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">1</span><span class="sc">:</span>n)<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>prob_pos_i <span class="sc">-</span> (<span class="dv">1</span><span class="sc">:</span>n)<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>prob_neg_i</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">draw_N_estimates_over_n</span>(</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">theta =</span> <span class="cf">function</span>(X){<span class="fu">sqrt</span>(<span class="fu">length</span>(X))<span class="sc">*</span>(<span class="fu">mean</span>(X) <span class="sc">-</span> <span class="dv">1</span>)},</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">N =</span> <span class="dv">200</span>,</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">n_vals =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">25</span>, <span class="dv">100</span>, <span class="dv">1000</span>),</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist =</span> draw_X,</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist_params =</span> <span class="fu">list</span>(<span class="dv">0</span>)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(estimate)) <span class="sc">+</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">color =</span> <span class="st">"black"</span>, <span class="at">fill =</span> <span class="st">"white"</span>, <span class="at">bins =</span> <span class="dv">50</span>) <span class="sc">+</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>sample_size, <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Estimates"</span>, <span class="at">y =</span> <span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot214" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot214-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-plot214-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot214-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.18: Histograms of estimates as sample size increases.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The apparent distribution of our estimates is not converging to a normal distribution, as we always have a few outliers that are too plentiful relative to their distance from the mean to be drawn from a normal distribution. As <span class="math inline">\(n\to\infty\)</span> these outliers become even more extreme. This is also evident from QQ-plots, where the points far away from the 45-degree line are drawn even farther away as <span class="math inline">\(n\to\infty\)</span>.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">sample =</span> estimate)) <span class="sc">+</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq_line</span>(<span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_qq</span>(<span class="at">size=</span> <span class="fl">0.5</span>) <span class="sc">+</span> </span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>sample_size) <span class="sc">+</span> </span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Quantiles of Simulated Estimates"</span>, <span class="at">y =</span> <span class="st">"Quantiles of Normal Distribution"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot215" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot215-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-plot215-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot215-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.19: The QQ-plot for the simulated distribution of the adjusted sample mean
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; While theoretically important, Lindeberg’s condition can be a bit hard to verify. It is much more common to appeal to a stronger assumption which gives rise to a second CLT that holds for independent, but not necessarily identically distributed, random variables. This final CLT may not be as general as the Lindeberg-Feller CLT, but it is much easier to work with.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.8 (Lyapunov CLT)</strong></span> Suppose <span class="math inline">\(\mathbf{X}=(X_1,\ldots, X_n)\)</span> is a sequence of independent random variables with <span class="math inline">\(\text{E}\left[X_i\right]=\mu_i\)</span> and <span class="math inline">\(\text{Var}\left(X_i\right)=\sigma_i^2\)</span>. If <span class="math inline">\(\text{E}\left[\left\lvert X_i - \mu_i\right\rvert^{2+\kappa}\right]\)</span> is finite for some <span class="math inline">\(\kappa &gt; 0,\)</span> then <span class="math inline">\(\sqrt n (\bar X-\mu)\overset{d}{\to}N(0, \bar\sigma^2)\)</span>, where <span class="math inline">\(\bar\sigma = (1/n)\sum_{i=1}^n \sigma_i.\)</span></p>
</div>
</section>
</section>
<section id="delta-method" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="delta-method"><span class="header-section-number">2.6</span> Delta Method</h2>
<p>Slutsky’s theorem and the continuous mapping theorem in tandem with the LLN give us the ability to prove that certain functions of sample means are convergent. Is it possible that we can do something similar with the CLT to find the asymptotic distribution of functions of sample means?</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Suppose <span class="math inline">\(g\)</span> is a function of <span class="math inline">\(\bar X\)</span>, where a CLT applies to the random sample <span class="math inline">\(\mathbf{X}\)</span>. We know <span class="math inline">\(\sqrt n(\bar X-\mu)\overset{a}{\sim}N(0, \sigma^2)\)</span>. Is it possible to conclude that <span class="math inline">\(\sqrt n(g(\bar X)-g(\mu))\overset{a}{\sim}N(0, \tilde\sigma^2)\)</span> for some <span class="math inline">\(\tilde\sigma^2\)</span>? Furthermore, can we determine <span class="math inline">\(\tilde\mu\)</span> and <span class="math inline">\(\tilde\sigma^2\)</span> only knowing <span class="math inline">\(g\)</span>, <span class="math inline">\(\mu\)</span>, and <span class="math inline">\(\sigma^2\)</span>?</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; We have information about the difference <span class="math inline">\(\bar X-\mu\)</span> and want information about the difference <span class="math inline">\(g(\bar X)-g(\mu)\)</span>. Situations where we know something about behavior in the domain of a function and want to relate it to the function’s behavior in the codomain are common place in math, but in particular in real analysis. This is where the mean value theorem saves the day. Assuming <span class="math inline">\(g\)</span> is continuously differentiable and fixing <span class="math inline">\(n\)</span>, there exists some <span class="math inline">\(T_n\)</span> in between <span class="math inline">\(\bar X\)</span> and <span class="math inline">\(\theta\)</span> (<span class="math inline">\(\bar X&lt; T_n &lt; \mu\)</span> or <span class="math inline">\(\mu &lt; T_n &lt; \bar X\)</span>) such that: <span class="math display">\[\begin{align*}
&amp; \frac{g(\bar X)-g(\mu)}{\bar X - \mu} = g'(T_n)\\
\implies &amp; g(\bar X) = g(\mu) + g'(T_n)(\bar X-\mu)\\
\implies &amp; \sqrt{n}[g(\bar X) - g(\mu)] = g'(T_n)\sqrt{n} (\bar X-\mu)
\end{align*}\]</span> If we let <span class="math inline">\(n\)</span> vary, we have a sequence of random variables <span class="math inline">\(\{T_n\}\)</span> such that <span class="math inline">\(\left\lvert T_n -\mu\right\rvert &lt; \left\lvert\bar X - \mu\right\rvert\)</span>. By the LLN <span class="math inline">\(\left\lvert\bar X - \mu\right\rvert \overset{p}{\to}0\)</span>, so <span class="math inline">\(\left\lvert T_n -\mu\right\rvert \overset{p}{\to}0\)</span>, which is equivalent to <span class="math inline">\(T_n \overset{p}{\to}\mu\)</span>. By the continuous mapping theorem, <span class="math inline">\(g(T_n) \overset{p}{\to}g(\mu)\)</span>. This means <span class="math display">\[\sqrt{n}[g(\bar X) - g(\mu)] = \underbrace{g'(T_n)}_{\overset{p}{\to}g(\mu)}\cdot\underbrace{\sqrt{n} (\bar X-\mu)}_{\overset{d}{\to}N(0, \sigma^2)},\]</span> so Slutsky’s theorem gives <span class="math display">\[\sqrt{n}[g(\bar X) - g(\mu)] \overset{d}{\to}g(\mu)\cdot N(0,\sigma^2) = N(0, \sigma^2[g(\mu)]^2).\]</span> This all is contingent on <span class="math inline">\(g\)</span> not being a function of <span class="math inline">\(n\)</span>, otherwise things fall apart when we apply limiting processes.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This result is known as the delta method, and applies to any sequence of random variables that is asymptotically normal. It is also readily generalized to higher dimensions where <span class="math inline">\(\mathbf g\)</span> is a vector valued function.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.9 (Delta Method)</strong></span> Suppose <span class="math inline">\((\mathbf{X}_1,\ldots, \mathbf{X}_n)\)</span> is a sequence of random vectors such that <span class="math inline">\(\sqrt n (\mathbf{X}_n - \mathbf t) \overset{d}{\to}N(\mathbf 0, \boldsymbol\Sigma)\)</span> for some vector <span class="math inline">\(\mathbf t\)</span> in the interior of <span class="math inline">\(\mathcal X\)</span>. If <span class="math inline">\(\mathbf g(\mathbf{X}_n)\)</span> is a vector valued function that:</p>
<ol type="1">
<li>is continuously differentiable,</li>
<li>does not involve <span class="math inline">\(n\)</span>,</li>
<li><span class="math inline">\(\frac{\partial \mathbf g}{\partial\mathbf x}(\mathbf t) \neq 0\)</span>,<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></li>
</ol>
<p>then,</p>
<p><span class="math display">\[ \sqrt n \left[\mathbf g(\mathbf{X}_n) - \mathbf g(\mathbf t)\right] \overset{d}{\to}N\left(\mathbf 0, \left[\frac{\partial \mathbf g}{\partial\mathbf x}(\mathbf t)\right]\boldsymbol\Sigma\left[\frac{\partial \mathbf g}{\partial\mathbf x}(\mathbf t)\right]'\right)\]</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.19</strong></span> Return to the example where <span class="math inline">\(X_i\overset{iid}{\sim}\text{Exp}(1)\)</span>, giving <span class="math inline">\(\text{E}\left[X_i\right] = 1\)</span> and <span class="math inline">\(\text{Var}\left(X_i\right) = 1\)</span>. By the CLT, <span class="math inline">\(\sqrt n(\bar X - 1)\overset{d}{\to}N(0,1)\)</span>. If <span class="math inline">\(g(t) = t^2 +3\)</span>, what is the asymptotic distribution of <span class="math inline">\(\sqrt{n}[g(\bar X) - g(1)]\)</span>? According to the delta method we have <span class="math display">\[\begin{align*}
&amp;\sqrt{n}[g(\bar X) - g(1)]  \overset{a}{\sim}N(0, 1[g'(1)]^2),\\
\implies &amp; \sqrt{n}\left[\bar X^2 - 1\right] \overset{a}{\sim}N(0, 4).
\end{align*}\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> <span class="cf">function</span>(t){</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>  t<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">3</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">draw_N_estimates</span>(</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">theta =</span> <span class="cf">function</span>(X){<span class="fu">sqrt</span>(<span class="fu">length</span>(X))<span class="sc">*</span>(<span class="fu">g</span>(<span class="fu">mean</span>(X)) <span class="sc">-</span> <span class="fu">g</span>(<span class="dv">1</span>))},</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">N =</span> <span class="fl">1e4</span>,</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="fl">1e5</span>,</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist =</span> rexp,</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist_params =</span> <span class="fu">list</span>(</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">rate =</span> <span class="dv">1</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can plot a histogram of our estimates and overlay the distribution <span class="math inline">\(N(0,4)\)</span>.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(estimate)) <span class="sc">+</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> <span class="fu">after_stat</span>(density)), <span class="at">color =</span> <span class="dv">1</span>, <span class="at">fill =</span> <span class="st">"white"</span>, <span class="at">bins =</span> <span class="dv">100</span>) <span class="sc">+</span> </span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"Estimates of √n(g(X) - g(1)) "</span>) <span class="sc">+</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> dnorm, <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">2</span>), <span class="at">color =</span> <span class="st">"red"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot216" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot216-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="asymptotics_files/figure-html/fig-plot216-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot216-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.20: Histogram of adjusted sample mean transformed by g, and theoretical distribution given by the delta method
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The big takeaway from the delta method is that estimators which are nice functions of sample means will be asymptotically distributed according to a normal distribution. Furthermore, any nice function of such an estimator will also have a normal asymptotic distribution!</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-plot217" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot217-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/meme.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="458">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot217-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.21: A mediocre meme
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="little-o_p-big-o_p-and-taylor-expansions" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="little-o_p-big-o_p-and-taylor-expansions"><span class="header-section-number">2.7</span> Little <span class="math inline">\(o_p\)</span>, Big <span class="math inline">\(O_p\)</span>, and Taylor Expansions</h2>
<p>We’ve talked a lot about whether or not random variables converge, and how they converge, but not the rate at which they converge. We can introduce some notation that allows us to quantify this rate.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.7</strong></span> Given a sequence of random variables <span class="math inline">\(X_n\)</span>, we say <span class="math inline">\(X_n\)</span> is <span style="color:red"><strong><em>little “O.P” of</em></strong> <span class="math inline">\(n^k\)</span></span>, denoted <span class="math inline">\(X_n = o_p(n^k)\)</span>, if <span class="math inline">\(X_n / n^k\overset{p}{\to}0\)</span>.</p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Note that <span class="math inline">\(X_n\overset{p}{\to}0\)</span> implies that <span class="math inline">\(X_n = o_p(1)\)</span>. The use of “=” is a bit misleading in this definition, as <span class="math inline">\(X_n = o_p(n^k)\)</span> does not establish any equality, instead referring to how <span class="math inline">\(X_n\)</span> behaves asymptotically. For instance, if we have two sequences of random variables <span class="math inline">\(X_n\)</span> and <span class="math inline">\(Y_n\)</span> such that <span class="math inline">\(X_n\overset{p}{\to}X\)</span> and <span class="math inline">\(Y_n\overset{p}{\to}0\)</span>, we have $ X_n + Y_n X + 0 $, but could write <span class="math inline">\(X_n + Y_n\)</span> as <span class="math inline">\(X_n + o_p(1)\)</span>. This emphasizes the sequence <span class="math inline">\(X_n\)</span>, and frames <span class="math inline">\(Y_n\)</span> as some negligible remainder term that tends to zero.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.8</strong></span> Given a sequence of random variables <span class="math inline">\(X_n\)</span>, we say <span class="math inline">\(X_n\)</span> is <span style="color:red"><strong><em>big “O.P” of</em></strong> <span class="math inline">\(n^k\)</span></span>, denoted <span class="math inline">\(X_n = O_p(n^k)\)</span>, if for all <span class="math inline">\(\varepsilon &gt; 0\)</span>, there exists some <span class="math inline">\(\delta\)</span> and <span class="math inline">\(N\)</span> such that <span class="math inline">\(\Pr(|X_n/n^k| \ge \delta) &lt;\varepsilon\)</span> for all <span class="math inline">\(n &gt; N\)</span>. In other words, <span class="math inline">\(X_n/n^k\)</span> is <span style="color:red"><strong><em>bounded in probability</em></strong></span>.</p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; We are most interested in the case where <span class="math inline">\(X_n = O_p(1)\)</span>. If this is the case, then as <span class="math inline">\(n\to\infty\)</span>, we can bound the area in the tails of <span class="math inline">\(f_{X_n}\)</span> by some constant <span class="math inline">\(\delta\)</span> such that the area is negligible (less than <span class="math inline">\(\varepsilon\)</span>).</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.20</strong></span> We know that <span class="math inline">\(\bar X \overset{d}{\to}N(\mu, \sigma^2/n)\)</span> when <span class="math inline">\(\mathbf{X}\)</span> is an iid sample. We have that <span class="math inline">\(\bar X = O_p(1)\)</span>. We have <span class="math display">\[\Pr(|\bar X/1| \ge \delta) = \Pr(-\bar X\ge -\delta \text{ and }\delta \le \bar X) = 2\cdot\Pr(\bar X\ge \delta) = 2\left[1 - \Phi\left(\frac{\delta - \mu}{\sigma/\sqrt n}\right)\right].\]</span> If we take the limit of this as <span class="math inline">\(n\to \infty\)</span> we have <span class="math display">\[ \lim_{n\to \infty}\Pr(|X\bar X/1| \ge \delta) = \lim_{n\to \infty}2\left[1 - \Phi\left(\frac{\delta - \mu}{\sigma/\sqrt n}\right)\right] = 0.\]</span> By the definition of a limit, there must exists some <span class="math inline">\(N\)</span> such that <span class="math inline">\(\Pr(|\bar X/1| \ge \delta) &lt; \varepsilon\)</span> for any <span class="math inline">\(n &gt; N\)</span>, so <span class="math inline">\(\bar X = O_p(1)\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.21</strong></span> If <span class="math inline">\(X_n \overset{d}{\to}X\)</span>, then <span class="math inline">\(X_n = O_p(1)\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.22</strong></span> If <span class="math inline">\(X_n = o_p(1)\)</span>, then <span class="math inline">\(X_n = O_p(1)\)</span>.</p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A common place to encounter <span class="math inline">\(o_p\)</span> is when performing Taylor expansions.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.23 (Taylor’s Theorem)</strong></span> Taylor’s theorem, as given in <span class="citation" data-cites="rudin1976principles">Rudin (<a href="#ref-rudin1976principles" role="doc-biblioref">1976</a>)</span>, tells us that if <span class="math inline">\(f:\mathbb R\to\mathbb R\)</span> is <span class="math inline">\(k-\)</span>times differentiable at a point <span class="math inline">\(a\in \mathbb R\)</span>, then there is some element <span class="math inline">\(c\in (a,b)\)</span> such that <span class="math display">\[\begin{align*}
f(b) &amp; = \sum_{j=0}^{k-1}\frac{f^{(j)}(a)}{k!}(b-a)^j + \frac{f^{(n)}(c)}{k!}(b-a)^k.
\end{align*}\]</span> For <span class="math inline">\(n = 2\)</span>, we have the mean value theorem: <span class="math display">\[ f(b)= f(a) + f'(c)(b-a).\]</span> If we let <span class="math inline">\(a\to b\)</span>, then <span class="math display">\[\begin{align*}
&amp;\lim_{a\to b}f(b)  = \sum_{j=0}^{k-1}\lim_{a\to b}\frac{f^{(j)}(a)}{j!}(b-a)^j + \lim_{a\to b}\frac{f^{(k)}(c)}{k!}(b-a)^k.\\
\implies &amp; f(b)  =\lim_{a\to b} f(a) + \sum_{j=1}^{k-1}\lim_{a\to b}\frac{f^{(j)}(a)}{j!}(b-a)^j +  \lim_{a\to b}\frac{f^{(k)}(c)}{k!}(b-a)^k\\
\implies &amp; f(b)  = f(b) + \underbrace{\sum_{j=1}^{k-1}\frac{f^{(j)}(b)}{j!}(b-b)^j}_0 + \lim_{a\to b}\frac{f^{(k)}(c)}{k!}(b-a)^k\\
\implies &amp; \lim_{a\to b}\frac{f^{(k)}(c)}{n!}(b-a)^k = 0\\
\implies &amp; \lim_{a\to b}\frac{f^{(k)}(c)}{n!(b-a)^k} = 0\\
\implies &amp; \frac{f^{(k)}(c)}{k!} = o(|a-b|^k)\\
\end{align*}\]</span> Here, <span class="math inline">\(o\)</span> is the deterministic counterpart of <span class="math inline">\(o_p\)</span> (we’re not working with random variables just yet). This means we can write Taylor’s theorem as <span class="math display">\[ f(b) = \sum_{j=0}^{k-1}\frac{f^{(j)}(a)}{j!}(b-a)^k +o(|a-b|^k).\]</span> In the event <span class="math inline">\(f\)</span> is infinitely differentiable we can make this approximation arbitrarily accurate, giving rise toa function’s Taylor series.</p>
<p>Now suppose <span class="math inline">\(f_n(X)\)</span> is a sequence of functions of random variables, where the subscript <span class="math inline">\(n\)</span> emphasizes that <span class="math inline">\(f_n\)</span> is a random variable. IF we apply Taylor’s theorem to <span class="math inline">\(f_n(X)\)</span> we have <span class="math display">\[f_n(b) = \sum_{j=0}^{k-1}\frac{f_n^{(j)}(a)}{j!}(b-a)^k +o_p(|a-b|^k)\]</span> for realizations of the random variable <span class="math inline">\(a,b,c\in\mathcal X\)</span> where <span class="math inline">\(c\in(a,b)\)</span>. Assuming <span class="math inline">\(k \ge 1\)</span>, then <span class="math inline">\(o_p(|a-b|^k)\)</span> implies <span class="math inline">\(o_p(1)\)</span>, so <span class="math display">\[f_n(b) = \sum_{j=0}^{k-1}\frac{f_n^{(j)}(a)}{j!}(b-a)^k +o_p(1).\]</span></p>
</div>
</section>
<section id="asymptotically-normal-estimators" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="asymptotically-normal-estimators"><span class="header-section-number">2.8</span> Asymptotically Normal Estimators</h2>
<p>When putting our asymptotic tools to work on an estimator of interest, we will almost always find that it converges to a normal distribution, is consistent, and that the rate of convergence is linked to <span class="math inline">\(\sqrt{n}\)</span>.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.9</strong></span> An estimator <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> is <span style="color:red"><span class="math inline">\(\sqrt{n}-\)</span>consistent asymptotically normal (root-n CAN)</span>, if <span class="math display">\[\sqrt{n}(\hat{\boldsymbol{\theta}}- \boldsymbol{\theta}) \overset{d}{\to}N(\mathbf 0, \mathbf V)\]</span> for a PSD matrix <span class="math inline">\(\mathbf V\)</span>. Equivalently, <span class="math display">\[ \hat{\boldsymbol{\theta}}\overset{a}{\sim}N(\boldsymbol{\theta}, \mathbf V/n).\]</span> We refer to <span class="math inline">\(\mathbf V/n\)</span> as the <span style="color:red"><strong><em>asymptotic variance</em></strong></span> of <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> and write <span class="math inline">\(\text{Avar}\left(\hat{\boldsymbol{\theta}}\right) = \mathbf V /n\)</span>.</p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “<span class="math inline">\(\sqrt n-\)</span>” emphasizes the fact that <span class="math inline">\(\sqrt{n}(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}) = O_p(1)\)</span>, which is equivalent to <span class="math inline">\(\hat{\boldsymbol{\theta}} = \boldsymbol{\theta}+ O_p(n^{-1/2})\)</span>. In other words, as <span class="math inline">\(n\to\infty\)</span> the error term associated with our estimate decreases at a rate of <span class="math inline">\(n^{1/2}\)</span>. A fourfold increase in observations results in half the error. We also have that <span class="math inline">\(\sqrt{n}(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}) = o_p(1)\)</span>, so <span class="math inline">\(\hat{\boldsymbol{\theta}}\overset{p}{\to}\boldsymbol{\theta}\)</span>, hence the “consistent” in the previous definition. We also have that <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> is asymptotically unbiased if it is root-n CAN, as <span class="math inline">\(\text{E}\left[\hat{\boldsymbol{\theta}}\right]\to \boldsymbol{\theta}\)</span>. This will be the one of, if not the, <strong><em>most important property</em></strong> an estimator can posses.</p>
</section>
<section id="further-reading" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">2.9</span> Further Reading</h2>
<ul>
<li>Eric Zivot’s <a href="http://faculty.washington.edu/ezivot/econ583/econ583asymptoticsprimer.pdf">primer on asymptotics</a></li>
<li><span class="citation" data-cites="wooldridge2010econometric">Wooldridge (<a href="#ref-wooldridge2010econometric" role="doc-biblioref">2010</a>)</span>, Chapter 3</li>
<li><span class="citation" data-cites="greene2003econometric">Greene (<a href="#ref-greene2003econometric" role="doc-biblioref">2018</a>)</span>, Appendix D</li>
<li><span class="citation" data-cites="van2000asymptotic">Van der Vaart (<a href="#ref-van2000asymptotic" role="doc-biblioref">2000</a>)</span></li>
<li><span class="citation" data-cites="white2014asymptotic">White (<a href="#ref-white2014asymptotic" role="doc-biblioref">1984</a>)</span></li>
</ul>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-billingsley2008probability" class="csl-entry" role="listitem">
Billingsley, Patrick. 2008. <em>Probability and Measure</em>. John Wiley &amp; Sons.
</div>
<div id="ref-durrett2019probability" class="csl-entry" role="listitem">
Durrett, Rick. 2019. <em>Probability: Theory and Examples</em>. Vol. 49. Cambridge university press.
</div>
<div id="ref-greene2003econometric" class="csl-entry" role="listitem">
Greene, William H. 2018. <em>Econometric Analysis</em>. 8th ed. Pearson Education.
</div>
<div id="ref-lehmann1999elements" class="csl-entry" role="listitem">
Lehmann, Erich Leo. 1999. <em>Elements of Large-Sample Theory</em>. Springer.
</div>
<div id="ref-newey1994large" class="csl-entry" role="listitem">
Newey, Whitney K, and Daniel McFadden. 1994. <span>“Large Sample Estimation and Hypothesis Testing.”</span> <em>Handbook of Econometrics</em> 4: 2111–2245.
</div>
<div id="ref-rudin1976principles" class="csl-entry" role="listitem">
Rudin, Walter. 1976. <em>Principles of Mathematical Analysis</em>. Vol. 3. McGraw-hill New York.
</div>
<div id="ref-van2000asymptotic" class="csl-entry" role="listitem">
Van der Vaart, Aad W. 2000. <em>Asymptotic Statistics</em>. Vol. 3. Cambridge university press.
</div>
<div id="ref-white2014asymptotic" class="csl-entry" role="listitem">
White, Halbert. 1984. <em>Asymptotic Theory for Econometricians</em>. Academic press.
</div>
<div id="ref-wooldridge2010econometric" class="csl-entry" role="listitem">
Wooldridge, Jeffrey M. 2010. <em>Econometric Analysis of Cross Section and Panel Data</em>. MIT press.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Convergence in distribution is also sometimes called convergence in law among mathematicians. The notation for this form of convergence is also as varied as it’s name. Some other ways of writing <span class="math inline">\(X_n\overset{d}{\to} X\)</span> are: <span class="math inline">\(X_n\rightsquigarrow X\)</span>, <span class="math inline">\(X_n\Rightarrow X\)</span>, <span class="math inline">\(X_n \overset{\mathcal L}\to X\)</span>, <span class="math inline">\(\mathcal L(X_n) \to \mathcal L(X)\)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The probability space on which the sequence of random variables is defined need not be common, but assume as much for the sake of this digression.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>where <span class="math inline">\(I\subseteq \mathcal B(\mathbb R)\)</span><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>We can actually draw observations of <em>any</em> random variable using the uniform distribution on <span class="math inline">\([0,1]\)</span> via <a href="https://en.wikipedia.org/wiki/Inverse_transform_sampling#Reduction_of_the_number_of_inversions"><strong><em>inverse transform sampling</em></strong></a>. This is one of the primary ways computers generate random observations from a given distribution.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><span class="math inline">\(\frac{\partial \mathbf g}{\partial\mathbf x}(\mathbf t)\)</span> is the <span class="math inline">\(\dim(\mathbf g(\mathbf{X}_n)) \times \dim(\mathbf X_n)\)</span> Jacobian matrix comprised of the partial derivatives of the components of <span class="math inline">\(\mathbf g\)</span>.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./estimators.html" class="pagination-link" aria-label="Finite Sample Properties of Estimators">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Finite Sample Properties of Estimators</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./testing.html" class="pagination-link" aria-label="Hypothesis Testing">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb28" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="an">editor:</span><span class="co"> </span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co">  markdown: </span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co">    wrap: 72</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\plim}{plim}</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\argmin}{argmin}</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\argmax}{argmax}</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>\newcommand{\var}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Var}\left(#1\right)}</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>\newcommand{\avar}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Avar}\left(#1\right)}</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>\newcommand{\E}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{E}\left<span class="co">[</span><span class="ot">#1\right</span><span class="co">]</span>}</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>\newcommand{\cov}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Cov}\left(#1\right)}</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>\newcommand{\mse}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{MSE}\left(#1\right)}</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>\newcommand{\se}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{se}\left(#1\right)}</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>\newcommand{\limfunc}{lim} </span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>\newcommand{\X}{\mathbf{X}}</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>\newcommand{\Xm}{\mathbb{X}}</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>\newcommand{\EER}{\bar{\thet}_\text{EE}}</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>\newcommand{\NLS}{\hat{\bet}_\text{NLLS}}</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>\newcommand{\z}{\mathbf{z}}</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>\newcommand{\rr}{\mathbf{r}}</span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>\newcommand{\C}{\mathbf{C}}</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>\newcommand{\Pe}{\mathbf{P}}</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>\newcommand{\y}{\mathbf{y}}</span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>\newcommand{\Y}{\mathbf{Y}}</span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>\newcommand{\uu}{\mathbf{u}}</span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>\newcommand{\e}{\mathbf{e}}</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>\newcommand{\D}{\mathbf{D}}</span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>\newcommand{\x}{\mathbf{x}}</span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a>\newcommand{\xm}{\mathbb{x}}</span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a>\newcommand{\Zm}{\mathbb{Z}}</span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>\newcommand{\Wm}{\mathbb{W}}</span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a>\newcommand{\Hm}{\mathbb{H}}</span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a>\newcommand{\W}{\mathbf{W}}</span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a>\newcommand{\Z}{\mathbf{Z}}</span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a>\newcommand{\Hess}{\mathbf{H}(\mathbf{\Z\mid\thet})}</span>
<span id="cb28-38"><a href="#cb28-38" aria-hidden="true" tabindex="-1"></a>\newcommand{\Score}{\mathbf{S}(\mathbf{\Z\mid\thet})}</span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a>\newcommand{\A}{\mathbf{A}}</span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a>\newcommand{\h}{\mathbf{h}}</span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a>\newcommand{\Q}{\mathbf{Q}}</span>
<span id="cb28-42"><a href="#cb28-42" aria-hidden="true" tabindex="-1"></a>\newcommand{\F}{\mathbf{F}}</span>
<span id="cb28-43"><a href="#cb28-43" aria-hidden="true" tabindex="-1"></a>\newcommand{\G}{\mathbf{G}}</span>
<span id="cb28-44"><a href="#cb28-44" aria-hidden="true" tabindex="-1"></a>\newcommand{\I}{\mathbf{I}}</span>
<span id="cb28-45"><a href="#cb28-45" aria-hidden="true" tabindex="-1"></a>\renewcommand{\D}{\mathbf{D}}</span>
<span id="cb28-46"><a href="#cb28-46" aria-hidden="true" tabindex="-1"></a>\renewcommand{\C}{\mathbf{C}}</span>
<span id="cb28-47"><a href="#cb28-47" aria-hidden="true" tabindex="-1"></a>\newcommand{\zer}{\mathbf{0}}</span>
<span id="cb28-48"><a href="#cb28-48" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLS}{\hat{\boldsymbol\beta}_\text{OLS} }</span>
<span id="cb28-49"><a href="#cb28-49" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLSOV}{\hat{\boldsymbol\beta}_\text{OLS,OV} }</span>
<span id="cb28-50"><a href="#cb28-50" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLSME}{\hat{\boldsymbol\beta}_\text{OLS,ME} }</span>
<span id="cb28-51"><a href="#cb28-51" aria-hidden="true" tabindex="-1"></a>\newcommand{\EE}{\hat{\boldsymbol\theta}_\text{EX} }</span>
<span id="cb28-52"><a href="#cb28-52" aria-hidden="true" tabindex="-1"></a>\newcommand{\ME}{\hat{\boldsymbol\theta}_\text{M} }</span>
<span id="cb28-53"><a href="#cb28-53" aria-hidden="true" tabindex="-1"></a>\newcommand{\MDE}{\hat{\boldsymbol\theta}_\text{MDE} }</span>
<span id="cb28-54"><a href="#cb28-54" aria-hidden="true" tabindex="-1"></a>\newcommand{\IV}{\hat{\boldsymbol\beta}_\text{IV} }</span>
<span id="cb28-55"><a href="#cb28-55" aria-hidden="true" tabindex="-1"></a>\newcommand{\TSLS}{\hat{\boldsymbol\beta}_\text{2SLS} }</span>
<span id="cb28-56"><a href="#cb28-56" aria-hidden="true" tabindex="-1"></a>\newcommand{\thet}{\boldsymbol{\theta}}</span>
<span id="cb28-57"><a href="#cb28-57" aria-hidden="true" tabindex="-1"></a>\newcommand{\et}{\boldsymbol{\eta}}</span>
<span id="cb28-58"><a href="#cb28-58" aria-hidden="true" tabindex="-1"></a>\newcommand{\R}{\mathbb{R}}</span>
<span id="cb28-59"><a href="#cb28-59" aria-hidden="true" tabindex="-1"></a>\newcommand{\Sig}{\boldsymbol{\Sigma}}</span>
<span id="cb28-60"><a href="#cb28-60" aria-hidden="true" tabindex="-1"></a>\newcommand{\ep}{\boldsymbol{\varepsilon}}</span>
<span id="cb28-61"><a href="#cb28-61" aria-hidden="true" tabindex="-1"></a>\newcommand{\Omeg}{\boldsymbol{\Omega}}</span>
<span id="cb28-62"><a href="#cb28-62" aria-hidden="true" tabindex="-1"></a>\newcommand{\Thet}{\boldsymbol{\Theta}}</span>
<span id="cb28-63"><a href="#cb28-63" aria-hidden="true" tabindex="-1"></a>\newcommand{\bet}{\boldsymbol{\beta}}</span>
<span id="cb28-64"><a href="#cb28-64" aria-hidden="true" tabindex="-1"></a>\newcommand{\rk}{rank}</span>
<span id="cb28-65"><a href="#cb28-65" aria-hidden="true" tabindex="-1"></a>\newcommand{\tsum}{\sum}</span>
<span id="cb28-66"><a href="#cb28-66" aria-hidden="true" tabindex="-1"></a>\newcommand{\tr}{tr}</span>
<span id="cb28-67"><a href="#cb28-67" aria-hidden="true" tabindex="-1"></a>\newcommand{\norm}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\left\lVert#1\right\rVert}</span>
<span id="cb28-68"><a href="#cb28-68" aria-hidden="true" tabindex="-1"></a>\newcommand{\abs}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\left\lvert#1\right\rvert}</span>
<span id="cb28-69"><a href="#cb28-69" aria-hidden="true" tabindex="-1"></a>\newcommand{\ms}{\overset{ms}{\to}}</span>
<span id="cb28-70"><a href="#cb28-70" aria-hidden="true" tabindex="-1"></a>\newcommand{\pto}{\overset{p}{\to}}</span>
<span id="cb28-71"><a href="#cb28-71" aria-hidden="true" tabindex="-1"></a>\newcommand{\asto}{\overset{as}{\to}}</span>
<span id="cb28-72"><a href="#cb28-72" aria-hidden="true" tabindex="-1"></a>\newcommand{\iid}{\overset{iid}{\sim}}</span>
<span id="cb28-73"><a href="#cb28-73" aria-hidden="true" tabindex="-1"></a>\newcommand{\dto}{\overset{d}{\to}}</span>
<span id="cb28-74"><a href="#cb28-74" aria-hidden="true" tabindex="-1"></a>\newcommand{\asim}{\overset{a}{\sim}}</span>
<span id="cb28-75"><a href="#cb28-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-76"><a href="#cb28-76" aria-hidden="true" tabindex="-1"></a><span class="fu"># Asymptotic Properties of Estimators {#sec-asy}</span></span>
<span id="cb28-77"><a href="#cb28-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-80"><a href="#cb28-80" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-81"><a href="#cb28-81" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb28-82"><a href="#cb28-82" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: false</span></span>
<span id="cb28-83"><a href="#cb28-83" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb28-84"><a href="#cb28-84" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gganimate)</span>
<span id="cb28-85"><a href="#cb28-85" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-86"><a href="#cb28-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-87"><a href="#cb28-87" aria-hidden="true" tabindex="-1"></a>When considering estimators in @sec-est, we kept the sample size</span>
<span id="cb28-88"><a href="#cb28-88" aria-hidden="true" tabindex="-1"></a>$n$ fixed when assessing estimators. We now consider how estimators</span>
<span id="cb28-89"><a href="#cb28-89" aria-hidden="true" tabindex="-1"></a>behave as $n\to\infty.$ In practice, we will never have infinite data,</span>
<span id="cb28-90"><a href="#cb28-90" aria-hidden="true" tabindex="-1"></a>asymptotics gives us an approximate idea of how estimators perform for</span>
<span id="cb28-91"><a href="#cb28-91" aria-hidden="true" tabindex="-1"></a>large data sets. A comprehensive reference in asymptotic theory is due</span>
<span id="cb28-92"><a href="#cb28-92" aria-hidden="true" tabindex="-1"></a>to @van2000asymptotic. For a treatment concerned purely with</span>
<span id="cb28-93"><a href="#cb28-93" aria-hidden="true" tabindex="-1"></a>econometrics, @newey1994large provide a phenomenal survey, most of which</span>
<span id="cb28-94"><a href="#cb28-94" aria-hidden="true" tabindex="-1"></a>we will touch on when discussing general classes of estimators.\</span>
<span id="cb28-95"><a href="#cb28-95" aria-hidden="true" tabindex="-1"></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;With loss of some generality, we will assume that all random</span>
<span id="cb28-96"><a href="#cb28-96" aria-hidden="true" tabindex="-1"></a>variables have finite expectation and variances. Dispensing with this</span>
<span id="cb28-97"><a href="#cb28-97" aria-hidden="true" tabindex="-1"></a>assumption is something for a probability course.</span>
<span id="cb28-98"><a href="#cb28-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-99"><a href="#cb28-99" aria-hidden="true" tabindex="-1"></a><span class="fu">## Convergence</span></span>
<span id="cb28-100"><a href="#cb28-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-101"><a href="#cb28-101" aria-hidden="true" tabindex="-1"></a>At some point in high school, most students encounter the concept of a</span>
<span id="cb28-102"><a href="#cb28-102" aria-hidden="true" tabindex="-1"></a>numeric sequence, and how they can converge to a limit. Later on,</span>
<span id="cb28-103"><a href="#cb28-103" aria-hidden="true" tabindex="-1"></a>perhaps when taking a real analysis course, sequences are generalized to</span>
<span id="cb28-104"><a href="#cb28-104" aria-hidden="true" tabindex="-1"></a>spaces of functions. A sequence of functions may also converge to a</span>
<span id="cb28-105"><a href="#cb28-105" aria-hidden="true" tabindex="-1"></a>limit, whether that be converging pointwise and/or converging uniformly</span>
<span id="cb28-106"><a href="#cb28-106" aria-hidden="true" tabindex="-1"></a>(for details see @rudin1976principles). Random variables are functions</span>
<span id="cb28-107"><a href="#cb28-107" aria-hidden="true" tabindex="-1"></a>from a sample space to $\mathbb R$, so we can consider how these</span>
<span id="cb28-108"><a href="#cb28-108" aria-hidden="true" tabindex="-1"></a>functions converge. </span>
<span id="cb28-109"><a href="#cb28-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-110"><a href="#cb28-110" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convergence in MSE</span></span>
<span id="cb28-111"><a href="#cb28-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-112"><a href="#cb28-112" aria-hidden="true" tabindex="-1"></a>The first type of convergence we'll work with deals with MSE.</span>
<span id="cb28-113"><a href="#cb28-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-114"><a href="#cb28-114" aria-hidden="true" tabindex="-1"></a>::: {#def-}</span>
<span id="cb28-115"><a href="#cb28-115" aria-hidden="true" tabindex="-1"></a>A sequence of random variables $X_n$ [***converges in mean</span>
<span id="cb28-116"><a href="#cb28-116" aria-hidden="true" tabindex="-1"></a>square***]{style="color:red"} to a random variable $X$, written as</span>
<span id="cb28-117"><a href="#cb28-117" aria-hidden="true" tabindex="-1"></a>$X_n\overset{ms}{\to} X$, if</span>
<span id="cb28-118"><a href="#cb28-118" aria-hidden="true" tabindex="-1"></a>$$\lim_{n\to\infty} \text{E}\left<span class="co">[</span><span class="ot">(X_n - X)^2\right</span><span class="co">]</span> = 0.$$ A sequence</span>
<span id="cb28-119"><a href="#cb28-119" aria-hidden="true" tabindex="-1"></a>of random vectors $\mathbf{X}_n = (X_{1,n},\ldots X_{k,n})$ converges in</span>
<span id="cb28-120"><a href="#cb28-120" aria-hidden="true" tabindex="-1"></a>mean square to $\mathbf{X}$ if $X_{i,n}\overset{ms}{\to} X_i$ for</span>
<span id="cb28-121"><a href="#cb28-121" aria-hidden="true" tabindex="-1"></a>$i=1,\ldots, k$.</span>
<span id="cb28-122"><a href="#cb28-122" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-123"><a href="#cb28-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-124"><a href="#cb28-124" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$X_n \overset{ms}{\to}X$ if the average distance between $X_n$ and</span></span>
<span id="cb28-125"><a href="#cb28-125" aria-hidden="true" tabindex="-1"></a>$X$ shrinks as $n\to\infty$ where distance is measured as $(X_n - X)^2$.</span>
<span id="cb28-126"><a href="#cb28-126" aria-hidden="true" tabindex="-1"></a>We can also have $X_n \overset{ms}{\to}c$ for some constant $c$, as $c$</span>
<span id="cb28-127"><a href="#cb28-127" aria-hidden="true" tabindex="-1"></a>is a trivial random variable.</span>
<span id="cb28-128"><a href="#cb28-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-129"><a href="#cb28-129" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb28-130"><a href="#cb28-130" aria-hidden="true" tabindex="-1"></a>Suppose we draw a sample of $n$ iid random variables $Z_i$ and define</span>
<span id="cb28-131"><a href="#cb28-131" aria-hidden="true" tabindex="-1"></a>$X_n$ to be the sample mean of our observations.</span>
<span id="cb28-132"><a href="#cb28-132" aria-hidden="true" tabindex="-1"></a>$$X_n = \frac{1}{n}\sum_{i=1}^n Z_i$$ If</span>
<span id="cb28-133"><a href="#cb28-133" aria-hidden="true" tabindex="-1"></a>$\text{E}\left<span class="co">[</span><span class="ot">Z_i\right</span><span class="co">]</span> = \mu$ and</span>
<span id="cb28-134"><a href="#cb28-134" aria-hidden="true" tabindex="-1"></a>$\text{Var}\left(Z_i\right) = \sigma^2$ for all $i$, we have</span>
<span id="cb28-135"><a href="#cb28-135" aria-hidden="true" tabindex="-1"></a>$X_n\overset{ms}{\to}\mu$: \begin{align*}</span>
<span id="cb28-136"><a href="#cb28-136" aria-hidden="true" tabindex="-1"></a>\lim_{n\to\infty}\text{E}\left<span class="co">[</span><span class="ot">(X_n - \mu)^2\right</span><span class="co">]</span> &amp; = \lim_{n\to\infty}\text{Var}\left(X_n\right) + \text{Bias}(X_n) <span class="sc">\\</span></span>
<span id="cb28-137"><a href="#cb28-137" aria-hidden="true" tabindex="-1"></a>&amp;= \lim_{n\to\infty} \frac{\sigma^2}{n} + 0 &amp; (X_n \text{ unbiased}) <span class="sc">\\</span></span>
<span id="cb28-138"><a href="#cb28-138" aria-hidden="true" tabindex="-1"></a>&amp; = \lim_{n\to\infty} \frac{\sigma^2}{n} <span class="sc">\\</span></span>
<span id="cb28-139"><a href="#cb28-139" aria-hidden="true" tabindex="-1"></a>&amp; = 0.</span>
<span id="cb28-140"><a href="#cb28-140" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb28-141"><a href="#cb28-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-142"><a href="#cb28-142" aria-hidden="true" tabindex="-1"></a>What does this convergence "look like"? If</span>
<span id="cb28-143"><a href="#cb28-143" aria-hidden="true" tabindex="-1"></a>$Z_i\overset{iid}{\sim}N(0,1)$, we know that</span>
<span id="cb28-144"><a href="#cb28-144" aria-hidden="true" tabindex="-1"></a>$X_n = \bar Z \sim N(\mu, \sigma^2/n)$. Let's plot this distribution for</span>
<span id="cb28-145"><a href="#cb28-145" aria-hidden="true" tabindex="-1"></a>increasing values of $n$.</span>
<span id="cb28-146"><a href="#cb28-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-149"><a href="#cb28-149" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-150"><a href="#cb28-150" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-151"><a href="#cb28-151" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot21</span></span>
<span id="cb28-152"><a href="#cb28-152" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb28-153"><a href="#cb28-153" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb28-154"><a href="#cb28-154" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-155"><a href="#cb28-155" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The distribution of X_n for increasing values of n"</span></span>
<span id="cb28-156"><a href="#cb28-156" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb28-157"><a href="#cb28-157" aria-hidden="true" tabindex="-1"></a><span class="fu">expand_grid</span>(</span>
<span id="cb28-158"><a href="#cb28-158" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length =</span> <span class="dv">500</span>),</span>
<span id="cb28-159"><a href="#cb28-159" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span></span>
<span id="cb28-160"><a href="#cb28-160" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb28-161"><a href="#cb28-161" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb28-162"><a href="#cb28-162" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="fu">dnorm</span>(x, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">/</span>n))</span>
<span id="cb28-163"><a href="#cb28-163" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb28-164"><a href="#cb28-164" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x,y)) <span class="sc">+</span></span>
<span id="cb28-165"><a href="#cb28-165" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb28-166"><a href="#cb28-166" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb28-167"><a href="#cb28-167" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Value of X_n"</span>, <span class="at">y =</span> <span class="st">"Density"</span>, <span class="at">color =</span> <span class="st">"Sample Size"</span>) <span class="sc">+</span></span>
<span id="cb28-168"><a href="#cb28-168" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb28-169"><a href="#cb28-169" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transition_states</span>(n) <span class="sc">+</span></span>
<span id="cb28-170"><a href="#cb28-170" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'n = {closest_state}'</span>)</span>
<span id="cb28-171"><a href="#cb28-171" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-172"><a href="#cb28-172" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-173"><a href="#cb28-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-174"><a href="#cb28-174" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This example betrays a useful property related to variables which</span></span>
<span id="cb28-175"><a href="#cb28-175" aria-hidden="true" tabindex="-1"></a>converge in mean square.</span>
<span id="cb28-176"><a href="#cb28-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-177"><a href="#cb28-177" aria-hidden="true" tabindex="-1"></a>::: {#prp-mse3}</span>
<span id="cb28-178"><a href="#cb28-178" aria-hidden="true" tabindex="-1"></a>A sequence of random variables $X_n$ converges in mean square to a</span>
<span id="cb28-179"><a href="#cb28-179" aria-hidden="true" tabindex="-1"></a>constant $c$ *if and only if* $\text{E}\left<span class="co">[</span><span class="ot">X_n\right</span><span class="co">]</span>\to c$ and</span>
<span id="cb28-180"><a href="#cb28-180" aria-hidden="true" tabindex="-1"></a>$\text{Var}\left(X_n\right)\to 0$.</span>
<span id="cb28-181"><a href="#cb28-181" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-182"><a href="#cb28-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-183"><a href="#cb28-183" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb28-184"><a href="#cb28-184" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">space</span><span class="co">]</span>{style="color:white"}</span>
<span id="cb28-185"><a href="#cb28-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-186"><a href="#cb28-186" aria-hidden="true" tabindex="-1"></a>$(\Longrightarrow)$ Suppose $X_n \overset{ms}{\to}c$. Then</span>
<span id="cb28-187"><a href="#cb28-187" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb28-188"><a href="#cb28-188" aria-hidden="true" tabindex="-1"></a>&amp; \lim_{n\to\infty}\text{E}\left<span class="co">[</span><span class="ot">(X_n - c)^2\right</span><span class="co">]</span> = 0<span class="sc">\\</span></span>
<span id="cb28-189"><a href="#cb28-189" aria-hidden="true" tabindex="-1"></a> \implies &amp; \lim_{n\to\infty}\text{E}\left<span class="co">[</span><span class="ot">(X_n - c)^2\right</span><span class="co">]</span> = 0<span class="sc">\\</span></span>
<span id="cb28-190"><a href="#cb28-190" aria-hidden="true" tabindex="-1"></a> \implies&amp; \lim_{n\to\infty}\left<span class="co">[</span><span class="ot">\text{E}\left[X_n\right]^2 -2c \text{E}\left[X_n\right] + c^2\right</span><span class="co">]</span>= 0<span class="sc">\\</span></span>
<span id="cb28-191"><a href="#cb28-191" aria-hidden="true" tabindex="-1"></a> \implies&amp; \lim_{n\to\infty}\left<span class="co">[</span><span class="ot">(\text{E}\left[X_n\right]^2 -\text{E}\left[X_n\right]^2) + \text{E}\left[X_n\right]^2 - 2c \text{E}\left[X_n\right] + c^2\right</span><span class="co">]</span>= 0 <span class="sc">\\</span> </span>
<span id="cb28-192"><a href="#cb28-192" aria-hidden="true" tabindex="-1"></a> \implies &amp;\lim_{n\to\infty} \text{Var}\left(X_n\right) + \lim_{n\to\infty}\left<span class="co">[</span><span class="ot">\text{E}\left[X_n\right] -c\right</span><span class="co">]</span>^2 = 0 </span>
<span id="cb28-193"><a href="#cb28-193" aria-hidden="true" tabindex="-1"></a>\end{align*} This final equality gives the desired result.</span>
<span id="cb28-194"><a href="#cb28-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-195"><a href="#cb28-195" aria-hidden="true" tabindex="-1"></a>$(\Longleftarrow)$ Suppose $\text{E}\left<span class="co">[</span><span class="ot">X_n\right</span><span class="co">]</span>\to c$ and</span>
<span id="cb28-196"><a href="#cb28-196" aria-hidden="true" tabindex="-1"></a>$\text{Var}\left(X_n\right)\to 0$. We have \begin{align*}</span>
<span id="cb28-197"><a href="#cb28-197" aria-hidden="true" tabindex="-1"></a>&amp;\lim_{n\to\infty} \text{Var}\left(X_n\right) + \lim_{n\to\infty}\left<span class="co">[</span><span class="ot">\text{E}\left[X_n\right] -c\right</span><span class="co">]</span>^2 = 0 <span class="sc">\\</span></span>
<span id="cb28-198"><a href="#cb28-198" aria-hidden="true" tabindex="-1"></a>\implies &amp; \lim_{n\to\infty}\text{E}\left<span class="co">[</span><span class="ot">(X_n - c)^2\right</span><span class="co">]</span> = 0<span class="sc">\\</span></span>
<span id="cb28-199"><a href="#cb28-199" aria-hidden="true" tabindex="-1"></a>\implies &amp; X_n\overset{ms}{\to}c</span>
<span id="cb28-200"><a href="#cb28-200" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb28-201"><a href="#cb28-201" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-202"><a href="#cb28-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-203"><a href="#cb28-203" aria-hidden="true" tabindex="-1"></a>::: {#cor-mseconv}</span>
<span id="cb28-204"><a href="#cb28-204" aria-hidden="true" tabindex="-1"></a>Suppose $X_n$ is a sequence of random variables such that</span>
<span id="cb28-205"><a href="#cb28-205" aria-hidden="true" tabindex="-1"></a>$\text{E}\left<span class="co">[</span><span class="ot">X_n\right</span><span class="co">]</span> = c$ for all $n$. Then $X_n\overset{ms}{\to}c$</span>
<span id="cb28-206"><a href="#cb28-206" aria-hidden="true" tabindex="-1"></a>*if and only if* $\text{Var}\left(X_n\right)\to 0$.</span>
<span id="cb28-207"><a href="#cb28-207" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-208"><a href="#cb28-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-209"><a href="#cb28-209" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convergence in Probability</span></span>
<span id="cb28-210"><a href="#cb28-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-211"><a href="#cb28-211" aria-hidden="true" tabindex="-1"></a>Convergence in mean square captures the idea that a random variable gets</span>
<span id="cb28-212"><a href="#cb28-212" aria-hidden="true" tabindex="-1"></a>"closer" to some value $c,$ but it is hardly the only way to define this</span>
<span id="cb28-213"><a href="#cb28-213" aria-hidden="true" tabindex="-1"></a>behavior. A more "traditional" approach would be defining convergence</span>
<span id="cb28-214"><a href="#cb28-214" aria-hidden="true" tabindex="-1"></a>using an inequality involving an arbitrarily small $\varepsilon &gt;0$</span>
<span id="cb28-215"><a href="#cb28-215" aria-hidden="true" tabindex="-1"></a>(akin the to $\varepsilon-\delta$ definition of a limit).</span>
<span id="cb28-216"><a href="#cb28-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-217"><a href="#cb28-217" aria-hidden="true" tabindex="-1"></a>::: {#def-}</span>
<span id="cb28-218"><a href="#cb28-218" aria-hidden="true" tabindex="-1"></a>A sequence of random variables $X_n$ [***converges in</span>
<span id="cb28-219"><a href="#cb28-219" aria-hidden="true" tabindex="-1"></a>probability***]{style="color:red"} to a random variable $X$, written as</span>
<span id="cb28-220"><a href="#cb28-220" aria-hidden="true" tabindex="-1"></a>$X_n\overset{p}{\to}X$ or $\mathop{\mathrm{plim}}X_n = X$, if</span>
<span id="cb28-221"><a href="#cb28-221" aria-hidden="true" tabindex="-1"></a>$$\lim_{n\to\infty} \Pr (|X_n - X| &gt; \varepsilon)= 0$$ for all</span>
<span id="cb28-222"><a href="#cb28-222" aria-hidden="true" tabindex="-1"></a>$\varepsilon &gt; 0$. Equivalently, $X_n\overset{p}{\to}X$ if for all</span>
<span id="cb28-223"><a href="#cb28-223" aria-hidden="true" tabindex="-1"></a>$\varepsilon &gt; 0$ and $\delta &gt; 0$, there exists some $N$ such that for</span>
<span id="cb28-224"><a href="#cb28-224" aria-hidden="true" tabindex="-1"></a>all $n \ge N$, $$ \Pr (|X_n - X| &gt; \varepsilon) &lt; \delta.$$ A sequence</span>
<span id="cb28-225"><a href="#cb28-225" aria-hidden="true" tabindex="-1"></a>of random vectors $\mathbf{X}_n = (X_{1,n},\ldots X_{k,n})$ converges in</span>
<span id="cb28-226"><a href="#cb28-226" aria-hidden="true" tabindex="-1"></a>probability to $\mathbf{X}$ if $X_{i,n}\overset{p}{\to} X_i$ for</span>
<span id="cb28-227"><a href="#cb28-227" aria-hidden="true" tabindex="-1"></a>$i=1,\ldots, k$.</span>
<span id="cb28-228"><a href="#cb28-228" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-229"><a href="#cb28-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-230"><a href="#cb28-230" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Intuitively, $X_n \overset{p}{\to}X$ if the probability that the</span></span>
<span id="cb28-231"><a href="#cb28-231" aria-hidden="true" tabindex="-1"></a>difference $|X_n - X|$ is not small (greater than some $\varepsilon$)</span>
<span id="cb28-232"><a href="#cb28-232" aria-hidden="true" tabindex="-1"></a>goes to zero as $n\to\infty$. In other words, for large values of $n$, there is </span>
<span id="cb28-233"><a href="#cb28-233" aria-hidden="true" tabindex="-1"></a>a large probability that $X_n$ is close to $X$</span>
<span id="cb28-234"><a href="#cb28-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-235"><a href="#cb28-235" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb28-236"><a href="#cb28-236" aria-hidden="true" tabindex="-1"></a>Return to the previous example where $X_n = \bar Z$, and assume</span>
<span id="cb28-237"><a href="#cb28-237" aria-hidden="true" tabindex="-1"></a>$Z_i \overset{iid}{\sim}N(\mu,\sigma^2)$. We will verify that</span>
<span id="cb28-238"><a href="#cb28-238" aria-hidden="true" tabindex="-1"></a>$X_n\overset{p}{\to}\mu$ using the definition of convergence in</span>
<span id="cb28-239"><a href="#cb28-239" aria-hidden="true" tabindex="-1"></a>probability using the fact that $X_n \sim N(\mu, \sigma^2/n)$.</span>
<span id="cb28-240"><a href="#cb28-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-243"><a href="#cb28-243" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-244"><a href="#cb28-244" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb28-245"><a href="#cb28-245" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot22</span></span>
<span id="cb28-246"><a href="#cb28-246" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb28-247"><a href="#cb28-247" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-248"><a href="#cb28-248" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "For any arbitrary ε, we can find some n such that the probability X_n falls outside the set |μ-ε| is arbitrarily small"</span></span>
<span id="cb28-249"><a href="#cb28-249" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb28-250"><a href="#cb28-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-251"><a href="#cb28-251" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"figures/converge.png"</span>)</span>
<span id="cb28-252"><a href="#cb28-252" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-253"><a href="#cb28-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-254"><a href="#cb28-254" aria-hidden="true" tabindex="-1"></a>For some $\varepsilon &gt; 0$, \begin{align*}</span>
<span id="cb28-255"><a href="#cb28-255" aria-hidden="true" tabindex="-1"></a>\Pr (|X_n - \mu| &gt; \varepsilon) &amp; = 1 - \Pr (\mu - \varepsilon &lt; X_n &lt; \mu + \varepsilon)<span class="sc">\\</span></span>
<span id="cb28-256"><a href="#cb28-256" aria-hidden="true" tabindex="-1"></a>&amp; = 1 - (F_{X_n}(\mu + \varepsilon) + F_{X_n}(\mu - \varepsilon))<span class="sc">\\</span></span>
<span id="cb28-257"><a href="#cb28-257" aria-hidden="true" tabindex="-1"></a>&amp; = 1 - 2\left<span class="co">[</span><span class="ot">F_{X_n}(\mu + \varepsilon) - \frac{1}{2}\right</span><span class="co">]</span> &amp; (F_{X_n} \text{symmetric about }\mu)<span class="sc">\\</span></span>
<span id="cb28-258"><a href="#cb28-258" aria-hidden="true" tabindex="-1"></a> &amp; = 1 - 2\left<span class="co">[</span><span class="ot">\Phi\left(\frac{(\mu + \varepsilon) - \mu}{\sigma/\sqrt{n}}\right) - \frac{1}{2}\right</span><span class="co">]</span> &amp; (\Phi\text{ standard normal distribution})<span class="sc">\\</span></span>
<span id="cb28-259"><a href="#cb28-259" aria-hidden="true" tabindex="-1"></a> &amp; = 1 - 2\left<span class="co">[</span><span class="ot">\Phi\left(\frac{\varepsilon}{\sigma/\sqrt{n}}\right) - \frac{1}{2}\right</span><span class="co">]</span>.</span>
<span id="cb28-260"><a href="#cb28-260" aria-hidden="true" tabindex="-1"></a>\end{align*} Given some $\delta &gt;0$, we can solve for the lowest value</span>
<span id="cb28-261"><a href="#cb28-261" aria-hidden="true" tabindex="-1"></a>of $n$ that satisfies $\Pr (|X_n - c| &gt; \varepsilon) &lt; \delta$.</span>
<span id="cb28-262"><a href="#cb28-262" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb28-263"><a href="#cb28-263" aria-hidden="true" tabindex="-1"></a>&amp;\Pr (|X_n - c| &gt; \varepsilon) &lt; \delta <span class="sc">\\</span> </span>
<span id="cb28-264"><a href="#cb28-264" aria-hidden="true" tabindex="-1"></a>\implies &amp; 1 - 2\left<span class="co">[</span><span class="ot">\Phi\left(\frac{\varepsilon}{\sigma/\sqrt{n}}\right) - \frac{1}{2}\right</span><span class="co">]</span> &lt; \delta <span class="sc">\\</span></span>
<span id="cb28-265"><a href="#cb28-265" aria-hidden="true" tabindex="-1"></a>\implies&amp;  n &gt; \left(\frac{\sigma \Phi^{-1}(1-\delta/2)}{\varepsilon}\right)^2</span>
<span id="cb28-266"><a href="#cb28-266" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>\implies &amp; n &gt; \left\lceil \left(\frac{\sigma \Phi^{-1}(1-\delta/2)}{\varepsilon}\right)^2 \right\rceil</span>
<span id="cb28-267"><a href="#cb28-267" aria-hidden="true" tabindex="-1"></a>\end{align*} Just to be excruciatingly pedantic, we rounded our solution</span>
<span id="cb28-268"><a href="#cb28-268" aria-hidden="true" tabindex="-1"></a>up to the closest positive integer, as $n$ corresponds to a sample size.</span>
<span id="cb28-269"><a href="#cb28-269" aria-hidden="true" tabindex="-1"></a>For fixed values of $\mu$ and $\sigma^2$ (say 3 and 2, respectively), we</span>
<span id="cb28-270"><a href="#cb28-270" aria-hidden="true" tabindex="-1"></a>can define a function of $(\varepsilon, \delta)$ which calculates the</span>
<span id="cb28-271"><a href="#cb28-271" aria-hidden="true" tabindex="-1"></a>sample size required to satisfy $\Pr(|X_n - c|&gt;\varepsilon)&lt;\delta$.</span>
<span id="cb28-272"><a href="#cb28-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-275"><a href="#cb28-275" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-276"><a href="#cb28-276" aria-hidden="true" tabindex="-1"></a>n_fun <span class="ot">&lt;-</span> <span class="cf">function</span>(delta, ep, mu, sigma){</span>
<span id="cb28-277"><a href="#cb28-277" aria-hidden="true" tabindex="-1"></a> <span class="fu">ceiling</span>(((sigma<span class="sc">*</span><span class="fu">qnorm</span>(<span class="dv">1</span><span class="sc">-</span>delta <span class="sc">/</span><span class="dv">2</span>))<span class="sc">/</span>ep)<span class="sc">^</span><span class="dv">2</span>) </span>
<span id="cb28-278"><a href="#cb28-278" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb28-279"><a href="#cb28-279" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-280"><a href="#cb28-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-281"><a href="#cb28-281" aria-hidden="true" tabindex="-1"></a>Let's plot this function for various values of $(\varepsilon, \delta)$.</span>
<span id="cb28-282"><a href="#cb28-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-285"><a href="#cb28-285" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-286"><a href="#cb28-286" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-287"><a href="#cb28-287" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot23</span></span>
<span id="cb28-288"><a href="#cb28-288" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb28-289"><a href="#cb28-289" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb28-290"><a href="#cb28-290" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-291"><a href="#cb28-291" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The sample size required to satisfy the inequality in the definition of convergence of probability for various values (ε,δ)"</span></span>
<span id="cb28-292"><a href="#cb28-292" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb28-293"><a href="#cb28-293" aria-hidden="true" tabindex="-1"></a><span class="fu">expand_grid</span>(</span>
<span id="cb28-294"><a href="#cb28-294" aria-hidden="true" tabindex="-1"></a>  <span class="at">e =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">0.1</span>, <span class="fl">0.01</span>, <span class="fl">0.001</span>, <span class="fl">0.0001</span>),</span>
<span id="cb28-295"><a href="#cb28-295" aria-hidden="true" tabindex="-1"></a>  <span class="at">d =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">9999</span><span class="sc">/</span><span class="dv">10000</span></span>
<span id="cb28-296"><a href="#cb28-296" aria-hidden="true" tabindex="-1"></a>)  <span class="sc">%&gt;%</span> </span>
<span id="cb28-297"><a href="#cb28-297" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">sample =</span> <span class="fu">map2_dbl</span>(d, e, n_fun, <span class="at">mu =</span> <span class="dv">3</span>, <span class="at">sigma =</span> <span class="fu">sqrt</span>(<span class="dv">2</span>))) <span class="sc">%&gt;%</span> </span>
<span id="cb28-298"><a href="#cb28-298" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(d, sample, <span class="at">color =</span> <span class="fu">as.factor</span>(e))) <span class="sc">+</span></span>
<span id="cb28-299"><a href="#cb28-299" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_reverse</span>() <span class="sc">+</span></span>
<span id="cb28-300"><a href="#cb28-300" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_log10</span>() <span class="sc">+</span> </span>
<span id="cb28-301"><a href="#cb28-301" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb28-302"><a href="#cb28-302" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb28-303"><a href="#cb28-303" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"δ"</span>, <span class="at">y =</span> <span class="st">"Sample Size"</span>, <span class="at">color =</span> <span class="st">"ε"</span>)<span class="sc">+</span></span>
<span id="cb28-304"><a href="#cb28-304" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span>
<span id="cb28-305"><a href="#cb28-305" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-306"><a href="#cb28-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-307"><a href="#cb28-307" aria-hidden="true" tabindex="-1"></a>We can also verify that</span>
<span id="cb28-308"><a href="#cb28-308" aria-hidden="true" tabindex="-1"></a>$\lim_{n\to\infty}\Pr (|X_n - \mu| &gt; \varepsilon) = 0$ for various</span>
<span id="cb28-309"><a href="#cb28-309" aria-hidden="true" tabindex="-1"></a>values of $\varepsilon.$</span>
<span id="cb28-310"><a href="#cb28-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-313"><a href="#cb28-313" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-314"><a href="#cb28-314" aria-hidden="true" tabindex="-1"></a>prob_ep <span class="ot">&lt;-</span> <span class="cf">function</span>(n, ep, mu, sigma){</span>
<span id="cb28-315"><a href="#cb28-315" aria-hidden="true" tabindex="-1"></a>  <span class="dv">1</span> <span class="sc">-</span> <span class="dv">2</span><span class="sc">*</span>(<span class="fu">pnorm</span>(ep <span class="sc">/</span> (sigma <span class="sc">/</span> <span class="fu">sqrt</span>(n))) <span class="sc">-</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb28-316"><a href="#cb28-316" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb28-317"><a href="#cb28-317" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-318"><a href="#cb28-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-321"><a href="#cb28-321" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-322"><a href="#cb28-322" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-323"><a href="#cb28-323" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot24</span></span>
<span id="cb28-324"><a href="#cb28-324" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb28-325"><a href="#cb28-325" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb28-326"><a href="#cb28-326" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-327"><a href="#cb28-327" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The probability that X_n falls outside the interval  |μ-ε| for various values of (ε,n) "</span></span>
<span id="cb28-328"><a href="#cb28-328" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb28-329"><a href="#cb28-329" aria-hidden="true" tabindex="-1"></a><span class="fu">expand_grid</span>(</span>
<span id="cb28-330"><a href="#cb28-330" aria-hidden="true" tabindex="-1"></a>  <span class="at">e =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">0.1</span>, <span class="fl">0.01</span>, <span class="fl">0.001</span>, <span class="fl">0.0001</span>),</span>
<span id="cb28-331"><a href="#cb28-331" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="dv">1</span><span class="sc">:</span><span class="fl">1e6</span></span>
<span id="cb28-332"><a href="#cb28-332" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb28-333"><a href="#cb28-333" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">prob =</span> <span class="fu">map2_dbl</span>(e, n, \(e, n) <span class="fu">prob_ep</span>(n, e, <span class="at">mu =</span> <span class="dv">3</span>, <span class="at">sigma =</span> <span class="fu">sqrt</span>(<span class="dv">3</span>)))) <span class="sc">%&gt;%</span> </span>
<span id="cb28-334"><a href="#cb28-334" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(n, prob, <span class="at">color =</span> <span class="fu">as.factor</span>(e))) <span class="sc">+</span></span>
<span id="cb28-335"><a href="#cb28-335" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb28-336"><a href="#cb28-336" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_log10</span>() <span class="sc">+</span></span>
<span id="cb28-337"><a href="#cb28-337" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb28-338"><a href="#cb28-338" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Sample Size"</span>, <span class="at">y =</span> <span class="st">"Pr(|X_n - mu| &gt; ε)"</span>, <span class="at">color =</span> <span class="st">"ε"</span>) <span class="sc">+</span></span>
<span id="cb28-339"><a href="#cb28-339" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) </span>
<span id="cb28-340"><a href="#cb28-340" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-341"><a href="#cb28-341" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-342"><a href="#cb28-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-343"><a href="#cb28-343" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;How does convergence in mean square related to convergence in</span></span>
<span id="cb28-344"><a href="#cb28-344" aria-hidden="true" tabindex="-1"></a>probability? As it turns out the latter is a weaker condition implied by</span>
<span id="cb28-345"><a href="#cb28-345" aria-hidden="true" tabindex="-1"></a>the prior. Before stating and proving this result, we will need a lemma.</span>
<span id="cb28-346"><a href="#cb28-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-347"><a href="#cb28-347" aria-hidden="true" tabindex="-1"></a>::: {#lem-markovineq}</span>
<span id="cb28-348"><a href="#cb28-348" aria-hidden="true" tabindex="-1"></a><span class="fu">## Markov's inequality</span></span>
<span id="cb28-349"><a href="#cb28-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-350"><a href="#cb28-350" aria-hidden="true" tabindex="-1"></a>If $X$ is a nonnegative random variable, and $a &gt; 0$, then</span>
<span id="cb28-351"><a href="#cb28-351" aria-hidden="true" tabindex="-1"></a>$$\Pr(X\ge a) \le \frac{\text{E}\left<span class="co">[</span><span class="ot">X\right</span><span class="co">]</span>}{a}$$</span>
<span id="cb28-352"><a href="#cb28-352" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-353"><a href="#cb28-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-354"><a href="#cb28-354" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb28-355"><a href="#cb28-355" aria-hidden="true" tabindex="-1"></a>The expectation of $X$ can be written as \begin{align*}</span>
<span id="cb28-356"><a href="#cb28-356" aria-hidden="true" tabindex="-1"></a>\text{E}\left<span class="co">[</span><span class="ot">X\right</span><span class="co">]</span> &amp; = \int_{-\infty}^\infty x\ dF_X(x) <span class="sc">\\</span> </span>
<span id="cb28-357"><a href="#cb28-357" aria-hidden="true" tabindex="-1"></a>&amp; = \int_{0}^\infty x\ dF_X(x) &amp; (X\text{ is nonnegative}) <span class="sc">\\</span> </span>
<span id="cb28-358"><a href="#cb28-358" aria-hidden="true" tabindex="-1"></a>&amp; = \int_{0}^a x\ dF_X(x) + \int_{a}^\infty x\ dF_X(x) <span class="sc">\\</span> </span>
<span id="cb28-359"><a href="#cb28-359" aria-hidden="true" tabindex="-1"></a>&amp; \ge \int_a^\infty x\ dF_X(x)<span class="sc">\\</span></span>
<span id="cb28-360"><a href="#cb28-360" aria-hidden="true" tabindex="-1"></a>&amp; \ge \int_a^\infty a\ dF_X(x) &amp; (a \ge x \text{ on }(a,\infty))<span class="sc">\\</span></span>
<span id="cb28-361"><a href="#cb28-361" aria-hidden="true" tabindex="-1"></a>&amp; = a \int_a^\infty\ dF_X(x) <span class="sc">\\</span></span>
<span id="cb28-362"><a href="#cb28-362" aria-hidden="true" tabindex="-1"></a>&amp; = a\Pr(X \ge a).</span>
<span id="cb28-363"><a href="#cb28-363" aria-hidden="true" tabindex="-1"></a>\end{align*} Dividing both sides of this inequality by $a$ gives</span>
<span id="cb28-364"><a href="#cb28-364" aria-hidden="true" tabindex="-1"></a>$\Pr(X\ge a) \le \text{E}\left<span class="co">[</span><span class="ot">X\right</span><span class="co">]</span>/a$.</span>
<span id="cb28-365"><a href="#cb28-365" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-366"><a href="#cb28-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-367"><a href="#cb28-367" aria-hidden="true" tabindex="-1"></a>::: {#prp-conv}</span>
<span id="cb28-368"><a href="#cb28-368" aria-hidden="true" tabindex="-1"></a><span class="fu">## Convergence in MSE --\&gt; Convergence in Probability</span></span>
<span id="cb28-369"><a href="#cb28-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-370"><a href="#cb28-370" aria-hidden="true" tabindex="-1"></a>Let $X_n$ be a sequence of random variables. If $X_n\overset{ms}{\to}X$,</span>
<span id="cb28-371"><a href="#cb28-371" aria-hidden="true" tabindex="-1"></a>then $X_n\overset{p}{\to}X$.</span>
<span id="cb28-372"><a href="#cb28-372" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-373"><a href="#cb28-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-374"><a href="#cb28-374" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb28-375"><a href="#cb28-375" aria-hidden="true" tabindex="-1"></a>Suppose $X_n\overset{ms}{\to}X$. For all $\varepsilon &gt; 0$</span>
<span id="cb28-376"><a href="#cb28-376" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb28-377"><a href="#cb28-377" aria-hidden="true" tabindex="-1"></a>\lim_{n\to\infty} \Pr (|X_n - X| &gt; \varepsilon) &amp; = \lim_{n\to\infty} \Pr ((X_n - X)^2 &gt; \varepsilon^2) <span class="sc">\\</span></span>
<span id="cb28-378"><a href="#cb28-378" aria-hidden="true" tabindex="-1"></a>&amp; \le \lim_{n\to\infty} \frac{\text{E}\left<span class="co">[</span><span class="ot">(X_n - X)^2\right</span><span class="co">]</span>}{\varepsilon^2} &amp; (\text{Markov's Inequality}) <span class="sc">\\</span></span>
<span id="cb28-379"><a href="#cb28-379" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{0}{\varepsilon^2} &amp; (X_n\overset{ms}{\to}X)<span class="sc">\\</span></span>
<span id="cb28-380"><a href="#cb28-380" aria-hidden="true" tabindex="-1"></a>&amp; = 0.</span>
<span id="cb28-381"><a href="#cb28-381" aria-hidden="true" tabindex="-1"></a>\end{align*} Therefore $X_n\overset{p}{\to}c$.</span>
<span id="cb28-382"><a href="#cb28-382" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-383"><a href="#cb28-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-384"><a href="#cb28-384" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The usefulness of @prp-conv cannot be emphasized enough. Proving</span></span>
<span id="cb28-385"><a href="#cb28-385" aria-hidden="true" tabindex="-1"></a>convergence in probability using the definition is cumbersome, so we</span>
<span id="cb28-386"><a href="#cb28-386" aria-hidden="true" tabindex="-1"></a>will almost show convergence in mean square and then appeal to @prp-conv</span>
<span id="cb28-387"><a href="#cb28-387" aria-hidden="true" tabindex="-1"></a>to verify convergence in probability. Nevertheless, situations can arise</span>
<span id="cb28-388"><a href="#cb28-388" aria-hidden="true" tabindex="-1"></a>where $X_n\overset{p}{\to} X$, but $X_n \not\overset{ms}{\to} X$.</span>
<span id="cb28-389"><a href="#cb28-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-390"><a href="#cb28-390" aria-hidden="true" tabindex="-1"></a>::: {#exm-pnotmse}</span>
<span id="cb28-391"><a href="#cb28-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-392"><a href="#cb28-392" aria-hidden="true" tabindex="-1"></a><span class="fu">## Convergence in Probability but not in Mean Square</span></span>
<span id="cb28-393"><a href="#cb28-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-394"><a href="#cb28-394" aria-hidden="true" tabindex="-1"></a>Suppose $X_n$ is defined on the sample space $<span class="sc">\{</span>1,n^2<span class="sc">\}</span>$ such that:</span>
<span id="cb28-395"><a href="#cb28-395" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb28-396"><a href="#cb28-396" aria-hidden="true" tabindex="-1"></a>\Pr(X_n = 0) &amp;= 1-1/n<span class="sc">\\</span></span>
<span id="cb28-397"><a href="#cb28-397" aria-hidden="true" tabindex="-1"></a>\Pr(X_n = n^2) &amp;= 1/n</span>
<span id="cb28-398"><a href="#cb28-398" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb28-399"><a href="#cb28-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-402"><a href="#cb28-402" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-403"><a href="#cb28-403" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-404"><a href="#cb28-404" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot24.2</span></span>
<span id="cb28-405"><a href="#cb28-405" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb28-406"><a href="#cb28-406" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb28-407"><a href="#cb28-407" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-408"><a href="#cb28-408" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The probability density function associated with X_n for increasing values of n."</span></span>
<span id="cb28-409"><a href="#cb28-409" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb28-410"><a href="#cb28-410" aria-hidden="true" tabindex="-1"></a><span class="fu">expand_grid</span>(<span class="at">n =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>, <span class="at">x =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb28-411"><a href="#cb28-411" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb28-412"><a href="#cb28-412" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> x<span class="sc">*</span>(n<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb28-413"><a href="#cb28-413" aria-hidden="true" tabindex="-1"></a>    <span class="at">p =</span> (<span class="dv">1-1</span><span class="sc">/</span>n)<span class="sc">*</span>(x <span class="sc">==</span> <span class="dv">0</span>) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">/</span>n)<span class="sc">*</span>(x <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb28-414"><a href="#cb28-414" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb28-415"><a href="#cb28-415" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> p)) <span class="sc">+</span></span>
<span id="cb28-416"><a href="#cb28-416" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb28-417"><a href="#cb28-417" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">xend =</span> x, <span class="at">y =</span> <span class="dv">0</span>, <span class="at">yend =</span> p)) <span class="sc">+</span></span>
<span id="cb28-418"><a href="#cb28-418" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb28-419"><a href="#cb28-419" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_sqrt</span>(<span class="at">breaks =</span> (<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>)<span class="sc">*</span><span class="dv">500</span>) <span class="sc">+</span> </span>
<span id="cb28-420"><a href="#cb28-420" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">color =</span> <span class="st">"n"</span>, <span class="at">y =</span> <span class="st">"Probability Density"</span>, <span class="at">x =</span> <span class="st">"X_n"</span>) <span class="sc">+</span></span>
<span id="cb28-421"><a href="#cb28-421" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)  <span class="sc">+</span> </span>
<span id="cb28-422"><a href="#cb28-422" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transition_states</span>(n) <span class="sc">+</span></span>
<span id="cb28-423"><a href="#cb28-423" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'n = {closest_state}'</span>)</span>
<span id="cb28-424"><a href="#cb28-424" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-425"><a href="#cb28-425" aria-hidden="true" tabindex="-1"></a>The expected value of $X_n$ is</span>
<span id="cb28-426"><a href="#cb28-426" aria-hidden="true" tabindex="-1"></a>$$\text{E}\left<span class="co">[</span><span class="ot">X_n\right</span><span class="co">]</span> = 0(1-1/n) + n^2(1/n) = n,$$ so</span>
<span id="cb28-427"><a href="#cb28-427" aria-hidden="true" tabindex="-1"></a>$\text{E}\left<span class="co">[</span><span class="ot">X_n\right</span><span class="co">]</span>\to\infty$ as $n\to \infty$. This rules out</span>
<span id="cb28-428"><a href="#cb28-428" aria-hidden="true" tabindex="-1"></a>$X_n$ converging in mean square to any value. Nevertheless, we have</span>
<span id="cb28-429"><a href="#cb28-429" aria-hidden="true" tabindex="-1"></a>$X_n\overset{p}{\to}0$. For all $\varepsilon &gt; 0$,</span>
<span id="cb28-430"><a href="#cb28-430" aria-hidden="true" tabindex="-1"></a>$$\Pr(|X_n - 0| &gt; \varepsilon) = \Pr(X_n \neq 0) = \Pr(X_n = n^2) = 1/n \to 0.$$</span>
<span id="cb28-431"><a href="#cb28-431" aria-hidden="true" tabindex="-1"></a>This disagreement among definitions of convergence arises because the convergence in probability</span>
<span id="cb28-432"><a href="#cb28-432" aria-hidden="true" tabindex="-1"></a>only takes into consideration the probability assigned to each value in the sample space, whereas convergence in MSE is based on an expectation which takes into consideration the probability assigned to each value in the sample space *weighted* by the value in the sample space. In this particular example, the probability that $X_n = n^2$ is $1/n$, and the growth of $n^2$ as $n\to \infty$ outpaces the growth of $1/n$, so the expected value of $X_n$ grows indefinitely.</span>
<span id="cb28-433"><a href="#cb28-433" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-434"><a href="#cb28-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-435"><a href="#cb28-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-436"><a href="#cb28-436" aria-hidden="true" tabindex="-1"></a>::: remark </span>
<span id="cb28-437"><a href="#cb28-437" aria-hidden="true" tabindex="-1"></a>We can think of the counterexample in @exm-pnotmse arising from the "tail" of a density, where the "tail" happened to just be a single point because the random variable was discrete. This is a pattern that comes up often in asymptotics -- the tails of distributions and densities can cause trouble. Many theorems and results depend on these tails behaving well.</span>
<span id="cb28-438"><a href="#cb28-438" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-439"><a href="#cb28-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-440"><a href="#cb28-440" aria-hidden="true" tabindex="-1"></a><span class="fu">### Almost Sure Convergence</span></span>
<span id="cb28-441"><a href="#cb28-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-442"><a href="#cb28-442" aria-hidden="true" tabindex="-1"></a>An third type of convergence is almost sure convergence. Recalling that a random variable is just a function, we can define a stochastic analog to pointwise convergence of a sequence of functions. </span>
<span id="cb28-443"><a href="#cb28-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-444"><a href="#cb28-444" aria-hidden="true" tabindex="-1"></a>::: {#def-}</span>
<span id="cb28-445"><a href="#cb28-445" aria-hidden="true" tabindex="-1"></a>A sequence of random variables $X_n$ <span class="co">[</span><span class="ot">***converges almost surely***</span><span class="co">]</span>{style="color:red"} to a random variable $X$, written as</span>
<span id="cb28-446"><a href="#cb28-446" aria-hidden="true" tabindex="-1"></a>$X_n\overset{a.s}{\to}X$ if</span>
<span id="cb28-447"><a href="#cb28-447" aria-hidden="true" tabindex="-1"></a>$$ \Pr \left(\lim_{n\to\infty} X_n = X\right) = 1.$$ A sequence</span>
<span id="cb28-448"><a href="#cb28-448" aria-hidden="true" tabindex="-1"></a>of random vectors $\mathbf{X}_n = (X_{1,n},\ldots X_{k,n})$ converges almost surely to $\mathbf{X}$ if $X_{i,n}\overset{a.s}{\to} X_i$ for</span>
<span id="cb28-449"><a href="#cb28-449" aria-hidden="true" tabindex="-1"></a>$i=1,\ldots, k$.</span>
<span id="cb28-450"><a href="#cb28-450" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-451"><a href="#cb28-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-452"><a href="#cb28-452" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Almost sure convergence is the probabilistic equivalent of a sequence of functions converging pointwise almost everywhere. The difference between $X_n\overset{a.s}{\to} X$ and $X_n\pto X$ is subtle, and arises from the limit being taken before or after we take the probability of the event. While convergence in probability tells us that the chance that $X_n$ and $X$ are far approaches zero, almost sure convergence says that the probability that $X_n\to X$ as $n\to \infty$ is 1. We're certain that $X_n$ will eventually coincide with $X$, although we don't know when that will happen. The next example makes the difference between these two definitions a bit more concrete. Later on in @exm-strongvsweak an illustration will be presented to distinguish the two definitions.</span></span>
<span id="cb28-453"><a href="#cb28-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-454"><a href="#cb28-454" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb28-455"><a href="#cb28-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-456"><a href="#cb28-456" aria-hidden="true" tabindex="-1"></a><span class="fu">## Convergence in Probability but not in Almost Surely</span></span>
<span id="cb28-457"><a href="#cb28-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-458"><a href="#cb28-458" aria-hidden="true" tabindex="-1"></a>Define $X_n$ on $<span class="sc">\{</span>0,1<span class="sc">\}</span>$ such that: </span>
<span id="cb28-459"><a href="#cb28-459" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb28-460"><a href="#cb28-460" aria-hidden="true" tabindex="-1"></a>\Pr(X_n = 1) &amp; = 1/n<span class="sc">\\</span></span>
<span id="cb28-461"><a href="#cb28-461" aria-hidden="true" tabindex="-1"></a>\Pr(X_n = 0) &amp; = 1-1/n</span>
<span id="cb28-462"><a href="#cb28-462" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb28-463"><a href="#cb28-463" aria-hidden="true" tabindex="-1"></a>We have $X_n\pto 0$ since </span>
<span id="cb28-464"><a href="#cb28-464" aria-hidden="true" tabindex="-1"></a>$$ \lim_{n\to\infty}\Pr(|X_n - 0| &gt; \varepsilon) =  \lim_{n\to\infty}\Pr(X_n &gt; \varepsilon) = \lim_{n\to\infty}\Pr(X_n = 1) = \lim_{n\to\infty} 1/n = 0$$ for all $\varepsilon &gt; 0$.</span>
<span id="cb28-465"><a href="#cb28-465" aria-hidden="true" tabindex="-1"></a>At the same time, we have </span>
<span id="cb28-466"><a href="#cb28-466" aria-hidden="true" tabindex="-1"></a>$$\sum_{n=1}^\infty \Pr(X_n = 1) =  \sum_{n=1}^\infty 1/n = \infty,$$ so by a form of the <span class="co">[</span><span class="ot">Borel-Cantelli Lemma</span><span class="co">](http://www.columbia.edu/~ks20/stochastic-I/stochastic-I-BC.pdf)</span> then $X_n = 1$ occurs an infinite number of times. This rules out the possibility that $\lim_{n\to\infty} X_n = 0$ with probability 1.</span>
<span id="cb28-467"><a href="#cb28-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-468"><a href="#cb28-468" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-469"><a href="#cb28-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-470"><a href="#cb28-470" aria-hidden="true" tabindex="-1"></a>::: {#prp-}</span>
<span id="cb28-471"><a href="#cb28-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-472"><a href="#cb28-472" aria-hidden="true" tabindex="-1"></a><span class="fu">## Convergence A.S -&gt; Convergence in Probability</span></span>
<span id="cb28-473"><a href="#cb28-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-474"><a href="#cb28-474" aria-hidden="true" tabindex="-1"></a>Let $X_n$ be a sequence of random variables. If $X_n\overset{a.s}\to X$ then $X_n\pto X$. </span>
<span id="cb28-475"><a href="#cb28-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-476"><a href="#cb28-476" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-477"><a href="#cb28-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-478"><a href="#cb28-478" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The proof of this result can be found in @van2000asymptotic. Almost sure convergence is not nearly as important as convergence in probability when it comes to assessing estimators. In fact, @lehmann1999elements doesn't even mention it in his treatment of asymptotic statistics. The math underlying almost sure convergence is also a bit complex, so the related proofs aren't very informative and will be omitted.</span></span>
<span id="cb28-479"><a href="#cb28-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-480"><a href="#cb28-480" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convergence in Distribution</span></span>
<span id="cb28-481"><a href="#cb28-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-482"><a href="#cb28-482" aria-hidden="true" tabindex="-1"></a>The final notion of convergence we will use related to the probability</span>
<span id="cb28-483"><a href="#cb28-483" aria-hidden="true" tabindex="-1"></a>distribution of random variables.</span>
<span id="cb28-484"><a href="#cb28-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-485"><a href="#cb28-485" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb28-486"><a href="#cb28-486" aria-hidden="true" tabindex="-1"></a>A sequence of random variables $X_n$ [***converges in distribution</span>
<span id="cb28-487"><a href="#cb28-487" aria-hidden="true" tabindex="-1"></a>(converges weakly)***]{style="color:red"} to a random variable $X$,</span>
<span id="cb28-488"><a href="#cb28-488" aria-hidden="true" tabindex="-1"></a>written as $X_n \overset{d}{\to}X$, if</span>
<span id="cb28-489"><a href="#cb28-489" aria-hidden="true" tabindex="-1"></a>$$\lim_{n\to\infty} F_{X_n}(x)= F_X(x)$$ for all points $x$ where $F_{X}$ is continuous. In this case, we refer to</span>
<span id="cb28-490"><a href="#cb28-490" aria-hidden="true" tabindex="-1"></a>$F_X$ as the <span class="co">[</span><span class="ot">***asymptotic distribution***</span><span class="co">]</span>{style="color:red"} of</span>
<span id="cb28-491"><a href="#cb28-491" aria-hidden="true" tabindex="-1"></a>$X_n$, and write $X_n \overset{a}{\sim}F_X$.^<span class="co">[</span><span class="ot">Convergence in distribution is also sometimes called convergence in law among mathematicians. The notation for this form of convergence is also as varied as it's name. Some other ways of writing $X_n\overset{d}{\to} X$ are: $X_n\rightsquigarrow X$, $X_n\Rightarrow X$, $X_n \overset{\mathcal L}\to X$, $\mathcal L(X_n) \to \mathcal L(X)$.</span><span class="co">]</span> A sequence</span>
<span id="cb28-492"><a href="#cb28-492" aria-hidden="true" tabindex="-1"></a>of random vectors $\mathbf{X}_n = (X_{1,n},\ldots X_{k,n})$ converges in distribution to $\X$ if $\lim_{n\to\infty} F_{\X_n}(x)= F_{\X}(x)$.</span>
<span id="cb28-493"><a href="#cb28-493" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-494"><a href="#cb28-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-495"><a href="#cb28-495" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For our purposes, $X_n\overset{d}{\to}X$ means the distribution of $X_n$</span></span>
<span id="cb28-496"><a href="#cb28-496" aria-hidden="true" tabindex="-1"></a>can be approximated by $F_X$, and this approximation becomes</span>
<span id="cb28-497"><a href="#cb28-497" aria-hidden="true" tabindex="-1"></a>increasingly better as $n\to\infty$.</span>
<span id="cb28-498"><a href="#cb28-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-499"><a href="#cb28-499" aria-hidden="true" tabindex="-1"></a>::: {#exm-tdist}</span>
<span id="cb28-500"><a href="#cb28-500" aria-hidden="true" tabindex="-1"></a>One example of convergence in distribution you may be familiar with</span>
<span id="cb28-501"><a href="#cb28-501" aria-hidden="true" tabindex="-1"></a>deals with the student's $t-$distribution where the degrees of freedom</span>
<span id="cb28-502"><a href="#cb28-502" aria-hidden="true" tabindex="-1"></a>$n\to\infty$. If $X_n\sim t_n$, then $X_n \overset{d}{\to}X$ where</span>
<span id="cb28-503"><a href="#cb28-503" aria-hidden="true" tabindex="-1"></a>$X\sim N(0,1)$.</span>
<span id="cb28-504"><a href="#cb28-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-507"><a href="#cb28-507" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-508"><a href="#cb28-508" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-509"><a href="#cb28-509" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot25</span></span>
<span id="cb28-510"><a href="#cb28-510" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb28-511"><a href="#cb28-511" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb28-512"><a href="#cb28-512" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-513"><a href="#cb28-513" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The t-distribution converges to the standard normal distribution (represented by the dashed red line) as the degrees of freedom increase."</span></span>
<span id="cb28-514"><a href="#cb28-514" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb28-515"><a href="#cb28-515" aria-hidden="true" tabindex="-1"></a><span class="fu">expand_grid</span>(</span>
<span id="cb28-516"><a href="#cb28-516" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="at">length =</span> <span class="fl">1e3</span>),</span>
<span id="cb28-517"><a href="#cb28-517" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,</span>
<span id="cb28-518"><a href="#cb28-518" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist =</span> <span class="st">"Student's t"</span></span>
<span id="cb28-519"><a href="#cb28-519" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb28-520"><a href="#cb28-520" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">val =</span> <span class="fu">dt</span>(x, n)) <span class="sc">%&gt;%</span> </span>
<span id="cb28-521"><a href="#cb28-521" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, val)) <span class="sc">+</span></span>
<span id="cb28-522"><a href="#cb28-522" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb28-523"><a href="#cb28-523" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(</span>
<span id="cb28-524"><a href="#cb28-524" aria-hidden="true" tabindex="-1"></a>    <span class="at">fun =</span> dnorm, </span>
<span id="cb28-525"><a href="#cb28-525" aria-hidden="true" tabindex="-1"></a>    <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>), </span>
<span id="cb28-526"><a href="#cb28-526" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">"red"</span>, </span>
<span id="cb28-527"><a href="#cb28-527" aria-hidden="true" tabindex="-1"></a>    <span class="at">linetype=</span><span class="st">"dashed"</span></span>
<span id="cb28-528"><a href="#cb28-528" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb28-529"><a href="#cb28-529" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb28-530"><a href="#cb28-530" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">color =</span> <span class="st">"t-distribution degrees of freedom, n"</span>, <span class="at">y =</span> <span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb28-531"><a href="#cb28-531" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb28-532"><a href="#cb28-532" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transition_states</span>(n) <span class="sc">+</span></span>
<span id="cb28-533"><a href="#cb28-533" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'n = {closest_state}'</span>)</span>
<span id="cb28-534"><a href="#cb28-534" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-535"><a href="#cb28-535" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-536"><a href="#cb28-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-537"><a href="#cb28-537" aria-hidden="true" tabindex="-1"></a>::: {#prp-}</span>
<span id="cb28-538"><a href="#cb28-538" aria-hidden="true" tabindex="-1"></a><span class="fu">## Convergence in Probability --\&gt; Convergence in Distribution</span></span>
<span id="cb28-539"><a href="#cb28-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-540"><a href="#cb28-540" aria-hidden="true" tabindex="-1"></a>Let $X_n$ be a sequence of random variables. If $X_n\overset{p}{\to}X$,</span>
<span id="cb28-541"><a href="#cb28-541" aria-hidden="true" tabindex="-1"></a>then $X_n\overset{d}{\to}X$.</span>
<span id="cb28-542"><a href="#cb28-542" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-543"><a href="#cb28-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-544"><a href="#cb28-544" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb28-545"><a href="#cb28-545" aria-hidden="true" tabindex="-1"></a>Suppose $X_n\overset{p}{\to}X$ and let $\varepsilon &gt; 0$. We have,</span>
<span id="cb28-546"><a href="#cb28-546" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb28-547"><a href="#cb28-547" aria-hidden="true" tabindex="-1"></a>\Pr(X_n \le x) &amp; = \Pr(X_n\le x \text{ and } X \le x + \varepsilon) + \Pr(X_n\le x \text{ and } X &gt; x + \varepsilon) <span class="sc">\\</span></span>
<span id="cb28-548"><a href="#cb28-548" aria-hidden="true" tabindex="-1"></a>  &amp; = \Pr(X \le x + \varepsilon) + \Pr(X_n - X\le x - X \text{ and } x - X &lt; -\varepsilon)<span class="sc">\\</span> </span>
<span id="cb28-549"><a href="#cb28-549" aria-hidden="true" tabindex="-1"></a>  &amp; \le \Pr(X \le x + \varepsilon) + \Pr(X_n - X &lt; -\varepsilon)<span class="sc">\\</span></span>
<span id="cb28-550"><a href="#cb28-550" aria-hidden="true" tabindex="-1"></a>  &amp; \le \Pr(X \le x + \varepsilon) + \Pr(X_n - X &lt; -\varepsilon) + \Pr(X - X_n &gt; \varepsilon) <span class="sc">\\</span></span>
<span id="cb28-551"><a href="#cb28-551" aria-hidden="true" tabindex="-1"></a>  &amp; = \Pr(X \le x + \varepsilon) + \Pr(|X_n - X| &gt; \varepsilon)</span>
<span id="cb28-552"><a href="#cb28-552" aria-hidden="true" tabindex="-1"></a>\end{align*} Similarly,</span>
<span id="cb28-553"><a href="#cb28-553" aria-hidden="true" tabindex="-1"></a>$$ \Pr(X \le x-\varepsilon) \le \Pr(X_n \le x) + \Pr(|X_n - X| &gt; \varepsilon).$$</span>
<span id="cb28-554"><a href="#cb28-554" aria-hidden="true" tabindex="-1"></a>We can use these inequalities to find an upper and lower bound of</span>
<span id="cb28-555"><a href="#cb28-555" aria-hidden="true" tabindex="-1"></a>$\Pr(X_n \le x)$: </span>
<span id="cb28-556"><a href="#cb28-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-557"><a href="#cb28-557" aria-hidden="true" tabindex="-1"></a>::: {.column-screen-inset-right}</span>
<span id="cb28-558"><a href="#cb28-558" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb28-559"><a href="#cb28-559" aria-hidden="true" tabindex="-1"></a>&amp; \Pr(X \le x-\varepsilon) - \Pr(|X_n - X| &gt; \varepsilon)\le \Pr(X_n \le x) \le \Pr(X \le x + \varepsilon) + \Pr(|X_n - X| &gt; \varepsilon) <span class="sc">\\</span></span>
<span id="cb28-560"><a href="#cb28-560" aria-hidden="true" tabindex="-1"></a>\implies &amp; \lim_{n\to\infty}<span class="co">[</span><span class="ot">\Pr(X \le x-\varepsilon) - \Pr(|X_n - X| &gt; \varepsilon)</span><span class="co">]</span>\le \lim_{n\to\infty}\Pr(X_n \le x) \le \lim_{n\to\infty}<span class="co">[</span><span class="ot">\Pr(X \le x + \varepsilon) + \Pr(|X_n - X| &gt; \varepsilon) </span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb28-561"><a href="#cb28-561" aria-hidden="true" tabindex="-1"></a>\implies &amp; \Pr(X \le x-\varepsilon) - \underbrace{\lim_{n\to\infty}\Pr(|X_n - X| &gt; \varepsilon) }_0\le \lim_{n\to\infty}\Pr(X_n \le x) \le \Pr(X \le x + \varepsilon) + \underbrace{\lim_{n\to\infty}\Pr(|X_n - X| &gt; \varepsilon)}_0 <span class="sc">\\</span></span>
<span id="cb28-562"><a href="#cb28-562" aria-hidden="true" tabindex="-1"></a>\implies &amp; \Pr(X \le x-\varepsilon)\le \lim_{n\to\infty} \Pr(X_n \le x) \le  \Pr(X \le x + \varepsilon) &amp; (X_n\overset{p}{\to}X)<span class="sc">\\</span> </span>
<span id="cb28-563"><a href="#cb28-563" aria-hidden="true" tabindex="-1"></a>\implies &amp; F_X(x-\varepsilon)\le \lim_{n\to\infty} F_{X_n}(x) \le  F_X(x-\varepsilon) </span>
<span id="cb28-564"><a href="#cb28-564" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb28-565"><a href="#cb28-565" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-566"><a href="#cb28-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-567"><a href="#cb28-567" aria-hidden="true" tabindex="-1"></a>This holds for all $\varepsilon &gt; 0$, so it must be the</span>
<span id="cb28-568"><a href="#cb28-568" aria-hidden="true" tabindex="-1"></a>case that $\lim_{n\to\infty} F_{X_n}(x) = F_X(x)$</span>
<span id="cb28-569"><a href="#cb28-569" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-570"><a href="#cb28-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-571"><a href="#cb28-571" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Of the different concepts of stochastic convergence, convergence in distribution is the weakest (as the alternate name weak convergence implies). If $X_n \overset{d}{\to} X$, we're not saying that $X_n$ and $X$ become close, or that the probability that $X_n$ and $X$ are close approaches 1. We're saying that, given a common probability space $(\mathcal X, \mathcal F, P)$,^[The probability space on which the sequence of random variables is defined need not be common, but assume as much for the sake of this digression.] that the distribution function $F_{X_n}$ defined via the distribution $P(X^{-1}(I))$^[where $I\subseteq \mathcal B(\mathbb R)$] happens to converge to the same function corresponding to the random variable $X$. Furthermore, the convergence of $F_{X_n}$ to $F_X$ only needs to occur at the points which $F_X$ is continuous, *and* even then the convergence only needs to be pointwise (opposed to uniform).</span></span>
<span id="cb28-572"><a href="#cb28-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-573"><a href="#cb28-573" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-574"><a href="#cb28-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-575"><a href="#cb28-575" aria-hidden="true" tabindex="-1"></a><span class="fu">### Putting the Pieces Together</span></span>
<span id="cb28-576"><a href="#cb28-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-577"><a href="#cb28-577" aria-hidden="true" tabindex="-1"></a>The biggest takeaway from this should be the following relations:</span>
<span id="cb28-578"><a href="#cb28-578" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb28-579"><a href="#cb28-579" aria-hidden="true" tabindex="-1"></a>&amp;X_n \overset{ms}{\to}X \implies X_n \overset{p}{\to}X \implies X_n \overset{d}{\to}X<span class="sc">\\</span></span>
<span id="cb28-580"><a href="#cb28-580" aria-hidden="true" tabindex="-1"></a>&amp;X_n \overset{a.s}{\to}X \implies X_n \overset{p}{\to}X</span>
<span id="cb28-581"><a href="#cb28-581" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb28-582"><a href="#cb28-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-583"><a href="#cb28-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-584"><a href="#cb28-584" aria-hidden="true" tabindex="-1"></a><span class="fu">## Consistency</span></span>
<span id="cb28-585"><a href="#cb28-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-586"><a href="#cb28-586" aria-hidden="true" tabindex="-1"></a>Our three modes of convergence were defined for any sequence of random</span>
<span id="cb28-587"><a href="#cb28-587" aria-hidden="true" tabindex="-1"></a>variables. It should come as no surprise, considering the previous</span>
<span id="cb28-588"><a href="#cb28-588" aria-hidden="true" tabindex="-1"></a>examples considering whether the sample mean converged, that we are</span>
<span id="cb28-589"><a href="#cb28-589" aria-hidden="true" tabindex="-1"></a>interested in the convergence of estimators $\hat\theta(\mathbf{X})$ as</span>
<span id="cb28-590"><a href="#cb28-590" aria-hidden="true" tabindex="-1"></a>sample size increases. In particular we are interested in whether</span>
<span id="cb28-591"><a href="#cb28-591" aria-hidden="true" tabindex="-1"></a>$\hat\theta(\mathbf{X})$ converges to the constant $\theta\in\Theta$ it</span>
<span id="cb28-592"><a href="#cb28-592" aria-hidden="true" tabindex="-1"></a>is estimating.</span>
<span id="cb28-593"><a href="#cb28-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-594"><a href="#cb28-594" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb28-595"><a href="#cb28-595" aria-hidden="true" tabindex="-1"></a>An estimator $\hat\theta$ is [***consistent (for estimand***</span>
<span id="cb28-596"><a href="#cb28-596" aria-hidden="true" tabindex="-1"></a>$\theta$)]{style="color:red"} if $\hat\theta \overset{p}{\to}\theta$.</span>
<span id="cb28-597"><a href="#cb28-597" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-598"><a href="#cb28-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-599"><a href="#cb28-599" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; We've already seen that $\bar X$ is a consistent estimator for $\mu$</span></span>
<span id="cb28-600"><a href="#cb28-600" aria-hidden="true" tabindex="-1"></a>when we take an iid sample from a normal distribution. Let's investigate</span>
<span id="cb28-601"><a href="#cb28-601" aria-hidden="true" tabindex="-1"></a>it's variance-counterpart $S^2$.</span>
<span id="cb28-602"><a href="#cb28-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-603"><a href="#cb28-603" aria-hidden="true" tabindex="-1"></a>::: {#exm-consvarnorm}</span>
<span id="cb28-604"><a href="#cb28-604" aria-hidden="true" tabindex="-1"></a>For $X_i \overset{iid}{\sim}N(\mu,\sigma^2)$,</span>
<span id="cb28-605"><a href="#cb28-605" aria-hidden="true" tabindex="-1"></a>$S^2 = \sum_{i=1}^n (X_i - \bar X)/(n-1)$ is an unbiased estimator for</span>
<span id="cb28-606"><a href="#cb28-606" aria-hidden="true" tabindex="-1"></a>$\sigma^2$. This estimator's MSE (which is just its variance as it is</span>
<span id="cb28-607"><a href="#cb28-607" aria-hidden="true" tabindex="-1"></a>unbiased) is $2\sigma^4/(n-1)$ which converges to $0$ as $n\to\infty$,</span>
<span id="cb28-608"><a href="#cb28-608" aria-hidden="true" tabindex="-1"></a>so $S^2$ is consistent by @prp-conv.</span>
<span id="cb28-609"><a href="#cb28-609" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-610"><a href="#cb28-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-611"><a href="#cb28-611" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This example highlights the fact that proving an unbiased estimator is</span></span>
<span id="cb28-612"><a href="#cb28-612" aria-hidden="true" tabindex="-1"></a>consistent is a matter of showing its variance converges to 0.</span>
<span id="cb28-613"><a href="#cb28-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-614"><a href="#cb28-614" aria-hidden="true" tabindex="-1"></a>::: {#cor-unbcon}</span>
<span id="cb28-615"><a href="#cb28-615" aria-hidden="true" tabindex="-1"></a>Suppose $\hat\theta$ is an unbiased estimator for $\theta$. Then</span>
<span id="cb28-616"><a href="#cb28-616" aria-hidden="true" tabindex="-1"></a>$\hat\theta$ is consistent *if and only if*</span>
<span id="cb28-617"><a href="#cb28-617" aria-hidden="true" tabindex="-1"></a>$\text{Var}\left(\hat\theta\right)\to 0$.</span>
<span id="cb28-618"><a href="#cb28-618" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-619"><a href="#cb28-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-620"><a href="#cb28-620" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb28-621"><a href="#cb28-621" aria-hidden="true" tabindex="-1"></a>Apply @cor-mseconv to an unbiased estimator.</span>
<span id="cb28-622"><a href="#cb28-622" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-623"><a href="#cb28-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-624"><a href="#cb28-624" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A second type of convergence related to estimators pertains to the bias</span></span>
<span id="cb28-625"><a href="#cb28-625" aria-hidden="true" tabindex="-1"></a>of an estimator. In @sec-est we saw a few estimators that were biased,</span>
<span id="cb28-626"><a href="#cb28-626" aria-hidden="true" tabindex="-1"></a>but this bias was such that it diminished as $n\to\infty$. In effect,</span>
<span id="cb28-627"><a href="#cb28-627" aria-hidden="true" tabindex="-1"></a>they were unbiased in an asymptotic sense.</span>
<span id="cb28-628"><a href="#cb28-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-629"><a href="#cb28-629" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb28-630"><a href="#cb28-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-631"><a href="#cb28-631" aria-hidden="true" tabindex="-1"></a>An estimator $\hat\theta$ is [***asymptotically</span>
<span id="cb28-632"><a href="#cb28-632" aria-hidden="true" tabindex="-1"></a>unbiased***]{style="color:red"} if</span>
<span id="cb28-633"><a href="#cb28-633" aria-hidden="true" tabindex="-1"></a>$\lim_{n\to\infty}\text{Bias}(\hat\theta, \theta) = 0$.</span>
<span id="cb28-634"><a href="#cb28-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-635"><a href="#cb28-635" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-636"><a href="#cb28-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-637"><a href="#cb28-637" aria-hidden="true" tabindex="-1"></a>:::{#exm}</span>
<span id="cb28-638"><a href="#cb28-638" aria-hidden="true" tabindex="-1"></a>For $X_i \overset{iid}{\sim}N(\mu,\sigma^2)$,</span>
<span id="cb28-639"><a href="#cb28-639" aria-hidden="true" tabindex="-1"></a>$\hat\theta = \sum_{i=1}^n (X_i - \bar X)/n$ is a biased estimator for</span>
<span id="cb28-640"><a href="#cb28-640" aria-hidden="true" tabindex="-1"></a>$\sigma^2$. Its bias is</span>
<span id="cb28-641"><a href="#cb28-641" aria-hidden="true" tabindex="-1"></a>$$\text{Bias}(\hat\theta, \sigma^2) = \frac{n-1}{n}\sigma^2 - \sigma^2.$$</span>
<span id="cb28-642"><a href="#cb28-642" aria-hidden="true" tabindex="-1"></a>As $n\to\infty$, this bias vanishes. To illustrate this, we can simulate</span>
<span id="cb28-643"><a href="#cb28-643" aria-hidden="true" tabindex="-1"></a>estimates for various sample sizes, taking $X_i \sim N(0,1)$.</span>
<span id="cb28-644"><a href="#cb28-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-647"><a href="#cb28-647" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-648"><a href="#cb28-648" aria-hidden="true" tabindex="-1"></a>draw_estimate <span class="ot">&lt;-</span> <span class="cf">function</span>(theta, n, dist, dist_params, s){</span>
<span id="cb28-649"><a href="#cb28-649" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">do.call</span>(dist, <span class="fu">append</span>(n, dist_params))</span>
<span id="cb28-650"><a href="#cb28-650" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb28-651"><a href="#cb28-651" aria-hidden="true" tabindex="-1"></a>    <span class="at">sample_size =</span> n,</span>
<span id="cb28-652"><a href="#cb28-652" aria-hidden="true" tabindex="-1"></a>    <span class="at">iter_num =</span> s,</span>
<span id="cb28-653"><a href="#cb28-653" aria-hidden="true" tabindex="-1"></a>    <span class="at">estimate =</span> <span class="fu">theta</span>(X)</span>
<span id="cb28-654"><a href="#cb28-654" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb28-655"><a href="#cb28-655" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb28-656"><a href="#cb28-656" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb28-657"><a href="#cb28-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-658"><a href="#cb28-658" aria-hidden="true" tabindex="-1"></a>draw_N_estimates <span class="ot">&lt;-</span> <span class="cf">function</span>(theta, N, n, dist, dist_params){</span>
<span id="cb28-659"><a href="#cb28-659" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>N <span class="sc">%&gt;%</span> </span>
<span id="cb28-660"><a href="#cb28-660" aria-hidden="true" tabindex="-1"></a>    <span class="fu">map_df</span>(\(s) <span class="fu">draw_estimate</span>(theta, n, dist, dist_params, s)) </span>
<span id="cb28-661"><a href="#cb28-661" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb28-662"><a href="#cb28-662" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb28-663"><a href="#cb28-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-664"><a href="#cb28-664" aria-hidden="true" tabindex="-1"></a>draw_N_estimates_over_n <span class="ot">&lt;-</span> <span class="cf">function</span>(theta, N, n_vals, dist, dist_params){</span>
<span id="cb28-665"><a href="#cb28-665" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> n_vals <span class="sc">%&gt;%</span> </span>
<span id="cb28-666"><a href="#cb28-666" aria-hidden="true" tabindex="-1"></a>    <span class="fu">map_df</span>(\(n) <span class="fu">draw_N_estimates</span>(theta, N, n, dist, dist_params)) </span>
<span id="cb28-667"><a href="#cb28-667" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb28-668"><a href="#cb28-668" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb28-669"><a href="#cb28-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-670"><a href="#cb28-670" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">draw_N_estimates_over_n</span>(</span>
<span id="cb28-671"><a href="#cb28-671" aria-hidden="true" tabindex="-1"></a>  <span class="at">theta =</span> <span class="cf">function</span>(X){<span class="fu">sum</span>((X <span class="sc">-</span> <span class="fu">mean</span>(X))<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="fu">length</span>(X)},</span>
<span id="cb28-672"><a href="#cb28-672" aria-hidden="true" tabindex="-1"></a>  <span class="at">N =</span> <span class="fl">1e6</span>,</span>
<span id="cb28-673"><a href="#cb28-673" aria-hidden="true" tabindex="-1"></a>  <span class="at">n_vals =</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">25</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>),</span>
<span id="cb28-674"><a href="#cb28-674" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist =</span> rnorm,</span>
<span id="cb28-675"><a href="#cb28-675" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist_params =</span> <span class="fu">list</span>(</span>
<span id="cb28-676"><a href="#cb28-676" aria-hidden="true" tabindex="-1"></a>    <span class="at">mean =</span> <span class="dv">0</span>,</span>
<span id="cb28-677"><a href="#cb28-677" aria-hidden="true" tabindex="-1"></a>    <span class="at">sd =</span> <span class="dv">1</span></span>
<span id="cb28-678"><a href="#cb28-678" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb28-679"><a href="#cb28-679" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb28-680"><a href="#cb28-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-681"><a href="#cb28-681" aria-hidden="true" tabindex="-1"></a><span class="co">#Print bias calculated over 1,000,000 simulations</span></span>
<span id="cb28-682"><a href="#cb28-682" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb28-683"><a href="#cb28-683" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(sample_size) <span class="sc">%&gt;%</span> </span>
<span id="cb28-684"><a href="#cb28-684" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">bias =</span> <span class="fu">mean</span>(estimate) <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb28-685"><a href="#cb28-685" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-686"><a href="#cb28-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-689"><a href="#cb28-689" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-690"><a href="#cb28-690" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-691"><a href="#cb28-691" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot26</span></span>
<span id="cb28-692"><a href="#cb28-692" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb28-693"><a href="#cb28-693" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb28-694"><a href="#cb28-694" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-695"><a href="#cb28-695" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "As the sample size increases, the bias of our estimator converges to zero."</span></span>
<span id="cb28-696"><a href="#cb28-696" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb28-697"><a href="#cb28-697" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb28-698"><a href="#cb28-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-699"><a href="#cb28-699" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb28-700"><a href="#cb28-700" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(estimate, <span class="at">color =</span> <span class="fu">as.factor</span>(sample_size))) <span class="sc">+</span></span>
<span id="cb28-701"><a href="#cb28-701" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>() <span class="sc">+</span></span>
<span id="cb28-702"><a href="#cb28-702" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>,<span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb28-703"><a href="#cb28-703" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb28-704"><a href="#cb28-704" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb28-705"><a href="#cb28-705" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Estimates of Variance, True Value = 1"</span>, <span class="at">y =</span> <span class="st">"Density"</span>, <span class="at">color =</span> <span class="st">"Sample Size"</span>)</span>
<span id="cb28-706"><a href="#cb28-706" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-707"><a href="#cb28-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-708"><a href="#cb28-708" aria-hidden="true" tabindex="-1"></a>The estimator $\hat\theta$ is also consistent, as it converges in mean</span>
<span id="cb28-709"><a href="#cb28-709" aria-hidden="true" tabindex="-1"></a>square.</span>
<span id="cb28-710"><a href="#cb28-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-711"><a href="#cb28-711" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-712"><a href="#cb28-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-713"><a href="#cb28-713" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Asymptotic unbiasedness does not imply consistency, and</span></span>
<span id="cb28-714"><a href="#cb28-714" aria-hidden="true" tabindex="-1"></a>consistency does not imply asymptotic unbiasedness.</span>
<span id="cb28-715"><a href="#cb28-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-716"><a href="#cb28-716" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb28-717"><a href="#cb28-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-718"><a href="#cb28-718" aria-hidden="true" tabindex="-1"></a><span class="fu">## Consistent, Not Asymptotically Unbiased</span></span>
<span id="cb28-719"><a href="#cb28-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-720"><a href="#cb28-720" aria-hidden="true" tabindex="-1"></a>Recall that the sequence of discrete random variables $X_n$ with denisty</span>
<span id="cb28-721"><a href="#cb28-721" aria-hidden="true" tabindex="-1"></a>$$ f_{X_n}(x) = \begin{cases}1-1/n&amp; x=0<span class="sc">\\</span> 1/n &amp; x = n^2 \end{cases}.$$</span>
<span id="cb28-722"><a href="#cb28-722" aria-hidden="true" tabindex="-1"></a>We established that $X_n\overset{p}{\to}0$, so an estimator with this</span>
<span id="cb28-723"><a href="#cb28-723" aria-hidden="true" tabindex="-1"></a>distribution would be a consistent estimator for $0$. Despite this, the</span>
<span id="cb28-724"><a href="#cb28-724" aria-hidden="true" tabindex="-1"></a>estimator would not be asymptotically unbiased, as</span>
<span id="cb28-725"><a href="#cb28-725" aria-hidden="true" tabindex="-1"></a>$\text{E}\left<span class="co">[</span><span class="ot">X_n\right</span><span class="co">]</span> = n$, which tends to infinity as $n$ grows.</span>
<span id="cb28-726"><a href="#cb28-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-727"><a href="#cb28-727" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-728"><a href="#cb28-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-729"><a href="#cb28-729" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb28-730"><a href="#cb28-730" aria-hidden="true" tabindex="-1"></a><span class="fu">## Asymptotically Unbiased, Not Consistent</span></span>
<span id="cb28-731"><a href="#cb28-731" aria-hidden="true" tabindex="-1"></a>For $X_i\overset{iid}{\sim}N(\mu,\sigma^2)$, define the estimator</span>
<span id="cb28-732"><a href="#cb28-732" aria-hidden="true" tabindex="-1"></a>$\hat\mu(\mathbf{X}) = X_1$. We simply take the first observation to be</span>
<span id="cb28-733"><a href="#cb28-733" aria-hidden="true" tabindex="-1"></a>our estimate of $\mu$. This estimator is unbiased,</span>
<span id="cb28-734"><a href="#cb28-734" aria-hidden="true" tabindex="-1"></a>$$\text{E}\left<span class="co">[</span><span class="ot">\hat\mu\right</span><span class="co">]</span> = \text{E}\left<span class="co">[</span><span class="ot">X_1\right</span><span class="co">]</span> = \mu,$$ so it</span>
<span id="cb28-735"><a href="#cb28-735" aria-hidden="true" tabindex="-1"></a>is asymptotically unbiased. Nevertheless, the estimator fails to be</span>
<span id="cb28-736"><a href="#cb28-736" aria-hidden="true" tabindex="-1"></a>consistent, as</span>
<span id="cb28-737"><a href="#cb28-737" aria-hidden="true" tabindex="-1"></a>$$\lim_{n\to\infty} \Pr (|\hat\mu - \mu| &gt; \varepsilon)= \lim_{n\to\infty} \left<span class="sc">\{</span>1 - 2\left<span class="co">[</span><span class="ot">\Phi\left(\frac{\varepsilon}{\sigma}\right) - \frac{1}{2}\right</span><span class="co">]</span>\right<span class="sc">\}</span> \neq 0 .$$</span>
<span id="cb28-738"><a href="#cb28-738" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-739"><a href="#cb28-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-740"><a href="#cb28-740" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The incompatibility of asymptotic unbiasedness and consistency is due to</span></span>
<span id="cb28-741"><a href="#cb28-741" aria-hidden="true" tabindex="-1"></a>the behavior of $\text{Var}\left(X_n\right)$ as $n\to\infty$.</span>
<span id="cb28-742"><a href="#cb28-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-743"><a href="#cb28-743" aria-hidden="true" tabindex="-1"></a>::: {#prp-consbias}</span>
<span id="cb28-744"><a href="#cb28-744" aria-hidden="true" tabindex="-1"></a><span class="fu">## Relating Consistency and Asymptotic Unbiasedness</span></span>
<span id="cb28-745"><a href="#cb28-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-746"><a href="#cb28-746" aria-hidden="true" tabindex="-1"></a>Suppose $\hat\theta$ is an estimator for $\theta$.</span>
<span id="cb28-747"><a href="#cb28-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-748"><a href="#cb28-748" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>If $\hat\theta$ is consistent and there exists some $M$ such that</span>
<span id="cb28-749"><a href="#cb28-749" aria-hidden="true" tabindex="-1"></a>    $\text{Var}\left(\hat\theta\right) \le M$ for all $n$ (bounded</span>
<span id="cb28-750"><a href="#cb28-750" aria-hidden="true" tabindex="-1"></a>    variance), then $\hat\theta$ is asymptotically unbiased.</span>
<span id="cb28-751"><a href="#cb28-751" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>If $\hat\theta$ is asymptotically unbiased and</span>
<span id="cb28-752"><a href="#cb28-752" aria-hidden="true" tabindex="-1"></a>    $\lim_{n\to\infty}\text{Var}\left(\hat\theta\right) = 0$ (vanishing</span>
<span id="cb28-753"><a href="#cb28-753" aria-hidden="true" tabindex="-1"></a>    variance), then $\hat\theta$ is consistent</span>
<span id="cb28-754"><a href="#cb28-754" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-755"><a href="#cb28-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-756"><a href="#cb28-756" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb28-757"><a href="#cb28-757" aria-hidden="true" tabindex="-1"></a>test</span>
<span id="cb28-758"><a href="#cb28-758" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-759"><a href="#cb28-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-762"><a href="#cb28-762" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-763"><a href="#cb28-763" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb28-764"><a href="#cb28-764" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot27</span></span>
<span id="cb28-765"><a href="#cb28-765" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb28-766"><a href="#cb28-766" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-767"><a href="#cb28-767" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Relationship between various concepts of convergence in the context of estimators"</span></span>
<span id="cb28-768"><a href="#cb28-768" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb28-769"><a href="#cb28-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-770"><a href="#cb28-770" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"figures/relating_convergence.png"</span>)</span>
<span id="cb28-771"><a href="#cb28-771" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-772"><a href="#cb28-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-773"><a href="#cb28-773" aria-hidden="true" tabindex="-1"></a><span class="fu">## Laws of Large Numbers</span></span>
<span id="cb28-774"><a href="#cb28-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-775"><a href="#cb28-775" aria-hidden="true" tabindex="-1"></a>In most examples until now, the properties of estimators were implicitly</span>
<span id="cb28-776"><a href="#cb28-776" aria-hidden="true" tabindex="-1"></a>a function of the underlying model the data is generated from. We</span>
<span id="cb28-777"><a href="#cb28-777" aria-hidden="true" tabindex="-1"></a>established that $\bar X$ and $S^2$ are consistent estimators for $\mu$</span>
<span id="cb28-778"><a href="#cb28-778" aria-hidden="true" tabindex="-1"></a>and $\sigma^2$, respectively, *when*</span>
<span id="cb28-779"><a href="#cb28-779" aria-hidden="true" tabindex="-1"></a>$X_i \overset{iid}{\sim}N(\mu,\sigma^2)$. We know the distribution of</span>
<span id="cb28-780"><a href="#cb28-780" aria-hidden="true" tabindex="-1"></a>$\bar X$, *when* $X_i \overset{iid}{\sim}N(\mu,\sigma^2)$. In an ideal</span>
<span id="cb28-781"><a href="#cb28-781" aria-hidden="true" tabindex="-1"></a>world, we would be able to establish desirable properties of estimators</span>
<span id="cb28-782"><a href="#cb28-782" aria-hidden="true" tabindex="-1"></a>under more robust settings where our specified model may include a wide</span>
<span id="cb28-783"><a href="#cb28-783" aria-hidden="true" tabindex="-1"></a>array of distributions. Our first step in doing this will be introducing</span>
<span id="cb28-784"><a href="#cb28-784" aria-hidden="true" tabindex="-1"></a>variants of one of the most important results in all of probability -- the law of</span>
<span id="cb28-785"><a href="#cb28-785" aria-hidden="true" tabindex="-1"></a>large numbers (LLN). In the context of estimation, LLNs give sufficient conditions for our favorite estimator, $\bar X$, to be consistent. </span>
<span id="cb28-786"><a href="#cb28-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-787"><a href="#cb28-787" aria-hidden="true" tabindex="-1"></a><span class="fu">### Khinchine's Weak Law of Large Numbers</span></span>
<span id="cb28-788"><a href="#cb28-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-789"><a href="#cb28-789" aria-hidden="true" tabindex="-1"></a>The version of the LLN we'll use the most often deals with convergence in probability. To prove that $\bar X \overset{p}{\to}\mu$ we'll rely on an inequality similar to</span>
<span id="cb28-790"><a href="#cb28-790" aria-hidden="true" tabindex="-1"></a>@lem-markovineq.</span>
<span id="cb28-791"><a href="#cb28-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-792"><a href="#cb28-792" aria-hidden="true" tabindex="-1"></a>::: {#lem-}</span>
<span id="cb28-793"><a href="#cb28-793" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chebyshev's Inequality</span></span>
<span id="cb28-794"><a href="#cb28-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-795"><a href="#cb28-795" aria-hidden="true" tabindex="-1"></a>If $X$ is a random variable with an expected value $\mu$ and variance</span>
<span id="cb28-796"><a href="#cb28-796" aria-hidden="true" tabindex="-1"></a>$\sigma^2$, then for all $a &gt; 0$</span>
<span id="cb28-797"><a href="#cb28-797" aria-hidden="true" tabindex="-1"></a>$$\Pr(|X - \mu| \ge k) \le \frac{\sigma^2}{k^2}.$$</span>
<span id="cb28-798"><a href="#cb28-798" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-799"><a href="#cb28-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-800"><a href="#cb28-800" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb28-801"><a href="#cb28-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-802"><a href="#cb28-802" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb28-803"><a href="#cb28-803" aria-hidden="true" tabindex="-1"></a>\Pr(|X - \mu| \ge k) &amp;= \Pr((X - \mu)^2 \ge k^2)<span class="sc">\\</span></span>
<span id="cb28-804"><a href="#cb28-804" aria-hidden="true" tabindex="-1"></a>&amp; \le \frac{\text{E}\left<span class="co">[</span><span class="ot">(X-\mu)^2\right</span><span class="co">]</span>}{k^2} &amp; (\text{Markov's Inequality})<span class="sc">\\</span></span>
<span id="cb28-805"><a href="#cb28-805" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{\sigma^2}{k^2}</span>
<span id="cb28-806"><a href="#cb28-806" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb28-807"><a href="#cb28-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-808"><a href="#cb28-808" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-809"><a href="#cb28-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-810"><a href="#cb28-810" aria-hidden="true" tabindex="-1"></a>::: {#thm-}</span>
<span id="cb28-811"><a href="#cb28-811" aria-hidden="true" tabindex="-1"></a><span class="fu">## LLN I</span></span>
<span id="cb28-812"><a href="#cb28-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-813"><a href="#cb28-813" aria-hidden="true" tabindex="-1"></a>If $(X_1,\ldots, X_n)$ are a set of iid random variables where</span>
<span id="cb28-814"><a href="#cb28-814" aria-hidden="true" tabindex="-1"></a>$\text{E}\left<span class="co">[</span><span class="ot">X_i\right</span><span class="co">]</span> = \mu$, then $\bar X \overset{p}{\to}\mu$.</span>
<span id="cb28-815"><a href="#cb28-815" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-816"><a href="#cb28-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-817"><a href="#cb28-817" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb28-818"><a href="#cb28-818" aria-hidden="true" tabindex="-1"></a>Recall that $\text{Var}\left(\bar X\right) = \sigma^2/n$. By Chebyshev's</span>
<span id="cb28-819"><a href="#cb28-819" aria-hidden="true" tabindex="-1"></a>Inequality, \begin{align*}</span>
<span id="cb28-820"><a href="#cb28-820" aria-hidden="true" tabindex="-1"></a>\lim_{n\to\infty}\Pr(|X_n - \mu| \ge \varepsilon) \le \lim_{n\to\infty}\frac{(\sigma^2/n)}{\varepsilon^2} =   \lim_{n\to\infty} \frac{\sigma^2}{n\varepsilon} = 0.</span>
<span id="cb28-821"><a href="#cb28-821" aria-hidden="true" tabindex="-1"></a>\end{align*} Therefore, $\bar X\overset{p}{\to}\mu$.</span>
<span id="cb28-822"><a href="#cb28-822" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-823"><a href="#cb28-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-824"><a href="#cb28-824" aria-hidden="true" tabindex="-1"></a>:::{#exm-lln1}</span>
<span id="cb28-825"><a href="#cb28-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-826"><a href="#cb28-826" aria-hidden="true" tabindex="-1"></a>To illustrate the LLN, let's simulate realizations of iid random</span>
<span id="cb28-827"><a href="#cb28-827" aria-hidden="true" tabindex="-1"></a>variables from a series of different distributions and show that</span>
<span id="cb28-828"><a href="#cb28-828" aria-hidden="true" tabindex="-1"></a>regardless of the distribution, $\bar X \to \mu$. We will use the</span>
<span id="cb28-829"><a href="#cb28-829" aria-hidden="true" tabindex="-1"></a>following distributions: \begin{align*}</span>
<span id="cb28-830"><a href="#cb28-830" aria-hidden="true" tabindex="-1"></a>X_i &amp; \overset{iid}{\sim}\text{Exp}(1/\mu)<span class="sc">\\</span></span>
<span id="cb28-831"><a href="#cb28-831" aria-hidden="true" tabindex="-1"></a>X_i &amp; \overset{iid}{\sim}\chi_\mu^2<span class="sc">\\</span></span>
<span id="cb28-832"><a href="#cb28-832" aria-hidden="true" tabindex="-1"></a>X_i &amp; \overset{iid}{\sim}\text{Uni}(0, 2\mu)<span class="sc">\\</span></span>
<span id="cb28-833"><a href="#cb28-833" aria-hidden="true" tabindex="-1"></a>X_i &amp; \overset{iid}{\sim}\text{Gamma}(2\mu, 2)<span class="sc">\\</span></span>
<span id="cb28-834"><a href="#cb28-834" aria-hidden="true" tabindex="-1"></a>X_i &amp; \overset{iid}{\sim}\text{HyperGeo}(10, 20, 15\mu)</span>
<span id="cb28-835"><a href="#cb28-835" aria-hidden="true" tabindex="-1"></a>\end{align*} All these distributions have been selected such that</span>
<span id="cb28-836"><a href="#cb28-836" aria-hidden="true" tabindex="-1"></a>$\text{E}\left<span class="co">[</span><span class="ot">X_i\right</span><span class="co">]</span> = \mu$. For our simulations, we will take</span>
<span id="cb28-837"><a href="#cb28-837" aria-hidden="true" tabindex="-1"></a>$\mu = 5$. If we plot the value of the sample mean versus the sample</span>
<span id="cb28-838"><a href="#cb28-838" aria-hidden="true" tabindex="-1"></a>size $n$, we see that the values converge to the true value $\mu = 5$</span>
<span id="cb28-839"><a href="#cb28-839" aria-hidden="true" tabindex="-1"></a>regardless of the distribution of $X_i$.</span>
<span id="cb28-840"><a href="#cb28-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-843"><a href="#cb28-843" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-844"><a href="#cb28-844" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-845"><a href="#cb28-845" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot28</span></span>
<span id="cb28-846"><a href="#cb28-846" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb28-847"><a href="#cb28-847" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb28-848"><a href="#cb28-848" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-849"><a href="#cb28-849" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The sample mean of all samples converges to the population mean by the LLN"</span></span>
<span id="cb28-850"><a href="#cb28-850" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb28-851"><a href="#cb28-851" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb28-852"><a href="#cb28-852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-853"><a href="#cb28-853" aria-hidden="true" tabindex="-1"></a>mean_n_increases <span class="ot">&lt;-</span> <span class="cf">function</span>(n, dist, dist_label, dist_params, t){</span>
<span id="cb28-854"><a href="#cb28-854" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb28-855"><a href="#cb28-855" aria-hidden="true" tabindex="-1"></a>    <span class="at">sample_size =</span> <span class="dv">1</span><span class="sc">:</span>n,</span>
<span id="cb28-856"><a href="#cb28-856" aria-hidden="true" tabindex="-1"></a>    <span class="at">prob_dist =</span> dist_label,</span>
<span id="cb28-857"><a href="#cb28-857" aria-hidden="true" tabindex="-1"></a>    <span class="at">random_values =</span> <span class="fu">do.call</span>(dist, <span class="fu">append</span>(n, dist_params)),</span>
<span id="cb28-858"><a href="#cb28-858" aria-hidden="true" tabindex="-1"></a>    <span class="at">iter_num =</span> t</span>
<span id="cb28-859"><a href="#cb28-859" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">%&gt;%</span> </span>
<span id="cb28-860"><a href="#cb28-860" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">estimate =</span> <span class="fu">cummean</span>(random_values))</span>
<span id="cb28-861"><a href="#cb28-861" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb28-862"><a href="#cb28-862" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb28-863"><a href="#cb28-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-864"><a href="#cb28-864" aria-hidden="true" tabindex="-1"></a><span class="fu">list</span>(</span>
<span id="cb28-865"><a href="#cb28-865" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(rexp,  rchisq, runif, rgamma, rhyper),</span>
<span id="cb28-866"><a href="#cb28-866" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="st">"rexp"</span>, <span class="st">"rchisq"</span>, <span class="st">"runif"</span>, <span class="st">"rgamma"</span>, <span class="st">"rhyper"</span>),</span>
<span id="cb28-867"><a href="#cb28-867" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="fu">list</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">5</span>), <span class="fu">list</span>(<span class="dv">5</span>), <span class="fu">list</span>(<span class="dv">0</span>, <span class="dv">10</span>), <span class="fu">list</span>(<span class="dv">10</span>, <span class="dv">2</span>), <span class="fu">list</span>(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">15</span>))</span>
<span id="cb28-868"><a href="#cb28-868" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb28-869"><a href="#cb28-869" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pmap_df</span>(\(dist, dist_label, dist_params) <span class="fu">mean_n_increases</span>(<span class="at">n =</span> <span class="fl">1e5</span>, dist, dist_label, dist_params, <span class="at">t =</span> <span class="dv">1</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb28-870"><a href="#cb28-870" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(sample_size, estimate, <span class="at">color =</span> prob_dist)) <span class="sc">+</span></span>
<span id="cb28-871"><a href="#cb28-871" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">alpha=</span><span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb28-872"><a href="#cb28-872" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb28-873"><a href="#cb28-873" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Sample Size, n"</span>, <span class="at">y =</span> <span class="st">"Sample Mean"</span>, <span class="at">color =</span> <span class="st">"Distribution of iid Random Sample"</span>) <span class="sc">+</span></span>
<span id="cb28-874"><a href="#cb28-874" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb28-875"><a href="#cb28-875" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb28-876"><a href="#cb28-876" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="fl">4.5</span>, <span class="fl">5.5</span>)</span>
<span id="cb28-877"><a href="#cb28-877" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-878"><a href="#cb28-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-879"><a href="#cb28-879" aria-hidden="true" tabindex="-1"></a>For the sake of an even better illustration, let's focus on the case where $X_i \overset{iid}{\sim}\text{Exp}(1/\mu)$ where $\mu = 5$. We'll draw 10,000 samples of $X_i$, and calculate $\bar X$ for each sample as the sample size ranges from 1 all the way to 1,000.</span>
<span id="cb28-880"><a href="#cb28-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-883"><a href="#cb28-883" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-884"><a href="#cb28-884" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-885"><a href="#cb28-885" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot28.2</span></span>
<span id="cb28-886"><a href="#cb28-886" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb28-887"><a href="#cb28-887" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb28-888"><a href="#cb28-888" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-889"><a href="#cb28-889" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "10,000 lines, each of which corresponds to a simulated realization of the sample mean over a range of sample sizes. The area within the red dashed lines is the set |μ - Sample Mean| &lt; 0.5"</span></span>
<span id="cb28-890"><a href="#cb28-890" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb28-891"><a href="#cb28-891" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb28-892"><a href="#cb28-892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-893"><a href="#cb28-893" aria-hidden="true" tabindex="-1"></a>mean_n_increases_N_draws <span class="ot">&lt;-</span> <span class="cf">function</span>(N, n, dist, dist_label, dist_params){</span>
<span id="cb28-894"><a href="#cb28-894" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>N <span class="sc">%&gt;%</span> </span>
<span id="cb28-895"><a href="#cb28-895" aria-hidden="true" tabindex="-1"></a>    <span class="fu">map_df</span>(\(t) <span class="fu">mean_n_increases</span>(n, dist, dist_label, dist_params, t))</span>
<span id="cb28-896"><a href="#cb28-896" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb28-897"><a href="#cb28-897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-898"><a href="#cb28-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-899"><a href="#cb28-899" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">mean_n_increases_N_draws</span>(</span>
<span id="cb28-900"><a href="#cb28-900" aria-hidden="true" tabindex="-1"></a>  <span class="at">N =</span> <span class="fl">1e4</span>, </span>
<span id="cb28-901"><a href="#cb28-901" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="fl">1e3</span>, </span>
<span id="cb28-902"><a href="#cb28-902" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist =</span> rexp, </span>
<span id="cb28-903"><a href="#cb28-903" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist_label =</span> <span class="st">"rexp"</span>, </span>
<span id="cb28-904"><a href="#cb28-904" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist_params =</span> <span class="fu">list</span>(</span>
<span id="cb28-905"><a href="#cb28-905" aria-hidden="true" tabindex="-1"></a>    <span class="at">rate =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">5</span></span>
<span id="cb28-906"><a href="#cb28-906" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb28-907"><a href="#cb28-907" aria-hidden="true" tabindex="-1"></a>) </span>
<span id="cb28-908"><a href="#cb28-908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-909"><a href="#cb28-909" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb28-910"><a href="#cb28-910" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> sample_size, <span class="at">y =</span> estimate)) <span class="sc">+</span></span>
<span id="cb28-911"><a href="#cb28-911" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">group =</span> iter_num), <span class="at">alpha =</span> <span class="fl">0.1</span>, <span class="at">size =</span> <span class="fl">0.05</span>) <span class="sc">+</span></span>
<span id="cb28-912"><a href="#cb28-912" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="dv">15</span>) <span class="sc">+</span> </span>
<span id="cb28-913"><a href="#cb28-913" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">5</span> <span class="sc">-</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linetype=</span><span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb28-914"><a href="#cb28-914" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">5</span> <span class="sc">+</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linetype=</span><span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb28-915"><a href="#cb28-915" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb28-916"><a href="#cb28-916" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-917"><a href="#cb28-917" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-918"><a href="#cb28-918" aria-hidden="true" tabindex="-1"></a>The weak LLN tells us that since $\bar X\pto 5$, then by the definition of convergence in probability, the probability one of the lines falls in @fig-plot28.2 outside the interval within the red dashed lines at a particular sample size $n$ approaches zero as $n\to \infty$. Furthermore, this will hold regardless of how close we make the red lines to the true value $\mu = 5$. For the sake of illustration we took $\varepsilon = 0.5$, but it will hold *for all* $\varepsilon &gt; 0$. In fact, we can go one step further and illustrate $\Pr(|\bar X-\mu| &gt; \varepsilon)$ by looking at the proportion of times that $|\bar X-\mu| &gt; \varepsilon$ holds across out 10,000 simulations. We'll do this for $\varepsilon=0.5,0.6,\ldots,1$.</span>
<span id="cb28-919"><a href="#cb28-919" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-922"><a href="#cb28-922" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-923"><a href="#cb28-923" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-924"><a href="#cb28-924" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot28.3</span></span>
<span id="cb28-925"><a href="#cb28-925" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb28-926"><a href="#cb28-926" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb28-927"><a href="#cb28-927" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-928"><a href="#cb28-928" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "For each ε, the observed probability in question approaches 0 as the sample size grows."</span></span>
<span id="cb28-929"><a href="#cb28-929" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb28-930"><a href="#cb28-930" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb28-931"><a href="#cb28-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-932"><a href="#cb28-932" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb28-933"><a href="#cb28-933" aria-hidden="true" tabindex="-1"></a>  <span class="fu">expand_grid</span>(<span class="at">epsilon =</span> <span class="dv">5</span><span class="sc">:</span><span class="dv">10</span><span class="sc">/</span><span class="dv">10</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb28-934"><a href="#cb28-934" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">outside =</span> estimate <span class="sc">&gt;=</span> <span class="dv">5</span> <span class="sc">+</span> epsilon <span class="sc">|</span> estimate <span class="sc">&lt;=</span> <span class="dv">5</span> <span class="sc">-</span> epsilon) <span class="sc">%&gt;%</span> </span>
<span id="cb28-935"><a href="#cb28-935" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(sample_size, epsilon) <span class="sc">%&gt;%</span> </span>
<span id="cb28-936"><a href="#cb28-936" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">prob =</span> <span class="fu">sum</span>(outside) <span class="sc">/</span> <span class="fu">n</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb28-937"><a href="#cb28-937" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> sample_size, prob, <span class="at">color =</span> <span class="fu">factor</span>(epsilon))) <span class="sc">+</span></span>
<span id="cb28-938"><a href="#cb28-938" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb28-939"><a href="#cb28-939" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb28-940"><a href="#cb28-940" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb28-941"><a href="#cb28-941" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">"Observed Pr(|Estimate-μ|&gt;ε)"</span>, <span class="at">x =</span> <span class="st">"Sample Size n"</span>, <span class="at">color =</span> <span class="st">"ε"</span>)</span>
<span id="cb28-942"><a href="#cb28-942" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-943"><a href="#cb28-943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-944"><a href="#cb28-944" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-945"><a href="#cb28-945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-946"><a href="#cb28-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-947"><a href="#cb28-947" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb28-948"><a href="#cb28-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-949"><a href="#cb28-949" aria-hidden="true" tabindex="-1"></a><span class="fu">## Monte Carlo Simulations</span></span>
<span id="cb28-950"><a href="#cb28-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-951"><a href="#cb28-951" aria-hidden="true" tabindex="-1"></a>In @exm-var we performed a Monte Carlo simulation to illustrate the bias</span>
<span id="cb28-952"><a href="#cb28-952" aria-hidden="true" tabindex="-1"></a>of $\hat\theta(\mathbf{X}) = \sum_{i=1}^n (X_i - \bar X)/n$ and</span>
<span id="cb28-953"><a href="#cb28-953" aria-hidden="true" tabindex="-1"></a>unbiasedness of $S^2$. We did this by fixing $n=20$, drawing a random</span>
<span id="cb28-954"><a href="#cb28-954" aria-hidden="true" tabindex="-1"></a>sample, recording estimates $\hat\theta(\mathbf{x})$ and</span>
<span id="cb28-955"><a href="#cb28-955" aria-hidden="true" tabindex="-1"></a>$S^2(\mathbf{x})$, and repeating this $k$ times. This is nothing more</span>
<span id="cb28-956"><a href="#cb28-956" aria-hidden="true" tabindex="-1"></a>than drawing $j=1,\ldots,k$ observations from the random variables</span>
<span id="cb28-957"><a href="#cb28-957" aria-hidden="true" tabindex="-1"></a>$\hat\theta(\mathbf{X})$ and $S^2(\mathbf{X})$. By the LLN,</span>
<span id="cb28-958"><a href="#cb28-958" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb28-959"><a href="#cb28-959" aria-hidden="true" tabindex="-1"></a>\frac{1}{k}\sum_{i=1}^k \hat\theta_j(\mathbf{x}) &amp;\overset{p}{\to}\text{E}\left<span class="co">[</span><span class="ot">\hat\theta(\mathbf{X})\right</span><span class="co">]</span>,<span class="sc">\\</span></span>
<span id="cb28-960"><a href="#cb28-960" aria-hidden="true" tabindex="-1"></a>\frac{1}{k}\sum_{i=1}^k S_j^2(\mathbf{x}) &amp;\overset{p}{\to}\text{E}\left<span class="co">[</span><span class="ot">S^2(\mathbf{X})\right</span><span class="co">]</span>,</span>
<span id="cb28-961"><a href="#cb28-961" aria-hidden="true" tabindex="-1"></a>\end{align*} so for a large enough $k$, we can approximate the expected</span>
<span id="cb28-962"><a href="#cb28-962" aria-hidden="true" tabindex="-1"></a>value with its sample counterpart.</span>
<span id="cb28-963"><a href="#cb28-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-964"><a href="#cb28-964" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-965"><a href="#cb28-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-966"><a href="#cb28-966" aria-hidden="true" tabindex="-1"></a><span class="fu">### Kolmogorov's Strong Law of Large Numbers</span></span>
<span id="cb28-967"><a href="#cb28-967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-968"><a href="#cb28-968" aria-hidden="true" tabindex="-1"></a>A stronger version of the LLN is stated in terms of almost sure convergence. Since almost sure convergence is stronger than convergence in probability (which is all that is needed for an estimator to be consistent), this version is referred to as the "strong" LLN. </span>
<span id="cb28-969"><a href="#cb28-969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-970"><a href="#cb28-970" aria-hidden="true" tabindex="-1"></a>::: {#thm-}</span>
<span id="cb28-971"><a href="#cb28-971" aria-hidden="true" tabindex="-1"></a><span class="fu">## LLN II</span></span>
<span id="cb28-972"><a href="#cb28-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-973"><a href="#cb28-973" aria-hidden="true" tabindex="-1"></a>If $(X_1,\ldots, X_n)$ are a set of iid random variables where</span>
<span id="cb28-974"><a href="#cb28-974" aria-hidden="true" tabindex="-1"></a>$\text{E}\left<span class="co">[</span><span class="ot">X_i\right</span><span class="co">]</span> = \mu$, then $\bar X \overset{as}{\to}\mu$.</span>
<span id="cb28-975"><a href="#cb28-975" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-976"><a href="#cb28-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-977"><a href="#cb28-977" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb28-978"><a href="#cb28-978" aria-hidden="true" tabindex="-1"></a>See the proof of Theorem 6.1 in @billingsley2008probability.</span>
<span id="cb28-979"><a href="#cb28-979" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb28-980"><a href="#cb28-980" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-981"><a href="#cb28-981" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-982"><a href="#cb28-982" aria-hidden="true" tabindex="-1"></a>:::{#exm-strongvsweak}</span>
<span id="cb28-983"><a href="#cb28-983" aria-hidden="true" tabindex="-1"></a><span class="fu">## Strong vs Weak LLN</span></span>
<span id="cb28-984"><a href="#cb28-984" aria-hidden="true" tabindex="-1"></a>We can actually visualize the difference between the strong and weak LLNs, and in doing so highlight the difference between almost sure convergence and convergence in probability. Like @exm-lln1, assume $X_i \sim\text{Exp}(1/\mu)$ where $\mu = 5$. We'll perform a similar simulation to that which gave @fig-plot28.2, but this time we'll only look at one sample of $X_i$ (instead of 10,000). </span>
<span id="cb28-985"><a href="#cb28-985" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-988"><a href="#cb28-988" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-989"><a href="#cb28-989" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-990"><a href="#cb28-990" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot28.4</span></span>
<span id="cb28-991"><a href="#cb28-991" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb28-992"><a href="#cb28-992" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb28-993"><a href="#cb28-993" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-994"><a href="#cb28-994" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "A single simulation realization of the sample mean over a range of sample sizes. The area within the red dashed lines is the set |μ - Sample Mean| &lt; 0.5"</span></span>
<span id="cb28-995"><a href="#cb28-995" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb28-996"><a href="#cb28-996" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb28-997"><a href="#cb28-997" aria-hidden="true" tabindex="-1"></a><span class="fu">mean_n_increases</span>(</span>
<span id="cb28-998"><a href="#cb28-998" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="fl">1e5</span>,</span>
<span id="cb28-999"><a href="#cb28-999" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist =</span> rexp,</span>
<span id="cb28-1000"><a href="#cb28-1000" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist_label =</span> <span class="st">"rexp"</span>,</span>
<span id="cb28-1001"><a href="#cb28-1001" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist_params =</span> <span class="fu">list</span>(</span>
<span id="cb28-1002"><a href="#cb28-1002" aria-hidden="true" tabindex="-1"></a>    <span class="at">rate =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">5</span></span>
<span id="cb28-1003"><a href="#cb28-1003" aria-hidden="true" tabindex="-1"></a>  ),</span>
<span id="cb28-1004"><a href="#cb28-1004" aria-hidden="true" tabindex="-1"></a>  <span class="at">t =</span> <span class="dv">1</span></span>
<span id="cb28-1005"><a href="#cb28-1005" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb28-1006"><a href="#cb28-1006" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(sample_size, estimate)) <span class="sc">+</span> </span>
<span id="cb28-1007"><a href="#cb28-1007" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span> </span>
<span id="cb28-1008"><a href="#cb28-1008" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">5</span> <span class="sc">-</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linetype=</span><span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb28-1009"><a href="#cb28-1009" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">5</span> <span class="sc">+</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linetype=</span><span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb28-1010"><a href="#cb28-1010" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() </span>
<span id="cb28-1011"><a href="#cb28-1011" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-1012"><a href="#cb28-1012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1013"><a href="#cb28-1013" aria-hidden="true" tabindex="-1"></a>The strong LLN tells us that for some finite sample size $N$ the line in @fig-plot28.4 will fall within the red lines for all $n &gt; N$.</span>
<span id="cb28-1014"><a href="#cb28-1014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1015"><a href="#cb28-1015" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1016"><a href="#cb28-1016" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1017"><a href="#cb28-1017" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chebyshev's Weak Law of Large Numbers</span></span>
<span id="cb28-1018"><a href="#cb28-1018" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1019"><a href="#cb28-1019" aria-hidden="true" tabindex="-1"></a>The crucial assumption made by both LLNs up to this point is that $\bar X$ is calculated</span>
<span id="cb28-1020"><a href="#cb28-1020" aria-hidden="true" tabindex="-1"></a>with an iid random sample. If we drop this assumption, then $\bar X$</span>
<span id="cb28-1021"><a href="#cb28-1021" aria-hidden="true" tabindex="-1"></a>needn't estimate $\mu$ consistently.</span>
<span id="cb28-1022"><a href="#cb28-1022" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1023"><a href="#cb28-1023" aria-hidden="true" tabindex="-1"></a>::: {#exm-noiid}</span>
<span id="cb28-1024"><a href="#cb28-1024" aria-hidden="true" tabindex="-1"></a><span class="fu">## LLN Failing with Non-IID Data</span></span>
<span id="cb28-1025"><a href="#cb28-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1026"><a href="#cb28-1026" aria-hidden="true" tabindex="-1"></a>Suppose $\mathbf{X}= (X_1, \ldots, X_n)$ where $X_i \sim N(-1, i)$ if</span>
<span id="cb28-1027"><a href="#cb28-1027" aria-hidden="true" tabindex="-1"></a>$i$ is odd, and $X_i \sim N(1,i)$ is $i$ is even. The data is</span>
<span id="cb28-1028"><a href="#cb28-1028" aria-hidden="true" tabindex="-1"></a>independent, but not identically distributed. If some LLN would hold</span>
<span id="cb28-1029"><a href="#cb28-1029" aria-hidden="true" tabindex="-1"></a>here, we would suspect that $\bar X$ would converge $0$ since the average of</span>
<span id="cb28-1030"><a href="#cb28-1030" aria-hidden="true" tabindex="-1"></a>the underlying population means $1$ and $-1$. Let's simulate $\bar X$ for</span>
<span id="cb28-1031"><a href="#cb28-1031" aria-hidden="true" tabindex="-1"></a>$n$ ranging from 1 to 100,000.</span>
<span id="cb28-1032"><a href="#cb28-1032" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1035"><a href="#cb28-1035" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-1036"><a href="#cb28-1036" aria-hidden="true" tabindex="-1"></a>draw_X_i <span class="ot">&lt;-</span> <span class="cf">function</span>(i){</span>
<span id="cb28-1037"><a href="#cb28-1037" aria-hidden="true" tabindex="-1"></a>  mu_i <span class="ot">&lt;-</span> (<span class="sc">-</span><span class="dv">1</span>)<span class="sc">^</span>(i <span class="sc">%%</span> <span class="dv">2</span>) <span class="sc">*</span> (<span class="dv">1</span>)<span class="sc">^</span>(<span class="dv">1</span> <span class="sc">-</span> i <span class="sc">%%</span> <span class="dv">2</span>)</span>
<span id="cb28-1038"><a href="#cb28-1038" aria-hidden="true" tabindex="-1"></a>  sigma_i <span class="ot">&lt;-</span> i </span>
<span id="cb28-1039"><a href="#cb28-1039" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb28-1040"><a href="#cb28-1040" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, mu_i, sigma_i)</span>
<span id="cb28-1041"><a href="#cb28-1041" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb28-1042"><a href="#cb28-1042" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb28-1043"><a href="#cb28-1043" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1044"><a href="#cb28-1044" aria-hidden="true" tabindex="-1"></a>draw_X <span class="ot">&lt;-</span> <span class="cf">function</span>(n){</span>
<span id="cb28-1045"><a href="#cb28-1045" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb28-1046"><a href="#cb28-1046" aria-hidden="true" tabindex="-1"></a>    <span class="at">i =</span> <span class="dv">1</span><span class="sc">:</span>n,</span>
<span id="cb28-1047"><a href="#cb28-1047" aria-hidden="true" tabindex="-1"></a>    <span class="at">value =</span> <span class="fu">map_dbl</span>(<span class="dv">1</span><span class="sc">:</span>n, draw_X_i)</span>
<span id="cb28-1048"><a href="#cb28-1048" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb28-1049"><a href="#cb28-1049" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">estimate =</span> <span class="fu">cummean</span>(value))</span>
<span id="cb28-1050"><a href="#cb28-1050" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb28-1051"><a href="#cb28-1051" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb28-1052"><a href="#cb28-1052" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1053"><a href="#cb28-1053" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">draw_X</span>(<span class="fl">1e5</span>)</span>
<span id="cb28-1054"><a href="#cb28-1054" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-1055"><a href="#cb28-1055" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1056"><a href="#cb28-1056" aria-hidden="true" tabindex="-1"></a>We see that our estimates very much do not converge to any particular</span>
<span id="cb28-1057"><a href="#cb28-1057" aria-hidden="true" tabindex="-1"></a>value.</span>
<span id="cb28-1058"><a href="#cb28-1058" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1061"><a href="#cb28-1061" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-1062"><a href="#cb28-1062" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-1063"><a href="#cb28-1063" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot29</span></span>
<span id="cb28-1064"><a href="#cb28-1064" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb28-1065"><a href="#cb28-1065" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb28-1066"><a href="#cb28-1066" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-1067"><a href="#cb28-1067" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The sample mean of non-IID data does not satisfy the LLN"</span></span>
<span id="cb28-1068"><a href="#cb28-1068" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb28-1069"><a href="#cb28-1069" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb28-1070"><a href="#cb28-1070" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(i, estimate)) <span class="sc">+</span></span>
<span id="cb28-1071"><a href="#cb28-1071" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb28-1072"><a href="#cb28-1072" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb28-1073"><a href="#cb28-1073" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Sample Size"</span>, <span class="at">y =</span> <span class="st">"Sample Mean"</span>)</span>
<span id="cb28-1074"><a href="#cb28-1074" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-1075"><a href="#cb28-1075" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1076"><a href="#cb28-1076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1077"><a href="#cb28-1077" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The proof of the LLN relied on Chebyshev's equality and the fact that</span></span>
<span id="cb28-1078"><a href="#cb28-1078" aria-hidden="true" tabindex="-1"></a>$\text{Var}\left(\bar X\right) = \sigma^2/n \to 0$ when</span>
<span id="cb28-1079"><a href="#cb28-1079" aria-hidden="true" tabindex="-1"></a>$\text{Var}\left(X_i\right) = \sigma^2$. Perhaps if we added an</span>
<span id="cb28-1080"><a href="#cb28-1080" aria-hidden="true" tabindex="-1"></a>assumption regarding the variance of a non-iid sample, then we could</span>
<span id="cb28-1081"><a href="#cb28-1081" aria-hidden="true" tabindex="-1"></a>salvage a result similar to the LLN. This is precisely what Chebyshev's</span>
<span id="cb28-1082"><a href="#cb28-1082" aria-hidden="true" tabindex="-1"></a>LLN does.</span>
<span id="cb28-1083"><a href="#cb28-1083" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1084"><a href="#cb28-1084" aria-hidden="true" tabindex="-1"></a>:::{#prp-chebylln}</span>
<span id="cb28-1085"><a href="#cb28-1085" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1086"><a href="#cb28-1086" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chebyshev's (Weak) Law of Large Numbers</span></span>
<span id="cb28-1087"><a href="#cb28-1087" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1088"><a href="#cb28-1088" aria-hidden="true" tabindex="-1"></a>Suppose $(X_1,\ldots, X_n)$ are a sample such that</span>
<span id="cb28-1089"><a href="#cb28-1089" aria-hidden="true" tabindex="-1"></a>$\text{E}\left<span class="co">[</span><span class="ot">X_i\right</span><span class="co">]</span> = \mu_i$,</span>
<span id="cb28-1090"><a href="#cb28-1090" aria-hidden="true" tabindex="-1"></a>$\text{Cov}\left(X_i, X_j\right) = \sigma_{ij}^2$, and</span>
<span id="cb28-1091"><a href="#cb28-1091" aria-hidden="true" tabindex="-1"></a>$\lim_{n\to\infty}\frac{1}{n^2}\sum_{i=1}^n \sigma_{ij}^2 =0$. If</span>
<span id="cb28-1092"><a href="#cb28-1092" aria-hidden="true" tabindex="-1"></a>$\frac{1}{n}\sum_{i=1}^n\mu_i \overset{p}{\to}\mu$, then</span>
<span id="cb28-1093"><a href="#cb28-1093" aria-hidden="true" tabindex="-1"></a>$\bar X \overset{p}{\to}\mu$.</span>
<span id="cb28-1094"><a href="#cb28-1094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1095"><a href="#cb28-1095" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1096"><a href="#cb28-1096" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1097"><a href="#cb28-1097" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb28-1098"><a href="#cb28-1098" aria-hidden="true" tabindex="-1"></a>The expected value of $\bar X$ is</span>
<span id="cb28-1099"><a href="#cb28-1099" aria-hidden="true" tabindex="-1"></a>$$\text{E}\left<span class="co">[</span><span class="ot">\bar X\right</span><span class="co">]</span> = \frac{1}{n}\sum_{i=1}^n\text{E}\left<span class="co">[</span><span class="ot">X_i\right</span><span class="co">]</span>= \frac{1}{n}\sum_{i=1}^n\mu_i \overset{p}{\to}\mu.$$</span>
<span id="cb28-1100"><a href="#cb28-1100" aria-hidden="true" tabindex="-1"></a>The variance is</span>
<span id="cb28-1101"><a href="#cb28-1101" aria-hidden="true" tabindex="-1"></a>$$ \text{Var}\left(\bar X\right) = \text{Var}\left(\frac{1}{n}\sum_{i=1}^nX_i\right) = \frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n\text{Cov}\left(X_i,X_j\right) = \frac{1}{n}\sum_{i=1}^n \sigma_{ij}^2\to 0.$$</span>
<span id="cb28-1102"><a href="#cb28-1102" aria-hidden="true" tabindex="-1"></a>By Proposition @prp-mse3, $\bar X\overset{ms}{\to}\mu$, so</span>
<span id="cb28-1103"><a href="#cb28-1103" aria-hidden="true" tabindex="-1"></a>$\bar X \overset{p}{\to}\mu$.</span>
<span id="cb28-1104"><a href="#cb28-1104" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1105"><a href="#cb28-1105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1106"><a href="#cb28-1106" aria-hidden="true" tabindex="-1"></a>:::{#cor-}</span>
<span id="cb28-1107"><a href="#cb28-1107" aria-hidden="true" tabindex="-1"></a>Suppose $(X_1,\ldots, X_n)$ are an independent sample such that</span>
<span id="cb28-1108"><a href="#cb28-1108" aria-hidden="true" tabindex="-1"></a>$\text{E}\left<span class="co">[</span><span class="ot">X_i\right</span><span class="co">]</span> = \mu_i$,</span>
<span id="cb28-1109"><a href="#cb28-1109" aria-hidden="true" tabindex="-1"></a>$\text{Var}\left(X_i\right) = \sigma_{i}^2$, and</span>
<span id="cb28-1110"><a href="#cb28-1110" aria-hidden="true" tabindex="-1"></a>$\lim_{n\to\infty}\frac{1}{n^2}\sum_{i=1}^n \sigma_i^2 =0$. If</span>
<span id="cb28-1111"><a href="#cb28-1111" aria-hidden="true" tabindex="-1"></a>$\frac{1}{n}\sum_{i=1}^n\mu_i \overset{p}{\to}\mu$, then</span>
<span id="cb28-1112"><a href="#cb28-1112" aria-hidden="true" tabindex="-1"></a>$\bar X \overset{p}{\to}\mu$.</span>
<span id="cb28-1113"><a href="#cb28-1113" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1114"><a href="#cb28-1114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1115"><a href="#cb28-1115" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb28-1116"><a href="#cb28-1116" aria-hidden="true" tabindex="-1"></a>If the sample is independent, then</span>
<span id="cb28-1117"><a href="#cb28-1117" aria-hidden="true" tabindex="-1"></a>$$\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n\text{Cov}\left(X_i,X_j\right) =\frac{1}{n^2}\sum_{i=1}^n\text{Var}\left(X_i\right). $$</span>
<span id="cb28-1118"><a href="#cb28-1118" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1119"><a href="#cb28-1119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1120"><a href="#cb28-1120" aria-hidden="true" tabindex="-1"></a>The reason our non-iid sample in Example @exm-noiid did not converge was</span>
<span id="cb28-1121"><a href="#cb28-1121" aria-hidden="true" tabindex="-1"></a>because</span>
<span id="cb28-1122"><a href="#cb28-1122" aria-hidden="true" tabindex="-1"></a>$$ \frac{1}{n^2}\sum_{i=1}^n \sigma_i^2 = \frac{1}{n^2}\sum_{i=1}^n i = \frac{1}{n^2}\frac{n(n+1)}{2} = \frac{n^2 + n}{2n^2} \to \frac{1}{2} \neq 0.$$</span>
<span id="cb28-1123"><a href="#cb28-1123" aria-hidden="true" tabindex="-1"></a>Let's modify it slightly so the sum of the variances does converge to</span>
<span id="cb28-1124"><a href="#cb28-1124" aria-hidden="true" tabindex="-1"></a>zero.</span>
<span id="cb28-1125"><a href="#cb28-1125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1126"><a href="#cb28-1126" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb28-1127"><a href="#cb28-1127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1128"><a href="#cb28-1128" aria-hidden="true" tabindex="-1"></a><span class="fu">## LLN with Non-IID Data</span></span>
<span id="cb28-1129"><a href="#cb28-1129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1130"><a href="#cb28-1130" aria-hidden="true" tabindex="-1"></a>Suppose $\mathbf{X}= (X_1, \ldots, X_n)$ where $X_i \sim N(-1,i^{-1})$</span>
<span id="cb28-1131"><a href="#cb28-1131" aria-hidden="true" tabindex="-1"></a>if $i$ is odd, and $X_i \sim N(1,i^{-1})$ is $i$ is even. Now we have</span>
<span id="cb28-1132"><a href="#cb28-1132" aria-hidden="true" tabindex="-1"></a>$$\lim_{n\to\infty}\frac{1}{n^2}\sum_{i=1}^n \sigma_i^2 = \left<span class="co">[</span><span class="ot">\lim_{n\to\infty}\frac{1}{n^2}\right</span><span class="co">]</span>\sum_{i=1}^\infty i^{-1} \to 0 .$$</span>
<span id="cb28-1133"><a href="#cb28-1133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1136"><a href="#cb28-1136" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-1137"><a href="#cb28-1137" aria-hidden="true" tabindex="-1"></a>draw_X_i <span class="ot">&lt;-</span> <span class="cf">function</span>(i){</span>
<span id="cb28-1138"><a href="#cb28-1138" aria-hidden="true" tabindex="-1"></a>  mu_i <span class="ot">&lt;-</span> (<span class="sc">-</span><span class="dv">1</span>)<span class="sc">^</span>(i <span class="sc">%%</span> <span class="dv">2</span>) <span class="sc">*</span> (<span class="dv">1</span>)<span class="sc">^</span>(<span class="dv">1</span> <span class="sc">-</span> i <span class="sc">%%</span> <span class="dv">2</span>)</span>
<span id="cb28-1139"><a href="#cb28-1139" aria-hidden="true" tabindex="-1"></a>  sigma_i <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>i </span>
<span id="cb28-1140"><a href="#cb28-1140" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb28-1141"><a href="#cb28-1141" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, mu_i, sigma_i)</span>
<span id="cb28-1142"><a href="#cb28-1142" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb28-1143"><a href="#cb28-1143" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb28-1144"><a href="#cb28-1144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1145"><a href="#cb28-1145" aria-hidden="true" tabindex="-1"></a>draw_X <span class="ot">&lt;-</span> <span class="cf">function</span>(n){</span>
<span id="cb28-1146"><a href="#cb28-1146" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb28-1147"><a href="#cb28-1147" aria-hidden="true" tabindex="-1"></a>    <span class="at">i =</span> <span class="dv">1</span><span class="sc">:</span>n,</span>
<span id="cb28-1148"><a href="#cb28-1148" aria-hidden="true" tabindex="-1"></a>    <span class="at">value =</span> <span class="fu">map_dbl</span>(<span class="dv">1</span><span class="sc">:</span>n, draw_X_i)</span>
<span id="cb28-1149"><a href="#cb28-1149" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb28-1150"><a href="#cb28-1150" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">estimate =</span> <span class="fu">cummean</span>(value))</span>
<span id="cb28-1151"><a href="#cb28-1151" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb28-1152"><a href="#cb28-1152" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb28-1153"><a href="#cb28-1153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1154"><a href="#cb28-1154" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">draw_X</span>(<span class="dv">150</span>)</span>
<span id="cb28-1155"><a href="#cb28-1155" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-1156"><a href="#cb28-1156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1157"><a href="#cb28-1157" aria-hidden="true" tabindex="-1"></a>Now we see that $\bar X$ is converging to $0$, and doing so rather</span>
<span id="cb28-1158"><a href="#cb28-1158" aria-hidden="true" tabindex="-1"></a>quickly.</span>
<span id="cb28-1159"><a href="#cb28-1159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1162"><a href="#cb28-1162" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-1163"><a href="#cb28-1163" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-1164"><a href="#cb28-1164" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot210</span></span>
<span id="cb28-1165"><a href="#cb28-1165" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb28-1166"><a href="#cb28-1166" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb28-1167"><a href="#cb28-1167" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-1168"><a href="#cb28-1168" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Despite the data not being IID, the sample mean still converges to the population mean because the variance shrinks in the correct way."</span></span>
<span id="cb28-1169"><a href="#cb28-1169" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb28-1170"><a href="#cb28-1170" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb28-1171"><a href="#cb28-1171" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(i, estimate)) <span class="sc">+</span></span>
<span id="cb28-1172"><a href="#cb28-1172" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb28-1173"><a href="#cb28-1173" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb28-1174"><a href="#cb28-1174" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Sample Size"</span>, <span class="at">y =</span> <span class="st">"Sample Mean"</span>)</span>
<span id="cb28-1175"><a href="#cb28-1175" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-1176"><a href="#cb28-1176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1177"><a href="#cb28-1177" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1178"><a href="#cb28-1178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1179"><a href="#cb28-1179" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Continuous Mapping Theorem and Slutsky's Theorem</span></span>
<span id="cb28-1180"><a href="#cb28-1180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1181"><a href="#cb28-1181" aria-hidden="true" tabindex="-1"></a>At first glance, the LLN may not seem especially useful as it only</span>
<span id="cb28-1182"><a href="#cb28-1182" aria-hidden="true" tabindex="-1"></a>applies to the sample mean. However, when paired with two key results</span>
<span id="cb28-1183"><a href="#cb28-1183" aria-hidden="true" tabindex="-1"></a>about convergence, the LLN becomes an indispensable tool to analyze the</span>
<span id="cb28-1184"><a href="#cb28-1184" aria-hidden="true" tabindex="-1"></a>convergence of many random variables and estimators. The first of these</span>
<span id="cb28-1185"><a href="#cb28-1185" aria-hidden="true" tabindex="-1"></a>is an extension of a key result in real analysis. A useful, and defining</span>
<span id="cb28-1186"><a href="#cb28-1186" aria-hidden="true" tabindex="-1"></a>property, of continuous functions is that they preserve limits of</span>
<span id="cb28-1187"><a href="#cb28-1187" aria-hidden="true" tabindex="-1"></a>numeric sequences. If $<span class="sc">\{</span>a_n<span class="sc">\}</span>$ is a numeric sequence, then</span>
<span id="cb28-1188"><a href="#cb28-1188" aria-hidden="true" tabindex="-1"></a>$$\lim_{n \to\infty} f(a_n) = f\left(\lim_{n\to\infty} a_n\right) \iff f\text{ continuous}.$$</span>
<span id="cb28-1189"><a href="#cb28-1189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1190"><a href="#cb28-1190" aria-hidden="true" tabindex="-1"></a>:::{#thm-}</span>
<span id="cb28-1191"><a href="#cb28-1191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1192"><a href="#cb28-1192" aria-hidden="true" tabindex="-1"></a><span class="fu">## Continuous Mapping Theorem I</span></span>
<span id="cb28-1193"><a href="#cb28-1193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1194"><a href="#cb28-1194" aria-hidden="true" tabindex="-1"></a>Suppose $X_n \overset{p}{\to}X$, and let $g$ be a continuous function.</span>
<span id="cb28-1195"><a href="#cb28-1195" aria-hidden="true" tabindex="-1"></a>Then $$g(X_n)\overset{p}{\to}g(X).$$ In other words we are able to</span>
<span id="cb28-1196"><a href="#cb28-1196" aria-hidden="true" tabindex="-1"></a>interchange the $\mathop{\mathrm{plim}}$ operator with a continuous</span>
<span id="cb28-1197"><a href="#cb28-1197" aria-hidden="true" tabindex="-1"></a>function:</span>
<span id="cb28-1198"><a href="#cb28-1198" aria-hidden="true" tabindex="-1"></a>$$\mathop{\mathrm{plim}}g(X_n) = g\left(\mathop{\mathrm{plim}}X_n\right).$$</span>
<span id="cb28-1199"><a href="#cb28-1199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1200"><a href="#cb28-1200" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1201"><a href="#cb28-1201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1202"><a href="#cb28-1202" aria-hidden="true" tabindex="-1"></a>The proof of this result can be found in @van2000asymptotic. An</span>
<span id="cb28-1203"><a href="#cb28-1203" aria-hidden="true" tabindex="-1"></a>immediate corollary follows from the fact that convergence in</span>
<span id="cb28-1204"><a href="#cb28-1204" aria-hidden="true" tabindex="-1"></a>probability implies convergence in distribution.</span>
<span id="cb28-1205"><a href="#cb28-1205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1206"><a href="#cb28-1206" aria-hidden="true" tabindex="-1"></a>::: {#cor-}</span>
<span id="cb28-1207"><a href="#cb28-1207" aria-hidden="true" tabindex="-1"></a><span class="fu">## Continuous Mapping Theorem II</span></span>
<span id="cb28-1208"><a href="#cb28-1208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1209"><a href="#cb28-1209" aria-hidden="true" tabindex="-1"></a>Suppose $X_n \overset{p}{\to}X$, and let $g$ be a continuous function.</span>
<span id="cb28-1210"><a href="#cb28-1210" aria-hidden="true" tabindex="-1"></a>Then $$g(X_n)\overset{d}{\to}g(X)$$</span>
<span id="cb28-1211"><a href="#cb28-1211" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1212"><a href="#cb28-1212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1213"><a href="#cb28-1213" aria-hidden="true" tabindex="-1"></a>An equally useful result involves the limit of a sums and products of</span>
<span id="cb28-1214"><a href="#cb28-1214" aria-hidden="true" tabindex="-1"></a>convergent random variables.</span>
<span id="cb28-1215"><a href="#cb28-1215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1216"><a href="#cb28-1216" aria-hidden="true" tabindex="-1"></a>:::{#thm-}</span>
<span id="cb28-1217"><a href="#cb28-1217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1218"><a href="#cb28-1218" aria-hidden="true" tabindex="-1"></a><span class="fu">## Slusky's Theorem</span></span>
<span id="cb28-1219"><a href="#cb28-1219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1220"><a href="#cb28-1220" aria-hidden="true" tabindex="-1"></a>Let $X_n$ and $Y_n$ be sequences of random variables. If</span>
<span id="cb28-1221"><a href="#cb28-1221" aria-hidden="true" tabindex="-1"></a>$X_n\overset{d}{\to}X$ for some random variable $X$, and</span>
<span id="cb28-1222"><a href="#cb28-1222" aria-hidden="true" tabindex="-1"></a>$Y_n\overset{p}{\to}c$ for some constant $c$, then \begin{align*}</span>
<span id="cb28-1223"><a href="#cb28-1223" aria-hidden="true" tabindex="-1"></a>X_n + Y_n &amp;\overset{d}{\to}X + c<span class="sc">\\</span></span>
<span id="cb28-1224"><a href="#cb28-1224" aria-hidden="true" tabindex="-1"></a>X_nY_n &amp; \overset{d}{\to}Xc.</span>
<span id="cb28-1225"><a href="#cb28-1225" aria-hidden="true" tabindex="-1"></a>\end{align*} Furthermore, if $c\neq 0$,</span>
<span id="cb28-1226"><a href="#cb28-1226" aria-hidden="true" tabindex="-1"></a>$$ X_n/Y_n \overset{d}{\to}X/c.$$</span>
<span id="cb28-1227"><a href="#cb28-1227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1228"><a href="#cb28-1228" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1229"><a href="#cb28-1229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1230"><a href="#cb28-1230" aria-hidden="true" tabindex="-1"></a>::: proof</span>
<span id="cb28-1231"><a href="#cb28-1231" aria-hidden="true" tabindex="-1"></a>Define a random vector to be $\mathbf Z_n = (X_n,Y_n)$. We have</span>
<span id="cb28-1232"><a href="#cb28-1232" aria-hidden="true" tabindex="-1"></a>$\mathbf Z_n \overset{d}{\to}(X,c)$ as $X_n\overset{d}{\to}X$ and</span>
<span id="cb28-1233"><a href="#cb28-1233" aria-hidden="true" tabindex="-1"></a>$Y_n \overset{d}{\to}c$ (convergence in probability implies convergence</span>
<span id="cb28-1234"><a href="#cb28-1234" aria-hidden="true" tabindex="-1"></a>in distribution). We can apply the continuous mapping theorem to</span>
<span id="cb28-1235"><a href="#cb28-1235" aria-hidden="true" tabindex="-1"></a>$g(x,y) = x + y$, $g(x,y)=xy$, and $g(x/y)$ to establish the result.</span>
<span id="cb28-1236"><a href="#cb28-1236" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1237"><a href="#cb28-1237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1238"><a href="#cb28-1238" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Slutsky's theorem can be a bit hard to remember because it involves a</span></span>
<span id="cb28-1239"><a href="#cb28-1239" aria-hidden="true" tabindex="-1"></a>sequence of random variable which converges in distribution to a random</span>
<span id="cb28-1240"><a href="#cb28-1240" aria-hidden="true" tabindex="-1"></a>variable, and a sequence of random variables which converges in</span>
<span id="cb28-1241"><a href="#cb28-1241" aria-hidden="true" tabindex="-1"></a>probability to a constant. These asymmetries in mode of convergence and</span>
<span id="cb28-1242"><a href="#cb28-1242" aria-hidden="true" tabindex="-1"></a>the type of limit are essential, otherwise the result will not hold.</span>
<span id="cb28-1243"><a href="#cb28-1243" aria-hidden="true" tabindex="-1"></a>Fortunately, the result does hold if we replace all convergences in</span>
<span id="cb28-1244"><a href="#cb28-1244" aria-hidden="true" tabindex="-1"></a>distribution with convergence in probability (as the later implies the</span>
<span id="cb28-1245"><a href="#cb28-1245" aria-hidden="true" tabindex="-1"></a>prior).</span>
<span id="cb28-1246"><a href="#cb28-1246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1247"><a href="#cb28-1247" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb28-1248"><a href="#cb28-1248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1249"><a href="#cb28-1249" aria-hidden="true" tabindex="-1"></a>Suppose $X_n \sim\text{Uni}(0,1)$ and $Y_n = - X_n$. We have</span>
<span id="cb28-1250"><a href="#cb28-1250" aria-hidden="true" tabindex="-1"></a>$X_n \overset{d}{\to}\text{Uni}(0,1)$ and</span>
<span id="cb28-1251"><a href="#cb28-1251" aria-hidden="true" tabindex="-1"></a>$Y_n \overset{d}{\to}\text{Uni}(-1,0)$. Despite this</span>
<span id="cb28-1252"><a href="#cb28-1252" aria-hidden="true" tabindex="-1"></a>$X_n + Y_n = 0 \not\overset{d}{\to}\text{Uni}(0,1) + \text{Uni}(-1,0).$</span>
<span id="cb28-1253"><a href="#cb28-1253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1254"><a href="#cb28-1254" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1255"><a href="#cb28-1255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1256"><a href="#cb28-1256" aria-hidden="true" tabindex="-1"></a>::: {#exm-convar}</span>
<span id="cb28-1257"><a href="#cb28-1257" aria-hidden="true" tabindex="-1"></a><span class="fu">## Consistency of Sample Variance</span></span>
<span id="cb28-1258"><a href="#cb28-1258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1259"><a href="#cb28-1259" aria-hidden="true" tabindex="-1"></a>@exm-consvarnorm showed that $S^2$ is a consistent estimator for</span>
<span id="cb28-1260"><a href="#cb28-1260" aria-hidden="true" tabindex="-1"></a>$\sigma^2$ when $X_i\overset{iid}{\sim}N(\mu,\sigma^2)$. We can use the</span>
<span id="cb28-1261"><a href="#cb28-1261" aria-hidden="true" tabindex="-1"></a>continuous mapping theorem, Slutsky's theorem, and the LLN to show that</span>
<span id="cb28-1262"><a href="#cb28-1262" aria-hidden="true" tabindex="-1"></a>$S^2$ is consistent regardless of the distribution of our iid sample.</span>
<span id="cb28-1263"><a href="#cb28-1263" aria-hidden="true" tabindex="-1"></a>Suppose $\text{E}\left<span class="co">[</span><span class="ot">X_i\right</span><span class="co">]</span> = \mu$,</span>
<span id="cb28-1264"><a href="#cb28-1264" aria-hidden="true" tabindex="-1"></a>$\text{E}\left<span class="co">[</span><span class="ot">X_i^2\right</span><span class="co">]</span>=\mu_2$, and</span>
<span id="cb28-1265"><a href="#cb28-1265" aria-hidden="true" tabindex="-1"></a>$\text{E}\left<span class="co">[</span><span class="ot">X_i^4\right</span><span class="co">]</span>=\mu_4$ for all $i$. If we define our</span>
<span id="cb28-1266"><a href="#cb28-1266" aria-hidden="true" tabindex="-1"></a>continuous function to be $g(x) = x^2$, then \begin{align*}</span>
<span id="cb28-1267"><a href="#cb28-1267" aria-hidden="true" tabindex="-1"></a>S^2 &amp; = \frac{1}{n-1} \sum_{i=1}^nX_i - \frac{1}{n-1} \sum_{i=1}^n\bar X^2 <span class="sc">\\</span></span>
<span id="cb28-1268"><a href="#cb28-1268" aria-hidden="true" tabindex="-1"></a>    &amp; = \frac{1}{n-1} \sum_{i=1}^nX_i^2 + \frac{n}{n-1}\bar X^2  <span class="sc">\\</span></span>
<span id="cb28-1269"><a href="#cb28-1269" aria-hidden="true" tabindex="-1"></a>    &amp; = \frac{n}{n-1}\left<span class="co">[</span><span class="ot">\frac{1}{n}\sum_{i=1}^{n}X_i^2 - \left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)^2\right</span><span class="co">]</span></span>
<span id="cb28-1270"><a href="#cb28-1270" aria-hidden="true" tabindex="-1"></a>\end{align*} The first term in the brackets is an unbiased estimator of</span>
<span id="cb28-1271"><a href="#cb28-1271" aria-hidden="true" tabindex="-1"></a>$\mu_2$ with vanishing variance, so by @prp-consbias it is a consistent</span>
<span id="cb28-1272"><a href="#cb28-1272" aria-hidden="true" tabindex="-1"></a>estimator for $\text{E}\left<span class="co">[</span><span class="ot">X^2\right</span><span class="co">]</span>$: \begin{align*}</span>
<span id="cb28-1273"><a href="#cb28-1273" aria-hidden="true" tabindex="-1"></a>\text{E}\left<span class="co">[</span><span class="ot">\frac{1}{n}\sum_{i=1}^{n}X_i^2\right</span><span class="co">]</span> &amp;= \frac{1}{n}\left(n \mu_2\right) = \mu^2<span class="sc">\\</span></span>
<span id="cb28-1274"><a href="#cb28-1274" aria-hidden="true" tabindex="-1"></a>\lim_{n\to\infty}\text{Var}\left(\frac{1}{n}\sum_{i=1}^{n}X_i^2\right) &amp; = \lim_{n\to\infty}\frac{1}{n^2}n\left(\text{E}\left<span class="co">[</span><span class="ot">X_i^4\right</span><span class="co">]</span> - \text{E}\left<span class="co">[</span><span class="ot">X_i^2\right</span><span class="co">]</span>^2 \right) = \lim_{n\to\infty}\frac{\mu_4 + \mu_2^2}{n} = 0</span>
<span id="cb28-1275"><a href="#cb28-1275" aria-hidden="true" tabindex="-1"></a>\end{align*} The second term in the brackets can be written as</span>
<span id="cb28-1276"><a href="#cb28-1276" aria-hidden="true" tabindex="-1"></a>$g(\bar X)$, so by the continuous mapping theorem and the LLN,</span>
<span id="cb28-1277"><a href="#cb28-1277" aria-hidden="true" tabindex="-1"></a>$$ g(\bar X) = \left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)^2 \overset{p}{\to}\mu^2 = g(\mu).$$</span>
<span id="cb28-1278"><a href="#cb28-1278" aria-hidden="true" tabindex="-1"></a>So</span>
<span id="cb28-1279"><a href="#cb28-1279" aria-hidden="true" tabindex="-1"></a>$$S^2= \underbrace{\frac{n}{n-1}}_{\to 1}\Bigg[\underbrace{\frac{1}{n}\sum_{i=1}^{n}X_i^2}_{\overset{p}{\to}\mu_2} - \underbrace{\left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)^2}_{\overset{p}{\to}\mu^2} \Bigg].$$</span>
<span id="cb28-1280"><a href="#cb28-1280" aria-hidden="true" tabindex="-1"></a>We can apply Slutsky's theorem to the sum of sequences of random</span>
<span id="cb28-1281"><a href="#cb28-1281" aria-hidden="true" tabindex="-1"></a>variables which converge in probability to constants, so</span>
<span id="cb28-1282"><a href="#cb28-1282" aria-hidden="true" tabindex="-1"></a>$$ S^2 \overset{p}{\to}\mu_2 - \mu^2 = \sigma^2,$$ making $S^2$</span>
<span id="cb28-1283"><a href="#cb28-1283" aria-hidden="true" tabindex="-1"></a>consistent.</span>
<span id="cb28-1284"><a href="#cb28-1284" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1285"><a href="#cb28-1285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1286"><a href="#cb28-1286" aria-hidden="true" tabindex="-1"></a><span class="fu">## Central Limit Theorems</span></span>
<span id="cb28-1287"><a href="#cb28-1287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1288"><a href="#cb28-1288" aria-hidden="true" tabindex="-1"></a>The LLN told us that our favorite estimator for $\mu$, $\bar X$, is</span>
<span id="cb28-1289"><a href="#cb28-1289" aria-hidden="true" tabindex="-1"></a>consistent. We know turn to what is perhaps an even more important</span>
<span id="cb28-1290"><a href="#cb28-1290" aria-hidden="true" tabindex="-1"></a>result regarding $\bar X$, one that may in fact be the most important</span>
<span id="cb28-1291"><a href="#cb28-1291" aria-hidden="true" tabindex="-1"></a>results in all of probability -- the asymptotic distribution of $\bar X$ is a normal distribution.</span>
<span id="cb28-1292"><a href="#cb28-1292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1293"><a href="#cb28-1293" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lindeberg-Lévy CLT</span></span>
<span id="cb28-1294"><a href="#cb28-1294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1295"><a href="#cb28-1295" aria-hidden="true" tabindex="-1"></a>The *classic* version of the CLT is formally known as the Lindeberg-Lévy CLT, and is likely familiar.</span>
<span id="cb28-1296"><a href="#cb28-1296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1297"><a href="#cb28-1297" aria-hidden="true" tabindex="-1"></a>:::{#thm-}</span>
<span id="cb28-1298"><a href="#cb28-1298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1299"><a href="#cb28-1299" aria-hidden="true" tabindex="-1"></a><span class="fu">## CLT I</span></span>
<span id="cb28-1300"><a href="#cb28-1300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1301"><a href="#cb28-1301" aria-hidden="true" tabindex="-1"></a>Suppose $\mathbf{X}=(X_1,\ldots, X_n)$ is a sequence of random variables</span>
<span id="cb28-1302"><a href="#cb28-1302" aria-hidden="true" tabindex="-1"></a>with $\text{E}\left<span class="co">[</span><span class="ot">X_i\right</span><span class="co">]</span>=\mu$ and</span>
<span id="cb28-1303"><a href="#cb28-1303" aria-hidden="true" tabindex="-1"></a>$\text{Var}\left(X_i\right)=\sigma^2$. Then</span>
<span id="cb28-1304"><a href="#cb28-1304" aria-hidden="true" tabindex="-1"></a>$$\sqrt{n}(\bar X - \mu) \overset{d}{\to}N(0,\sigma^2),$$ which is also</span>
<span id="cb28-1305"><a href="#cb28-1305" aria-hidden="true" tabindex="-1"></a>often written as $$\bar X\overset{d}{\to}N(\mu, \sigma^2/n),$$ or</span>
<span id="cb28-1306"><a href="#cb28-1306" aria-hidden="true" tabindex="-1"></a>$$\sum_{i=1}^n X_i \overset{d}{\to}N(n\mu, \sqrt n \sigma^2) $$</span>
<span id="cb28-1307"><a href="#cb28-1307" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1308"><a href="#cb28-1308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1309"><a href="#cb28-1309" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The proof is a bit technical and requires some measure-theoretic based</span></span>
<span id="cb28-1310"><a href="#cb28-1310" aria-hidden="true" tabindex="-1"></a>probability theory. It can be found in @billingsley2008probability or</span>
<span id="cb28-1311"><a href="#cb28-1311" aria-hidden="true" tabindex="-1"></a>@durrett2019probability.</span>
<span id="cb28-1312"><a href="#cb28-1312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1313"><a href="#cb28-1313" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb28-1314"><a href="#cb28-1314" aria-hidden="true" tabindex="-1"></a>Suppose we have an iid sample from $\text{Exp}(1)$. If we simulate 1000</span>
<span id="cb28-1315"><a href="#cb28-1315" aria-hidden="true" tabindex="-1"></a>realizations of $\sqrt n(\bar X - \mu)$ for various sample sizes, we</span>
<span id="cb28-1316"><a href="#cb28-1316" aria-hidden="true" tabindex="-1"></a>should see that the distribution of our realizations becomes</span>
<span id="cb28-1317"><a href="#cb28-1317" aria-hidden="true" tabindex="-1"></a>approximately normal as we increase the sample size.</span>
<span id="cb28-1318"><a href="#cb28-1318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1321"><a href="#cb28-1321" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-1322"><a href="#cb28-1322" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">draw_N_estimates_over_n</span>(</span>
<span id="cb28-1323"><a href="#cb28-1323" aria-hidden="true" tabindex="-1"></a>  <span class="at">theta =</span> <span class="cf">function</span>(X){<span class="fu">sqrt</span>(<span class="fu">length</span>(X))<span class="sc">*</span>(<span class="fu">mean</span>(X) <span class="sc">-</span> <span class="dv">1</span>)},</span>
<span id="cb28-1324"><a href="#cb28-1324" aria-hidden="true" tabindex="-1"></a>  <span class="at">N =</span> <span class="fl">1e4</span>,</span>
<span id="cb28-1325"><a href="#cb28-1325" aria-hidden="true" tabindex="-1"></a>  <span class="at">n_vals =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">25</span>, <span class="dv">50</span>, (<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>)<span class="sc">*</span><span class="dv">100</span>),</span>
<span id="cb28-1326"><a href="#cb28-1326" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist =</span> rexp,</span>
<span id="cb28-1327"><a href="#cb28-1327" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist_params =</span> <span class="fu">list</span>(</span>
<span id="cb28-1328"><a href="#cb28-1328" aria-hidden="true" tabindex="-1"></a>    <span class="at">rate =</span> <span class="dv">1</span></span>
<span id="cb28-1329"><a href="#cb28-1329" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb28-1330"><a href="#cb28-1330" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb28-1331"><a href="#cb28-1331" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-1332"><a href="#cb28-1332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1333"><a href="#cb28-1333" aria-hidden="true" tabindex="-1"></a>Even for modest values of $n$, we can see that</span>
<span id="cb28-1334"><a href="#cb28-1334" aria-hidden="true" tabindex="-1"></a>$\sqrt n(\bar X - \mu) \overset{a}{\sim}N(0, \sigma^2)$.</span>
<span id="cb28-1335"><a href="#cb28-1335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1338"><a href="#cb28-1338" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-1339"><a href="#cb28-1339" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-1340"><a href="#cb28-1340" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot211</span></span>
<span id="cb28-1341"><a href="#cb28-1341" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb28-1342"><a href="#cb28-1342" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb28-1343"><a href="#cb28-1343" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-1344"><a href="#cb28-1344" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "As the sample size increases, the distribution of the adjusted sample mean looks more and more like the standard normal."</span></span>
<span id="cb28-1345"><a href="#cb28-1345" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb28-1346"><a href="#cb28-1346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1347"><a href="#cb28-1347" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb28-1348"><a href="#cb28-1348" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(estimate)) <span class="sc">+</span></span>
<span id="cb28-1349"><a href="#cb28-1349" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> <span class="fu">after_stat</span>(density)), <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">fill =</span> <span class="st">"white"</span>, <span class="at">bins =</span> <span class="dv">50</span>) <span class="sc">+</span></span>
<span id="cb28-1350"><a href="#cb28-1350" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb28-1351"><a href="#cb28-1351" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_function</span>(<span class="at">fun =</span> dnorm, <span class="at">colour =</span> <span class="st">"red"</span>) <span class="sc">+</span> </span>
<span id="cb28-1352"><a href="#cb28-1352" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transition_states</span>(sample_size) <span class="sc">+</span></span>
<span id="cb28-1353"><a href="#cb28-1353" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'Sample Size: {closest_state}'</span>, <span class="at">x =</span> <span class="st">"Estimates"</span>, <span class="at">y =</span> <span class="st">""</span>) </span>
<span id="cb28-1354"><a href="#cb28-1354" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-1355"><a href="#cb28-1355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1356"><a href="#cb28-1356" aria-hidden="true" tabindex="-1"></a>An alternate way to visually test whether our estimates are normally</span>
<span id="cb28-1357"><a href="#cb28-1357" aria-hidden="true" tabindex="-1"></a>distributed is with a quantile-quantile plot (QQ-plot), which graphs the</span>
<span id="cb28-1358"><a href="#cb28-1358" aria-hidden="true" tabindex="-1"></a>observed quantiles of our estimates against the theoretical quantiles of</span>
<span id="cb28-1359"><a href="#cb28-1359" aria-hidden="true" tabindex="-1"></a>a normal distribution (or those of any distribution we suspect our data</span>
<span id="cb28-1360"><a href="#cb28-1360" aria-hidden="true" tabindex="-1"></a>is drawn from). If our estimates are (approximately) normally</span>
<span id="cb28-1361"><a href="#cb28-1361" aria-hidden="true" tabindex="-1"></a>distributed, then the observed quantiles should be approximately equal</span>
<span id="cb28-1362"><a href="#cb28-1362" aria-hidden="true" tabindex="-1"></a>to the theoretical quantiles of a normal distribution, forming a</span>
<span id="cb28-1363"><a href="#cb28-1363" aria-hidden="true" tabindex="-1"></a>45-degree line.</span>
<span id="cb28-1364"><a href="#cb28-1364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1367"><a href="#cb28-1367" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-1368"><a href="#cb28-1368" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-1369"><a href="#cb28-1369" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot212</span></span>
<span id="cb28-1370"><a href="#cb28-1370" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb28-1371"><a href="#cb28-1371" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb28-1372"><a href="#cb28-1372" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-1373"><a href="#cb28-1373" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The QQ-plot for the simulated distribution of the adjusted sample mean"</span></span>
<span id="cb28-1374"><a href="#cb28-1374" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb28-1375"><a href="#cb28-1375" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb28-1376"><a href="#cb28-1376" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">sample =</span> estimate)) <span class="sc">+</span></span>
<span id="cb28-1377"><a href="#cb28-1377" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq_line</span>(<span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb28-1378"><a href="#cb28-1378" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_qq</span>(<span class="at">size=</span> <span class="fl">0.5</span>) <span class="sc">+</span> </span>
<span id="cb28-1379"><a href="#cb28-1379" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb28-1380"><a href="#cb28-1380" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transition_states</span>(sample_size) <span class="sc">+</span> </span>
<span id="cb28-1381"><a href="#cb28-1381" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'Sample Size: {closest_state}'</span>, <span class="at">x =</span> <span class="st">"Quantiles of Simulated Estimates"</span>, <span class="at">y =</span> <span class="st">"Quantiles of Normal Distribution"</span>)</span>
<span id="cb28-1382"><a href="#cb28-1382" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-1383"><a href="#cb28-1383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1384"><a href="#cb28-1384" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1385"><a href="#cb28-1385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1386"><a href="#cb28-1386" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The central limit theorem is similar to the LLN insofar that they only</span></span>
<span id="cb28-1387"><a href="#cb28-1387" aria-hidden="true" tabindex="-1"></a>concern the estimator $\bar X$, so how useful can they really be? Well</span>
<span id="cb28-1388"><a href="#cb28-1388" aria-hidden="true" tabindex="-1"></a>with the continuous mapping theorem and Slutsky's theorem, the answer is</span>
<span id="cb28-1389"><a href="#cb28-1389" aria-hidden="true" tabindex="-1"></a>*very useful*!</span>
<span id="cb28-1390"><a href="#cb28-1390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1391"><a href="#cb28-1391" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb28-1392"><a href="#cb28-1392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1393"><a href="#cb28-1393" aria-hidden="true" tabindex="-1"></a>In @exm-tdist we illustrated the fact that</span>
<span id="cb28-1394"><a href="#cb28-1394" aria-hidden="true" tabindex="-1"></a>$X_n \overset{d}{\to}N(0,1)$ where $X_n \sim t_n$, but we didn't</span>
<span id="cb28-1395"><a href="#cb28-1395" aria-hidden="true" tabindex="-1"></a>actually prove it. Directly proving this result is a matter of verify</span>
<span id="cb28-1396"><a href="#cb28-1396" aria-hidden="true" tabindex="-1"></a>that</span>
<span id="cb28-1397"><a href="#cb28-1397" aria-hidden="true" tabindex="-1"></a>$$\lim_{n\to\infty} F_{X_n}(x) =\lim_{n\to\infty}\int_{-\infty}^x \frac{\Gamma\left(\frac{n+1}{2}\right)}{\Gamma\left(\frac{n}{2}\right)\sqrt{n\pi}}\left(1 + \frac{x^2}{n}\right)^{-\frac{n+1}{2}} = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^x e^{-x^2/2} = F_X(x)$$</span>
<span id="cb28-1398"><a href="#cb28-1398" aria-hidden="true" tabindex="-1"></a>where $\Gamma$ is the gamma function defined as</span>
<span id="cb28-1399"><a href="#cb28-1399" aria-hidden="true" tabindex="-1"></a>$$\Gamma(t) = \int_0^\infty s^{t-1}e^{-s}\ ds.$$ A much easier way to</span>
<span id="cb28-1400"><a href="#cb28-1400" aria-hidden="true" tabindex="-1"></a>prove that $X_n \overset{d}{\to}N(0,1)$, is by using Slutsky's theorem</span>
<span id="cb28-1401"><a href="#cb28-1401" aria-hidden="true" tabindex="-1"></a>and the continuous mapping theorem. First recall that</span>
<span id="cb28-1402"><a href="#cb28-1402" aria-hidden="true" tabindex="-1"></a>$$\frac{\bar X - \mu}{S/\sqrt n} \sim t_n,$$ so we can write $X_n$ as</span>
<span id="cb28-1403"><a href="#cb28-1403" aria-hidden="true" tabindex="-1"></a>$X_n = \frac{\bar X - \mu}{S/\sqrt n}$ due to the fact that random</span>
<span id="cb28-1404"><a href="#cb28-1404" aria-hidden="true" tabindex="-1"></a>variables are uniquely determined by their distributions. From</span>
<span id="cb28-1405"><a href="#cb28-1405" aria-hidden="true" tabindex="-1"></a>@exm-consvarnorm, we have $S^2 \overset{p}{\to}\sigma^2$. By the continuous</span>
<span id="cb28-1406"><a href="#cb28-1406" aria-hidden="true" tabindex="-1"></a>mapping theorem</span>
<span id="cb28-1407"><a href="#cb28-1407" aria-hidden="true" tabindex="-1"></a>$$ \sqrt{S^2} = S \overset{p}{\to}\sigma = \sqrt{\sigma^2},$$ which gives</span>
<span id="cb28-1408"><a href="#cb28-1408" aria-hidden="true" tabindex="-1"></a>$$ X_n = \frac{\bar X - \mu}{S/\sqrt n}= \underbrace{\sqrt{n}(\bar X - \mu)}_{\overset{d}{\to}N(0, \sigma^2)}\underbrace{\frac{1}{s}}_{\overset{p}{\to}\sigma}.$$</span>
<span id="cb28-1409"><a href="#cb28-1409" aria-hidden="true" tabindex="-1"></a>Putting all the pieces together, Slutsky's theorem yields</span>
<span id="cb28-1410"><a href="#cb28-1410" aria-hidden="true" tabindex="-1"></a>$$X_n \overset{d}{\to}\frac{N(0,\sigma^2)}{\sigma} = N(0,1).$$</span>
<span id="cb28-1411"><a href="#cb28-1411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1412"><a href="#cb28-1412" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1413"><a href="#cb28-1413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1414"><a href="#cb28-1414" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The CLT can be generalized to samples of random vectors $\mathbf{X}_i$.</span></span>
<span id="cb28-1415"><a href="#cb28-1415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1416"><a href="#cb28-1416" aria-hidden="true" tabindex="-1"></a>::: {#thm-}</span>
<span id="cb28-1417"><a href="#cb28-1417" aria-hidden="true" tabindex="-1"></a>Suppose $(\mathbf{X}_1,\ldots, \mathbf{X}_n)$ is a sequence of iid</span>
<span id="cb28-1418"><a href="#cb28-1418" aria-hidden="true" tabindex="-1"></a>random vectors with $\text{E}\left<span class="co">[</span><span class="ot">\mathbf{X}_i\right</span><span class="co">]</span>=\boldsymbol\mu$</span>
<span id="cb28-1419"><a href="#cb28-1419" aria-hidden="true" tabindex="-1"></a>and $\text{Var}\left(\mathbf{X}_i\right)=\boldsymbol\Sigma$. Then</span>
<span id="cb28-1420"><a href="#cb28-1420" aria-hidden="true" tabindex="-1"></a>$$\sqrt{n}(\bar {\mathbf{X}}- \boldsymbol\mu) \overset{d}{\to}N(\mathbf{0},\boldsymbol\Sigma).$$</span>
<span id="cb28-1421"><a href="#cb28-1421" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1422"><a href="#cb28-1422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1423"><a href="#cb28-1423" aria-hidden="true" tabindex="-1"></a><span class="fu">### Not Identically Distributed</span></span>
<span id="cb28-1424"><a href="#cb28-1424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1425"><a href="#cb28-1425" aria-hidden="true" tabindex="-1"></a>@prp-chebylln allowed us to salvage a LLN when the iid assumption failed, so can we do the same with the CLT? Sort of. While we need an independent sample, we don't necessarily need realizations to be drawn from an identical distribution. Instead, the theorem will rely on the tails of distributions meeting a certion criterion. This version of the CLT is known as the Lindeberg-Feller CLT.</span>
<span id="cb28-1426"><a href="#cb28-1426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1427"><a href="#cb28-1427" aria-hidden="true" tabindex="-1"></a>:::{#thm-}</span>
<span id="cb28-1428"><a href="#cb28-1428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1429"><a href="#cb28-1429" aria-hidden="true" tabindex="-1"></a><span class="fu">## Lindeberg-Feller CLT</span></span>
<span id="cb28-1430"><a href="#cb28-1430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1431"><a href="#cb28-1431" aria-hidden="true" tabindex="-1"></a>Suppose $\mathbf{X}=(X_1,\ldots, X_n)$ is a sequence of independent</span>
<span id="cb28-1432"><a href="#cb28-1432" aria-hidden="true" tabindex="-1"></a>random variables with $\text{E}\left<span class="co">[</span><span class="ot">X_i\right</span><span class="co">]</span>=\mu_i$ and</span>
<span id="cb28-1433"><a href="#cb28-1433" aria-hidden="true" tabindex="-1"></a>$\text{Var}\left(X_i\right)=\sigma_i^2$, and define \begin{align*}</span>
<span id="cb28-1434"><a href="#cb28-1434" aria-hidden="true" tabindex="-1"></a>\bar \mu = \frac{1}{n}\sum_{i=1}^n\mu_i;<span class="sc">\\</span></span>
<span id="cb28-1435"><a href="#cb28-1435" aria-hidden="true" tabindex="-1"></a>\bar \sigma_n^2 = \frac{1}{n}\sum_{i=1}^n\sigma_i^2.</span>
<span id="cb28-1436"><a href="#cb28-1436" aria-hidden="true" tabindex="-1"></a>\end{align*} If the collection of variances $\sigma_i^2$ satisfies:</span>
<span id="cb28-1437"><a href="#cb28-1437" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb28-1438"><a href="#cb28-1438" aria-hidden="true" tabindex="-1"></a>\lim_{n\to\infty} &amp;\frac{\max<span class="sc">\{</span>\sigma_i<span class="sc">\}</span>}{n\bar\sigma} = 0;<span class="sc">\\</span></span>
<span id="cb28-1439"><a href="#cb28-1439" aria-hidden="true" tabindex="-1"></a>\lim_{n\to\infty} &amp;\bar\sigma_n^2 = \bar\sigma^2,</span>
<span id="cb28-1440"><a href="#cb28-1440" aria-hidden="true" tabindex="-1"></a>\end{align*} then</span>
<span id="cb28-1441"><a href="#cb28-1441" aria-hidden="true" tabindex="-1"></a>$\sqrt n (\bar X-\mu)\overset{d}{\to}N(0, \bar\sigma^2).$</span>
<span id="cb28-1442"><a href="#cb28-1442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1443"><a href="#cb28-1443" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1444"><a href="#cb28-1444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1445"><a href="#cb28-1445" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb28-1446"><a href="#cb28-1446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1447"><a href="#cb28-1447" aria-hidden="true" tabindex="-1"></a><span class="fu">## Lindeberg's Condition</span></span>
<span id="cb28-1448"><a href="#cb28-1448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1449"><a href="#cb28-1449" aria-hidden="true" tabindex="-1"></a>The Lindeberg-Feller conditions stipulates that</span>
<span id="cb28-1450"><a href="#cb28-1450" aria-hidden="true" tabindex="-1"></a>$\lim_{n\to\infty}\bar\sigma_n^2 = \bar\sigma^2$ and</span>
<span id="cb28-1451"><a href="#cb28-1451" aria-hidden="true" tabindex="-1"></a>$\lim_{n\to\infty} \frac{\max<span class="sc">\{</span>\sigma_i<span class="sc">\}</span>}{n\bar\sigma} = 0$, a</span>
<span id="cb28-1452"><a href="#cb28-1452" aria-hidden="true" tabindex="-1"></a>condition known as the ***Lindeberg's condition***. The condition is</span>
<span id="cb28-1453"><a href="#cb28-1453" aria-hidden="true" tabindex="-1"></a>often presented in more general terms, but the intuition remains the</span>
<span id="cb28-1454"><a href="#cb28-1454" aria-hidden="true" tabindex="-1"></a>same. For the CLT to hold for random variables that with different</span>
<span id="cb28-1455"><a href="#cb28-1455" aria-hidden="true" tabindex="-1"></a>variances, we need to makes sure that no single term $\sigma_i$</span>
<span id="cb28-1456"><a href="#cb28-1456" aria-hidden="true" tabindex="-1"></a>dominates the standard deviation. We can think about the sample mean</span>
<span id="cb28-1457"><a href="#cb28-1457" aria-hidden="true" tabindex="-1"></a>$\bar X$ as "mixing" many random variables $X_i.$ After mixing these</span>
<span id="cb28-1458"><a href="#cb28-1458" aria-hidden="true" tabindex="-1"></a>random variables, we hope to have a normal distribution, but that only</span>
<span id="cb28-1459"><a href="#cb28-1459" aria-hidden="true" tabindex="-1"></a>happens if tails of the various distributions of $X_i$ are negligible as</span>
<span id="cb28-1460"><a href="#cb28-1460" aria-hidden="true" tabindex="-1"></a>$n\to\infty$, giving us the trademark tails of a normal distribution</span>
<span id="cb28-1461"><a href="#cb28-1461" aria-hidden="true" tabindex="-1"></a>which tapper off. Let's consider a counterexample. Suppose $X_i$ is</span>
<span id="cb28-1462"><a href="#cb28-1462" aria-hidden="true" tabindex="-1"></a>defined on the sample space $<span class="sc">\{</span>-i,0,i<span class="sc">\}</span>$ is distributed such that</span>
<span id="cb28-1463"><a href="#cb28-1463" aria-hidden="true" tabindex="-1"></a>$$\Pr(X_i = k) = \begin{cases} 1/2i^2 &amp; k = -i^2 <span class="sc">\\</span> 1/2i^2 &amp; k = i^2 <span class="sc">\\</span> 1 - 1/i^2&amp;k=0\end{cases}.$$</span>
<span id="cb28-1464"><a href="#cb28-1464" aria-hidden="true" tabindex="-1"></a>Graphing the density function for a few values of $i$ gives us a better</span>
<span id="cb28-1465"><a href="#cb28-1465" aria-hidden="true" tabindex="-1"></a>sense of how $X_i$ behaves.</span>
<span id="cb28-1466"><a href="#cb28-1466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1469"><a href="#cb28-1469" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-1470"><a href="#cb28-1470" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-1471"><a href="#cb28-1471" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot213</span></span>
<span id="cb28-1472"><a href="#cb28-1472" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb28-1473"><a href="#cb28-1473" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb28-1474"><a href="#cb28-1474" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-1475"><a href="#cb28-1475" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Density of X_i for i = 1,...,5."</span></span>
<span id="cb28-1476"><a href="#cb28-1476" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb28-1477"><a href="#cb28-1477" aria-hidden="true" tabindex="-1"></a><span class="fu">expand_grid</span>(<span class="at">i =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">x =</span> <span class="sc">-</span><span class="dv">100</span><span class="sc">:</span><span class="dv">100</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb28-1478"><a href="#cb28-1478" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(x <span class="sc">==</span> i<span class="sc">^</span><span class="dv">2</span> <span class="sc">|</span> x <span class="sc">==</span> <span class="sc">-</span>i<span class="sc">^</span><span class="dv">2</span><span class="sc">|</span> x<span class="sc">==</span> <span class="dv">0</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb28-1479"><a href="#cb28-1479" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> <span class="fu">ifelse</span>(x <span class="sc">==</span> <span class="dv">0</span>, <span class="dv">1</span> <span class="sc">-</span> <span class="dv">1</span><span class="sc">/</span>(i<span class="sc">^</span><span class="dv">2</span>), <span class="dv">1</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>i<span class="sc">^</span><span class="dv">2</span>))) <span class="sc">%&gt;%</span> </span>
<span id="cb28-1480"><a href="#cb28-1480" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, y)) <span class="sc">+</span></span>
<span id="cb28-1481"><a href="#cb28-1481" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb28-1482"><a href="#cb28-1482" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"k"</span>) <span class="sc">+</span></span>
<span id="cb28-1483"><a href="#cb28-1483" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">xend =</span> x, <span class="at">y=</span> <span class="dv">0</span>, <span class="at">yend =</span> y)) <span class="sc">+</span></span>
<span id="cb28-1484"><a href="#cb28-1484" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span> </span>
<span id="cb28-1485"><a href="#cb28-1485" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transition_states</span>(i) <span class="sc">+</span> </span>
<span id="cb28-1486"><a href="#cb28-1486" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">'i = {closest_state}'</span>, <span class="at">y =</span> <span class="st">"Pr(X_i = k)"</span>)</span>
<span id="cb28-1487"><a href="#cb28-1487" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-1488"><a href="#cb28-1488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1489"><a href="#cb28-1489" aria-hidden="true" tabindex="-1"></a>As $i\to\infty$, nearly all the probability is concentrated as $k = 0$.</span>
<span id="cb28-1490"><a href="#cb28-1490" aria-hidden="true" tabindex="-1"></a>The remaining probability is at the extreme tails of the distribution</span>
<span id="cb28-1491"><a href="#cb28-1491" aria-hidden="true" tabindex="-1"></a>$\pm i^2$, and these tails become more and more extreme (quadritically</span>
<span id="cb28-1492"><a href="#cb28-1492" aria-hidden="true" tabindex="-1"></a>so) as $i\to\infty$. This is the exact type of behavior Lindeberg's</span>
<span id="cb28-1493"><a href="#cb28-1493" aria-hidden="true" tabindex="-1"></a>condition rules out. The expectation, expectation squared, and variance</span>
<span id="cb28-1494"><a href="#cb28-1494" aria-hidden="true" tabindex="-1"></a>of $X_i$ are: \begin{align*}</span>
<span id="cb28-1495"><a href="#cb28-1495" aria-hidden="true" tabindex="-1"></a>\text{E}\left<span class="co">[</span><span class="ot">X_i\right</span><span class="co">]</span> &amp; = (1/2i^2)(-i^2) + (1/2i^2)(i^2) +  (1 - 1/i^2)(0) = 0<span class="sc">\\</span></span>
<span id="cb28-1496"><a href="#cb28-1496" aria-hidden="true" tabindex="-1"></a>\text{E}\left<span class="co">[</span><span class="ot">X_i^2\right</span><span class="co">]</span> &amp; = (1/2i^2)(i^4) + (1/2i^2)(i^4) +  (1 - 1/i^2)(0) = i^2<span class="sc">\\</span></span>
<span id="cb28-1497"><a href="#cb28-1497" aria-hidden="true" tabindex="-1"></a>\text{Var}\left(X_i\right) &amp; = i^2 - 0^2 = i^2</span>
<span id="cb28-1498"><a href="#cb28-1498" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb28-1499"><a href="#cb28-1499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1500"><a href="#cb28-1500" aria-hidden="true" tabindex="-1"></a>We can verify that Lindeberg's condition does not hold. \begin{align*}</span>
<span id="cb28-1501"><a href="#cb28-1501" aria-hidden="true" tabindex="-1"></a>\lim_{n\to\infty}\bar \sigma_n = \lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^n i^2 = \lim_{n\to\infty}\frac{(n+1)(2n+1)}{6} \to\infty</span>
<span id="cb28-1502"><a href="#cb28-1502" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb28-1503"><a href="#cb28-1503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1504"><a href="#cb28-1504" aria-hidden="true" tabindex="-1"></a>To simulate realizations of this random variable, we can define $X_i$</span>
<span id="cb28-1505"><a href="#cb28-1505" aria-hidden="true" tabindex="-1"></a>using $U_i\sim\text{Uni}(0,1)$.<span class="ot">[^asymptotics-1]</span></span>
<span id="cb28-1506"><a href="#cb28-1506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1507"><a href="#cb28-1507" aria-hidden="true" tabindex="-1"></a>$$X_i = \begin{cases}-i^2 &amp; U_i\in<span class="co">[</span><span class="ot">0, 1/2i^2)\\ i^2 &amp; U_i\in[1/2i^2, 1/i^2) \\ 0 &amp; U_i\in [1/i^2,1</span><span class="co">]</span>\end{cases}$$</span>
<span id="cb28-1508"><a href="#cb28-1508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1511"><a href="#cb28-1511" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-1512"><a href="#cb28-1512" aria-hidden="true" tabindex="-1"></a>draw_X <span class="ot">&lt;-</span> <span class="cf">function</span>(n,...){</span>
<span id="cb28-1513"><a href="#cb28-1513" aria-hidden="true" tabindex="-1"></a>  U <span class="ot">&lt;-</span> <span class="fu">runif</span>(n)</span>
<span id="cb28-1514"><a href="#cb28-1514" aria-hidden="true" tabindex="-1"></a>  prob_pos_i <span class="ot">&lt;-</span> (U <span class="sc">&lt;</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">:</span>n)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb28-1515"><a href="#cb28-1515" aria-hidden="true" tabindex="-1"></a>  prob_neg_i <span class="ot">&lt;-</span> (U <span class="sc">&gt;</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">:</span>n)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">&amp;</span> U <span class="sc">&lt;</span> <span class="dv">1</span><span class="sc">/</span>((<span class="dv">1</span><span class="sc">:</span>n)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb28-1516"><a href="#cb28-1516" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">1</span><span class="sc">:</span>n)<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>prob_pos_i <span class="sc">-</span> (<span class="dv">1</span><span class="sc">:</span>n)<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>prob_neg_i</span>
<span id="cb28-1517"><a href="#cb28-1517" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb28-1518"><a href="#cb28-1518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1519"><a href="#cb28-1519" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">draw_N_estimates_over_n</span>(</span>
<span id="cb28-1520"><a href="#cb28-1520" aria-hidden="true" tabindex="-1"></a>  <span class="at">theta =</span> <span class="cf">function</span>(X){<span class="fu">sqrt</span>(<span class="fu">length</span>(X))<span class="sc">*</span>(<span class="fu">mean</span>(X) <span class="sc">-</span> <span class="dv">1</span>)},</span>
<span id="cb28-1521"><a href="#cb28-1521" aria-hidden="true" tabindex="-1"></a>  <span class="at">N =</span> <span class="dv">200</span>,</span>
<span id="cb28-1522"><a href="#cb28-1522" aria-hidden="true" tabindex="-1"></a>  <span class="at">n_vals =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">25</span>, <span class="dv">100</span>, <span class="dv">1000</span>),</span>
<span id="cb28-1523"><a href="#cb28-1523" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist =</span> draw_X,</span>
<span id="cb28-1524"><a href="#cb28-1524" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist_params =</span> <span class="fu">list</span>(<span class="dv">0</span>)</span>
<span id="cb28-1525"><a href="#cb28-1525" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb28-1526"><a href="#cb28-1526" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-1527"><a href="#cb28-1527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1530"><a href="#cb28-1530" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-1531"><a href="#cb28-1531" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-1532"><a href="#cb28-1532" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot214</span></span>
<span id="cb28-1533"><a href="#cb28-1533" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb28-1534"><a href="#cb28-1534" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb28-1535"><a href="#cb28-1535" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-1536"><a href="#cb28-1536" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Histograms of estimates as sample size increases."</span></span>
<span id="cb28-1537"><a href="#cb28-1537" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb28-1538"><a href="#cb28-1538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1539"><a href="#cb28-1539" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb28-1540"><a href="#cb28-1540" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(estimate)) <span class="sc">+</span></span>
<span id="cb28-1541"><a href="#cb28-1541" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">color =</span> <span class="st">"black"</span>, <span class="at">fill =</span> <span class="st">"white"</span>, <span class="at">bins =</span> <span class="dv">50</span>) <span class="sc">+</span></span>
<span id="cb28-1542"><a href="#cb28-1542" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>sample_size, <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb28-1543"><a href="#cb28-1543" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb28-1544"><a href="#cb28-1544" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Estimates"</span>, <span class="at">y =</span> <span class="st">""</span>)</span>
<span id="cb28-1545"><a href="#cb28-1545" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-1546"><a href="#cb28-1546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1547"><a href="#cb28-1547" aria-hidden="true" tabindex="-1"></a>The apparent distribution of our estimates is not converging to a normal</span>
<span id="cb28-1548"><a href="#cb28-1548" aria-hidden="true" tabindex="-1"></a>distribution, as we always have a few outliers that are too plentiful</span>
<span id="cb28-1549"><a href="#cb28-1549" aria-hidden="true" tabindex="-1"></a>relative to their distance from the mean to be drawn from a normal</span>
<span id="cb28-1550"><a href="#cb28-1550" aria-hidden="true" tabindex="-1"></a>distribution. As $n\to\infty$ these outliers become even more extreme.</span>
<span id="cb28-1551"><a href="#cb28-1551" aria-hidden="true" tabindex="-1"></a>This is also evident from QQ-plots, where the points far away from the</span>
<span id="cb28-1552"><a href="#cb28-1552" aria-hidden="true" tabindex="-1"></a>45-degree line are drawn even farther away as $n\to\infty$.</span>
<span id="cb28-1553"><a href="#cb28-1553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1556"><a href="#cb28-1556" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-1557"><a href="#cb28-1557" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-1558"><a href="#cb28-1558" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot215</span></span>
<span id="cb28-1559"><a href="#cb28-1559" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb28-1560"><a href="#cb28-1560" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb28-1561"><a href="#cb28-1561" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-1562"><a href="#cb28-1562" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The QQ-plot for the simulated distribution of the adjusted sample mean"</span></span>
<span id="cb28-1563"><a href="#cb28-1563" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb28-1564"><a href="#cb28-1564" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb28-1565"><a href="#cb28-1565" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">sample =</span> estimate)) <span class="sc">+</span></span>
<span id="cb28-1566"><a href="#cb28-1566" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_qq_line</span>(<span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb28-1567"><a href="#cb28-1567" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_qq</span>(<span class="at">size=</span> <span class="fl">0.5</span>) <span class="sc">+</span> </span>
<span id="cb28-1568"><a href="#cb28-1568" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>sample_size) <span class="sc">+</span> </span>
<span id="cb28-1569"><a href="#cb28-1569" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb28-1570"><a href="#cb28-1570" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Quantiles of Simulated Estimates"</span>, <span class="at">y =</span> <span class="st">"Quantiles of Normal Distribution"</span>)</span>
<span id="cb28-1571"><a href="#cb28-1571" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-1572"><a href="#cb28-1572" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1573"><a href="#cb28-1573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1574"><a href="#cb28-1574" aria-hidden="true" tabindex="-1"></a><span class="ot">[^asymptotics-1]: </span>We can actually draw observations of *any* random</span>
<span id="cb28-1575"><a href="#cb28-1575" aria-hidden="true" tabindex="-1"></a>    variable using the uniform distribution on $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ via [***inverse</span>
<span id="cb28-1576"><a href="#cb28-1576" aria-hidden="true" tabindex="-1"></a>    transform</span>
<span id="cb28-1577"><a href="#cb28-1577" aria-hidden="true" tabindex="-1"></a>    sampling***](https://en.wikipedia.org/wiki/Inverse_transform_sampling#Reduction_of_the_number_of_inversions).</span>
<span id="cb28-1578"><a href="#cb28-1578" aria-hidden="true" tabindex="-1"></a>    This is one of the primary ways computers generate random</span>
<span id="cb28-1579"><a href="#cb28-1579" aria-hidden="true" tabindex="-1"></a>    observations from a given distribution.</span>
<span id="cb28-1580"><a href="#cb28-1580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1581"><a href="#cb28-1581" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; While theoretically important, Lindeberg's condition can be a bit hard</span></span>
<span id="cb28-1582"><a href="#cb28-1582" aria-hidden="true" tabindex="-1"></a>to verify. It is much more common to appeal to a stronger assumption</span>
<span id="cb28-1583"><a href="#cb28-1583" aria-hidden="true" tabindex="-1"></a>which gives rise to a second CLT that holds for independent, but not</span>
<span id="cb28-1584"><a href="#cb28-1584" aria-hidden="true" tabindex="-1"></a>necessarily identically distributed, random variables. This final CLT</span>
<span id="cb28-1585"><a href="#cb28-1585" aria-hidden="true" tabindex="-1"></a>may not be as general as the Lindeberg-Feller CLT, but it is much easier</span>
<span id="cb28-1586"><a href="#cb28-1586" aria-hidden="true" tabindex="-1"></a>to work with.</span>
<span id="cb28-1587"><a href="#cb28-1587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1588"><a href="#cb28-1588" aria-hidden="true" tabindex="-1"></a>:::{#thm-}</span>
<span id="cb28-1589"><a href="#cb28-1589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1590"><a href="#cb28-1590" aria-hidden="true" tabindex="-1"></a><span class="fu">## Lyapunov CLT</span></span>
<span id="cb28-1591"><a href="#cb28-1591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1592"><a href="#cb28-1592" aria-hidden="true" tabindex="-1"></a>Suppose $\mathbf{X}=(X_1,\ldots, X_n)$ is a sequence of independent</span>
<span id="cb28-1593"><a href="#cb28-1593" aria-hidden="true" tabindex="-1"></a>random variables with $\text{E}\left<span class="co">[</span><span class="ot">X_i\right</span><span class="co">]</span>=\mu_i$ and</span>
<span id="cb28-1594"><a href="#cb28-1594" aria-hidden="true" tabindex="-1"></a>$\text{Var}\left(X_i\right)=\sigma_i^2$. If</span>
<span id="cb28-1595"><a href="#cb28-1595" aria-hidden="true" tabindex="-1"></a>$\text{E}\left<span class="co">[</span><span class="ot">\left\lvert X_i - \mu_i\right\rvert^{2+\kappa}\right</span><span class="co">]</span>$ is</span>
<span id="cb28-1596"><a href="#cb28-1596" aria-hidden="true" tabindex="-1"></a>finite for some $\kappa &gt; 0,$ then</span>
<span id="cb28-1597"><a href="#cb28-1597" aria-hidden="true" tabindex="-1"></a>$\sqrt n (\bar X-\mu)\overset{d}{\to}N(0, \bar\sigma^2)$, where</span>
<span id="cb28-1598"><a href="#cb28-1598" aria-hidden="true" tabindex="-1"></a>$\bar\sigma = (1/n)\sum_{i=1}^n \sigma_i.$</span>
<span id="cb28-1599"><a href="#cb28-1599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1600"><a href="#cb28-1600" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1601"><a href="#cb28-1601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1602"><a href="#cb28-1602" aria-hidden="true" tabindex="-1"></a><span class="fu">## Delta Method</span></span>
<span id="cb28-1603"><a href="#cb28-1603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1604"><a href="#cb28-1604" aria-hidden="true" tabindex="-1"></a>Slutsky's theorem and the continuous mapping theorem in tandem with the</span>
<span id="cb28-1605"><a href="#cb28-1605" aria-hidden="true" tabindex="-1"></a>LLN give us the ability to prove that certain functions of sample means</span>
<span id="cb28-1606"><a href="#cb28-1606" aria-hidden="true" tabindex="-1"></a>are convergent. Is it possible that we can do something similar with the</span>
<span id="cb28-1607"><a href="#cb28-1607" aria-hidden="true" tabindex="-1"></a>CLT to find the asymptotic distribution of functions of sample means?</span>
<span id="cb28-1608"><a href="#cb28-1608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1609"><a href="#cb28-1609" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Suppose $g$ is a function of $\bar X$, where a CLT applies to the random</span></span>
<span id="cb28-1610"><a href="#cb28-1610" aria-hidden="true" tabindex="-1"></a>sample $\mathbf{X}$. We know</span>
<span id="cb28-1611"><a href="#cb28-1611" aria-hidden="true" tabindex="-1"></a>$\sqrt n(\bar X-\mu)\overset{a}{\sim}N(0, \sigma^2)$. Is it possible to</span>
<span id="cb28-1612"><a href="#cb28-1612" aria-hidden="true" tabindex="-1"></a>conclude that</span>
<span id="cb28-1613"><a href="#cb28-1613" aria-hidden="true" tabindex="-1"></a>$\sqrt n(g(\bar X)-g(\mu))\overset{a}{\sim}N(0, \tilde\sigma^2)$ for</span>
<span id="cb28-1614"><a href="#cb28-1614" aria-hidden="true" tabindex="-1"></a>some $\tilde\sigma^2$? Furthermore, can we determine $\tilde\mu$ and</span>
<span id="cb28-1615"><a href="#cb28-1615" aria-hidden="true" tabindex="-1"></a>$\tilde\sigma^2$ only knowing $g$, $\mu$, and $\sigma^2$?</span>
<span id="cb28-1616"><a href="#cb28-1616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1617"><a href="#cb28-1617" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; We have information about the difference $\bar X-\mu$ and want</span></span>
<span id="cb28-1618"><a href="#cb28-1618" aria-hidden="true" tabindex="-1"></a>information about the difference $g(\bar X)-g(\mu)$. Situations where we</span>
<span id="cb28-1619"><a href="#cb28-1619" aria-hidden="true" tabindex="-1"></a>know something about behavior in the domain of a function and want to</span>
<span id="cb28-1620"><a href="#cb28-1620" aria-hidden="true" tabindex="-1"></a>relate it to the function's behavior in the codomain are common place in</span>
<span id="cb28-1621"><a href="#cb28-1621" aria-hidden="true" tabindex="-1"></a>math, but in particular in real analysis. This is where the mean value</span>
<span id="cb28-1622"><a href="#cb28-1622" aria-hidden="true" tabindex="-1"></a>theorem saves the day. Assuming $g$ is continuously differentiable and</span>
<span id="cb28-1623"><a href="#cb28-1623" aria-hidden="true" tabindex="-1"></a>fixing $n$, there exists some $T_n$ in between $\bar X$ and $\theta$</span>
<span id="cb28-1624"><a href="#cb28-1624" aria-hidden="true" tabindex="-1"></a>($\bar X&lt; T_n &lt; \mu$ or $\mu &lt; T_n &lt; \bar X$) such that: \begin{align*}</span>
<span id="cb28-1625"><a href="#cb28-1625" aria-hidden="true" tabindex="-1"></a>&amp; \frac{g(\bar X)-g(\mu)}{\bar X - \mu} = g'(T_n)<span class="sc">\\</span></span>
<span id="cb28-1626"><a href="#cb28-1626" aria-hidden="true" tabindex="-1"></a>\implies &amp; g(\bar X) = g(\mu) + g'(T_n)(\bar X-\mu)<span class="sc">\\</span></span>
<span id="cb28-1627"><a href="#cb28-1627" aria-hidden="true" tabindex="-1"></a>\implies &amp; \sqrt{n}<span class="co">[</span><span class="ot">g(\bar X) - g(\mu)</span><span class="co">]</span> = g'(T_n)\sqrt{n} (\bar X-\mu)</span>
<span id="cb28-1628"><a href="#cb28-1628" aria-hidden="true" tabindex="-1"></a>\end{align*} If we let $n$ vary, we have a sequence of random variables</span>
<span id="cb28-1629"><a href="#cb28-1629" aria-hidden="true" tabindex="-1"></a>$<span class="sc">\{</span>T_n<span class="sc">\}</span>$ such that</span>
<span id="cb28-1630"><a href="#cb28-1630" aria-hidden="true" tabindex="-1"></a>$\left\lvert T_n -\mu\right\rvert &lt; \left\lvert\bar X - \mu\right\rvert$.</span>
<span id="cb28-1631"><a href="#cb28-1631" aria-hidden="true" tabindex="-1"></a>By the LLN $\left\lvert\bar X - \mu\right\rvert \overset{p}{\to}0$, so</span>
<span id="cb28-1632"><a href="#cb28-1632" aria-hidden="true" tabindex="-1"></a>$\left\lvert T_n -\mu\right\rvert \overset{p}{\to}0$, which is</span>
<span id="cb28-1633"><a href="#cb28-1633" aria-hidden="true" tabindex="-1"></a>equivalent to $T_n \overset{p}{\to}\mu$. By the continuous mapping</span>
<span id="cb28-1634"><a href="#cb28-1634" aria-hidden="true" tabindex="-1"></a>theorem, $g(T_n) \overset{p}{\to}g(\mu)$. This means</span>
<span id="cb28-1635"><a href="#cb28-1635" aria-hidden="true" tabindex="-1"></a>$$\sqrt{n}<span class="co">[</span><span class="ot">g(\bar X) - g(\mu)</span><span class="co">]</span> = \underbrace{g'(T_n)}_{\overset{p}{\to}g(\mu)}\cdot\underbrace{\sqrt{n} (\bar X-\mu)}_{\overset{d}{\to}N(0, \sigma^2)},$$</span>
<span id="cb28-1636"><a href="#cb28-1636" aria-hidden="true" tabindex="-1"></a>so Slutsky's theorem gives</span>
<span id="cb28-1637"><a href="#cb28-1637" aria-hidden="true" tabindex="-1"></a>$$\sqrt{n}<span class="co">[</span><span class="ot">g(\bar X) - g(\mu)</span><span class="co">]</span> \overset{d}{\to}g(\mu)\cdot N(0,\sigma^2) = N(0, \sigma^2<span class="co">[</span><span class="ot">g(\mu)</span><span class="co">]</span>^2).$$</span>
<span id="cb28-1638"><a href="#cb28-1638" aria-hidden="true" tabindex="-1"></a>This all is contingent on $g$ not being a function of $n$, otherwise</span>
<span id="cb28-1639"><a href="#cb28-1639" aria-hidden="true" tabindex="-1"></a>things fall apart when we apply limiting processes.</span>
<span id="cb28-1640"><a href="#cb28-1640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1641"><a href="#cb28-1641" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; This result is known as the delta method, and applies to any sequence of</span></span>
<span id="cb28-1642"><a href="#cb28-1642" aria-hidden="true" tabindex="-1"></a>random variables that is asymptotically normal. It is also readily</span>
<span id="cb28-1643"><a href="#cb28-1643" aria-hidden="true" tabindex="-1"></a>generalized to higher dimensions where $\mathbf g$ is a vector valued</span>
<span id="cb28-1644"><a href="#cb28-1644" aria-hidden="true" tabindex="-1"></a>function.</span>
<span id="cb28-1645"><a href="#cb28-1645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1646"><a href="#cb28-1646" aria-hidden="true" tabindex="-1"></a>:::{#thm-}</span>
<span id="cb28-1647"><a href="#cb28-1647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1648"><a href="#cb28-1648" aria-hidden="true" tabindex="-1"></a><span class="fu">## Delta Method</span></span>
<span id="cb28-1649"><a href="#cb28-1649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1650"><a href="#cb28-1650" aria-hidden="true" tabindex="-1"></a>Suppose $(\mathbf{X}_1,\ldots, \mathbf{X}_n)$ is a sequence of random</span>
<span id="cb28-1651"><a href="#cb28-1651" aria-hidden="true" tabindex="-1"></a>vectors such that</span>
<span id="cb28-1652"><a href="#cb28-1652" aria-hidden="true" tabindex="-1"></a>$\sqrt n (\mathbf{X}_n - \mathbf t) \overset{d}{\to}N(\mathbf 0, \boldsymbol\Sigma)$</span>
<span id="cb28-1653"><a href="#cb28-1653" aria-hidden="true" tabindex="-1"></a>for some vector $\mathbf t$ in the interior of $\mathcal X$. If</span>
<span id="cb28-1654"><a href="#cb28-1654" aria-hidden="true" tabindex="-1"></a>$\mathbf g(\mathbf{X}_n)$ is a vector valued function that:</span>
<span id="cb28-1655"><a href="#cb28-1655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1656"><a href="#cb28-1656" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>is continuously differentiable,</span>
<span id="cb28-1657"><a href="#cb28-1657" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>does not involve $n$,</span>
<span id="cb28-1658"><a href="#cb28-1658" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>$\frac{\partial \mathbf g}{\partial\mathbf x}(\mathbf t) \neq 0$,<span class="ot">[^asymptotics-2]</span></span>
<span id="cb28-1659"><a href="#cb28-1659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1660"><a href="#cb28-1660" aria-hidden="true" tabindex="-1"></a>then,</span>
<span id="cb28-1661"><a href="#cb28-1661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1662"><a href="#cb28-1662" aria-hidden="true" tabindex="-1"></a>$$ \sqrt n \left<span class="co">[</span><span class="ot">\mathbf g(\mathbf{X}_n) - \mathbf g(\mathbf t)\right</span><span class="co">]</span> \overset{d}{\to}N\left(\mathbf 0, \left<span class="co">[</span><span class="ot">\frac{\partial \mathbf g}{\partial\mathbf x}(\mathbf t)\right</span><span class="co">]</span>\boldsymbol\Sigma\left<span class="co">[</span><span class="ot">\frac{\partial \mathbf g}{\partial\mathbf x}(\mathbf t)\right</span><span class="co">]</span>'\right)$$</span>
<span id="cb28-1663"><a href="#cb28-1663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1664"><a href="#cb28-1664" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1665"><a href="#cb28-1665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1666"><a href="#cb28-1666" aria-hidden="true" tabindex="-1"></a><span class="ot">[^asymptotics-2]: </span>$\frac{\partial \mathbf g}{\partial\mathbf x}(\mathbf t)$</span>
<span id="cb28-1667"><a href="#cb28-1667" aria-hidden="true" tabindex="-1"></a>    is the $\dim(\mathbf g(\mathbf{X}_n)) \times \dim(\mathbf X_n)$</span>
<span id="cb28-1668"><a href="#cb28-1668" aria-hidden="true" tabindex="-1"></a>    Jacobian matrix comprised of the partial derivatives of the</span>
<span id="cb28-1669"><a href="#cb28-1669" aria-hidden="true" tabindex="-1"></a>    components of $\mathbf g$.</span>
<span id="cb28-1670"><a href="#cb28-1670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1671"><a href="#cb28-1671" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb28-1672"><a href="#cb28-1672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1673"><a href="#cb28-1673" aria-hidden="true" tabindex="-1"></a>Return to the example where $X_i\overset{iid}{\sim}\text{Exp}(1)$,</span>
<span id="cb28-1674"><a href="#cb28-1674" aria-hidden="true" tabindex="-1"></a>giving $\text{E}\left<span class="co">[</span><span class="ot">X_i\right</span><span class="co">]</span> = 1$ and</span>
<span id="cb28-1675"><a href="#cb28-1675" aria-hidden="true" tabindex="-1"></a>$\text{Var}\left(X_i\right) = 1$. By the CLT,</span>
<span id="cb28-1676"><a href="#cb28-1676" aria-hidden="true" tabindex="-1"></a>$\sqrt n(\bar X - 1)\overset{d}{\to}N(0,1)$. If $g(t) = t^2 +3$, what is</span>
<span id="cb28-1677"><a href="#cb28-1677" aria-hidden="true" tabindex="-1"></a>the asymptotic distribution of $\sqrt{n}<span class="co">[</span><span class="ot">g(\bar X) - g(1)</span><span class="co">]</span>$? According</span>
<span id="cb28-1678"><a href="#cb28-1678" aria-hidden="true" tabindex="-1"></a>to the delta method we have \begin{align*}</span>
<span id="cb28-1679"><a href="#cb28-1679" aria-hidden="true" tabindex="-1"></a>&amp;\sqrt{n}<span class="co">[</span><span class="ot">g(\bar X) - g(1)</span><span class="co">]</span>  \overset{a}{\sim}N(0, 1<span class="co">[</span><span class="ot">g'(1)</span><span class="co">]</span>^2),<span class="sc">\\</span></span>
<span id="cb28-1680"><a href="#cb28-1680" aria-hidden="true" tabindex="-1"></a>\implies &amp; \sqrt{n}\left<span class="co">[</span><span class="ot">\bar X^2 - 1\right</span><span class="co">]</span> \overset{a}{\sim}N(0, 4).</span>
<span id="cb28-1681"><a href="#cb28-1681" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb28-1682"><a href="#cb28-1682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1685"><a href="#cb28-1685" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-1686"><a href="#cb28-1686" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> <span class="cf">function</span>(t){</span>
<span id="cb28-1687"><a href="#cb28-1687" aria-hidden="true" tabindex="-1"></a>  t<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">3</span></span>
<span id="cb28-1688"><a href="#cb28-1688" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb28-1689"><a href="#cb28-1689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1690"><a href="#cb28-1690" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">draw_N_estimates</span>(</span>
<span id="cb28-1691"><a href="#cb28-1691" aria-hidden="true" tabindex="-1"></a>  <span class="at">theta =</span> <span class="cf">function</span>(X){<span class="fu">sqrt</span>(<span class="fu">length</span>(X))<span class="sc">*</span>(<span class="fu">g</span>(<span class="fu">mean</span>(X)) <span class="sc">-</span> <span class="fu">g</span>(<span class="dv">1</span>))},</span>
<span id="cb28-1692"><a href="#cb28-1692" aria-hidden="true" tabindex="-1"></a>  <span class="at">N =</span> <span class="fl">1e4</span>,</span>
<span id="cb28-1693"><a href="#cb28-1693" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="fl">1e5</span>,</span>
<span id="cb28-1694"><a href="#cb28-1694" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist =</span> rexp,</span>
<span id="cb28-1695"><a href="#cb28-1695" aria-hidden="true" tabindex="-1"></a>  <span class="at">dist_params =</span> <span class="fu">list</span>(</span>
<span id="cb28-1696"><a href="#cb28-1696" aria-hidden="true" tabindex="-1"></a>    <span class="at">rate =</span> <span class="dv">1</span></span>
<span id="cb28-1697"><a href="#cb28-1697" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb28-1698"><a href="#cb28-1698" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb28-1699"><a href="#cb28-1699" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-1700"><a href="#cb28-1700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1701"><a href="#cb28-1701" aria-hidden="true" tabindex="-1"></a>We can plot a histogram of our estimates and overlay the distribution</span>
<span id="cb28-1702"><a href="#cb28-1702" aria-hidden="true" tabindex="-1"></a>$N(0,4)$.</span>
<span id="cb28-1703"><a href="#cb28-1703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1706"><a href="#cb28-1706" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-1707"><a href="#cb28-1707" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb28-1708"><a href="#cb28-1708" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot216</span></span>
<span id="cb28-1709"><a href="#cb28-1709" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb28-1710"><a href="#cb28-1710" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb28-1711"><a href="#cb28-1711" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-1712"><a href="#cb28-1712" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Histogram of adjusted sample mean transformed by g, and theoretical distribution given by the delta method"</span></span>
<span id="cb28-1713"><a href="#cb28-1713" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb28-1714"><a href="#cb28-1714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1715"><a href="#cb28-1715" aria-hidden="true" tabindex="-1"></a>results <span class="sc">%&gt;%</span> </span>
<span id="cb28-1716"><a href="#cb28-1716" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(estimate)) <span class="sc">+</span></span>
<span id="cb28-1717"><a href="#cb28-1717" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> <span class="fu">after_stat</span>(density)), <span class="at">color =</span> <span class="dv">1</span>, <span class="at">fill =</span> <span class="st">"white"</span>, <span class="at">bins =</span> <span class="dv">100</span>) <span class="sc">+</span> </span>
<span id="cb28-1718"><a href="#cb28-1718" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"Estimates of √n(g(X) - g(1)) "</span>) <span class="sc">+</span></span>
<span id="cb28-1719"><a href="#cb28-1719" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb28-1720"><a href="#cb28-1720" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> dnorm, <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">2</span>), <span class="at">color =</span> <span class="st">"red"</span>)</span>
<span id="cb28-1721"><a href="#cb28-1721" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-1722"><a href="#cb28-1722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1723"><a href="#cb28-1723" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1724"><a href="#cb28-1724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1725"><a href="#cb28-1725" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The big takeaway from the delta method is that estimators which are nice</span></span>
<span id="cb28-1726"><a href="#cb28-1726" aria-hidden="true" tabindex="-1"></a>functions of sample means will be asymptotically distributed according</span>
<span id="cb28-1727"><a href="#cb28-1727" aria-hidden="true" tabindex="-1"></a>to a normal distribution. Furthermore, any nice function of such an</span>
<span id="cb28-1728"><a href="#cb28-1728" aria-hidden="true" tabindex="-1"></a>estimator will also have a normal asymptotic distribution!</span>
<span id="cb28-1729"><a href="#cb28-1729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1732"><a href="#cb28-1732" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-1733"><a href="#cb28-1733" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb28-1734"><a href="#cb28-1734" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot217</span></span>
<span id="cb28-1735"><a href="#cb28-1735" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb28-1736"><a href="#cb28-1736" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-1737"><a href="#cb28-1737" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "A mediocre meme"</span></span>
<span id="cb28-1738"><a href="#cb28-1738" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb28-1739"><a href="#cb28-1739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1740"><a href="#cb28-1740" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"figures/meme.png"</span>)</span>
<span id="cb28-1741"><a href="#cb28-1741" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-1742"><a href="#cb28-1742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1743"><a href="#cb28-1743" aria-hidden="true" tabindex="-1"></a><span class="fu">## Little $o_p$, Big $O_p$, and Taylor Expansions</span></span>
<span id="cb28-1744"><a href="#cb28-1744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1745"><a href="#cb28-1745" aria-hidden="true" tabindex="-1"></a>We've talked a lot about whether or not random variables converge, and</span>
<span id="cb28-1746"><a href="#cb28-1746" aria-hidden="true" tabindex="-1"></a>how they converge, but not the rate at which they converge. We can</span>
<span id="cb28-1747"><a href="#cb28-1747" aria-hidden="true" tabindex="-1"></a>introduce some notation that allows us to quantify this rate.</span>
<span id="cb28-1748"><a href="#cb28-1748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1749"><a href="#cb28-1749" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb28-1750"><a href="#cb28-1750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1751"><a href="#cb28-1751" aria-hidden="true" tabindex="-1"></a>Given a sequence of random variables $X_n$, we say $X_n$ is [***little</span>
<span id="cb28-1752"><a href="#cb28-1752" aria-hidden="true" tabindex="-1"></a>"O.P" of*** $n^k$]{style="color:red"}, denoted $X_n = o_p(n^k)$, if</span>
<span id="cb28-1753"><a href="#cb28-1753" aria-hidden="true" tabindex="-1"></a>$X_n / n^k\overset{p}{\to}0$.</span>
<span id="cb28-1754"><a href="#cb28-1754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1755"><a href="#cb28-1755" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1756"><a href="#cb28-1756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1757"><a href="#cb28-1757" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Note that $X_n\overset{p}{\to}0$ implies that $X_n = o_p(1)$. The use of</span></span>
<span id="cb28-1758"><a href="#cb28-1758" aria-hidden="true" tabindex="-1"></a>"=" is a bit misleading in this definition, as $X_n = o_p(n^k)$ does not</span>
<span id="cb28-1759"><a href="#cb28-1759" aria-hidden="true" tabindex="-1"></a>establish any equality, instead referring to how $X_n$ behaves</span>
<span id="cb28-1760"><a href="#cb28-1760" aria-hidden="true" tabindex="-1"></a>asymptotically. For instance, if we have two sequences of random</span>
<span id="cb28-1761"><a href="#cb28-1761" aria-hidden="true" tabindex="-1"></a>variables $X_n$ and $Y_n$ such that $X_n\overset{p}{\to}X$ and</span>
<span id="cb28-1762"><a href="#cb28-1762" aria-hidden="true" tabindex="-1"></a>$Y_n\overset{p}{\to}0$, we have \$ X_n + Y_n \overset{p}{\to} X + 0 \$,</span>
<span id="cb28-1763"><a href="#cb28-1763" aria-hidden="true" tabindex="-1"></a>but could write $X_n + Y_n$ as $X_n + o_p(1)$. This emphasizes the</span>
<span id="cb28-1764"><a href="#cb28-1764" aria-hidden="true" tabindex="-1"></a>sequence $X_n$, and frames $Y_n$ as some negligible remainder term that</span>
<span id="cb28-1765"><a href="#cb28-1765" aria-hidden="true" tabindex="-1"></a>tends to zero.</span>
<span id="cb28-1766"><a href="#cb28-1766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1767"><a href="#cb28-1767" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb28-1768"><a href="#cb28-1768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1769"><a href="#cb28-1769" aria-hidden="true" tabindex="-1"></a>Given a sequence of random variables $X_n$, we say $X_n$ is [***big</span>
<span id="cb28-1770"><a href="#cb28-1770" aria-hidden="true" tabindex="-1"></a>"O.P" of*** $n^k$]{style="color:red"}, denoted $X_n = O_p(n^k)$, if for</span>
<span id="cb28-1771"><a href="#cb28-1771" aria-hidden="true" tabindex="-1"></a>all $\varepsilon &gt; 0$, there exists some $\delta$ and $N$ such that</span>
<span id="cb28-1772"><a href="#cb28-1772" aria-hidden="true" tabindex="-1"></a>$\Pr(|X_n/n^k| \ge \delta) &lt;\varepsilon$ for all $n &gt; N$. In other</span>
<span id="cb28-1773"><a href="#cb28-1773" aria-hidden="true" tabindex="-1"></a>words, $X_n/n^k$ is <span class="co">[</span><span class="ot">***bounded in probability***</span><span class="co">]</span>{style="color:red"}.</span>
<span id="cb28-1774"><a href="#cb28-1774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1775"><a href="#cb28-1775" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1776"><a href="#cb28-1776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1777"><a href="#cb28-1777" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; We are most interested in the case where $X_n = O_p(1)$. If this is the</span></span>
<span id="cb28-1778"><a href="#cb28-1778" aria-hidden="true" tabindex="-1"></a>case, then as $n\to\infty$, we can bound the area in the tails of</span>
<span id="cb28-1779"><a href="#cb28-1779" aria-hidden="true" tabindex="-1"></a>$f_{X_n}$ by some constant $\delta$ such that the area is negligible</span>
<span id="cb28-1780"><a href="#cb28-1780" aria-hidden="true" tabindex="-1"></a>(less than $\varepsilon$).</span>
<span id="cb28-1781"><a href="#cb28-1781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1782"><a href="#cb28-1782" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb28-1783"><a href="#cb28-1783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1784"><a href="#cb28-1784" aria-hidden="true" tabindex="-1"></a>We know that $\bar X \overset{d}{\to}N(\mu, \sigma^2/n)$ when</span>
<span id="cb28-1785"><a href="#cb28-1785" aria-hidden="true" tabindex="-1"></a>$\mathbf{X}$ is an iid sample. We have that $\bar X = O_p(1)$. We have</span>
<span id="cb28-1786"><a href="#cb28-1786" aria-hidden="true" tabindex="-1"></a>$$\Pr(|\bar X/1| \ge \delta) = \Pr(-\bar X\ge -\delta \text{ and }\delta \le \bar X) = 2\cdot\Pr(\bar X\ge \delta) = 2\left<span class="co">[</span><span class="ot">1 - \Phi\left(\frac{\delta - \mu}{\sigma/\sqrt n}\right)\right</span><span class="co">]</span>.$$</span>
<span id="cb28-1787"><a href="#cb28-1787" aria-hidden="true" tabindex="-1"></a>If we take the limit of this as $n\to \infty$ we have</span>
<span id="cb28-1788"><a href="#cb28-1788" aria-hidden="true" tabindex="-1"></a>$$ \lim_{n\to \infty}\Pr(|X\bar X/1| \ge \delta) = \lim_{n\to \infty}2\left<span class="co">[</span><span class="ot">1 - \Phi\left(\frac{\delta - \mu}{\sigma/\sqrt n}\right)\right</span><span class="co">]</span> = 0.$$</span>
<span id="cb28-1789"><a href="#cb28-1789" aria-hidden="true" tabindex="-1"></a>By the definition of a limit, there must exists some $N$ such that</span>
<span id="cb28-1790"><a href="#cb28-1790" aria-hidden="true" tabindex="-1"></a>$\Pr(|\bar X/1| \ge \delta) &lt; \varepsilon$ for any $n &gt; N$, so</span>
<span id="cb28-1791"><a href="#cb28-1791" aria-hidden="true" tabindex="-1"></a>$\bar X = O_p(1)$.</span>
<span id="cb28-1792"><a href="#cb28-1792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1793"><a href="#cb28-1793" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1794"><a href="#cb28-1794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1795"><a href="#cb28-1795" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb28-1796"><a href="#cb28-1796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1797"><a href="#cb28-1797" aria-hidden="true" tabindex="-1"></a>If $X_n \overset{d}{\to}X$, then $X_n = O_p(1)$.</span>
<span id="cb28-1798"><a href="#cb28-1798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1799"><a href="#cb28-1799" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1800"><a href="#cb28-1800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1801"><a href="#cb28-1801" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb28-1802"><a href="#cb28-1802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1803"><a href="#cb28-1803" aria-hidden="true" tabindex="-1"></a>If $X_n = o_p(1)$, then $X_n = O_p(1)$.</span>
<span id="cb28-1804"><a href="#cb28-1804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1805"><a href="#cb28-1805" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1806"><a href="#cb28-1806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1807"><a href="#cb28-1807" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A common place to encounter $o_p$ is when performing Taylor expansions.</span></span>
<span id="cb28-1808"><a href="#cb28-1808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1809"><a href="#cb28-1809" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb28-1810"><a href="#cb28-1810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1811"><a href="#cb28-1811" aria-hidden="true" tabindex="-1"></a><span class="fu">## Taylor's Theorem</span></span>
<span id="cb28-1812"><a href="#cb28-1812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1813"><a href="#cb28-1813" aria-hidden="true" tabindex="-1"></a>Taylor's theorem, as given in @rudin1976principles, tells us that if</span>
<span id="cb28-1814"><a href="#cb28-1814" aria-hidden="true" tabindex="-1"></a>$f:\mathbb R\to\mathbb R$ is $k-$times differentiable at a point</span>
<span id="cb28-1815"><a href="#cb28-1815" aria-hidden="true" tabindex="-1"></a>$a\in \mathbb R$, then there is some element $c\in (a,b)$ such that</span>
<span id="cb28-1816"><a href="#cb28-1816" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb28-1817"><a href="#cb28-1817" aria-hidden="true" tabindex="-1"></a>f(b) &amp; = \sum_{j=0}^{k-1}\frac{f^{(j)}(a)}{k!}(b-a)^j + \frac{f^{(n)}(c)}{k!}(b-a)^k.</span>
<span id="cb28-1818"><a href="#cb28-1818" aria-hidden="true" tabindex="-1"></a>\end{align*} For $n = 2$, we have the mean value theorem:</span>
<span id="cb28-1819"><a href="#cb28-1819" aria-hidden="true" tabindex="-1"></a>$$ f(b)= f(a) + f'(c)(b-a).$$ If we let $a\to b$, then \begin{align*}</span>
<span id="cb28-1820"><a href="#cb28-1820" aria-hidden="true" tabindex="-1"></a>&amp;\lim_{a\to b}f(b)  = \sum_{j=0}^{k-1}\lim_{a\to b}\frac{f^{(j)}(a)}{j!}(b-a)^j + \lim_{a\to b}\frac{f^{(k)}(c)}{k!}(b-a)^k.<span class="sc">\\</span></span>
<span id="cb28-1821"><a href="#cb28-1821" aria-hidden="true" tabindex="-1"></a>\implies &amp; f(b)  =\lim_{a\to b} f(a) + \sum_{j=1}^{k-1}\lim_{a\to b}\frac{f^{(j)}(a)}{j!}(b-a)^j +  \lim_{a\to b}\frac{f^{(k)}(c)}{k!}(b-a)^k<span class="sc">\\</span></span>
<span id="cb28-1822"><a href="#cb28-1822" aria-hidden="true" tabindex="-1"></a>\implies &amp; f(b)  = f(b) + \underbrace{\sum_{j=1}^{k-1}\frac{f^{(j)}(b)}{j!}(b-b)^j}_0 + \lim_{a\to b}\frac{f^{(k)}(c)}{k!}(b-a)^k<span class="sc">\\</span></span>
<span id="cb28-1823"><a href="#cb28-1823" aria-hidden="true" tabindex="-1"></a>\implies &amp; \lim_{a\to b}\frac{f^{(k)}(c)}{n!}(b-a)^k = 0<span class="sc">\\</span></span>
<span id="cb28-1824"><a href="#cb28-1824" aria-hidden="true" tabindex="-1"></a>\implies &amp; \lim_{a\to b}\frac{f^{(k)}(c)}{n!(b-a)^k} = 0<span class="sc">\\</span></span>
<span id="cb28-1825"><a href="#cb28-1825" aria-hidden="true" tabindex="-1"></a>\implies &amp; \frac{f^{(k)}(c)}{k!} = o(|a-b|^k)<span class="sc">\\</span></span>
<span id="cb28-1826"><a href="#cb28-1826" aria-hidden="true" tabindex="-1"></a>\end{align*} Here, $o$ is the deterministic counterpart of $o_p$ (we're</span>
<span id="cb28-1827"><a href="#cb28-1827" aria-hidden="true" tabindex="-1"></a>not working with random variables just yet). This means we can write</span>
<span id="cb28-1828"><a href="#cb28-1828" aria-hidden="true" tabindex="-1"></a>Taylor's theorem as</span>
<span id="cb28-1829"><a href="#cb28-1829" aria-hidden="true" tabindex="-1"></a>$$ f(b) = \sum_{j=0}^{k-1}\frac{f^{(j)}(a)}{j!}(b-a)^k +o(|a-b|^k).$$ In</span>
<span id="cb28-1830"><a href="#cb28-1830" aria-hidden="true" tabindex="-1"></a>the event $f$ is infinitely differentiable we can make this</span>
<span id="cb28-1831"><a href="#cb28-1831" aria-hidden="true" tabindex="-1"></a>approximation arbitrarily accurate, giving rise toa function's Taylor</span>
<span id="cb28-1832"><a href="#cb28-1832" aria-hidden="true" tabindex="-1"></a>series.</span>
<span id="cb28-1833"><a href="#cb28-1833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1834"><a href="#cb28-1834" aria-hidden="true" tabindex="-1"></a>Now suppose $f_n(X)$ is a sequence of functions of random variables,</span>
<span id="cb28-1835"><a href="#cb28-1835" aria-hidden="true" tabindex="-1"></a>where the subscript $n$ emphasizes that $f_n$ is a random variable. IF</span>
<span id="cb28-1836"><a href="#cb28-1836" aria-hidden="true" tabindex="-1"></a>we apply Taylor's theorem to $f_n(X)$ we have</span>
<span id="cb28-1837"><a href="#cb28-1837" aria-hidden="true" tabindex="-1"></a>$$f_n(b) = \sum_{j=0}^{k-1}\frac{f_n^{(j)}(a)}{j!}(b-a)^k +o_p(|a-b|^k)$$</span>
<span id="cb28-1838"><a href="#cb28-1838" aria-hidden="true" tabindex="-1"></a>for realizations of the random variable $a,b,c\in\mathcal X$ where</span>
<span id="cb28-1839"><a href="#cb28-1839" aria-hidden="true" tabindex="-1"></a>$c\in(a,b)$. Assuming $k \ge 1$, then $o_p(|a-b|^k)$ implies $o_p(1)$,</span>
<span id="cb28-1840"><a href="#cb28-1840" aria-hidden="true" tabindex="-1"></a>so $$f_n(b) = \sum_{j=0}^{k-1}\frac{f_n^{(j)}(a)}{j!}(b-a)^k +o_p(1).$$</span>
<span id="cb28-1841"><a href="#cb28-1841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1842"><a href="#cb28-1842" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1843"><a href="#cb28-1843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1844"><a href="#cb28-1844" aria-hidden="true" tabindex="-1"></a><span class="fu">## Asymptotically Normal Estimators</span></span>
<span id="cb28-1845"><a href="#cb28-1845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1846"><a href="#cb28-1846" aria-hidden="true" tabindex="-1"></a>When putting our asymptotic tools to work on an estimator of interest,</span>
<span id="cb28-1847"><a href="#cb28-1847" aria-hidden="true" tabindex="-1"></a>we will almost always find that it converges to a normal distribution,</span>
<span id="cb28-1848"><a href="#cb28-1848" aria-hidden="true" tabindex="-1"></a>is consistent, and that the rate of convergence is linked to $\sqrt{n}$.</span>
<span id="cb28-1849"><a href="#cb28-1849" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1850"><a href="#cb28-1850" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb28-1851"><a href="#cb28-1851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1852"><a href="#cb28-1852" aria-hidden="true" tabindex="-1"></a>An estimator $\hat{\boldsymbol{\theta}}$ is [$\sqrt{n}-$consistent</span>
<span id="cb28-1853"><a href="#cb28-1853" aria-hidden="true" tabindex="-1"></a>asymptotically normal (root-n CAN)]{style="color:red"}, if</span>
<span id="cb28-1854"><a href="#cb28-1854" aria-hidden="true" tabindex="-1"></a>$$\sqrt{n}(\hat{\boldsymbol{\theta}}- \boldsymbol{\theta}) \overset{d}{\to}N(\mathbf 0, \mathbf V)$$</span>
<span id="cb28-1855"><a href="#cb28-1855" aria-hidden="true" tabindex="-1"></a>for a PSD matrix $\mathbf V$. Equivalently,</span>
<span id="cb28-1856"><a href="#cb28-1856" aria-hidden="true" tabindex="-1"></a>$$ \hat{\boldsymbol{\theta}}\overset{a}{\sim}N(\boldsymbol{\theta}, \mathbf V/n).$$</span>
<span id="cb28-1857"><a href="#cb28-1857" aria-hidden="true" tabindex="-1"></a>We refer to $\mathbf V/n$ as the [***asymptotic</span>
<span id="cb28-1858"><a href="#cb28-1858" aria-hidden="true" tabindex="-1"></a>variance***]{style="color:red"} of $\hat{\boldsymbol{\theta}}$ and write</span>
<span id="cb28-1859"><a href="#cb28-1859" aria-hidden="true" tabindex="-1"></a>$\text{Avar}\left(\hat{\boldsymbol{\theta}}\right) = \mathbf V /n$.</span>
<span id="cb28-1860"><a href="#cb28-1860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1861"><a href="#cb28-1861" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-1862"><a href="#cb28-1862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1863"><a href="#cb28-1863" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "$\sqrt n-$" emphasizes the fact that</span></span>
<span id="cb28-1864"><a href="#cb28-1864" aria-hidden="true" tabindex="-1"></a>$\sqrt{n}(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}) = O_p(1)$,</span>
<span id="cb28-1865"><a href="#cb28-1865" aria-hidden="true" tabindex="-1"></a>which is equivalent to</span>
<span id="cb28-1866"><a href="#cb28-1866" aria-hidden="true" tabindex="-1"></a>$\hat{\boldsymbol{\theta}} = \boldsymbol{\theta}+ O_p(n^{-1/2})$. In</span>
<span id="cb28-1867"><a href="#cb28-1867" aria-hidden="true" tabindex="-1"></a>other words, as $n\to\infty$ the error term associated with our estimate</span>
<span id="cb28-1868"><a href="#cb28-1868" aria-hidden="true" tabindex="-1"></a>decreases at a rate of $n^{1/2}$. A fourfold increase in observations</span>
<span id="cb28-1869"><a href="#cb28-1869" aria-hidden="true" tabindex="-1"></a>results in half the error. We also have that</span>
<span id="cb28-1870"><a href="#cb28-1870" aria-hidden="true" tabindex="-1"></a>$\sqrt{n}(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}) = o_p(1)$, so</span>
<span id="cb28-1871"><a href="#cb28-1871" aria-hidden="true" tabindex="-1"></a>$\hat{\boldsymbol{\theta}}\overset{p}{\to}\boldsymbol{\theta}$, hence</span>
<span id="cb28-1872"><a href="#cb28-1872" aria-hidden="true" tabindex="-1"></a>the "consistent" in the previous definition. We also have that</span>
<span id="cb28-1873"><a href="#cb28-1873" aria-hidden="true" tabindex="-1"></a>$\hat{\boldsymbol{\theta}}$ is asymptotically unbiased if it is root-n</span>
<span id="cb28-1874"><a href="#cb28-1874" aria-hidden="true" tabindex="-1"></a>CAN, as</span>
<span id="cb28-1875"><a href="#cb28-1875" aria-hidden="true" tabindex="-1"></a>$\text{E}\left<span class="co">[</span><span class="ot">\hat{\boldsymbol{\theta}}\right</span><span class="co">]</span>\to \boldsymbol{\theta}$.</span>
<span id="cb28-1876"><a href="#cb28-1876" aria-hidden="true" tabindex="-1"></a>This will be the one of, if not the, ***most important property*** an</span>
<span id="cb28-1877"><a href="#cb28-1877" aria-hidden="true" tabindex="-1"></a>estimator can posses.</span>
<span id="cb28-1878"><a href="#cb28-1878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1879"><a href="#cb28-1879" aria-hidden="true" tabindex="-1"></a><span class="fu">## Further Reading</span></span>
<span id="cb28-1880"><a href="#cb28-1880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-1881"><a href="#cb28-1881" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Eric Zivot's [primer on</span>
<span id="cb28-1882"><a href="#cb28-1882" aria-hidden="true" tabindex="-1"></a>    asymptotics](http://faculty.washington.edu/ezivot/econ583/econ583asymptoticsprimer.pdf)</span>
<span id="cb28-1883"><a href="#cb28-1883" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>@wooldridge2010econometric, Chapter 3</span>
<span id="cb28-1884"><a href="#cb28-1884" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>@greene2003econometric, Appendix D</span>
<span id="cb28-1885"><a href="#cb28-1885" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>@van2000asymptotic</span>
<span id="cb28-1886"><a href="#cb28-1886" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>@white2014asymptotic</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>