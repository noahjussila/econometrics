{"title":"Asymptotic Properties of Estimators","markdown":{"yaml":{"editor":{"markdown":{"wrap":72}}},"headingText":"Asymptotic Properties of Estimators","headingAttr":{"id":"sec-asy","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n\\DeclareMathOperator{\\plim}{plim}\n\\DeclareMathOperator{\\argmin}{argmin}\n\\DeclareMathOperator{\\argmax}{argmax}\n\\newcommand{\\var}[1]{\\text{Var}\\left(#1\\right)}\n\\newcommand{\\avar}[1]{\\text{Avar}\\left(#1\\right)}\n\\newcommand{\\E}[1]{\\text{E}\\left[#1\\right]}\n\\newcommand{\\cov}[1]{\\text{Cov}\\left(#1\\right)}\n\\newcommand{\\mse}[1]{\\text{MSE}\\left(#1\\right)}\n\\newcommand{\\se}[1]{\\text{se}\\left(#1\\right)}\n\\newcommand{\\limfunc}{lim} \n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\Xm}{\\mathbb{X}}\n\\newcommand{\\EER}{\\bar{\\thet}_\\text{EE}}\n\\newcommand{\\NLS}{\\hat{\\bet}_\\text{NLLS}}\n\\newcommand{\\z}{\\mathbf{z}}\n\\newcommand{\\rr}{\\mathbf{r}}\n\\newcommand{\\C}{\\mathbf{C}}\n\\newcommand{\\Pe}{\\mathbf{P}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\uu}{\\mathbf{u}}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\xm}{\\mathbb{x}}\n\\newcommand{\\Zm}{\\mathbb{Z}}\n\\newcommand{\\Wm}{\\mathbb{W}}\n\\newcommand{\\Hm}{\\mathbb{H}}\n\\newcommand{\\W}{\\mathbf{W}}\n\\newcommand{\\Z}{\\mathbf{Z}}\n\\newcommand{\\Hess}{\\mathbf{H}(\\mathbf{\\Z\\mid\\thet})}\n\\newcommand{\\Score}{\\mathbf{S}(\\mathbf{\\Z\\mid\\thet})}\n\\newcommand{\\A}{\\mathbf{A}}\n\\newcommand{\\h}{\\mathbf{h}}\n\\newcommand{\\Q}{\\mathbf{Q}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\G}{\\mathbf{G}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\renewcommand{\\D}{\\mathbf{D}}\n\\renewcommand{\\C}{\\mathbf{C}}\n\\newcommand{\\zer}{\\mathbf{0}}\n\\newcommand{\\OLS}{\\hat{\\boldsymbol\\beta}_\\text{OLS} }\n\\newcommand{\\OLSOV}{\\hat{\\boldsymbol\\beta}_\\text{OLS,OV} }\n\\newcommand{\\OLSME}{\\hat{\\boldsymbol\\beta}_\\text{OLS,ME} }\n\\newcommand{\\EE}{\\hat{\\boldsymbol\\theta}_\\text{EX} }\n\\newcommand{\\ME}{\\hat{\\boldsymbol\\theta}_\\text{M} }\n\\newcommand{\\MDE}{\\hat{\\boldsymbol\\theta}_\\text{MDE} }\n\\newcommand{\\IV}{\\hat{\\boldsymbol\\beta}_\\text{IV} }\n\\newcommand{\\TSLS}{\\hat{\\boldsymbol\\beta}_\\text{2SLS} }\n\\newcommand{\\thet}{\\boldsymbol{\\theta}}\n\\newcommand{\\et}{\\boldsymbol{\\eta}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Sig}{\\boldsymbol{\\Sigma}}\n\\newcommand{\\ep}{\\boldsymbol{\\varepsilon}}\n\\newcommand{\\Omeg}{\\boldsymbol{\\Omega}}\n\\newcommand{\\Thet}{\\boldsymbol{\\Theta}}\n\\newcommand{\\bet}{\\boldsymbol{\\beta}}\n\\newcommand{\\rk}{rank}\n\\newcommand{\\tsum}{\\sum}\n\\newcommand{\\tr}{tr}\n\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\\newcommand{\\abs}[1]{\\left\\lvert#1\\right\\rvert}\n\\newcommand{\\ms}{\\overset{ms}{\\to}}\n\\newcommand{\\pto}{\\overset{p}{\\to}}\n\\newcommand{\\asto}{\\overset{as}{\\to}}\n\\newcommand{\\iid}{\\overset{iid}{\\sim}}\n\\newcommand{\\dto}{\\overset{d}{\\to}}\n\\newcommand{\\asim}{\\overset{a}{\\sim}}\n\n\n```{r}\n#| echo: false\n#| output: false\nlibrary(tidyverse)\nlibrary(gganimate)\n```\n\nWhen considering estimators in @sec-est, we kept the sample size\n$n$ fixed when assessing estimators. We now consider how estimators\nbehave as $n\\to\\infty.$ In practice, we will never have infinite data,\nasymptotics gives us an approximate idea of how estimators perform for\nlarge data sets. A comprehensive reference in asymptotic theory is due\nto @van2000asymptotic. For a treatment concerned purely with\neconometrics, @newey1994large provide a phenomenal survey, most of which\nwe will touch on when discussing general classes of estimators.\\\n      With loss of some generality, we will assume that all random\nvariables have finite expectation and variances. Dispensing with this\nassumption is something for a probability course.\n\n## Convergence\n\nAt some point in high school, most students encounter the concept of a\nnumeric sequence, and how they can converge to a limit. Later on,\nperhaps when taking a real analysis course, sequences are generalized to\nspaces of functions. A sequence of functions may also converge to a\nlimit, whether that be converging pointwise and/or converging uniformly\n(for details see @rudin1976principles). Random variables are functions\nfrom a sample space to $\\mathbb R$, so we can consider how these\nfunctions converge. \n\n### Convergence in MSE\n\nThe first type of convergence we'll work with deals with MSE.\n\n::: {#def-}\nA sequence of random variables $X_n$ [***converges in mean\nsquare***]{style=\"color:red\"} to a random variable $X$, written as\n$X_n\\overset{ms}{\\to} X$, if\n$$\\lim_{n\\to\\infty} \\text{E}\\left[(X_n - X)^2\\right] = 0.$$ A sequence\nof random vectors $\\mathbf{X}_n = (X_{1,n},\\ldots X_{k,n})$ converges in\nmean square to $\\mathbf{X}$ if $X_{i,n}\\overset{ms}{\\to} X_i$ for\n$i=1,\\ldots, k$.\n:::\n\n      $X_n \\overset{ms}{\\to}X$ if the average distance between $X_n$ and\n$X$ shrinks as $n\\to\\infty$ where distance is measured as $(X_n - X)^2$.\nWe can also have $X_n \\overset{ms}{\\to}c$ for some constant $c$, as $c$\nis a trivial random variable.\n\n::: {#exm-}\nSuppose we draw a sample of $n$ iid random variables $Z_i$ and define\n$X_n$ to be the sample mean of our observations.\n$$X_n = \\frac{1}{n}\\sum_{i=1}^n Z_i$$ If\n$\\text{E}\\left[Z_i\\right] = \\mu$ and\n$\\text{Var}\\left(Z_i\\right) = \\sigma^2$ for all $i$, we have\n$X_n\\overset{ms}{\\to}\\mu$: \\begin{align*}\n\\lim_{n\\to\\infty}\\text{E}\\left[(X_n - \\mu)^2\\right] & = \\lim_{n\\to\\infty}\\text{Var}\\left(X_n\\right) + \\text{Bias}(X_n) \\\\\n&= \\lim_{n\\to\\infty} \\frac{\\sigma^2}{n} + 0 & (X_n \\text{ unbiased}) \\\\\n& = \\lim_{n\\to\\infty} \\frac{\\sigma^2}{n} \\\\\n& = 0.\n\\end{align*}\n\nWhat does this convergence \"look like\"? If\n$Z_i\\overset{iid}{\\sim}N(0,1)$, we know that\n$X_n = \\bar Z \\sim N(\\mu, \\sigma^2/n)$. Let's plot this distribution for\nincreasing values of $n$.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot21\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"The distribution of X_n for increasing values of n\"\n#| code-summary: \"Show code which generates figure\"\nexpand_grid(\n  x = seq(-3, 3, length = 500),\n  n = 1:100\n) %>% \n  mutate(\n    y = dnorm(x, 0, sqrt(1/n))\n  ) %>% \n  ggplot(aes(x,y)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Value of X_n\", y = \"Density\", color = \"Sample Size\") +\n  theme(legend.position = \"bottom\") +\n  transition_states(n) +\n  labs(title = 'n = {closest_state}')\n```\n:::\n\n      This example betrays a useful property related to variables which\nconverge in mean square.\n\n::: {#prp-mse3}\nA sequence of random variables $X_n$ converges in mean square to a\nconstant $c$ *if and only if* $\\text{E}\\left[X_n\\right]\\to c$ and\n$\\text{Var}\\left(X_n\\right)\\to 0$.\n:::\n\n::: proof\n[space]{style=\"color:white\"}\n\n$(\\Longrightarrow)$ Suppose $X_n \\overset{ms}{\\to}c$. Then\n\\begin{align*}\n& \\lim_{n\\to\\infty}\\text{E}\\left[(X_n - c)^2\\right] = 0\\\\\n \\implies & \\lim_{n\\to\\infty}\\text{E}\\left[(X_n - c)^2\\right] = 0\\\\\n \\implies& \\lim_{n\\to\\infty}\\left[\\text{E}\\left[X_n\\right]^2 -2c \\text{E}\\left[X_n\\right] + c^2\\right]= 0\\\\\n \\implies& \\lim_{n\\to\\infty}\\left[(\\text{E}\\left[X_n\\right]^2 -\\text{E}\\left[X_n\\right]^2) + \\text{E}\\left[X_n\\right]^2 - 2c \\text{E}\\left[X_n\\right] + c^2\\right]= 0 \\\\ \n \\implies &\\lim_{n\\to\\infty} \\text{Var}\\left(X_n\\right) + \\lim_{n\\to\\infty}\\left[\\text{E}\\left[X_n\\right] -c\\right]^2 = 0 \n\\end{align*} This final equality gives the desired result.\n\n$(\\Longleftarrow)$ Suppose $\\text{E}\\left[X_n\\right]\\to c$ and\n$\\text{Var}\\left(X_n\\right)\\to 0$. We have \\begin{align*}\n&\\lim_{n\\to\\infty} \\text{Var}\\left(X_n\\right) + \\lim_{n\\to\\infty}\\left[\\text{E}\\left[X_n\\right] -c\\right]^2 = 0 \\\\\n\\implies & \\lim_{n\\to\\infty}\\text{E}\\left[(X_n - c)^2\\right] = 0\\\\\n\\implies & X_n\\overset{ms}{\\to}c\n\\end{align*}\n:::\n\n::: {#cor-mseconv}\nSuppose $X_n$ is a sequence of random variables such that\n$\\text{E}\\left[X_n\\right] = c$ for all $n$. Then $X_n\\overset{ms}{\\to}c$\n*if and only if* $\\text{Var}\\left(X_n\\right)\\to 0$.\n:::\n\n### Convergence in Probability\n\nConvergence in mean square captures the idea that a random variable gets\n\"closer\" to some value $c,$ but it is hardly the only way to define this\nbehavior. A more \"traditional\" approach would be defining convergence\nusing an inequality involving an arbitrarily small $\\varepsilon >0$\n(akin the to $\\varepsilon-\\delta$ definition of a limit).\n\n::: {#def-}\nA sequence of random variables $X_n$ [***converges in\nprobability***]{style=\"color:red\"} to a random variable $X$, written as\n$X_n\\overset{p}{\\to}X$ or $\\mathop{\\mathrm{plim}}X_n = X$, if\n$$\\lim_{n\\to\\infty} \\Pr (|X_n - X| > \\varepsilon)= 0$$ for all\n$\\varepsilon > 0$. Equivalently, $X_n\\overset{p}{\\to}X$ if for all\n$\\varepsilon > 0$ and $\\delta > 0$, there exists some $N$ such that for\nall $n \\ge N$, $$ \\Pr (|X_n - X| > \\varepsilon) < \\delta.$$ A sequence\nof random vectors $\\mathbf{X}_n = (X_{1,n},\\ldots X_{k,n})$ converges in\nprobability to $\\mathbf{X}$ if $X_{i,n}\\overset{p}{\\to} X_i$ for\n$i=1,\\ldots, k$.\n:::\n\n      Intuitively, $X_n \\overset{p}{\\to}X$ if the probability that the\ndifference $|X_n - X|$ is not small (greater than some $\\varepsilon$)\ngoes to zero as $n\\to\\infty$. In other words, for large values of $n$, there is \na large probability that $X_n$ is close to $X$\n\n::: {#exm-}\nReturn to the previous example where $X_n = \\bar Z$, and assume\n$Z_i \\overset{iid}{\\sim}N(\\mu,\\sigma^2)$. We will verify that\n$X_n\\overset{p}{\\to}\\mu$ using the definition of convergence in\nprobability using the fact that $X_n \\sim N(\\mu, \\sigma^2/n)$.\n\n```{r}\n#| fig-align: center\n#| label: fig-plot22\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"For any arbitrary ε, we can find some n such that the probability X_n falls outside the set |μ-ε| is arbitrarily small\"\n#| echo: false\n\nknitr::include_graphics(\"figures/converge.png\")\n```\n\nFor some $\\varepsilon > 0$, \\begin{align*}\n\\Pr (|X_n - \\mu| > \\varepsilon) & = 1 - \\Pr (\\mu - \\varepsilon < X_n < \\mu + \\varepsilon)\\\\\n& = 1 - (F_{X_n}(\\mu + \\varepsilon) + F_{X_n}(\\mu - \\varepsilon))\\\\\n& = 1 - 2\\left[F_{X_n}(\\mu + \\varepsilon) - \\frac{1}{2}\\right] & (F_{X_n} \\text{symmetric about }\\mu)\\\\\n & = 1 - 2\\left[\\Phi\\left(\\frac{(\\mu + \\varepsilon) - \\mu}{\\sigma/\\sqrt{n}}\\right) - \\frac{1}{2}\\right] & (\\Phi\\text{ standard normal distribution})\\\\\n & = 1 - 2\\left[\\Phi\\left(\\frac{\\varepsilon}{\\sigma/\\sqrt{n}}\\right) - \\frac{1}{2}\\right].\n\\end{align*} Given some $\\delta >0$, we can solve for the lowest value\nof $n$ that satisfies $\\Pr (|X_n - c| > \\varepsilon) < \\delta$.\n\\begin{align*}\n&\\Pr (|X_n - c| > \\varepsilon) < \\delta \\\\ \n\\implies & 1 - 2\\left[\\Phi\\left(\\frac{\\varepsilon}{\\sigma/\\sqrt{n}}\\right) - \\frac{1}{2}\\right] < \\delta \\\\\n\\implies&  n > \\left(\\frac{\\sigma \\Phi^{-1}(1-\\delta/2)}{\\varepsilon}\\right)^2\n\\\\\\implies & n > \\left\\lceil \\left(\\frac{\\sigma \\Phi^{-1}(1-\\delta/2)}{\\varepsilon}\\right)^2 \\right\\rceil\n\\end{align*} Just to be excruciatingly pedantic, we rounded our solution\nup to the closest positive integer, as $n$ corresponds to a sample size.\nFor fixed values of $\\mu$ and $\\sigma^2$ (say 3 and 2, respectively), we\ncan define a function of $(\\varepsilon, \\delta)$ which calculates the\nsample size required to satisfy $\\Pr(|X_n - c|>\\varepsilon)<\\delta$.\n\n```{r}\nmu <- 3\nsigma <- sqrt(2)\n\nn_fun <- function(delta, ep){\n ceiling(((sigma*qnorm(1-delta /2))/ep)^2) \n}\n```\n\nLet's plot this function for various values of $(\\varepsilon, \\delta)$.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot23\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"The sample size required to satisfy the inequality in the definition of convergence of probability for various values (ε,δ)\"\n#| code-summary: \"Show code which generates figure\"\nexpand_grid(\n  e = c(1, 0.1, 0.01, 0.001, 0.0001),\n  d = 1:9999/10000\n) %>% \n  mutate(sample = n_fun(d,e)) %>% \n  ggplot(aes(d, sample, color = as.factor(e))) +\n  scale_x_reverse() +\n  scale_y_log10() + \n  geom_line() +\n  theme_minimal() +\n  labs(x = \"δ\", y = \"Sample Size\", color = \"ε\")+\n  theme(legend.position = \"bottom\")\n```\n\nWe can also verify that\n$\\lim_{n\\to\\infty}\\Pr (|X_n - \\mu| > \\varepsilon) = 0$ for various\nvalues of $\\varepsilon.$\n\n```{r}\nprob_ep <- function(n, ep){\n  1 - 2*(pnorm(ep / (sigma / sqrt(n))) - 1/2)\n}\n```\n\n```{r}\n#| code-fold: true\n#| label: fig-plot24\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"The probability that X_n falls outside the interval  |μ-ε| for various values of (ε,n) \"\n#| code-summary: \"Show code which generates figure\"\nexpand_grid(\n  e = c(1, 0.1, 0.01, 0.001, 0.0001),\n  n = 1:100\n) %>% \n  mutate(prob = prob_ep(e,n)) %>% \n  ggplot(aes(n, prob, color = as.factor(e))) +\n  geom_line() +\n  scale_x_log10() +\n  theme_minimal() +\n  labs(x = \"Sample Size\", y = \"Pr(|X_n - mu| > ε)\", color = \"ε\") +\n  theme(legend.position = \"bottom\") \n```\n:::\n\n      How does convergence in mean square related to convergence in\nprobability? As it turns out the latter is a weaker condition implied by\nthe prior. Before stating and proving this result, we will need a lemma.\n\n::: {#lem-markovineq}\n## Markov's inequality\n\nIf $X$ is a nonnegative random variable, and $a > 0$, then\n$$\\Pr(X\\ge a) \\le \\frac{\\text{E}\\left[X\\right]}{a}$$\n:::\n\n::: proof\nThe expectation of $X$ can be written as \\begin{align*}\n\\text{E}\\left[X\\right] & = \\int_{-\\infty}^\\infty x\\ dF_X(x) \\\\ \n& = \\int_{0}^\\infty x\\ dF_X(x) & (X\\text{ is nonnegative}) \\\\ \n& = \\int_{0}^a x\\ dF_X(x) + \\int_{a}^\\infty x\\ dF_X(x) \\\\ \n& \\ge \\int_a^\\infty x\\ dF_X(x)\\\\\n& \\ge \\int_a^\\infty a\\ dF_X(x) & (a \\ge x \\text{ on }(a,\\infty))\\\\\n& = a \\int_a^\\infty\\ dF_X(x) \\\\\n& = a\\Pr(X \\ge a).\n\\end{align*} Dividing both sides of this inequality by $a$ gives\n$\\Pr(X\\ge a) \\le \\text{E}\\left[X\\right]/a$.\n:::\n\n::: {#prp-conv}\n## Convergence in MSE --\\> Convergence in Probability\n\nLet $X_n$ be a sequence of random variables. If $X_n\\overset{ms}{\\to}X$,\nthen $X_n\\overset{p}{\\to}X$.\n:::\n\n::: proof\nSuppose $X_n\\overset{ms}{\\to}X$. For all $\\varepsilon > 0$\n\\begin{align*}\n\\lim_{n\\to\\infty} \\Pr (|X_n - X| > \\varepsilon) & = \\lim_{n\\to\\infty} \\Pr ((X_n - X)^2 > \\varepsilon^2) \\\\\n& \\le \\lim_{n\\to\\infty} \\frac{\\text{E}\\left[(X_n - X)^2\\right]}{\\varepsilon^2} & (\\text{Markov's Inequality}) \\\\\n& = \\frac{0}{\\varepsilon^2} & (X_n\\overset{ms}{\\to}X)\\\\\n& = 0.\n\\end{align*} Therefore $X_n\\overset{p}{\\to}c$.\n:::\n\n      The usefulness of @prp-conv cannot be emphasized enough. Proving\nconvergence in probability using the definition is cumbersome, so we\nwill almost show convergence in mean square and then appeal to @prp-conv\nto verify convergence in probability. Nevertheless, situations can arise\nwhere $X_n\\overset{p}{\\to} X$, but $X_n \\not\\overset{ms}{\\to} X$.\n\n::: {#exm-pnotmse}\n\n## Convergence in Probability but not in Mean Square\n\nSuppose $X_n$ is defined on the sample space $\\{1,n^2\\}$ such that:\n\\begin{align*}\n\\Pr(X_n = 0) &= 1-1/n\\\\\n\\Pr(X_n = n^2) &= 1/n\n\\end{align*}\n\n```{r}\n#| code-fold: true\n#| label: fig-plot24.2\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"The probability density function associated with X_n for increasing values of n.\"\n#| code-summary: \"Show code which generates figure\"\nexpand_grid(n = 1:50, x = 0:1) %>% \n  mutate(\n    x = x*(n^2),\n    p = (1-1/n)*(x == 0) + (1/n)*(x != 0)\n  ) %>% \n  ggplot(aes(x = x, y = p)) +\n  geom_point() +\n  geom_segment(aes(x = x, xend = x, y = 0, yend = p)) +\n  theme_minimal() +\n  scale_x_sqrt(breaks = (1:10)*500) + \n  labs(color = \"n\", y = \"Probability Density\", x = \"X_n\") +\n  theme(legend.position = \"bottom\")  + \n  transition_states(n) +\n  labs(title = 'n = {closest_state}')\n```\nThe expected value of $X_n$ is\n$$\\text{E}\\left[X_n\\right] = 0(1-1/n) + n^2(1/n) = n,$$ so\n$\\text{E}\\left[X_n\\right]\\to\\infty$ as $n\\to \\infty$. This rules out\n$X_n$ converging in mean square to any value. Nevertheless, we have\n$X_n\\overset{p}{\\to}0$. For all $\\varepsilon > 0$,\n$$\\Pr(|X_n - 0| > \\varepsilon) = \\Pr(X_n \\neq 0) = \\Pr(X_n = n^2) = 1/n \\to 0.$$\nThis disagreement among definitions of convergence arises because the convergence in probability\nonly takes into consideration the probability assigned to each value in the sample space, whereas convergence in MSE is based on an expectation which takes into consideration the probability assigned to each value in the sample space *weighted* by the value in the sample space. In this particular example, the probability that $X_n = n^2$ is $1/n$, and the growth of $n^2$ as $n\\to \\infty$ outpaces the growth of $1/n$, so the expected value of $X_n$ grows indefinitely.\n:::\n\n\n::: remark \nWe can think of the counterexample in @exm-pnotmse arising from the \"tail\" of a density, where the \"tail\" happened to just be a single point because the random variable was discrete. This is a pattern that comes up often in asymptotics -- the tails of distributions and densities can cause trouble. Many theorems and results depend on these tails behaving well.\n:::\n\n### Almost Sure Convergence\n\nAn third type of convergence is almost sure convergence. Recalling that a random variable is just a function, we can define a stochastic analog to pointwise convergence of a sequence of functions. \n\n::: {#def-}\nA sequence of random variables $X_n$ [***converges almost surely***]{style=\"color:red\"} to a random variable $X$, written as\n$X_n\\overset{a.s}{\\to}X$ if\n$$ \\Pr \\left(\\lim_{n\\to\\infty} X_n = X\\right) = 1.$$ A sequence\nof random vectors $\\mathbf{X}_n = (X_{1,n},\\ldots X_{k,n})$ converges almost surely to $\\mathbf{X}$ if $X_{i,n}\\overset{a.s}{\\to} X_i$ for\n$i=1,\\ldots, k$.\n:::\n\n       Almost sure convergence is the probabilistic equivalent of a sequence of functions converging pointwise almost everywhere. The difference between $X_n\\overset{a.s}{\\to} X$ and $X_n\\pto X$ is subtle, and arises from the limit being taken before or after we take the probability of the event. While convergence in probability tells us that the chance that $X_n$ and $X$ are far approaches zero, almost sure convergence says that the probability that $X_n\\to X$ as $n\\to \\infty$ is 1. We're certain that $X_n$ will eventually coincide with $X$, although we don't know when that will happen. The next example makes the difference between these two definitions a bit more concrete. Later on in @exm-strongvsweak an illustration will be presented to distinguish the two definitions.\n\n::: {#exm-}\n\n## Convergence in Probability but not in Almost Surely\n\nDefine $X_n$ on $\\{0,1\\}$ such that: \n\\begin{align*}\n\\Pr(X_n = 1) & = 1/n\\\\\n\\Pr(X_n = 0) & = 1-1/n\n\\end{align*}\nWe have $X_n\\pto 0$ since \n$$ \\lim_{n\\to\\infty}\\Pr(|X_n - 0| > \\varepsilon) =  \\lim_{n\\to\\infty}\\Pr(X_n > \\varepsilon) = \\lim_{n\\to\\infty}\\Pr(X_n = 1) = \\lim_{n\\to\\infty} 1/n = 0$$ for all $\\varepsilon > 0$.\nAt the same time, we have \n$$\\sum_{n=1}^\\infty \\Pr(X_n = 1) =  \\sum_{n=1}^\\infty 1/n = \\infty,$$ so by a form of the [Borel-Cantelli Lemma](http://www.columbia.edu/~ks20/stochastic-I/stochastic-I-BC.pdf) then $X_n = 1$ occurs an infinite number of times. This rules out the possibility that $\\lim_{n\\to\\infty} X_n = 0$ with probability 1.\n\n:::\n\n::: {#prp-}\n\n## Convergence A.S -> Convergence in Probability\n\nLet $X_n$ be a sequence of random variables. If $X_n\\overset{a.s}\\to X$ then $X_n\\pto X$. \n\n:::\n\n       The proof of this result can be found in @van2000asymptotic. Almost sure convergence is not nearly as important as convergence in probability when it comes to assessing estimators. In fact, @lehmann1999elements doesn't even mention it in his treatment of asymptotic statistics. The math underlying almost sure convergence is also a bit complex, so the related proofs aren't very informative and will be omitted.\n\n### Convergence in Distribution\n\nThe final notion of convergence we will use related to the probability\ndistribution of random variables.\n\n:::{#def-}\nA sequence of random variables $X_n$ [***converges in distribution\n(converges weakly)***]{style=\"color:red\"} to a random variable $X$,\nwritten as $X_n \\overset{d}{\\to}X$, if\n$$\\lim_{n\\to\\infty} F_{X_n}(x)= F_X(x)$$ for all points $x$ where $F_{X}$ is continuous. In this case, we refer to\n$F_X$ as the [***asymptotic distribution***]{style=\"color:red\"} of\n$X_n$, and write $X_n \\overset{a}{\\sim}F_X$.^[Convergence in distribution is also sometimes called convergence in law among mathematicians. The notation for this form of convergence is also as varied as it's name. Some other ways of writing $X_n\\overset{d}{\\to} X$ are: $X_n\\rightsquigarrow X$, $X_n\\Rightarrow X$, $X_n \\overset{\\mathcal L}\\to X$, $\\mathcal L(X_n) \\to \\mathcal L(X)$.] A sequence\nof random vectors $\\mathbf{X}_n = (X_{1,n},\\ldots X_{k,n})$ converges in distribution to $\\X$ if $\\lim_{n\\to\\infty} F_{\\X_n}(x)= F_{\\X}(x)$.\n:::\n\n      For our purposes, $X_n\\overset{d}{\\to}X$ means the distribution of $X_n$\ncan be approximated by $F_X$, and this approximation becomes\nincreasingly better as $n\\to\\infty$.\n\n::: {#exm-tdist}\nOne example of convergence in distribution you may be familiar with\ndeals with the student's $t-$distribution where the degrees of freedom\n$n\\to\\infty$. If $X_n\\sim t_n$, then $X_n \\overset{d}{\\to}X$ where\n$X\\sim N(0,1)$.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot25\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"The t-distribution converges to the standard normal distribution (represented by the dashed red line) as the degrees of freedom increase.\"\n#| code-summary: \"Show code which generates figure\"\nexpand_grid(\n  x = seq(-4, 4, length = 1e3),\n  n = 1:10,\n  dist = \"Student's t\"\n) %>% \n  mutate(val = dt(x, n)) %>% \n  ggplot(aes(x, val)) +\n  geom_line() +\n  stat_function(\n    fun = dnorm, \n    args = list(mean = 0, sd = 1), \n    color = \"red\", \n    linetype=\"dashed\"\n  ) +\n  theme_minimal() +\n  labs(color = \"t-distribution degrees of freedom, n\", y = \"Density\") +\n  theme(legend.position = \"bottom\") +\n  transition_states(n) +\n  labs(title = 'n = {closest_state}')\n```\n:::\n\n::: {#prp-}\n## Convergence in Probability --\\> Convergence in Distribution\n\nLet $X_n$ be a sequence of random variables. If $X_n\\overset{p}{\\to}X$,\nthen $X_n\\overset{d}{\\to}X$.\n:::\n\n::: proof\nSuppose $X_n\\overset{p}{\\to}X$ and let $\\varepsilon > 0$. We have,\n\\begin{align*}\n\\Pr(X_n \\le x) & = \\Pr(X_n\\le x \\text{ and } X \\le x + \\varepsilon) + \\Pr(X_n\\le x \\text{ and } X > x + \\varepsilon) \\\\\n  & = \\Pr(X \\le x + \\varepsilon) + \\Pr(X_n - X\\le x - X \\text{ and } x - X < -\\varepsilon)\\\\ \n  & \\le \\Pr(X \\le x + \\varepsilon) + \\Pr(X_n - X < -\\varepsilon)\\\\\n  & \\le \\Pr(X \\le x + \\varepsilon) + \\Pr(X_n - X < -\\varepsilon) + \\Pr(X - X_n > \\varepsilon) \\\\\n  & = \\Pr(X \\le x + \\varepsilon) + \\Pr(|X_n - X| > \\varepsilon)\n\\end{align*} Similarly,\n$$ \\Pr(X \\le x-\\varepsilon) \\le \\Pr(X_n \\le x) + \\Pr(|X_n - X| > \\varepsilon).$$\nWe can use these inequalities to find an upper and lower bound of\n$\\Pr(X_n \\le x)$: \n\n::: {.column-screen-inset-right}\n\\begin{align*}\n& \\Pr(X \\le x-\\varepsilon) - \\Pr(|X_n - X| > \\varepsilon)\\le \\Pr(X_n \\le x) \\le \\Pr(X \\le x + \\varepsilon) + \\Pr(|X_n - X| > \\varepsilon) \\\\\n\\implies & \\lim_{n\\to\\infty}[\\Pr(X \\le x-\\varepsilon) - \\Pr(|X_n - X| > \\varepsilon)]\\le \\lim_{n\\to\\infty}\\Pr(X_n \\le x) \\le \\lim_{n\\to\\infty}[\\Pr(X \\le x + \\varepsilon) + \\Pr(|X_n - X| > \\varepsilon) ]\\\\\n\\implies & \\Pr(X \\le x-\\varepsilon) - \\underbrace{\\lim_{n\\to\\infty}\\Pr(|X_n - X| > \\varepsilon) }_0\\le \\lim_{n\\to\\infty}\\Pr(X_n \\le x) \\le \\Pr(X \\le x + \\varepsilon) + \\underbrace{\\lim_{n\\to\\infty}\\Pr(|X_n - X| > \\varepsilon)}_0 \\\\\n\\implies & \\Pr(X \\le x-\\varepsilon)\\le \\lim_{n\\to\\infty} \\Pr(X_n \\le x) \\le  \\Pr(X \\le x + \\varepsilon) & (X_n\\overset{p}{\\to}X)\\\\ \n\\implies & F_X(x-\\varepsilon)\\le \\lim_{n\\to\\infty} F_{X_n}(x) \\le  F_X(x-\\varepsilon) \n\\end{align*}\n:::\n\nThis holds for all $\\varepsilon > 0$, so it must be the\ncase that $\\lim_{n\\to\\infty} F_{X_n}(x) = F_X(x)$\n:::\n\n       Of the different concepts of stochastic convergence, convergence in distribution is the weakest (as the alternate name weak convergence implies). If $X_n \\overset{d}{\\to} X$, we're not saying that $X_n$ and $X$ become close, or that the probability that $X_n$ and $X$ are close approaches 1. We're saying that, given a common probability space $(\\mathcal X, \\mathcal F, P)$,^[The probability space on which the sequence of random variables is defined need not be common, but assume as much for the sake of this digression.] that the distribution function $F_{X_n}$ defined via the distribution $P(X^{-1}(I))$^[where $I\\subseteq \\mathcal B(\\mathbb R)$] happens to converge to the same function corresponding to the random variable $X$. Furthermore, the convergence of $F_{X_n}$ to $F_X$ only needs to occur at the points which $F_X$ is continuous, *and* even then the convergence only needs to be pointwise (opposed to uniform).\n\n:::\n\n### Putting the Pieces Together\n\nThe biggest takeaway from this should be the following relations:\n\\begin{align*}\n&X_n \\overset{ms}{\\to}X \\implies X_n \\overset{p}{\\to}X \\implies X_n \\overset{d}{\\to}X\\\\\n&X_n \\overset{a.s}{\\to}X \\implies X_n \\overset{p}{\\to}X\n\\end{align*}\n\n\n## Consistency\n\nOur three modes of convergence were defined for any sequence of random\nvariables. It should come as no surprise, considering the previous\nexamples considering whether the sample mean converged, that we are\ninterested in the convergence of estimators $\\hat\\theta(\\mathbf{X})$ as\nsample size increases. In particular we are interested in whether\n$\\hat\\theta(\\mathbf{X})$ converges to the constant $\\theta\\in\\Theta$ it\nis estimating.\n\n:::{#def-}\nAn estimator $\\hat\\theta$ is [***consistent (for estimand***\n$\\theta$)]{style=\"color:red\"} if $\\hat\\theta \\overset{p}{\\to}\\theta$.\n:::\n\n       We've already seen that $\\bar X$ is a consistent estimator for $\\mu$\nwhen we take an iid sample from a normal distribution. Let's investigate\nit's variance-counterpart $S^2$.\n\n::: {#exm-consvarnorm}\nFor $X_i \\overset{iid}{\\sim}N(\\mu,\\sigma^2)$,\n$S^2 = \\sum_{i=1}^n (X_i - \\bar X)/(n-1)$ is an unbiased estimator for\n$\\sigma^2$. This estimator's MSE (which is just its variance as it is\nunbiased) is $2\\sigma^4/(n-1)$ which converges to $0$ as $n\\to\\infty$,\nso $S^2$ is consistent by @prp-conv.\n:::\n\n       This example highlights the fact that proving an unbiased estimator is\nconsistent is a matter of showing its variance converges to 0.\n\n::: {#cor-unbcon}\nSuppose $\\hat\\theta$ is an unbiased estimator for $\\theta$. Then\n$\\hat\\theta$ is consistent *if and only if*\n$\\text{Var}\\left(\\hat\\theta\\right)\\to 0$.\n:::\n\n::: proof\nApply @cor-mseconv to an unbiased estimator.\n:::\n\n      A second type of convergence related to estimators pertains to the bias\nof an estimator. In @sec-est we saw a few estimators that were biased,\nbut this bias was such that it diminished as $n\\to\\infty$. In effect,\nthey were unbiased in an asymptotic sense.\n\n:::{#def-}\n\nAn estimator $\\hat\\theta$ is [***asymptotically\nunbiased***]{style=\"color:red\"} if\n$\\lim_{n\\to\\infty}\\text{Bias}(\\hat\\theta, \\theta) = 0$.\n\n:::\n\n:::{#exm}\nFor $X_i \\overset{iid}{\\sim}N(\\mu,\\sigma^2)$,\n$\\hat\\theta = \\sum_{i=1}^n (X_i - \\bar X)/n$ is a biased estimator for\n$\\sigma^2$. Its bias is\n$$\\text{Bias}(\\hat\\theta, \\sigma^2) = \\frac{n-1}{n}\\sigma^2 - \\sigma^2.$$\nAs $n\\to\\infty$, this bias vanishes. To illustrate this, we can simulate\nestimates for various sample sizes, taking $X_i \\sim N(0,1)$.\n\n```{r}\ntheta <- function(X){\n  sum((X - mean(X))^2)/length(X)\n}\n\niter <- function(n, dist, dist_params, s){\n  X <- do.call(dist, append(n, dist_params))\n  output <- tibble(\n    sample_size = n,\n    iter_num = s,\n    estimate = theta(X)\n  )\n  return(output)\n}\n\nsim <- function(N, n, dist, dist_params){\n  output <- 1:N %>% \n    map(iter, n = n, dist = dist, dist_params = dist_params) %>% \n    bind_rows()\n}\n\nouter_sim <- function(N, n_vals, dist, dist_params){\n  output <- n_vals %>% \n    map(sim, N = N, dist = dist, dist_params = dist_params) %>% \n    bind_rows()\n}\n\nresults <- c(10, 25, 50, 100, 500) %>% \n  outer_sim(1e6, ., rnorm, list(0, 1))\n\n#Print bias calculated over 1,000,000 simulations\nresults %>% \n  group_by(sample_size) %>% \n  summarize(bias = mean(estimate) - 1)\n```\n\n```{r}\n#| code-fold: true\n#| label: fig-plot26\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"As the sample size increases, the bias of our estimator converges to zero.\"\n#| code-summary: \"Show code which generates figure\"\n#| warning: false\n\nresults %>% \n  ggplot(aes(estimate, color = as.factor(sample_size))) +\n  geom_density() +\n  xlim(0,2) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"Estimates of Variance, True Value = 1\", y = \"Density\", color = \"Sample Size\")\n```\n\nThe estimator $\\hat\\theta$ is also consistent, as it converges in mean\nsquare.\n\n:::\n\n       Asymptotic unbiasedness does not imply consistency, and\nconsistency does not imply asymptotic unbiasedness.\n\n:::{#exm-}\n\n## Consistent, Not Asymptotically Unbiased\n\nRecall that the sequence of discrete random variables $X_n$ with denisty\n$$ f_{X_n}(x) = \\begin{cases}1-1/n& x=0\\\\ 1/n & x = n^2 \\end{cases}.$$\nWe established that $X_n\\overset{p}{\\to}0$, so an estimator with this\ndistribution would be a consistent estimator for $0$. Despite this, the\nestimator would not be asymptotically unbiased, as\n$\\text{E}\\left[X_n\\right] = n$, which tends to infinity as $n$ grows.\n\n:::\n\n:::{#exm-}\n## Asymptotically Unbiased, Not Consistent\nFor $X_i\\overset{iid}{\\sim}N(\\mu,\\sigma^2)$, define the estimator\n$\\hat\\mu(\\mathbf{X}) = X_1$. We simply take the first observation to be\nour estimate of $\\mu$. This estimator is unbiased,\n$$\\text{E}\\left[\\hat\\mu\\right] = \\text{E}\\left[X_1\\right] = \\mu,$$ so it\nis asymptotically unbiased. Nevertheless, the estimator fails to be\nconsistent, as\n$$\\lim_{n\\to\\infty} \\Pr (|\\hat\\mu - \\mu| > \\varepsilon)= \\lim_{n\\to\\infty} \\left\\{1 - 2\\left[\\Phi\\left(\\frac{\\varepsilon}{\\sigma}\\right) - \\frac{1}{2}\\right]\\right\\} \\neq 0 .$$\n:::\n\n       The incompatibility of asymptotic unbiasedness and consistency is due to\nthe behavior of $\\text{Var}\\left(X_n\\right)$ as $n\\to\\infty$.\n\n::: {#prp-consbias}\n## Relating Consistency and Asymptotic Unbiasedness\n\nSuppose $\\hat\\theta$ is an estimator for $\\theta$.\n\n1.  If $\\hat\\theta$ is consistent and there exists some $M$ such that\n    $\\text{Var}\\left(\\hat\\theta\\right) \\le M$ for all $n$ (bounded\n    variance), then $\\hat\\theta$ is asymptotically unbiased.\n2.  If $\\hat\\theta$ is asymptotically unbiased and\n    $\\lim_{n\\to\\infty}\\text{Var}\\left(\\hat\\theta\\right) = 0$ (vanishing\n    variance), then $\\hat\\theta$ is consistent\n:::\n\n::: proof\ntest\n:::\n\n```{r}\n#| fig-align: center\n#| label: fig-plot27\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"Relationship between various concepts of convergence in the context of estimators\"\n#| echo: false\n\nknitr::include_graphics(\"figures/relating_convergence.png\")\n```\n\n## Laws of Large Numbers\n\nIn most examples until now, the properties of estimators were implicitly\na function of the underlying model the data is generated from. We\nestablished that $\\bar X$ and $S^2$ are consistent estimators for $\\mu$\nand $\\sigma^2$, respectively, *when*\n$X_i \\overset{iid}{\\sim}N(\\mu,\\sigma^2)$. We know the distribution of\n$\\bar X$, *when* $X_i \\overset{iid}{\\sim}N(\\mu,\\sigma^2)$. In an ideal\nworld, we would be able to establish desirable properties of estimators\nunder more robust settings where our specified model may include a wide\narray of distributions. Our first step in doing this will be introducing\nvariants of one of the most important results in all of probability -- the law of\nlarge numbers (LLN). In the context of estimation, LLNs give sufficient conditions for our favorite estimator, $\\bar X$, to be consistent. \n\n### Khinchine's Weak Law of Large Numbers\n\nThe version of the LLN we'll use the most often deals with convergence in probability. To prove that $\\bar X \\overset{p}{\\to}\\mu$ we'll rely on an inequality similar to\n@lem-markovineq.\n\n::: {#lem-}\n## Chebyshev's Inequality\n\nIf $X$ is a random variable with an expected value $\\mu$ and variance\n$\\sigma^2$, then for all $a > 0$\n$$\\Pr(|X - \\mu| \\ge k) \\le \\frac{\\sigma^2}{k^2}.$$\n:::\n\n::: proof\n```{=tex}\n\\begin{align*}\n\\Pr(|X - \\mu| \\ge k) &= \\Pr((X - \\mu)^2 \\ge k^2)\\\\\n& \\le \\frac{\\text{E}\\left[(X-\\mu)^2\\right]}{k^2} & (\\text{Markov's Inequality})\\\\\n& = \\frac{\\sigma^2}{k^2}\n\\end{align*}\n```\n:::\n\n::: {#thm-}\n## LLN I\n\nIf $(X_1,\\ldots, X_n)$ are a set of iid random variables where\n$\\text{E}\\left[X_i\\right] = \\mu$, then $\\bar X \\overset{p}{\\to}\\mu$.\n:::\n\n::: proof\nRecall that $\\text{Var}\\left(\\bar X\\right) = \\sigma^2/n$. By Chebyshev's\nInequality, \\begin{align*}\n\\lim_{n\\to\\infty}\\Pr(|X_n - \\mu| \\ge \\varepsilon) \\le \\lim_{n\\to\\infty}\\frac{(\\sigma^2/n)}{\\varepsilon^2} =   \\lim_{n\\to\\infty} \\frac{\\sigma^2}{n\\varepsilon} = 0.\n\\end{align*} Therefore, $\\bar X\\overset{p}{\\to}\\mu$.\n:::\n\n:::{#exm-lln1}\n\nTo illustrate the LLN, let's simulate realizations of iid random\nvariables from a series of different distributions and show that\nregardless of the distribution, $\\bar X \\to \\mu$. We will use the\nfollowing distributions: \\begin{align*}\nX_i & \\overset{iid}{\\sim}\\text{Exp}(1/\\mu)\\\\\nX_i & \\overset{iid}{\\sim}\\chi_\\mu^2\\\\\nX_i & \\overset{iid}{\\sim}\\text{Uni}(0, 2\\mu)\\\\\nX_i & \\overset{iid}{\\sim}\\text{Gamma}(2\\mu, 2)\\\\\nX_i & \\overset{iid}{\\sim}\\text{HyperGeo}(10, 20, 15\\mu)\n\\end{align*} All these distributions have been selected such that\n$\\text{E}\\left[X_i\\right] = \\mu$. For our simulations, we will take\n$\\mu = 5$. If we plot the value of the sample mean versus the sample\nsize $n$, we see that the values converge to the true value $\\mu = 5$\nregardless of the distribution of $X_i$.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot28\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"The sample mean of all samples converges to the population mean by the LLN\"\n#| code-summary: \"Show code which generates figure\"\n#| warning: false\niter <- function(n, dist, dist_params){\n  dist_string <- paste(substitute(dist))\n  output <- tibble(\n    sample_size = 1:n,\n    prob_dist = dist_string,\n    random_values = do.call(dist, append(n, dist_params))\n  ) %>% \n    mutate(estimate = cummean(random_values))\n  return(output)\n}\n\n# can likely use pmap here\nresults <- bind_rows(\n  iter(1e5, rexp, list(1/5)),\n  iter(1e5, rchisq, list(5)),\n  iter(1e5, runif, list(0, 10)),\n  iter(1e5, rgamma, list(10, 2)),\n  iter(1e5, rhyper, list(10, 20, 15))\n)\n\nresults %>% \n  ggplot(aes(sample_size, estimate, color = prob_dist)) +\n  geom_line(alpha=0.6) +\n  theme_minimal() +\n  labs(x = \"Sample Size, n\", y = \"Sample Mean\", color = \"Distribution of iid Random Sample\") +\n  theme(legend.position = \"bottom\") +\n  geom_hline(yintercept = 5) +\n  ylim(4.5, 5.5)\n```\n\nFor the sake of an even better illustration, let's focus on the case where $X_i \\overset{iid}{\\sim}\\text{Exp}(1/\\mu)$ where $\\mu = 5$. We'll draw 10,000 samples of $X_i$, and calculate $\\bar X$ for each sample as the sample size ranges from 1 all the way to 1,000.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot28.2\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"10,000 lines, each of which corresponds to a simulated realization of the sample mean over a range of sample sizes. The area within the red dashed lines is the set |μ - Sample Mean| < 0.5\"\n#| code-summary: \"Show code which generates figure\"\n#| warning: false\n\niter <- function(n, s, dist, dist_params){\n  output <- tibble(\n    sample_size = 1:n,\n    random_values = do.call(dist, append(n, dist_params)),\n    iter_num = s\n  ) %>% \n    mutate(estimate = cummean(random_values))\n  return(output)\n}\n\nsim <- function(N, n, dist, dist_params){\n  output <- 1:N %>% \n    map(iter, n = n, dist = dist, dist_params = dist_params) %>% \n    bind_rows()\n  return(output)\n}\n\nresults <- sim(1e4, 1e3, rexp, list(1/5))\n\nepsilon <- 0.5 \nresults %>% \n  ggplot(aes(x = sample_size, y = estimate)) +\n  geom_line(aes(group = iter_num), alpha = 0.1, size = 0.05) +\n  ylim(0, 15) + \n  geom_hline(yintercept = 5 - epsilon, color = \"red\", linetype=\"dashed\") +\n  geom_hline(yintercept = 5 + epsilon, color = \"red\", linetype=\"dashed\") +\n  theme_minimal()\n```\n\nThe weak LLN tells us that since $\\bar X\\pto 5$, then by the definition of convergence in probability, the probability one of the lines falls in @fig-plot28.2 outside the interval within the red dashed lines at a particular sample size $n$ approaches zero as $n\\to \\infty$. Furthermore, this will hold regardless of how close we make the red lines to the true value $\\mu = 5$. For the sake of illustration we took $\\varepsilon = 0.5$, but it will hold *for all* $\\varepsilon > 0$. In fact, we can go one step further and illustrate $\\Pr(|\\bar X-\\mu| > \\varepsilon)$ by looking at the proportion of times that $|\\bar X-\\mu| > \\varepsilon$ holds across out 10,000 simulations. We'll do this for $\\varepsilon=0.5,0.6,\\ldots,1$.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot28.3\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"For each ε, the observed probability in question approaches 0 as the sample size grows.\"\n#| code-summary: \"Show code which generates figure\"\n#| warning: false\n\nresults %>% \n  expand_grid(epsilon = 5:10/10) %>% \n  mutate(outside = estimate >= 5 + epsilon | estimate <= 5 - epsilon) %>% \n  group_by(sample_size, epsilon) %>% \n  summarize(prob = sum(outside) / n()) %>%\n  ggplot(aes(x = sample_size, prob, color = factor(epsilon))) +\n  geom_line() +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(y = \"Observed Pr(|Estimate-μ|>ε)\", x = \"Sample Size n\", color = \"ε\")\n```\n\n:::\n\n\n:::{#exm-}\n\n## Monte Carlo Simulations\n\nIn @exm-var we performed a Monte Carlo simulation to illustrate the bias\nof $\\hat\\theta(\\mathbf{X}) = \\sum_{i=1}^n (X_i - \\bar X)/n$ and\nunbiasedness of $S^2$. We did this by fixing $n=20$, drawing a random\nsample, recording estimates $\\hat\\theta(\\mathbf{x})$ and\n$S^2(\\mathbf{x})$, and repeating this $k$ times. This is nothing more\nthan drawing $j=1,\\ldots,k$ observations from the random variables\n$\\hat\\theta(\\mathbf{X})$ and $S^2(\\mathbf{X})$. By the LLN,\n\\begin{align*}\n\\frac{1}{k}\\sum_{i=1}^k \\hat\\theta_j(\\mathbf{x}) &\\overset{p}{\\to}\\text{E}\\left[\\hat\\theta(\\mathbf{X})\\right],\\\\\n\\frac{1}{k}\\sum_{i=1}^k S_j^2(\\mathbf{x}) &\\overset{p}{\\to}\\text{E}\\left[S^2(\\mathbf{X})\\right],\n\\end{align*} so for a large enough $k$, we can approximate the expected\nvalue with its sample counterpart.\n\n:::\n\n### Kolmogorov's Strong Law of Large Numbers\n\nA stronger version of the LLN is stated in terms of almost sure convergence. Since almost sure convergence is stronger than convergence in probability (which is all that is needed for an estimator to be consistent), this version is referred to as the \"strong\" LLN. \n\n::: {#thm-}\n## LLN II\n\nIf $(X_1,\\ldots, X_n)$ are a set of iid random variables where\n$\\text{E}\\left[X_i\\right] = \\mu$, then $\\bar X \\overset{as}{\\to}\\mu$.\n:::\n\n::: proof\nSee the proof of Theorem 6.1 in @billingsley2008probability.\n::: \n\n\n:::{#exm-strongvsweak}\n## Strong vs Weak LLN\nWe can actually visualize the difference between the strong and weak LLNs, and in doing so highlight the difference between almost sure convergence and convergence in probability. Like @exm-lln1, assume $X_i \\sim\\text{Exp}(1/\\mu)$ where $\\mu = 5$. We'll perform a similar simulation to that which gave @fig-plot28.2, but this time we'll only look at one sample of $X_i$ (instead of 10,000). \n\n```{r}\n#| code-fold: true\n#| label: fig-plot28.4\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"A single simulation realization of the sample mean over a range of sample sizes. The area within the red dashed lines is the set |μ - Sample Mean| < 0.5\"\n#| code-summary: \"Show code which generates figure\"\n#| warning: false\n\niter(1e5, 1, rexp, list(1/5)) %>% \n  ggplot(aes(sample_size, estimate)) + \n  geom_line() + \n  geom_hline(yintercept = 5 - epsilon, color = \"red\", linetype=\"dashed\") +\n  geom_hline(yintercept = 5 + epsilon, color = \"red\", linetype=\"dashed\") +\n  theme_minimal() \n```\n\nThe strong LLN tells us that for some finite sample size $N$ the line in @fig-plot28.4 will fall within the red lines for all $n > N$.\n\n:::\n\n### Chebyshev's Weak Law of Large Numbers\n\nThe crucial assumption made by both LLNs up to this point is that $\\bar X$ is calculated\nwith an iid random sample. If we drop this assumption, then $\\bar X$\nneedn't estimate $\\mu$ consistently.\n\n::: {#exm-noiid}\n## LLN Failing with Non-IID Data\n\nSuppose $\\mathbf{X}= (X_1, \\ldots, X_n)$ where $X_i \\sim N(-1, i)$ if\n$i$ is odd, and $X_i \\sim N(1,i)$ is $i$ is even. The data is\nindependent, but not identically distributed. If some LLN would hold\nhere, we would suspect that $\\bar X$ would converge $0$ since the average of\nthe underlying population means $1$ and $-1$. Let's simulate $\\bar X$ for\n$n$ ranging from 1 to 100,000.\n\n```{r}\ndraw_X_i <- function(i){\n  mu_i <- (-1)^(i %% 2) * (1)^(1 - i %% 2)\n  sigma_i <- i \n  \n  output <- rnorm(1, mu_i, sigma_i)\n  return(output)\n}\n\ndraw_X <- function(n){\n  output <- tibble(\n    i = 1:n,\n    value = map_dbl(1:n, draw_X_i)\n  ) %>% \n    mutate(estimate = cummean(value))\n  return(output)\n}\n\nresults <- draw_X(1e5)\n```\n\nWe see that our estimates very much do not converge to any particular\nvalue.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot29\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"The sample mean of non-IID data does not satisfy the LLN\"\n#| code-summary: \"Show code which generates figure\"\nresults %>% \n  ggplot(aes(i, estimate)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Sample Size\", y = \"Sample Mean\")\n```\n:::\n\n       The proof of the LLN relied on Chebyshev's equality and the fact that\n$\\text{Var}\\left(\\bar X\\right) = \\sigma^2/n \\to 0$ when\n$\\text{Var}\\left(X_i\\right) = \\sigma^2$. Perhaps if we added an\nassumption regarding the variance of a non-iid sample, then we could\nsalvage a result similar to the LLN. This is precisely what Chebyshev's\nLLN does.\n\n:::{#prp-chebylln}\n\n## Chebyshev's (Weak) Law of Large Numbers\n\nSuppose $(X_1,\\ldots, X_n)$ are a sample such that\n$\\text{E}\\left[X_i\\right] = \\mu_i$,\n$\\text{Cov}\\left(X_i, X_j\\right) = \\sigma_{ij}^2$, and\n$\\lim_{n\\to\\infty}\\frac{1}{n^2}\\sum_{i=1}^n \\sigma_{ij}^2 =0$. If\n$\\frac{1}{n}\\sum_{i=1}^n\\mu_i \\overset{p}{\\to}\\mu$, then\n$\\bar X \\overset{p}{\\to}\\mu$.\n\n:::\n\n::: proof\nThe expected value of $\\bar X$ is\n$$\\text{E}\\left[\\bar X\\right] = \\frac{1}{n}\\sum_{i=1}^n\\text{E}\\left[X_i\\right]= \\frac{1}{n}\\sum_{i=1}^n\\mu_i \\overset{p}{\\to}\\mu.$$\nThe variance is\n$$ \\text{Var}\\left(\\bar X\\right) = \\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right) = \\frac{1}{n^2}\\sum_{i=1}^n\\sum_{j=1}^n\\text{Cov}\\left(X_i,X_j\\right) = \\frac{1}{n}\\sum_{i=1}^n \\sigma_{ij}^2\\to 0.$$\nBy Proposition @prp-mse3, $\\bar X\\overset{ms}{\\to}\\mu$, so\n$\\bar X \\overset{p}{\\to}\\mu$.\n:::\n\n:::{#cor-}\nSuppose $(X_1,\\ldots, X_n)$ are an independent sample such that\n$\\text{E}\\left[X_i\\right] = \\mu_i$,\n$\\text{Var}\\left(X_i\\right) = \\sigma_{i}^2$, and\n$\\lim_{n\\to\\infty}\\frac{1}{n^2}\\sum_{i=1}^n \\sigma_i^2 =0$. If\n$\\frac{1}{n}\\sum_{i=1}^n\\mu_i \\overset{p}{\\to}\\mu$, then\n$\\bar X \\overset{p}{\\to}\\mu$.\n:::\n\n::: proof\nIf the sample is independent, then\n$$\\frac{1}{n^2}\\sum_{i=1}^n\\sum_{j=1}^n\\text{Cov}\\left(X_i,X_j\\right) =\\frac{1}{n^2}\\sum_{i=1}^n\\text{Var}\\left(X_i\\right). $$\n:::\n\nThe reason our non-iid sample in Example @exm-noiid did not converge was\nbecause\n$$ \\frac{1}{n^2}\\sum_{i=1}^n \\sigma_i^2 = \\frac{1}{n^2}\\sum_{i=1}^n i = \\frac{1}{n^2}\\frac{n(n+1)}{2} = \\frac{n^2 + n}{2n^2} \\to \\frac{1}{2} \\neq 0.$$\nLet's modify it slightly so the sum of the variances does converge to\nzero.\n\n:::{#exm-}\n\n## LLN with Non-IID Data\n\nSuppose $\\mathbf{X}= (X_1, \\ldots, X_n)$ where $X_i \\sim N(-1,i^{-1})$\nif $i$ is odd, and $X_i \\sim N(1,i^{-1})$ is $i$ is even. Now we have\n$$\\lim_{n\\to\\infty}\\frac{1}{n^2}\\sum_{i=1}^n \\sigma_i^2 = \\left[\\lim_{n\\to\\infty}\\frac{1}{n^2}\\right]\\sum_{i=1}^\\infty i^{-1} \\to 0 .$$\n\n```{r}\ndraw_X_i <- function(i){\n  mu_i <- (-1)^(i %% 2) * (1)^(1 - i %% 2)\n  sigma_i <- 1/i \n  \n  output <- rnorm(1, mu_i, sigma_i)\n  return(output)\n}\n\ndraw_X <- function(n){\n  output <- tibble(\n    i = 1:n,\n    value = map_dbl(1:n, draw_X_i)\n  ) %>% \n    mutate(estimate = cummean(value))\n  return(output)\n}\n\nresults <- draw_X(150)\n```\n\nNow we see that $\\bar X$ is converging to $0$, and doing so rather\nquickly.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot210\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"Despite the data not being IID, the sample mean still converges to the population mean because the variance shrinks in the correct way.\"\n#| code-summary: \"Show code which generates figure\"\nresults %>% \n  ggplot(aes(i, estimate)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Sample Size\", y = \"Sample Mean\")\n```\n\n:::\n\n## The Continuous Mapping Theorem and Slutsky's Theorem\n\nAt first glance, the LLN may not seem especially useful as it only\napplies to the sample mean. However, when paired with two key results\nabout convergence, the LLN becomes an indispensable tool to analyze the\nconvergence of many random variables and estimators. The first of these\nis an extension of a key result in real analysis. A useful, and defining\nproperty, of continuous functions is that they preserve limits of\nnumeric sequences. If $\\{a_n\\}$ is a numeric sequence, then\n$$\\lim_{n \\to\\infty} f(a_n) = f\\left(\\lim_{n\\to\\infty} a_n\\right) \\iff f\\text{ continuous}.$$\n\n:::{#thm-}\n\n## Continuous Mapping Theorem I\n\nSuppose $X_n \\overset{p}{\\to}X$, and let $g$ be a continuous function.\nThen $$g(X_n)\\overset{p}{\\to}g(X).$$ In other words we are able to\ninterchange the $\\mathop{\\mathrm{plim}}$ operator with a continuous\nfunction:\n$$\\mathop{\\mathrm{plim}}g(X_n) = g\\left(\\mathop{\\mathrm{plim}}X_n\\right).$$\n\n:::\n\nThe proof of this result can be found in @van2000asymptotic. An\nimmediate corollary follows from the fact that convergence in\nprobability implies convergence in distribution.\n\n::: {#cor-}\n## Continuous Mapping Theorem II\n\nSuppose $X_n \\overset{p}{\\to}X$, and let $g$ be a continuous function.\nThen $$g(X_n)\\overset{d}{\\to}g(X)$$\n:::\n\nAn equally useful result involves the limit of a sums and products of\nconvergent random variables.\n\n:::{#thm-}\n\n## Slusky's Theorem\n\nLet $X_n$ and $Y_n$ be sequences of random variables. If\n$X_n\\overset{d}{\\to}X$ for some random variable $X$, and\n$Y_n\\overset{p}{\\to}c$ for some constant $c$, then \\begin{align*}\nX_n + Y_n &\\overset{d}{\\to}X + c\\\\\nX_nY_n & \\overset{d}{\\to}Xc.\n\\end{align*} Furthermore, if $c\\neq 0$,\n$$ X_n/Y_n \\overset{d}{\\to}X/c.$$\n\n:::\n\n::: proof\nDefine a random vector to be $\\mathbf Z_n = (X_n,Y_n)$. We have\n$\\mathbf Z_n \\overset{d}{\\to}(X,c)$ as $X_n\\overset{d}{\\to}X$ and\n$Y_n \\overset{d}{\\to}c$ (convergence in probability implies convergence\nin distribution). We can apply the continuous mapping theorem to\n$g(x,y) = x + y$, $g(x,y)=xy$, and $g(x/y)$ to establish the result.\n:::\n\n       Slutsky's theorem can be a bit hard to remember because it involves a\nsequence of random variable which converges in distribution to a random\nvariable, and a sequence of random variables which converges in\nprobability to a constant. These asymmetries in mode of convergence and\nthe type of limit are essential, otherwise the result will not hold.\nFortunately, the result does hold if we replace all convergences in\ndistribution with convergence in probability (as the later implies the\nprior).\n\n:::{#exm-}\n\nSuppose $X_n \\sim\\text{Uni}(0,1)$ and $Y_n = - X_n$. We have\n$X_n \\overset{d}{\\to}\\text{Uni}(0,1)$ and\n$Y_n \\overset{d}{\\to}\\text{Uni}(-1,0)$. Despite this\n$X_n + Y_n = 0 \\not\\overset{d}{\\to}\\text{Uni}(0,1) + \\text{Uni}(-1,0).$\n\n:::\n\n::: {#exm-convar}\n## Consistency of Sample Variance\n\n@exm-consvarnorm showed that $S^2$ is a consistent estimator for\n$\\sigma^2$ when $X_i\\overset{iid}{\\sim}N(\\mu,\\sigma^2)$. We can use the\ncontinuous mapping theorem, Slutsky's theorem, and the LLN to show that\n$S^2$ is consistent regardless of the distribution of our iid sample.\nSuppose $\\text{E}\\left[X_i\\right] = \\mu$,\n$\\text{E}\\left[X_i^2\\right]=\\mu_2$, and\n$\\text{E}\\left[X_i^4\\right]=\\mu_4$ for all $i$. If we define our\ncontinuous function to be $g(x) = x^2$, then \\begin{align*}\nS^2 & = \\frac{1}{n-1} \\sum_{i=1}^nX_i - \\frac{1}{n-1} \\sum_{i=1}^n\\bar X^2 \\\\\n    & = \\frac{1}{n-1} \\sum_{i=1}^nX_i^2 + \\frac{n}{n-1}\\bar X^2  \\\\\n    & = \\frac{n}{n-1}\\left[\\frac{1}{n}\\sum_{i=1}^{n}X_i^2 - \\left(\\frac{1}{n}\\sum_{i=1}^{n}X_i\\right)^2\\right]\n\\end{align*} The first term in the brackets is an unbiased estimator of\n$\\mu_2$ with vanishing variance, so by @prp-consbias it is a consistent\nestimator for $\\text{E}\\left[X^2\\right]$: \\begin{align*}\n\\text{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}X_i^2\\right] &= \\frac{1}{n}\\left(n \\mu_2\\right) = \\mu^2\\\\\n\\lim_{n\\to\\infty}\\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_i^2\\right) & = \\lim_{n\\to\\infty}\\frac{1}{n^2}n\\left(\\text{E}\\left[X_i^4\\right] - \\text{E}\\left[X_i^2\\right]^2 \\right) = \\lim_{n\\to\\infty}\\frac{\\mu_4 + \\mu_2^2}{n} = 0\n\\end{align*} The second term in the brackets can be written as\n$g(\\bar X)$, so by the continuous mapping theorem and the LLN,\n$$ g(\\bar X) = \\left(\\frac{1}{n}\\sum_{i=1}^{n}X_i\\right)^2 \\overset{p}{\\to}\\mu^2 = g(\\mu).$$\nSo\n$$S^2= \\underbrace{\\frac{n}{n-1}}_{\\to 1}\\Bigg[\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}X_i^2}_{\\overset{p}{\\to}\\mu_2} - \\underbrace{\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_i\\right)^2}_{\\overset{p}{\\to}\\mu^2} \\Bigg].$$\nWe can apply Slutsky's theorem to the sum of sequences of random\nvariables which converge in probability to constants, so\n$$ S^2 \\overset{p}{\\to}\\mu_2 - \\mu^2 = \\sigma^2,$$ making $S^2$\nconsistent.\n:::\n\n## Central Limit Theorems\n\nThe LLN told us that our favorite estimator for $\\mu$, $\\bar X$, is\nconsistent. We know turn to what is perhaps an even more important\nresult regarding $\\bar X$, one that may in fact be the most important\nresults in all of probability -- the asymptotic distribution of $\\bar X$ is a normal distribution.\n\n### Lindeberg-Lévy CLT\n\nThe *classic* version of the CLT is formally known as the Lindeberg-Lévy CLT, and is likely familiar.\n\n:::{#thm-}\n\n## CLT I\n\nSuppose $\\mathbf{X}=(X_1,\\ldots, X_n)$ is a sequence of random variables\nwith $\\text{E}\\left[X_i\\right]=\\mu$ and\n$\\text{Var}\\left(X_i\\right)=\\sigma^2$. Then\n$$\\sqrt{n}(\\bar X - \\mu) \\overset{d}{\\to}N(0,\\sigma^2),$$ which is also\noften written as $$\\bar X\\overset{d}{\\to}N(\\mu, \\sigma^2/n),$$ or\n$$\\sum_{i=1}^n X_i \\overset{d}{\\to}N(n\\mu, \\sqrt n \\sigma^2) $$\n:::\n\n       The proof is a bit technical and requires some measure-theoretic based\nprobability theory. It can be found in @billingsley2008probability or\n@durrett2019probability.\n\n:::{#exm-}\nSuppose we have an iid sample from $\\text{Exp}(1)$. If we simulate 1000\nrealizations of $\\sqrt n(\\bar X - \\mu)$ for various sample sizes, we\nshould see that the distribution of our realizations becomes\napproximately normal as we increase the sample size.\n\n```{r}\niter <- function(n, dist, dist_params, s){\n  output <- tibble(\n    value = do.call(dist, append(n, dist_params))\n  ) %>% \n    summarize(\n      sample_size = n,\n      iter_num = s, \n      estimate = sqrt(n)*(mean(value) - 1)\n    ) \n  return(output)\n}\n\nsim <- function(N, n, dist, dist_params){\n  output <- 1:N %>% \n    map(iter, n = n, dist = dist, dist_params = dist_params) %>% \n    bind_rows()\n  return(output)\n}\n\nouter_sim <- function(n_vals, N, dist, dist_params){\n  output <- n_vals %>% \n    map(sim, N = N, dist = dist, dist_params = dist_params) %>% \n    bind_rows()\n  return(output)\n}\n\nresults <- outer_sim(c(1:25, 50, (1:10)*100), 1e4, rexp, list(1))\n```\n\nEven for modest values of $n$, we can see that\n$\\sqrt n(\\bar X - \\mu) \\overset{a}{\\sim}N(0, \\sigma^2)$.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot211\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"As the sample size increases, the distribution of the adjusted sample mean looks more and more like the standard normal.\"\n#| code-summary: \"Show code which generates figure\"\n\nresults %>% \n  ggplot(aes(estimate)) +\n  geom_histogram(aes(y = after_stat(density)), color = \"black\", fill = \"white\", bins = 50) +\n  theme_minimal() +\n  geom_function(fun = dnorm, colour = \"red\") + \n  transition_states(sample_size) +\n  labs(title = 'Sample Size: {closest_state}', x = \"Estimates\", y = \"\") \n```\n\nAn alternate way to visually test whether our estimates are normally\ndistributed is with a quantile-quantile plot (QQ-plot), which graphs the\nobserved quantiles of our estimates against the theoretical quantiles of\na normal distribution (or those of any distribution we suspect our data\nis drawn from). If our estimates are (approximately) normally\ndistributed, then the observed quantiles should be approximately equal\nto the theoretical quantiles of a normal distribution, forming a\n45-degree line.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot212\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"The QQ-plot for the simulated distribution of the adjusted sample mean\"\n#| code-summary: \"Show code which generates figure\"\nresults %>% \n  ggplot(aes(sample = estimate)) +\n  stat_qq_line(color = \"red\") +\n  geom_qq(size= 0.5) + \n  theme_minimal() +\n  transition_states(sample_size) + \n  labs(title = 'Sample Size: {closest_state}', x = \"Quantiles of Simulated Estimates\", y = \"Quantiles of Normal Distribution\")\n```\n\n:::\n\n       The central limit theorem is similar to the LLN insofar that they only\nconcern the estimator $\\bar X$, so how useful can they really be? Well\nwith the continuous mapping theorem and Slutsky's theorem, the answer is\n*very useful*!\n\n:::{#exm-}\n\nIn @exm-tdist we illustrated the fact that\n$X_n \\overset{d}{\\to}N(0,1)$ where $X_n \\sim t_n$, but we didn't\nactually prove it. Directly proving this result is a matter of verify\nthat\n$$\\lim_{n\\to\\infty} F_{X_n}(x) =\\lim_{n\\to\\infty}\\int_{-\\infty}^x \\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)}{\\Gamma\\left(\\frac{n}{2}\\right)\\sqrt{n\\pi}}\\left(1 + \\frac{x^2}{n}\\right)^{-\\frac{n+1}{2}} = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^x e^{-x^2/2} = F_X(x)$$\nwhere $\\Gamma$ is the gamma function defined as\n$$\\Gamma(t) = \\int_0^\\infty s^{t-1}e^{-s}\\ ds.$$ A much easier way to\nprove that $X_n \\overset{d}{\\to}N(0,1)$, is by using Slutsky's theorem\nand the continuous mapping theorem. First recall that\n$$\\frac{\\bar X - \\mu}{S/\\sqrt n} \\sim t_n,$$ so we can write $X_n$ as\n$X_n = \\frac{\\bar X - \\mu}{S/\\sqrt n}$ due to the fact that random\nvariables are uniquely determined by their distributions. From\n@exm-consvarnorm, we have $S^2 \\overset{p}{\\to}\\sigma^2$. By the continuous\nmapping theorem\n$$ \\sqrt{S^2} = S \\overset{p}{\\to}\\sigma = \\sqrt{\\sigma^2},$$ which gives\n$$ X_n = \\frac{\\bar X - \\mu}{S/\\sqrt n}= \\underbrace{\\sqrt{n}(\\bar X - \\mu)}_{\\overset{d}{\\to}N(0, \\sigma^2)}\\underbrace{\\frac{1}{s}}_{\\overset{p}{\\to}\\sigma}.$$\nPutting all the pieces together, Slutsky's theorem yields\n$$X_n \\overset{d}{\\to}\\frac{N(0,\\sigma^2)}{\\sigma} = N(0,1).$$\n\n:::\n\n       The CLT can be generalized to samples of random vectors $\\mathbf{X}_i$.\n\n::: {#thm-}\nSuppose $(\\mathbf{X}_1,\\ldots, \\mathbf{X}_n)$ is a sequence of iid\nrandom vectors with $\\text{E}\\left[\\mathbf{X}_i\\right]=\\boldsymbol\\mu$\nand $\\text{Var}\\left(\\mathbf{X}_i\\right)=\\boldsymbol\\Sigma$. Then\n$$\\sqrt{n}(\\bar {\\mathbf{X}}- \\boldsymbol\\mu) \\overset{d}{\\to}N(\\mathbf{0},\\boldsymbol\\Sigma).$$\n:::\n\n### Not Identically Distributed\n\n@prp-chebylln allowed us to salvage a LLN when the iid assumption failed, so can we do the same with the CLT? Sort of. While we need an independent sample, we don't necessarily need realizations to be drawn from an identical distribution. Instead, the theorem will rely on the tails of distributions meeting a certion criterion. This version of the CLT is known as the Lindeberg-Feller CLT.\n\n:::{#thm-}\n\n## Lindeberg-Feller CLT\n\nSuppose $\\mathbf{X}=(X_1,\\ldots, X_n)$ is a sequence of independent\nrandom variables with $\\text{E}\\left[X_i\\right]=\\mu_i$ and\n$\\text{Var}\\left(X_i\\right)=\\sigma_i^2$, and define \\begin{align*}\n\\bar \\mu = \\frac{1}{n}\\sum_{i=1}^n\\mu_i;\\\\\n\\bar \\sigma_n^2 = \\frac{1}{n}\\sum_{i=1}^n\\sigma_i^2.\n\\end{align*} If the collection of variances $\\sigma_i^2$ satisfies:\n\\begin{align*}\n\\lim_{n\\to\\infty} &\\frac{\\max\\{\\sigma_i\\}}{n\\bar\\sigma} = 0;\\\\\n\\lim_{n\\to\\infty} &\\bar\\sigma_n^2 = \\bar\\sigma^2,\n\\end{align*} then\n$\\sqrt n (\\bar X-\\mu)\\overset{d}{\\to}N(0, \\bar\\sigma^2).$\n\n:::\n\n::: {#exm-}\n\n## Lindeberg's Condition\n\nThe Lindeberg-Feller conditions stipulates that\n$\\lim_{n\\to\\infty}\\bar\\sigma_n^2 = \\bar\\sigma^2$ and\n$\\lim_{n\\to\\infty} \\frac{\\max\\{\\sigma_i\\}}{n\\bar\\sigma} = 0$, a\ncondition known as the ***Lindeberg's condition***. The condition is\noften presented in more general terms, but the intuition remains the\nsame. For the CLT to hold for random variables that with different\nvariances, we need to makes sure that no single term $\\sigma_i$\ndominates the standard deviation. We can think about the sample mean\n$\\bar X$ as \"mixing\" many random variables $X_i.$ After mixing these\nrandom variables, we hope to have a normal distribution, but that only\nhappens if tails of the various distributions of $X_i$ are negligible as\n$n\\to\\infty$, giving us the trademark tails of a normal distribution\nwhich tapper off. Let's consider a counterexample. Suppose $X_i$ is\ndefined on the sample space $\\{-i,0,i\\}$ is distributed such that\n$$\\Pr(X_i = k) = \\begin{cases} 1/2i^2 & k = -i^2 \\\\ 1/2i^2 & k = i^2 \\\\ 1 - 1/i^2&k=0\\end{cases}.$$\nGraphing the density function for a few values of $i$ gives us a better\nsense of how $X_i$ behaves.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot213\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"Density of X_i for i = 1,...,5.\"\n#| code-summary: \"Show code which generates figure\"\nexpand_grid(i = 1:5, x = -100:100) %>% \n  filter(x == i^2 | x == -i^2| x== 0) %>% \n  mutate(y = ifelse(x == 0, 1 - 1/(i^2), 1/(2*i^2))) %>% \n  ggplot(aes(x, y)) +\n  geom_point(size = 0.2) +\n  xlab(\"k\") +\n  geom_segment(aes(x = x, xend = x, y= 0, yend = y)) +\n  theme_minimal() + \n  transition_states(i) + \n  labs(title = 'i = {closest_state}', y = \"Pr(X_i = k)\")\n```\n\nAs $i\\to\\infty$, nearly all the probability is concentrated as $k = 0$.\nThe remaining probability is at the extreme tails of the distribution\n$\\pm i^2$, and these tails become more and more extreme (quadritically\nso) as $i\\to\\infty$. This is the exact type of behavior Lindeberg's\ncondition rules out. The expectation, expectation squared, and variance\nof $X_i$ are: \\begin{align*}\n\\text{E}\\left[X_i\\right] & = (1/2i^2)(-i^2) + (1/2i^2)(i^2) +  (1 - 1/i^2)(0) = 0\\\\\n\\text{E}\\left[X_i^2\\right] & = (1/2i^2)(i^4) + (1/2i^2)(i^4) +  (1 - 1/i^2)(0) = i^2\\\\\n\\text{Var}\\left(X_i\\right) & = i^2 - 0^2 = i^2\n\\end{align*}\n\nWe can verify that Lindeberg's condition does not hold. \\begin{align*}\n\\lim_{n\\to\\infty}\\bar \\sigma_n = \\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{i=1}^n i^2 = \\lim_{n\\to\\infty}\\frac{(n+1)(2n+1)}{6} \\to\\infty\n\\end{align*}\n\nTo simulate realizations of this random variable, we can define $X_i$\nusing $U_i\\sim\\text{Uni}(0,1)$.[^asymptotics-1]\n\n$$X_i = \\begin{cases}-i^2 & U_i\\in[0, 1/2i^2)\\\\ i^2 & U_i\\in[1/2i^2, 1/i^2) \\\\ 0 & U_i\\in [1/i^2,1]\\end{cases}$$\n\n```{r}\ndraw_X <- function(n,...){\n  U <- runif(n)\n  prob_pos_i <- (U < 1/(2*(1:n)^2))\n  prob_neg_i <- (U > 1/(2*(1:n)^2) & U < 1/((1:n)^2))\n  (1:n)^2*prob_pos_i - (1:n)^2*prob_neg_i\n}\n\nresults <- c(1, 5, 10, 25, 100, 1000) %>% \n  outer_sim(., 200, draw_X, list(0))\n```\n\n```{r}\n#| code-fold: true\n#| label: fig-plot214\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"Histograms of estimates as sample size increases.\"\n#| code-summary: \"Show code which generates figure\"\n\nresults %>% \n  ggplot(aes(estimate)) +\n  geom_histogram(color = \"black\", fill = \"white\", bins = 50) +\n  facet_wrap(~sample_size, scales = \"free\") +\n  theme_minimal() +\n  labs(x = \"Estimates\", y = \"\")\n```\n\nThe apparent distribution of our estimates is not converging to a normal\ndistribution, as we always have a few outliers that are too plentiful\nrelative to their distance from the mean to be drawn from a normal\ndistribution. As $n\\to\\infty$ these outliers become even more extreme.\nThis is also evident from QQ-plots, where the points far away from the\n45-degree line are drawn even farther away as $n\\to\\infty$.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot215\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"The QQ-plot for the simulated distribution of the adjusted sample mean\"\n#| code-summary: \"Show code which generates figure\"\nresults %>% \n  ggplot(aes(sample = estimate)) +\n  stat_qq_line(color = \"red\") +\n  geom_qq(size= 0.5) + \n  facet_wrap(~sample_size) + \n  theme_minimal() +\n  labs(x = \"Quantiles of Simulated Estimates\", y = \"Quantiles of Normal Distribution\")\n```\n:::\n\n[^asymptotics-1]: We can actually draw observations of *any* random\n    variable using the uniform distribution on $[0,1]$ via [***inverse\n    transform\n    sampling***](https://en.wikipedia.org/wiki/Inverse_transform_sampling#Reduction_of_the_number_of_inversions).\n    This is one of the primary ways computers generate random\n    observations from a given distribution.\n\n       While theoretically important, Lindeberg's condition can be a bit hard\nto verify. It is much more common to appeal to a stronger assumption\nwhich gives rise to a second CLT that holds for independent, but not\nnecessarily identically distributed, random variables. This final CLT\nmay not be as general as the Lindeberg-Feller CLT, but it is much easier\nto work with.\n\n:::{#thm-}\n\n## Lyapunov CLT\n\nSuppose $\\mathbf{X}=(X_1,\\ldots, X_n)$ is a sequence of independent\nrandom variables with $\\text{E}\\left[X_i\\right]=\\mu_i$ and\n$\\text{Var}\\left(X_i\\right)=\\sigma_i^2$. If\n$\\text{E}\\left[\\left\\lvert X_i - \\mu_i\\right\\rvert^{2+\\kappa}\\right]$ is\nfinite for some $\\kappa > 0,$ then\n$\\sqrt n (\\bar X-\\mu)\\overset{d}{\\to}N(0, \\bar\\sigma^2)$, where\n$\\bar\\sigma = (1/n)\\sum_{i=1}^n \\sigma_i.$\n\n:::\n\n## Delta Method\n\nSlutsky's theorem and the continuous mapping theorem in tandem with the\nLLN give us the ability to prove that certain functions of sample means\nare convergent. Is it possible that we can do something similar with the\nCLT to find the asymptotic distribution of functions of sample means?\n\n       Suppose $g$ is a function of $\\bar X$, where a CLT applies to the random\nsample $\\mathbf{X}$. We know\n$\\sqrt n(\\bar X-\\mu)\\overset{a}{\\sim}N(0, \\sigma^2)$. Is it possible to\nconclude that\n$\\sqrt n(g(\\bar X)-g(\\mu))\\overset{a}{\\sim}N(0, \\tilde\\sigma^2)$ for\nsome $\\tilde\\sigma^2$? Furthermore, can we determine $\\tilde\\mu$ and\n$\\tilde\\sigma^2$ only knowing $g$, $\\mu$, and $\\sigma^2$?\n\n       We have information about the difference $\\bar X-\\mu$ and want\ninformation about the difference $g(\\bar X)-g(\\mu)$. Situations where we\nknow something about behavior in the domain of a function and want to\nrelate it to the function's behavior in the codomain are common place in\nmath, but in particular in real analysis. This is where the mean value\ntheorem saves the day. Assuming $g$ is continuously differentiable and\nfixing $n$, there exists some $T_n$ in between $\\bar X$ and $\\theta$\n($\\bar X< T_n < \\mu$ or $\\mu < T_n < \\bar X$) such that: \\begin{align*}\n& \\frac{g(\\bar X)-g(\\mu)}{\\bar X - \\mu} = g'(T_n)\\\\\n\\implies & g(\\bar X) = g(\\mu) + g'(T_n)(\\bar X-\\mu)\\\\\n\\implies & \\sqrt{n}[g(\\bar X) - g(\\mu)] = g'(T_n)\\sqrt{n} (\\bar X-\\mu)\n\\end{align*} If we let $n$ vary, we have a sequence of random variables\n$\\{T_n\\}$ such that\n$\\left\\lvert T_n -\\mu\\right\\rvert < \\left\\lvert\\bar X - \\mu\\right\\rvert$.\nBy the LLN $\\left\\lvert\\bar X - \\mu\\right\\rvert \\overset{p}{\\to}0$, so\n$\\left\\lvert T_n -\\mu\\right\\rvert \\overset{p}{\\to}0$, which is\nequivalent to $T_n \\overset{p}{\\to}\\mu$. By the continuous mapping\ntheorem, $g(T_n) \\overset{p}{\\to}g(\\mu)$. This means\n$$\\sqrt{n}[g(\\bar X) - g(\\mu)] = \\underbrace{g'(T_n)}_{\\overset{p}{\\to}g(\\mu)}\\cdot\\underbrace{\\sqrt{n} (\\bar X-\\mu)}_{\\overset{d}{\\to}N(0, \\sigma^2)},$$\nso Slutsky's theorem gives\n$$\\sqrt{n}[g(\\bar X) - g(\\mu)] \\overset{d}{\\to}g(\\mu)\\cdot N(0,\\sigma^2) = N(0, \\sigma^2[g(\\mu)]^2).$$\nThis all is contingent on $g$ not being a function of $n$, otherwise\nthings fall apart when we apply limiting processes.\n\n       This result is known as the delta method, and applies to any sequence of\nrandom variables that is asymptotically normal. It is also readily\ngeneralized to higher dimensions where $\\mathbf g$ is a vector valued\nfunction.\n\n:::{#thm-}\n\n## Delta Method\n\nSuppose $(\\mathbf{X}_1,\\ldots, \\mathbf{X}_n)$ is a sequence of random\nvectors such that\n$\\sqrt n (\\mathbf{X}_n - \\mathbf t) \\overset{d}{\\to}N(\\mathbf 0, \\boldsymbol\\Sigma)$\nfor some vector $\\mathbf t$ in the interior of $\\mathcal X$. If\n$\\mathbf g(\\mathbf{X}_n)$ is a vector valued function that:\n\n1.  is continuously differentiable,\n2.  does not involve $n$,\n3.  $\\frac{\\partial \\mathbf g}{\\partial\\mathbf x}(\\mathbf t) \\neq 0$,[^asymptotics-2]\n\nthen,\n\n$$ \\sqrt n \\left[\\mathbf g(\\mathbf{X}_n) - \\mathbf g(\\mathbf t)\\right] \\overset{d}{\\to}N\\left(\\mathbf 0, \\left[\\frac{\\partial \\mathbf g}{\\partial\\mathbf x}(\\mathbf t)\\right]\\boldsymbol\\Sigma\\left[\\frac{\\partial \\mathbf g}{\\partial\\mathbf x}(\\mathbf t)\\right]'\\right)$$\n\n:::\n\n[^asymptotics-2]: $\\frac{\\partial \\mathbf g}{\\partial\\mathbf x}(\\mathbf t)$\n    is the $\\dim(\\mathbf g(\\mathbf{X}_n)) \\times \\dim(\\mathbf X_n)$\n    Jacobian matrix comprised of the partial derivatives of the\n    components of $\\mathbf g$.\n\n:::{#exm-}\n\nReturn to the example where $X_i\\overset{iid}{\\sim}\\text{Exp}(1)$,\ngiving $\\text{E}\\left[X_i\\right] = 1$ and\n$\\text{Var}\\left(X_i\\right) = 1$. By the CLT,\n$\\sqrt n(\\bar X - 1)\\overset{d}{\\to}N(0,1)$. If $g(t) = t^2 +3$, what is\nthe asymptotic distribution of $\\sqrt{n}[g(\\bar X) - g(1)]$? According\nto the delta method we have \\begin{align*}\n&\\sqrt{n}[g(\\bar X) - g(1)]  \\overset{a}{\\sim}N(0, 1[g'(1)]^2),\\\\\n\\implies & \\sqrt{n}\\left[\\bar X^2 - 1\\right] \\overset{a}{\\sim}N(0, 4).\n\\end{align*}\n\n```{r}\ng <- function(t){\n  t^2 + 3\n}\n\niter <- function(g, n, dist, dist_params, s){\n  output <- tibble(\n    value = do.call(dist, append(n, dist_params))\n  ) %>% \n    summarize(\n      sample_size = n,\n      iter_num = s, \n      estimate = sqrt(n)*(g(mean(value)) - g(1))\n    ) \n  return(output)\n}\n\nsim <- function(N, g, n, dist, dist_params){\n  output <- 1:N %>% \n    map(iter, g = g, n = n, dist = dist, dist_params = dist_params) %>% \n    bind_rows()\n}\n\nresults <- sim(1e4, g, 1e5, rexp, list(1))\n```\n\nWe can plot a histogram of our estimates and overlay the distribution\n$N(0,4)$.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot216\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"Histogram of adjusted sample mean transformed by g, and theoretical distribution given by the delta method\"\n#| code-summary: \"Show code which generates figure\"\n\nresults %>% \n  ggplot(aes(estimate)) +\n  geom_histogram(aes(y = ..density..), colour = 1, fill = \"white\", bins = 100) + \n  xlab(\"Estimates of √n(g(X) - g(1)) \") +\n  theme_minimal() +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 2), color = \"red\")\n```\n\n:::\n\n       The big takeaway from the delta method is that estimators which are nice\nfunctions of sample means will be asymptotically distributed according\nto a normal distribution. Furthermore, any nice function of such an\nestimator will also have a normal asymptotic distribution!\n\n```{r}\n#| fig-align: center\n#| label: fig-plot217\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"A mediocre meme\"\n#| echo: false\n\nknitr::include_graphics(\"figures/meme.png\")\n```\n\n## Little $o_p$, Big $O_p$, and Taylor Expansions\n\nWe've talked a lot about whether or not random variables converge, and\nhow they converge, but not the rate at which they converge. We can\nintroduce some notation that allows us to quantify this rate.\n\n:::{#def-}\n\nGiven a sequence of random variables $X_n$, we say $X_n$ is [***little\n\"O.P\" of*** $n^k$]{style=\"color:red\"}, denoted $X_n = o_p(n^k)$, if\n$X_n / n^k\\overset{p}{\\to}0$.\n\n:::\n\n       Note that $X_n\\overset{p}{\\to}0$ implies that $X_n = o_p(1)$. The use of\n\"=\" is a bit misleading in this definition, as $X_n = o_p(n^k)$ does not\nestablish any equality, instead referring to how $X_n$ behaves\nasymptotically. For instance, if we have two sequences of random\nvariables $X_n$ and $Y_n$ such that $X_n\\overset{p}{\\to}X$ and\n$Y_n\\overset{p}{\\to}0$, we have \\$ X_n + Y_n \\overset{p}{\\to} X + 0 \\$,\nbut could write $X_n + Y_n$ as $X_n + o_p(1)$. This emphasizes the\nsequence $X_n$, and frames $Y_n$ as some negligible remainder term that\ntends to zero.\n\n:::{#def-}\n\nGiven a sequence of random variables $X_n$, we say $X_n$ is [***big\n\"O.P\" of*** $n^k$]{style=\"color:red\"}, denoted $X_n = O_p(n^k)$, if for\nall $\\varepsilon > 0$, there exists some $\\delta$ and $N$ such that\n$\\Pr(|X_n/n^k| \\ge \\delta) <\\varepsilon$ for all $n > N$. In other\nwords, $X_n/n^k$ is [***bounded in probability***]{style=\"color:red\"}.\n\n:::\n\n       We are most interested in the case where $X_n = O_p(1)$. If this is the\ncase, then as $n\\to\\infty$, we can bound the area in the tails of\n$f_{X_n}$ by some constant $\\delta$ such that the area is negligible\n(less than $\\varepsilon$).\n\n:::{#exm-}\n\nWe know that $\\bar X \\overset{d}{\\to}N(\\mu, \\sigma^2/n)$ when\n$\\mathbf{X}$ is an iid sample. We have that $\\bar X = O_p(1)$. We have\n$$\\Pr(|\\bar X/1| \\ge \\delta) = \\Pr(-\\bar X\\ge -\\delta \\text{ and }\\delta \\le \\bar X) = 2\\cdot\\Pr(\\bar X\\ge \\delta) = 2\\left[1 - \\Phi\\left(\\frac{\\delta - \\mu}{\\sigma/\\sqrt n}\\right)\\right].$$\nIf we take the limit of this as $n\\to \\infty$ we have\n$$ \\lim_{n\\to \\infty}\\Pr(|X\\bar X/1| \\ge \\delta) = \\lim_{n\\to \\infty}2\\left[1 - \\Phi\\left(\\frac{\\delta - \\mu}{\\sigma/\\sqrt n}\\right)\\right] = 0.$$\nBy the definition of a limit, there must exists some $N$ such that\n$\\Pr(|\\bar X/1| \\ge \\delta) < \\varepsilon$ for any $n > N$, so\n$\\bar X = O_p(1)$.\n\n:::\n\n:::{#exm-}\n\nIf $X_n \\overset{d}{\\to}X$, then $X_n = O_p(1)$.\n\n:::\n\n:::{#exm-}\n\nIf $X_n = o_p(1)$, then $X_n = O_p(1)$.\n\n:::\n\n       A common place to encounter $o_p$ is when performing Taylor expansions.\n\n:::{#exm-}\n\n## Taylor's Theorem\n\nTaylor's theorem, as given in @rudin1976principles, tells us that if\n$f:\\mathbb R\\to\\mathbb R$ is $k-$times differentiable at a point\n$a\\in \\mathbb R$, then there is some element $c\\in (a,b)$ such that\n\\begin{align*}\nf(b) & = \\sum_{j=0}^{k-1}\\frac{f^{(j)}(a)}{k!}(b-a)^j + \\frac{f^{(n)}(c)}{k!}(b-a)^k.\n\\end{align*} For $n = 2$, we have the mean value theorem:\n$$ f(b)= f(a) + f'(c)(b-a).$$ If we let $a\\to b$, then \\begin{align*}\n&\\lim_{a\\to b}f(b)  = \\sum_{j=0}^{k-1}\\lim_{a\\to b}\\frac{f^{(j)}(a)}{j!}(b-a)^j + \\lim_{a\\to b}\\frac{f^{(k)}(c)}{k!}(b-a)^k.\\\\\n\\implies & f(b)  =\\lim_{a\\to b} f(a) + \\sum_{j=1}^{k-1}\\lim_{a\\to b}\\frac{f^{(j)}(a)}{j!}(b-a)^j +  \\lim_{a\\to b}\\frac{f^{(k)}(c)}{k!}(b-a)^k\\\\\n\\implies & f(b)  = f(b) + \\underbrace{\\sum_{j=1}^{k-1}\\frac{f^{(j)}(b)}{j!}(b-b)^j}_0 + \\lim_{a\\to b}\\frac{f^{(k)}(c)}{k!}(b-a)^k\\\\\n\\implies & \\lim_{a\\to b}\\frac{f^{(k)}(c)}{n!}(b-a)^k = 0\\\\\n\\implies & \\lim_{a\\to b}\\frac{f^{(k)}(c)}{n!(b-a)^k} = 0\\\\\n\\implies & \\frac{f^{(k)}(c)}{k!} = o(|a-b|^k)\\\\\n\\end{align*} Here, $o$ is the deterministic counterpart of $o_p$ (we're\nnot working with random variables just yet). This means we can write\nTaylor's theorem as\n$$ f(b) = \\sum_{j=0}^{k-1}\\frac{f^{(j)}(a)}{j!}(b-a)^k +o(|a-b|^k).$$ In\nthe event $f$ is infinitely differentiable we can make this\napproximation arbitrarily accurate, giving rise toa function's Taylor\nseries.\n\nNow suppose $f_n(X)$ is a sequence of functions of random variables,\nwhere the subscript $n$ emphasizes that $f_n$ is a random variable. IF\nwe apply Taylor's theorem to $f_n(X)$ we have\n$$f_n(b) = \\sum_{j=0}^{k-1}\\frac{f_n^{(j)}(a)}{j!}(b-a)^k +o_p(|a-b|^k)$$\nfor realizations of the random variable $a,b,c\\in\\mathcal X$ where\n$c\\in(a,b)$. Assuming $k \\ge 1$, then $o_p(|a-b|^k)$ implies $o_p(1)$,\nso $$f_n(b) = \\sum_{j=0}^{k-1}\\frac{f_n^{(j)}(a)}{j!}(b-a)^k +o_p(1).$$\n\n:::\n\n## Asymptotically Normal Estimators\n\nWhen putting our asymptotic tools to work on an estimator of interest,\nwe will almost always find that it converges to a normal distribution,\nis consistent, and that the rate of convergence is linked to $\\sqrt{n}$.\n\n:::{#def-}\n\nAn estimator $\\hat{\\boldsymbol{\\theta}}$ is [$\\sqrt{n}-$consistent\nasymptotically normal (root-n CAN)]{style=\"color:red\"}, if\n$$\\sqrt{n}(\\hat{\\boldsymbol{\\theta}}- \\boldsymbol{\\theta}) \\overset{d}{\\to}N(\\mathbf 0, \\mathbf V)$$\nfor a PSD matrix $\\mathbf V$. Equivalently,\n$$ \\hat{\\boldsymbol{\\theta}}\\overset{a}{\\sim}N(\\boldsymbol{\\theta}, \\mathbf V/n).$$\nWe refer to $\\mathbf V/n$ as the [***asymptotic\nvariance***]{style=\"color:red\"} of $\\hat{\\boldsymbol{\\theta}}$ and write\n$\\text{Avar}\\left(\\hat{\\boldsymbol{\\theta}}\\right) = \\mathbf V /n$.\n\n:::\n\n       \"$\\sqrt n-$\" emphasizes the fact that\n$\\sqrt{n}(\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}) = O_p(1)$,\nwhich is equivalent to\n$\\hat{\\boldsymbol{\\theta}} = \\boldsymbol{\\theta}+ O_p(n^{-1/2})$. In\nother words, as $n\\to\\infty$ the error term associated with our estimate\ndecreases at a rate of $n^{1/2}$. A fourfold increase in observations\nresults in half the error. We also have that\n$\\sqrt{n}(\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}) = o_p(1)$, so\n$\\hat{\\boldsymbol{\\theta}}\\overset{p}{\\to}\\boldsymbol{\\theta}$, hence\nthe \"consistent\" in the previous definition. We also have that\n$\\hat{\\boldsymbol{\\theta}}$ is asymptotically unbiased if it is root-n\nCAN, as\n$\\text{E}\\left[\\hat{\\boldsymbol{\\theta}}\\right]\\to \\boldsymbol{\\theta}$.\nThis will be the one of, if not the, ***most important property*** an\nestimator can posses.\n\n## Further Reading\n\n-   Eric Zivot's [primer on\n    asymptotics](http://faculty.washington.edu/ezivot/econ583/econ583asymptoticsprimer.pdf)\n-   @wooldridge2010econometric, Chapter 3\n-   @greene2003econometric, Appendix D\n-   @van2000asymptotic\n-   @white2014asymptotic\n","srcMarkdownNoYaml":"\n\n\\DeclareMathOperator{\\plim}{plim}\n\\DeclareMathOperator{\\argmin}{argmin}\n\\DeclareMathOperator{\\argmax}{argmax}\n\\newcommand{\\var}[1]{\\text{Var}\\left(#1\\right)}\n\\newcommand{\\avar}[1]{\\text{Avar}\\left(#1\\right)}\n\\newcommand{\\E}[1]{\\text{E}\\left[#1\\right]}\n\\newcommand{\\cov}[1]{\\text{Cov}\\left(#1\\right)}\n\\newcommand{\\mse}[1]{\\text{MSE}\\left(#1\\right)}\n\\newcommand{\\se}[1]{\\text{se}\\left(#1\\right)}\n\\newcommand{\\limfunc}{lim} \n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\Xm}{\\mathbb{X}}\n\\newcommand{\\EER}{\\bar{\\thet}_\\text{EE}}\n\\newcommand{\\NLS}{\\hat{\\bet}_\\text{NLLS}}\n\\newcommand{\\z}{\\mathbf{z}}\n\\newcommand{\\rr}{\\mathbf{r}}\n\\newcommand{\\C}{\\mathbf{C}}\n\\newcommand{\\Pe}{\\mathbf{P}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\uu}{\\mathbf{u}}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\xm}{\\mathbb{x}}\n\\newcommand{\\Zm}{\\mathbb{Z}}\n\\newcommand{\\Wm}{\\mathbb{W}}\n\\newcommand{\\Hm}{\\mathbb{H}}\n\\newcommand{\\W}{\\mathbf{W}}\n\\newcommand{\\Z}{\\mathbf{Z}}\n\\newcommand{\\Hess}{\\mathbf{H}(\\mathbf{\\Z\\mid\\thet})}\n\\newcommand{\\Score}{\\mathbf{S}(\\mathbf{\\Z\\mid\\thet})}\n\\newcommand{\\A}{\\mathbf{A}}\n\\newcommand{\\h}{\\mathbf{h}}\n\\newcommand{\\Q}{\\mathbf{Q}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\G}{\\mathbf{G}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\renewcommand{\\D}{\\mathbf{D}}\n\\renewcommand{\\C}{\\mathbf{C}}\n\\newcommand{\\zer}{\\mathbf{0}}\n\\newcommand{\\OLS}{\\hat{\\boldsymbol\\beta}_\\text{OLS} }\n\\newcommand{\\OLSOV}{\\hat{\\boldsymbol\\beta}_\\text{OLS,OV} }\n\\newcommand{\\OLSME}{\\hat{\\boldsymbol\\beta}_\\text{OLS,ME} }\n\\newcommand{\\EE}{\\hat{\\boldsymbol\\theta}_\\text{EX} }\n\\newcommand{\\ME}{\\hat{\\boldsymbol\\theta}_\\text{M} }\n\\newcommand{\\MDE}{\\hat{\\boldsymbol\\theta}_\\text{MDE} }\n\\newcommand{\\IV}{\\hat{\\boldsymbol\\beta}_\\text{IV} }\n\\newcommand{\\TSLS}{\\hat{\\boldsymbol\\beta}_\\text{2SLS} }\n\\newcommand{\\thet}{\\boldsymbol{\\theta}}\n\\newcommand{\\et}{\\boldsymbol{\\eta}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Sig}{\\boldsymbol{\\Sigma}}\n\\newcommand{\\ep}{\\boldsymbol{\\varepsilon}}\n\\newcommand{\\Omeg}{\\boldsymbol{\\Omega}}\n\\newcommand{\\Thet}{\\boldsymbol{\\Theta}}\n\\newcommand{\\bet}{\\boldsymbol{\\beta}}\n\\newcommand{\\rk}{rank}\n\\newcommand{\\tsum}{\\sum}\n\\newcommand{\\tr}{tr}\n\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\\newcommand{\\abs}[1]{\\left\\lvert#1\\right\\rvert}\n\\newcommand{\\ms}{\\overset{ms}{\\to}}\n\\newcommand{\\pto}{\\overset{p}{\\to}}\n\\newcommand{\\asto}{\\overset{as}{\\to}}\n\\newcommand{\\iid}{\\overset{iid}{\\sim}}\n\\newcommand{\\dto}{\\overset{d}{\\to}}\n\\newcommand{\\asim}{\\overset{a}{\\sim}}\n\n# Asymptotic Properties of Estimators {#sec-asy}\n\n```{r}\n#| echo: false\n#| output: false\nlibrary(tidyverse)\nlibrary(gganimate)\n```\n\nWhen considering estimators in @sec-est, we kept the sample size\n$n$ fixed when assessing estimators. We now consider how estimators\nbehave as $n\\to\\infty.$ In practice, we will never have infinite data,\nasymptotics gives us an approximate idea of how estimators perform for\nlarge data sets. A comprehensive reference in asymptotic theory is due\nto @van2000asymptotic. For a treatment concerned purely with\neconometrics, @newey1994large provide a phenomenal survey, most of which\nwe will touch on when discussing general classes of estimators.\\\n      With loss of some generality, we will assume that all random\nvariables have finite expectation and variances. Dispensing with this\nassumption is something for a probability course.\n\n## Convergence\n\nAt some point in high school, most students encounter the concept of a\nnumeric sequence, and how they can converge to a limit. Later on,\nperhaps when taking a real analysis course, sequences are generalized to\nspaces of functions. A sequence of functions may also converge to a\nlimit, whether that be converging pointwise and/or converging uniformly\n(for details see @rudin1976principles). Random variables are functions\nfrom a sample space to $\\mathbb R$, so we can consider how these\nfunctions converge. \n\n### Convergence in MSE\n\nThe first type of convergence we'll work with deals with MSE.\n\n::: {#def-}\nA sequence of random variables $X_n$ [***converges in mean\nsquare***]{style=\"color:red\"} to a random variable $X$, written as\n$X_n\\overset{ms}{\\to} X$, if\n$$\\lim_{n\\to\\infty} \\text{E}\\left[(X_n - X)^2\\right] = 0.$$ A sequence\nof random vectors $\\mathbf{X}_n = (X_{1,n},\\ldots X_{k,n})$ converges in\nmean square to $\\mathbf{X}$ if $X_{i,n}\\overset{ms}{\\to} X_i$ for\n$i=1,\\ldots, k$.\n:::\n\n      $X_n \\overset{ms}{\\to}X$ if the average distance between $X_n$ and\n$X$ shrinks as $n\\to\\infty$ where distance is measured as $(X_n - X)^2$.\nWe can also have $X_n \\overset{ms}{\\to}c$ for some constant $c$, as $c$\nis a trivial random variable.\n\n::: {#exm-}\nSuppose we draw a sample of $n$ iid random variables $Z_i$ and define\n$X_n$ to be the sample mean of our observations.\n$$X_n = \\frac{1}{n}\\sum_{i=1}^n Z_i$$ If\n$\\text{E}\\left[Z_i\\right] = \\mu$ and\n$\\text{Var}\\left(Z_i\\right) = \\sigma^2$ for all $i$, we have\n$X_n\\overset{ms}{\\to}\\mu$: \\begin{align*}\n\\lim_{n\\to\\infty}\\text{E}\\left[(X_n - \\mu)^2\\right] & = \\lim_{n\\to\\infty}\\text{Var}\\left(X_n\\right) + \\text{Bias}(X_n) \\\\\n&= \\lim_{n\\to\\infty} \\frac{\\sigma^2}{n} + 0 & (X_n \\text{ unbiased}) \\\\\n& = \\lim_{n\\to\\infty} \\frac{\\sigma^2}{n} \\\\\n& = 0.\n\\end{align*}\n\nWhat does this convergence \"look like\"? If\n$Z_i\\overset{iid}{\\sim}N(0,1)$, we know that\n$X_n = \\bar Z \\sim N(\\mu, \\sigma^2/n)$. Let's plot this distribution for\nincreasing values of $n$.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot21\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"The distribution of X_n for increasing values of n\"\n#| code-summary: \"Show code which generates figure\"\nexpand_grid(\n  x = seq(-3, 3, length = 500),\n  n = 1:100\n) %>% \n  mutate(\n    y = dnorm(x, 0, sqrt(1/n))\n  ) %>% \n  ggplot(aes(x,y)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Value of X_n\", y = \"Density\", color = \"Sample Size\") +\n  theme(legend.position = \"bottom\") +\n  transition_states(n) +\n  labs(title = 'n = {closest_state}')\n```\n:::\n\n      This example betrays a useful property related to variables which\nconverge in mean square.\n\n::: {#prp-mse3}\nA sequence of random variables $X_n$ converges in mean square to a\nconstant $c$ *if and only if* $\\text{E}\\left[X_n\\right]\\to c$ and\n$\\text{Var}\\left(X_n\\right)\\to 0$.\n:::\n\n::: proof\n[space]{style=\"color:white\"}\n\n$(\\Longrightarrow)$ Suppose $X_n \\overset{ms}{\\to}c$. Then\n\\begin{align*}\n& \\lim_{n\\to\\infty}\\text{E}\\left[(X_n - c)^2\\right] = 0\\\\\n \\implies & \\lim_{n\\to\\infty}\\text{E}\\left[(X_n - c)^2\\right] = 0\\\\\n \\implies& \\lim_{n\\to\\infty}\\left[\\text{E}\\left[X_n\\right]^2 -2c \\text{E}\\left[X_n\\right] + c^2\\right]= 0\\\\\n \\implies& \\lim_{n\\to\\infty}\\left[(\\text{E}\\left[X_n\\right]^2 -\\text{E}\\left[X_n\\right]^2) + \\text{E}\\left[X_n\\right]^2 - 2c \\text{E}\\left[X_n\\right] + c^2\\right]= 0 \\\\ \n \\implies &\\lim_{n\\to\\infty} \\text{Var}\\left(X_n\\right) + \\lim_{n\\to\\infty}\\left[\\text{E}\\left[X_n\\right] -c\\right]^2 = 0 \n\\end{align*} This final equality gives the desired result.\n\n$(\\Longleftarrow)$ Suppose $\\text{E}\\left[X_n\\right]\\to c$ and\n$\\text{Var}\\left(X_n\\right)\\to 0$. We have \\begin{align*}\n&\\lim_{n\\to\\infty} \\text{Var}\\left(X_n\\right) + \\lim_{n\\to\\infty}\\left[\\text{E}\\left[X_n\\right] -c\\right]^2 = 0 \\\\\n\\implies & \\lim_{n\\to\\infty}\\text{E}\\left[(X_n - c)^2\\right] = 0\\\\\n\\implies & X_n\\overset{ms}{\\to}c\n\\end{align*}\n:::\n\n::: {#cor-mseconv}\nSuppose $X_n$ is a sequence of random variables such that\n$\\text{E}\\left[X_n\\right] = c$ for all $n$. Then $X_n\\overset{ms}{\\to}c$\n*if and only if* $\\text{Var}\\left(X_n\\right)\\to 0$.\n:::\n\n### Convergence in Probability\n\nConvergence in mean square captures the idea that a random variable gets\n\"closer\" to some value $c,$ but it is hardly the only way to define this\nbehavior. A more \"traditional\" approach would be defining convergence\nusing an inequality involving an arbitrarily small $\\varepsilon >0$\n(akin the to $\\varepsilon-\\delta$ definition of a limit).\n\n::: {#def-}\nA sequence of random variables $X_n$ [***converges in\nprobability***]{style=\"color:red\"} to a random variable $X$, written as\n$X_n\\overset{p}{\\to}X$ or $\\mathop{\\mathrm{plim}}X_n = X$, if\n$$\\lim_{n\\to\\infty} \\Pr (|X_n - X| > \\varepsilon)= 0$$ for all\n$\\varepsilon > 0$. Equivalently, $X_n\\overset{p}{\\to}X$ if for all\n$\\varepsilon > 0$ and $\\delta > 0$, there exists some $N$ such that for\nall $n \\ge N$, $$ \\Pr (|X_n - X| > \\varepsilon) < \\delta.$$ A sequence\nof random vectors $\\mathbf{X}_n = (X_{1,n},\\ldots X_{k,n})$ converges in\nprobability to $\\mathbf{X}$ if $X_{i,n}\\overset{p}{\\to} X_i$ for\n$i=1,\\ldots, k$.\n:::\n\n      Intuitively, $X_n \\overset{p}{\\to}X$ if the probability that the\ndifference $|X_n - X|$ is not small (greater than some $\\varepsilon$)\ngoes to zero as $n\\to\\infty$. In other words, for large values of $n$, there is \na large probability that $X_n$ is close to $X$\n\n::: {#exm-}\nReturn to the previous example where $X_n = \\bar Z$, and assume\n$Z_i \\overset{iid}{\\sim}N(\\mu,\\sigma^2)$. We will verify that\n$X_n\\overset{p}{\\to}\\mu$ using the definition of convergence in\nprobability using the fact that $X_n \\sim N(\\mu, \\sigma^2/n)$.\n\n```{r}\n#| fig-align: center\n#| label: fig-plot22\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"For any arbitrary ε, we can find some n such that the probability X_n falls outside the set |μ-ε| is arbitrarily small\"\n#| echo: false\n\nknitr::include_graphics(\"figures/converge.png\")\n```\n\nFor some $\\varepsilon > 0$, \\begin{align*}\n\\Pr (|X_n - \\mu| > \\varepsilon) & = 1 - \\Pr (\\mu - \\varepsilon < X_n < \\mu + \\varepsilon)\\\\\n& = 1 - (F_{X_n}(\\mu + \\varepsilon) + F_{X_n}(\\mu - \\varepsilon))\\\\\n& = 1 - 2\\left[F_{X_n}(\\mu + \\varepsilon) - \\frac{1}{2}\\right] & (F_{X_n} \\text{symmetric about }\\mu)\\\\\n & = 1 - 2\\left[\\Phi\\left(\\frac{(\\mu + \\varepsilon) - \\mu}{\\sigma/\\sqrt{n}}\\right) - \\frac{1}{2}\\right] & (\\Phi\\text{ standard normal distribution})\\\\\n & = 1 - 2\\left[\\Phi\\left(\\frac{\\varepsilon}{\\sigma/\\sqrt{n}}\\right) - \\frac{1}{2}\\right].\n\\end{align*} Given some $\\delta >0$, we can solve for the lowest value\nof $n$ that satisfies $\\Pr (|X_n - c| > \\varepsilon) < \\delta$.\n\\begin{align*}\n&\\Pr (|X_n - c| > \\varepsilon) < \\delta \\\\ \n\\implies & 1 - 2\\left[\\Phi\\left(\\frac{\\varepsilon}{\\sigma/\\sqrt{n}}\\right) - \\frac{1}{2}\\right] < \\delta \\\\\n\\implies&  n > \\left(\\frac{\\sigma \\Phi^{-1}(1-\\delta/2)}{\\varepsilon}\\right)^2\n\\\\\\implies & n > \\left\\lceil \\left(\\frac{\\sigma \\Phi^{-1}(1-\\delta/2)}{\\varepsilon}\\right)^2 \\right\\rceil\n\\end{align*} Just to be excruciatingly pedantic, we rounded our solution\nup to the closest positive integer, as $n$ corresponds to a sample size.\nFor fixed values of $\\mu$ and $\\sigma^2$ (say 3 and 2, respectively), we\ncan define a function of $(\\varepsilon, \\delta)$ which calculates the\nsample size required to satisfy $\\Pr(|X_n - c|>\\varepsilon)<\\delta$.\n\n```{r}\nmu <- 3\nsigma <- sqrt(2)\n\nn_fun <- function(delta, ep){\n ceiling(((sigma*qnorm(1-delta /2))/ep)^2) \n}\n```\n\nLet's plot this function for various values of $(\\varepsilon, \\delta)$.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot23\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"The sample size required to satisfy the inequality in the definition of convergence of probability for various values (ε,δ)\"\n#| code-summary: \"Show code which generates figure\"\nexpand_grid(\n  e = c(1, 0.1, 0.01, 0.001, 0.0001),\n  d = 1:9999/10000\n) %>% \n  mutate(sample = n_fun(d,e)) %>% \n  ggplot(aes(d, sample, color = as.factor(e))) +\n  scale_x_reverse() +\n  scale_y_log10() + \n  geom_line() +\n  theme_minimal() +\n  labs(x = \"δ\", y = \"Sample Size\", color = \"ε\")+\n  theme(legend.position = \"bottom\")\n```\n\nWe can also verify that\n$\\lim_{n\\to\\infty}\\Pr (|X_n - \\mu| > \\varepsilon) = 0$ for various\nvalues of $\\varepsilon.$\n\n```{r}\nprob_ep <- function(n, ep){\n  1 - 2*(pnorm(ep / (sigma / sqrt(n))) - 1/2)\n}\n```\n\n```{r}\n#| code-fold: true\n#| label: fig-plot24\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"The probability that X_n falls outside the interval  |μ-ε| for various values of (ε,n) \"\n#| code-summary: \"Show code which generates figure\"\nexpand_grid(\n  e = c(1, 0.1, 0.01, 0.001, 0.0001),\n  n = 1:100\n) %>% \n  mutate(prob = prob_ep(e,n)) %>% \n  ggplot(aes(n, prob, color = as.factor(e))) +\n  geom_line() +\n  scale_x_log10() +\n  theme_minimal() +\n  labs(x = \"Sample Size\", y = \"Pr(|X_n - mu| > ε)\", color = \"ε\") +\n  theme(legend.position = \"bottom\") \n```\n:::\n\n      How does convergence in mean square related to convergence in\nprobability? As it turns out the latter is a weaker condition implied by\nthe prior. Before stating and proving this result, we will need a lemma.\n\n::: {#lem-markovineq}\n## Markov's inequality\n\nIf $X$ is a nonnegative random variable, and $a > 0$, then\n$$\\Pr(X\\ge a) \\le \\frac{\\text{E}\\left[X\\right]}{a}$$\n:::\n\n::: proof\nThe expectation of $X$ can be written as \\begin{align*}\n\\text{E}\\left[X\\right] & = \\int_{-\\infty}^\\infty x\\ dF_X(x) \\\\ \n& = \\int_{0}^\\infty x\\ dF_X(x) & (X\\text{ is nonnegative}) \\\\ \n& = \\int_{0}^a x\\ dF_X(x) + \\int_{a}^\\infty x\\ dF_X(x) \\\\ \n& \\ge \\int_a^\\infty x\\ dF_X(x)\\\\\n& \\ge \\int_a^\\infty a\\ dF_X(x) & (a \\ge x \\text{ on }(a,\\infty))\\\\\n& = a \\int_a^\\infty\\ dF_X(x) \\\\\n& = a\\Pr(X \\ge a).\n\\end{align*} Dividing both sides of this inequality by $a$ gives\n$\\Pr(X\\ge a) \\le \\text{E}\\left[X\\right]/a$.\n:::\n\n::: {#prp-conv}\n## Convergence in MSE --\\> Convergence in Probability\n\nLet $X_n$ be a sequence of random variables. If $X_n\\overset{ms}{\\to}X$,\nthen $X_n\\overset{p}{\\to}X$.\n:::\n\n::: proof\nSuppose $X_n\\overset{ms}{\\to}X$. For all $\\varepsilon > 0$\n\\begin{align*}\n\\lim_{n\\to\\infty} \\Pr (|X_n - X| > \\varepsilon) & = \\lim_{n\\to\\infty} \\Pr ((X_n - X)^2 > \\varepsilon^2) \\\\\n& \\le \\lim_{n\\to\\infty} \\frac{\\text{E}\\left[(X_n - X)^2\\right]}{\\varepsilon^2} & (\\text{Markov's Inequality}) \\\\\n& = \\frac{0}{\\varepsilon^2} & (X_n\\overset{ms}{\\to}X)\\\\\n& = 0.\n\\end{align*} Therefore $X_n\\overset{p}{\\to}c$.\n:::\n\n      The usefulness of @prp-conv cannot be emphasized enough. Proving\nconvergence in probability using the definition is cumbersome, so we\nwill almost show convergence in mean square and then appeal to @prp-conv\nto verify convergence in probability. Nevertheless, situations can arise\nwhere $X_n\\overset{p}{\\to} X$, but $X_n \\not\\overset{ms}{\\to} X$.\n\n::: {#exm-pnotmse}\n\n## Convergence in Probability but not in Mean Square\n\nSuppose $X_n$ is defined on the sample space $\\{1,n^2\\}$ such that:\n\\begin{align*}\n\\Pr(X_n = 0) &= 1-1/n\\\\\n\\Pr(X_n = n^2) &= 1/n\n\\end{align*}\n\n```{r}\n#| code-fold: true\n#| label: fig-plot24.2\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"The probability density function associated with X_n for increasing values of n.\"\n#| code-summary: \"Show code which generates figure\"\nexpand_grid(n = 1:50, x = 0:1) %>% \n  mutate(\n    x = x*(n^2),\n    p = (1-1/n)*(x == 0) + (1/n)*(x != 0)\n  ) %>% \n  ggplot(aes(x = x, y = p)) +\n  geom_point() +\n  geom_segment(aes(x = x, xend = x, y = 0, yend = p)) +\n  theme_minimal() +\n  scale_x_sqrt(breaks = (1:10)*500) + \n  labs(color = \"n\", y = \"Probability Density\", x = \"X_n\") +\n  theme(legend.position = \"bottom\")  + \n  transition_states(n) +\n  labs(title = 'n = {closest_state}')\n```\nThe expected value of $X_n$ is\n$$\\text{E}\\left[X_n\\right] = 0(1-1/n) + n^2(1/n) = n,$$ so\n$\\text{E}\\left[X_n\\right]\\to\\infty$ as $n\\to \\infty$. This rules out\n$X_n$ converging in mean square to any value. Nevertheless, we have\n$X_n\\overset{p}{\\to}0$. For all $\\varepsilon > 0$,\n$$\\Pr(|X_n - 0| > \\varepsilon) = \\Pr(X_n \\neq 0) = \\Pr(X_n = n^2) = 1/n \\to 0.$$\nThis disagreement among definitions of convergence arises because the convergence in probability\nonly takes into consideration the probability assigned to each value in the sample space, whereas convergence in MSE is based on an expectation which takes into consideration the probability assigned to each value in the sample space *weighted* by the value in the sample space. In this particular example, the probability that $X_n = n^2$ is $1/n$, and the growth of $n^2$ as $n\\to \\infty$ outpaces the growth of $1/n$, so the expected value of $X_n$ grows indefinitely.\n:::\n\n\n::: remark \nWe can think of the counterexample in @exm-pnotmse arising from the \"tail\" of a density, where the \"tail\" happened to just be a single point because the random variable was discrete. This is a pattern that comes up often in asymptotics -- the tails of distributions and densities can cause trouble. Many theorems and results depend on these tails behaving well.\n:::\n\n### Almost Sure Convergence\n\nAn third type of convergence is almost sure convergence. Recalling that a random variable is just a function, we can define a stochastic analog to pointwise convergence of a sequence of functions. \n\n::: {#def-}\nA sequence of random variables $X_n$ [***converges almost surely***]{style=\"color:red\"} to a random variable $X$, written as\n$X_n\\overset{a.s}{\\to}X$ if\n$$ \\Pr \\left(\\lim_{n\\to\\infty} X_n = X\\right) = 1.$$ A sequence\nof random vectors $\\mathbf{X}_n = (X_{1,n},\\ldots X_{k,n})$ converges almost surely to $\\mathbf{X}$ if $X_{i,n}\\overset{a.s}{\\to} X_i$ for\n$i=1,\\ldots, k$.\n:::\n\n       Almost sure convergence is the probabilistic equivalent of a sequence of functions converging pointwise almost everywhere. The difference between $X_n\\overset{a.s}{\\to} X$ and $X_n\\pto X$ is subtle, and arises from the limit being taken before or after we take the probability of the event. While convergence in probability tells us that the chance that $X_n$ and $X$ are far approaches zero, almost sure convergence says that the probability that $X_n\\to X$ as $n\\to \\infty$ is 1. We're certain that $X_n$ will eventually coincide with $X$, although we don't know when that will happen. The next example makes the difference between these two definitions a bit more concrete. Later on in @exm-strongvsweak an illustration will be presented to distinguish the two definitions.\n\n::: {#exm-}\n\n## Convergence in Probability but not in Almost Surely\n\nDefine $X_n$ on $\\{0,1\\}$ such that: \n\\begin{align*}\n\\Pr(X_n = 1) & = 1/n\\\\\n\\Pr(X_n = 0) & = 1-1/n\n\\end{align*}\nWe have $X_n\\pto 0$ since \n$$ \\lim_{n\\to\\infty}\\Pr(|X_n - 0| > \\varepsilon) =  \\lim_{n\\to\\infty}\\Pr(X_n > \\varepsilon) = \\lim_{n\\to\\infty}\\Pr(X_n = 1) = \\lim_{n\\to\\infty} 1/n = 0$$ for all $\\varepsilon > 0$.\nAt the same time, we have \n$$\\sum_{n=1}^\\infty \\Pr(X_n = 1) =  \\sum_{n=1}^\\infty 1/n = \\infty,$$ so by a form of the [Borel-Cantelli Lemma](http://www.columbia.edu/~ks20/stochastic-I/stochastic-I-BC.pdf) then $X_n = 1$ occurs an infinite number of times. This rules out the possibility that $\\lim_{n\\to\\infty} X_n = 0$ with probability 1.\n\n:::\n\n::: {#prp-}\n\n## Convergence A.S -> Convergence in Probability\n\nLet $X_n$ be a sequence of random variables. If $X_n\\overset{a.s}\\to X$ then $X_n\\pto X$. \n\n:::\n\n       The proof of this result can be found in @van2000asymptotic. Almost sure convergence is not nearly as important as convergence in probability when it comes to assessing estimators. In fact, @lehmann1999elements doesn't even mention it in his treatment of asymptotic statistics. The math underlying almost sure convergence is also a bit complex, so the related proofs aren't very informative and will be omitted.\n\n### Convergence in Distribution\n\nThe final notion of convergence we will use related to the probability\ndistribution of random variables.\n\n:::{#def-}\nA sequence of random variables $X_n$ [***converges in distribution\n(converges weakly)***]{style=\"color:red\"} to a random variable $X$,\nwritten as $X_n \\overset{d}{\\to}X$, if\n$$\\lim_{n\\to\\infty} F_{X_n}(x)= F_X(x)$$ for all points $x$ where $F_{X}$ is continuous. In this case, we refer to\n$F_X$ as the [***asymptotic distribution***]{style=\"color:red\"} of\n$X_n$, and write $X_n \\overset{a}{\\sim}F_X$.^[Convergence in distribution is also sometimes called convergence in law among mathematicians. The notation for this form of convergence is also as varied as it's name. Some other ways of writing $X_n\\overset{d}{\\to} X$ are: $X_n\\rightsquigarrow X$, $X_n\\Rightarrow X$, $X_n \\overset{\\mathcal L}\\to X$, $\\mathcal L(X_n) \\to \\mathcal L(X)$.] A sequence\nof random vectors $\\mathbf{X}_n = (X_{1,n},\\ldots X_{k,n})$ converges in distribution to $\\X$ if $\\lim_{n\\to\\infty} F_{\\X_n}(x)= F_{\\X}(x)$.\n:::\n\n      For our purposes, $X_n\\overset{d}{\\to}X$ means the distribution of $X_n$\ncan be approximated by $F_X$, and this approximation becomes\nincreasingly better as $n\\to\\infty$.\n\n::: {#exm-tdist}\nOne example of convergence in distribution you may be familiar with\ndeals with the student's $t-$distribution where the degrees of freedom\n$n\\to\\infty$. If $X_n\\sim t_n$, then $X_n \\overset{d}{\\to}X$ where\n$X\\sim N(0,1)$.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot25\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"The t-distribution converges to the standard normal distribution (represented by the dashed red line) as the degrees of freedom increase.\"\n#| code-summary: \"Show code which generates figure\"\nexpand_grid(\n  x = seq(-4, 4, length = 1e3),\n  n = 1:10,\n  dist = \"Student's t\"\n) %>% \n  mutate(val = dt(x, n)) %>% \n  ggplot(aes(x, val)) +\n  geom_line() +\n  stat_function(\n    fun = dnorm, \n    args = list(mean = 0, sd = 1), \n    color = \"red\", \n    linetype=\"dashed\"\n  ) +\n  theme_minimal() +\n  labs(color = \"t-distribution degrees of freedom, n\", y = \"Density\") +\n  theme(legend.position = \"bottom\") +\n  transition_states(n) +\n  labs(title = 'n = {closest_state}')\n```\n:::\n\n::: {#prp-}\n## Convergence in Probability --\\> Convergence in Distribution\n\nLet $X_n$ be a sequence of random variables. If $X_n\\overset{p}{\\to}X$,\nthen $X_n\\overset{d}{\\to}X$.\n:::\n\n::: proof\nSuppose $X_n\\overset{p}{\\to}X$ and let $\\varepsilon > 0$. We have,\n\\begin{align*}\n\\Pr(X_n \\le x) & = \\Pr(X_n\\le x \\text{ and } X \\le x + \\varepsilon) + \\Pr(X_n\\le x \\text{ and } X > x + \\varepsilon) \\\\\n  & = \\Pr(X \\le x + \\varepsilon) + \\Pr(X_n - X\\le x - X \\text{ and } x - X < -\\varepsilon)\\\\ \n  & \\le \\Pr(X \\le x + \\varepsilon) + \\Pr(X_n - X < -\\varepsilon)\\\\\n  & \\le \\Pr(X \\le x + \\varepsilon) + \\Pr(X_n - X < -\\varepsilon) + \\Pr(X - X_n > \\varepsilon) \\\\\n  & = \\Pr(X \\le x + \\varepsilon) + \\Pr(|X_n - X| > \\varepsilon)\n\\end{align*} Similarly,\n$$ \\Pr(X \\le x-\\varepsilon) \\le \\Pr(X_n \\le x) + \\Pr(|X_n - X| > \\varepsilon).$$\nWe can use these inequalities to find an upper and lower bound of\n$\\Pr(X_n \\le x)$: \n\n::: {.column-screen-inset-right}\n\\begin{align*}\n& \\Pr(X \\le x-\\varepsilon) - \\Pr(|X_n - X| > \\varepsilon)\\le \\Pr(X_n \\le x) \\le \\Pr(X \\le x + \\varepsilon) + \\Pr(|X_n - X| > \\varepsilon) \\\\\n\\implies & \\lim_{n\\to\\infty}[\\Pr(X \\le x-\\varepsilon) - \\Pr(|X_n - X| > \\varepsilon)]\\le \\lim_{n\\to\\infty}\\Pr(X_n \\le x) \\le \\lim_{n\\to\\infty}[\\Pr(X \\le x + \\varepsilon) + \\Pr(|X_n - X| > \\varepsilon) ]\\\\\n\\implies & \\Pr(X \\le x-\\varepsilon) - \\underbrace{\\lim_{n\\to\\infty}\\Pr(|X_n - X| > \\varepsilon) }_0\\le \\lim_{n\\to\\infty}\\Pr(X_n \\le x) \\le \\Pr(X \\le x + \\varepsilon) + \\underbrace{\\lim_{n\\to\\infty}\\Pr(|X_n - X| > \\varepsilon)}_0 \\\\\n\\implies & \\Pr(X \\le x-\\varepsilon)\\le \\lim_{n\\to\\infty} \\Pr(X_n \\le x) \\le  \\Pr(X \\le x + \\varepsilon) & (X_n\\overset{p}{\\to}X)\\\\ \n\\implies & F_X(x-\\varepsilon)\\le \\lim_{n\\to\\infty} F_{X_n}(x) \\le  F_X(x-\\varepsilon) \n\\end{align*}\n:::\n\nThis holds for all $\\varepsilon > 0$, so it must be the\ncase that $\\lim_{n\\to\\infty} F_{X_n}(x) = F_X(x)$\n:::\n\n       Of the different concepts of stochastic convergence, convergence in distribution is the weakest (as the alternate name weak convergence implies). If $X_n \\overset{d}{\\to} X$, we're not saying that $X_n$ and $X$ become close, or that the probability that $X_n$ and $X$ are close approaches 1. We're saying that, given a common probability space $(\\mathcal X, \\mathcal F, P)$,^[The probability space on which the sequence of random variables is defined need not be common, but assume as much for the sake of this digression.] that the distribution function $F_{X_n}$ defined via the distribution $P(X^{-1}(I))$^[where $I\\subseteq \\mathcal B(\\mathbb R)$] happens to converge to the same function corresponding to the random variable $X$. Furthermore, the convergence of $F_{X_n}$ to $F_X$ only needs to occur at the points which $F_X$ is continuous, *and* even then the convergence only needs to be pointwise (opposed to uniform).\n\n:::\n\n### Putting the Pieces Together\n\nThe biggest takeaway from this should be the following relations:\n\\begin{align*}\n&X_n \\overset{ms}{\\to}X \\implies X_n \\overset{p}{\\to}X \\implies X_n \\overset{d}{\\to}X\\\\\n&X_n \\overset{a.s}{\\to}X \\implies X_n \\overset{p}{\\to}X\n\\end{align*}\n\n\n## Consistency\n\nOur three modes of convergence were defined for any sequence of random\nvariables. It should come as no surprise, considering the previous\nexamples considering whether the sample mean converged, that we are\ninterested in the convergence of estimators $\\hat\\theta(\\mathbf{X})$ as\nsample size increases. In particular we are interested in whether\n$\\hat\\theta(\\mathbf{X})$ converges to the constant $\\theta\\in\\Theta$ it\nis estimating.\n\n:::{#def-}\nAn estimator $\\hat\\theta$ is [***consistent (for estimand***\n$\\theta$)]{style=\"color:red\"} if $\\hat\\theta \\overset{p}{\\to}\\theta$.\n:::\n\n       We've already seen that $\\bar X$ is a consistent estimator for $\\mu$\nwhen we take an iid sample from a normal distribution. Let's investigate\nit's variance-counterpart $S^2$.\n\n::: {#exm-consvarnorm}\nFor $X_i \\overset{iid}{\\sim}N(\\mu,\\sigma^2)$,\n$S^2 = \\sum_{i=1}^n (X_i - \\bar X)/(n-1)$ is an unbiased estimator for\n$\\sigma^2$. This estimator's MSE (which is just its variance as it is\nunbiased) is $2\\sigma^4/(n-1)$ which converges to $0$ as $n\\to\\infty$,\nso $S^2$ is consistent by @prp-conv.\n:::\n\n       This example highlights the fact that proving an unbiased estimator is\nconsistent is a matter of showing its variance converges to 0.\n\n::: {#cor-unbcon}\nSuppose $\\hat\\theta$ is an unbiased estimator for $\\theta$. Then\n$\\hat\\theta$ is consistent *if and only if*\n$\\text{Var}\\left(\\hat\\theta\\right)\\to 0$.\n:::\n\n::: proof\nApply @cor-mseconv to an unbiased estimator.\n:::\n\n      A second type of convergence related to estimators pertains to the bias\nof an estimator. In @sec-est we saw a few estimators that were biased,\nbut this bias was such that it diminished as $n\\to\\infty$. In effect,\nthey were unbiased in an asymptotic sense.\n\n:::{#def-}\n\nAn estimator $\\hat\\theta$ is [***asymptotically\nunbiased***]{style=\"color:red\"} if\n$\\lim_{n\\to\\infty}\\text{Bias}(\\hat\\theta, \\theta) = 0$.\n\n:::\n\n:::{#exm}\nFor $X_i \\overset{iid}{\\sim}N(\\mu,\\sigma^2)$,\n$\\hat\\theta = \\sum_{i=1}^n (X_i - \\bar X)/n$ is a biased estimator for\n$\\sigma^2$. Its bias is\n$$\\text{Bias}(\\hat\\theta, \\sigma^2) = \\frac{n-1}{n}\\sigma^2 - \\sigma^2.$$\nAs $n\\to\\infty$, this bias vanishes. To illustrate this, we can simulate\nestimates for various sample sizes, taking $X_i \\sim N(0,1)$.\n\n```{r}\ntheta <- function(X){\n  sum((X - mean(X))^2)/length(X)\n}\n\niter <- function(n, dist, dist_params, s){\n  X <- do.call(dist, append(n, dist_params))\n  output <- tibble(\n    sample_size = n,\n    iter_num = s,\n    estimate = theta(X)\n  )\n  return(output)\n}\n\nsim <- function(N, n, dist, dist_params){\n  output <- 1:N %>% \n    map(iter, n = n, dist = dist, dist_params = dist_params) %>% \n    bind_rows()\n}\n\nouter_sim <- function(N, n_vals, dist, dist_params){\n  output <- n_vals %>% \n    map(sim, N = N, dist = dist, dist_params = dist_params) %>% \n    bind_rows()\n}\n\nresults <- c(10, 25, 50, 100, 500) %>% \n  outer_sim(1e6, ., rnorm, list(0, 1))\n\n#Print bias calculated over 1,000,000 simulations\nresults %>% \n  group_by(sample_size) %>% \n  summarize(bias = mean(estimate) - 1)\n```\n\n```{r}\n#| code-fold: true\n#| label: fig-plot26\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"As the sample size increases, the bias of our estimator converges to zero.\"\n#| code-summary: \"Show code which generates figure\"\n#| warning: false\n\nresults %>% \n  ggplot(aes(estimate, color = as.factor(sample_size))) +\n  geom_density() +\n  xlim(0,2) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"Estimates of Variance, True Value = 1\", y = \"Density\", color = \"Sample Size\")\n```\n\nThe estimator $\\hat\\theta$ is also consistent, as it converges in mean\nsquare.\n\n:::\n\n       Asymptotic unbiasedness does not imply consistency, and\nconsistency does not imply asymptotic unbiasedness.\n\n:::{#exm-}\n\n## Consistent, Not Asymptotically Unbiased\n\nRecall that the sequence of discrete random variables $X_n$ with denisty\n$$ f_{X_n}(x) = \\begin{cases}1-1/n& x=0\\\\ 1/n & x = n^2 \\end{cases}.$$\nWe established that $X_n\\overset{p}{\\to}0$, so an estimator with this\ndistribution would be a consistent estimator for $0$. Despite this, the\nestimator would not be asymptotically unbiased, as\n$\\text{E}\\left[X_n\\right] = n$, which tends to infinity as $n$ grows.\n\n:::\n\n:::{#exm-}\n## Asymptotically Unbiased, Not Consistent\nFor $X_i\\overset{iid}{\\sim}N(\\mu,\\sigma^2)$, define the estimator\n$\\hat\\mu(\\mathbf{X}) = X_1$. We simply take the first observation to be\nour estimate of $\\mu$. This estimator is unbiased,\n$$\\text{E}\\left[\\hat\\mu\\right] = \\text{E}\\left[X_1\\right] = \\mu,$$ so it\nis asymptotically unbiased. Nevertheless, the estimator fails to be\nconsistent, as\n$$\\lim_{n\\to\\infty} \\Pr (|\\hat\\mu - \\mu| > \\varepsilon)= \\lim_{n\\to\\infty} \\left\\{1 - 2\\left[\\Phi\\left(\\frac{\\varepsilon}{\\sigma}\\right) - \\frac{1}{2}\\right]\\right\\} \\neq 0 .$$\n:::\n\n       The incompatibility of asymptotic unbiasedness and consistency is due to\nthe behavior of $\\text{Var}\\left(X_n\\right)$ as $n\\to\\infty$.\n\n::: {#prp-consbias}\n## Relating Consistency and Asymptotic Unbiasedness\n\nSuppose $\\hat\\theta$ is an estimator for $\\theta$.\n\n1.  If $\\hat\\theta$ is consistent and there exists some $M$ such that\n    $\\text{Var}\\left(\\hat\\theta\\right) \\le M$ for all $n$ (bounded\n    variance), then $\\hat\\theta$ is asymptotically unbiased.\n2.  If $\\hat\\theta$ is asymptotically unbiased and\n    $\\lim_{n\\to\\infty}\\text{Var}\\left(\\hat\\theta\\right) = 0$ (vanishing\n    variance), then $\\hat\\theta$ is consistent\n:::\n\n::: proof\ntest\n:::\n\n```{r}\n#| fig-align: center\n#| label: fig-plot27\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"Relationship between various concepts of convergence in the context of estimators\"\n#| echo: false\n\nknitr::include_graphics(\"figures/relating_convergence.png\")\n```\n\n## Laws of Large Numbers\n\nIn most examples until now, the properties of estimators were implicitly\na function of the underlying model the data is generated from. We\nestablished that $\\bar X$ and $S^2$ are consistent estimators for $\\mu$\nand $\\sigma^2$, respectively, *when*\n$X_i \\overset{iid}{\\sim}N(\\mu,\\sigma^2)$. We know the distribution of\n$\\bar X$, *when* $X_i \\overset{iid}{\\sim}N(\\mu,\\sigma^2)$. In an ideal\nworld, we would be able to establish desirable properties of estimators\nunder more robust settings where our specified model may include a wide\narray of distributions. Our first step in doing this will be introducing\nvariants of one of the most important results in all of probability -- the law of\nlarge numbers (LLN). In the context of estimation, LLNs give sufficient conditions for our favorite estimator, $\\bar X$, to be consistent. \n\n### Khinchine's Weak Law of Large Numbers\n\nThe version of the LLN we'll use the most often deals with convergence in probability. To prove that $\\bar X \\overset{p}{\\to}\\mu$ we'll rely on an inequality similar to\n@lem-markovineq.\n\n::: {#lem-}\n## Chebyshev's Inequality\n\nIf $X$ is a random variable with an expected value $\\mu$ and variance\n$\\sigma^2$, then for all $a > 0$\n$$\\Pr(|X - \\mu| \\ge k) \\le \\frac{\\sigma^2}{k^2}.$$\n:::\n\n::: proof\n```{=tex}\n\\begin{align*}\n\\Pr(|X - \\mu| \\ge k) &= \\Pr((X - \\mu)^2 \\ge k^2)\\\\\n& \\le \\frac{\\text{E}\\left[(X-\\mu)^2\\right]}{k^2} & (\\text{Markov's Inequality})\\\\\n& = \\frac{\\sigma^2}{k^2}\n\\end{align*}\n```\n:::\n\n::: {#thm-}\n## LLN I\n\nIf $(X_1,\\ldots, X_n)$ are a set of iid random variables where\n$\\text{E}\\left[X_i\\right] = \\mu$, then $\\bar X \\overset{p}{\\to}\\mu$.\n:::\n\n::: proof\nRecall that $\\text{Var}\\left(\\bar X\\right) = \\sigma^2/n$. By Chebyshev's\nInequality, \\begin{align*}\n\\lim_{n\\to\\infty}\\Pr(|X_n - \\mu| \\ge \\varepsilon) \\le \\lim_{n\\to\\infty}\\frac{(\\sigma^2/n)}{\\varepsilon^2} =   \\lim_{n\\to\\infty} \\frac{\\sigma^2}{n\\varepsilon} = 0.\n\\end{align*} Therefore, $\\bar X\\overset{p}{\\to}\\mu$.\n:::\n\n:::{#exm-lln1}\n\nTo illustrate the LLN, let's simulate realizations of iid random\nvariables from a series of different distributions and show that\nregardless of the distribution, $\\bar X \\to \\mu$. We will use the\nfollowing distributions: \\begin{align*}\nX_i & \\overset{iid}{\\sim}\\text{Exp}(1/\\mu)\\\\\nX_i & \\overset{iid}{\\sim}\\chi_\\mu^2\\\\\nX_i & \\overset{iid}{\\sim}\\text{Uni}(0, 2\\mu)\\\\\nX_i & \\overset{iid}{\\sim}\\text{Gamma}(2\\mu, 2)\\\\\nX_i & \\overset{iid}{\\sim}\\text{HyperGeo}(10, 20, 15\\mu)\n\\end{align*} All these distributions have been selected such that\n$\\text{E}\\left[X_i\\right] = \\mu$. For our simulations, we will take\n$\\mu = 5$. If we plot the value of the sample mean versus the sample\nsize $n$, we see that the values converge to the true value $\\mu = 5$\nregardless of the distribution of $X_i$.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot28\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"The sample mean of all samples converges to the population mean by the LLN\"\n#| code-summary: \"Show code which generates figure\"\n#| warning: false\niter <- function(n, dist, dist_params){\n  dist_string <- paste(substitute(dist))\n  output <- tibble(\n    sample_size = 1:n,\n    prob_dist = dist_string,\n    random_values = do.call(dist, append(n, dist_params))\n  ) %>% \n    mutate(estimate = cummean(random_values))\n  return(output)\n}\n\n# can likely use pmap here\nresults <- bind_rows(\n  iter(1e5, rexp, list(1/5)),\n  iter(1e5, rchisq, list(5)),\n  iter(1e5, runif, list(0, 10)),\n  iter(1e5, rgamma, list(10, 2)),\n  iter(1e5, rhyper, list(10, 20, 15))\n)\n\nresults %>% \n  ggplot(aes(sample_size, estimate, color = prob_dist)) +\n  geom_line(alpha=0.6) +\n  theme_minimal() +\n  labs(x = \"Sample Size, n\", y = \"Sample Mean\", color = \"Distribution of iid Random Sample\") +\n  theme(legend.position = \"bottom\") +\n  geom_hline(yintercept = 5) +\n  ylim(4.5, 5.5)\n```\n\nFor the sake of an even better illustration, let's focus on the case where $X_i \\overset{iid}{\\sim}\\text{Exp}(1/\\mu)$ where $\\mu = 5$. We'll draw 10,000 samples of $X_i$, and calculate $\\bar X$ for each sample as the sample size ranges from 1 all the way to 1,000.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot28.2\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"10,000 lines, each of which corresponds to a simulated realization of the sample mean over a range of sample sizes. The area within the red dashed lines is the set |μ - Sample Mean| < 0.5\"\n#| code-summary: \"Show code which generates figure\"\n#| warning: false\n\niter <- function(n, s, dist, dist_params){\n  output <- tibble(\n    sample_size = 1:n,\n    random_values = do.call(dist, append(n, dist_params)),\n    iter_num = s\n  ) %>% \n    mutate(estimate = cummean(random_values))\n  return(output)\n}\n\nsim <- function(N, n, dist, dist_params){\n  output <- 1:N %>% \n    map(iter, n = n, dist = dist, dist_params = dist_params) %>% \n    bind_rows()\n  return(output)\n}\n\nresults <- sim(1e4, 1e3, rexp, list(1/5))\n\nepsilon <- 0.5 \nresults %>% \n  ggplot(aes(x = sample_size, y = estimate)) +\n  geom_line(aes(group = iter_num), alpha = 0.1, size = 0.05) +\n  ylim(0, 15) + \n  geom_hline(yintercept = 5 - epsilon, color = \"red\", linetype=\"dashed\") +\n  geom_hline(yintercept = 5 + epsilon, color = \"red\", linetype=\"dashed\") +\n  theme_minimal()\n```\n\nThe weak LLN tells us that since $\\bar X\\pto 5$, then by the definition of convergence in probability, the probability one of the lines falls in @fig-plot28.2 outside the interval within the red dashed lines at a particular sample size $n$ approaches zero as $n\\to \\infty$. Furthermore, this will hold regardless of how close we make the red lines to the true value $\\mu = 5$. For the sake of illustration we took $\\varepsilon = 0.5$, but it will hold *for all* $\\varepsilon > 0$. In fact, we can go one step further and illustrate $\\Pr(|\\bar X-\\mu| > \\varepsilon)$ by looking at the proportion of times that $|\\bar X-\\mu| > \\varepsilon$ holds across out 10,000 simulations. We'll do this for $\\varepsilon=0.5,0.6,\\ldots,1$.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot28.3\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"For each ε, the observed probability in question approaches 0 as the sample size grows.\"\n#| code-summary: \"Show code which generates figure\"\n#| warning: false\n\nresults %>% \n  expand_grid(epsilon = 5:10/10) %>% \n  mutate(outside = estimate >= 5 + epsilon | estimate <= 5 - epsilon) %>% \n  group_by(sample_size, epsilon) %>% \n  summarize(prob = sum(outside) / n()) %>%\n  ggplot(aes(x = sample_size, prob, color = factor(epsilon))) +\n  geom_line() +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(y = \"Observed Pr(|Estimate-μ|>ε)\", x = \"Sample Size n\", color = \"ε\")\n```\n\n:::\n\n\n:::{#exm-}\n\n## Monte Carlo Simulations\n\nIn @exm-var we performed a Monte Carlo simulation to illustrate the bias\nof $\\hat\\theta(\\mathbf{X}) = \\sum_{i=1}^n (X_i - \\bar X)/n$ and\nunbiasedness of $S^2$. We did this by fixing $n=20$, drawing a random\nsample, recording estimates $\\hat\\theta(\\mathbf{x})$ and\n$S^2(\\mathbf{x})$, and repeating this $k$ times. This is nothing more\nthan drawing $j=1,\\ldots,k$ observations from the random variables\n$\\hat\\theta(\\mathbf{X})$ and $S^2(\\mathbf{X})$. By the LLN,\n\\begin{align*}\n\\frac{1}{k}\\sum_{i=1}^k \\hat\\theta_j(\\mathbf{x}) &\\overset{p}{\\to}\\text{E}\\left[\\hat\\theta(\\mathbf{X})\\right],\\\\\n\\frac{1}{k}\\sum_{i=1}^k S_j^2(\\mathbf{x}) &\\overset{p}{\\to}\\text{E}\\left[S^2(\\mathbf{X})\\right],\n\\end{align*} so for a large enough $k$, we can approximate the expected\nvalue with its sample counterpart.\n\n:::\n\n### Kolmogorov's Strong Law of Large Numbers\n\nA stronger version of the LLN is stated in terms of almost sure convergence. Since almost sure convergence is stronger than convergence in probability (which is all that is needed for an estimator to be consistent), this version is referred to as the \"strong\" LLN. \n\n::: {#thm-}\n## LLN II\n\nIf $(X_1,\\ldots, X_n)$ are a set of iid random variables where\n$\\text{E}\\left[X_i\\right] = \\mu$, then $\\bar X \\overset{as}{\\to}\\mu$.\n:::\n\n::: proof\nSee the proof of Theorem 6.1 in @billingsley2008probability.\n::: \n\n\n:::{#exm-strongvsweak}\n## Strong vs Weak LLN\nWe can actually visualize the difference between the strong and weak LLNs, and in doing so highlight the difference between almost sure convergence and convergence in probability. Like @exm-lln1, assume $X_i \\sim\\text{Exp}(1/\\mu)$ where $\\mu = 5$. We'll perform a similar simulation to that which gave @fig-plot28.2, but this time we'll only look at one sample of $X_i$ (instead of 10,000). \n\n```{r}\n#| code-fold: true\n#| label: fig-plot28.4\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"A single simulation realization of the sample mean over a range of sample sizes. The area within the red dashed lines is the set |μ - Sample Mean| < 0.5\"\n#| code-summary: \"Show code which generates figure\"\n#| warning: false\n\niter(1e5, 1, rexp, list(1/5)) %>% \n  ggplot(aes(sample_size, estimate)) + \n  geom_line() + \n  geom_hline(yintercept = 5 - epsilon, color = \"red\", linetype=\"dashed\") +\n  geom_hline(yintercept = 5 + epsilon, color = \"red\", linetype=\"dashed\") +\n  theme_minimal() \n```\n\nThe strong LLN tells us that for some finite sample size $N$ the line in @fig-plot28.4 will fall within the red lines for all $n > N$.\n\n:::\n\n### Chebyshev's Weak Law of Large Numbers\n\nThe crucial assumption made by both LLNs up to this point is that $\\bar X$ is calculated\nwith an iid random sample. If we drop this assumption, then $\\bar X$\nneedn't estimate $\\mu$ consistently.\n\n::: {#exm-noiid}\n## LLN Failing with Non-IID Data\n\nSuppose $\\mathbf{X}= (X_1, \\ldots, X_n)$ where $X_i \\sim N(-1, i)$ if\n$i$ is odd, and $X_i \\sim N(1,i)$ is $i$ is even. The data is\nindependent, but not identically distributed. If some LLN would hold\nhere, we would suspect that $\\bar X$ would converge $0$ since the average of\nthe underlying population means $1$ and $-1$. Let's simulate $\\bar X$ for\n$n$ ranging from 1 to 100,000.\n\n```{r}\ndraw_X_i <- function(i){\n  mu_i <- (-1)^(i %% 2) * (1)^(1 - i %% 2)\n  sigma_i <- i \n  \n  output <- rnorm(1, mu_i, sigma_i)\n  return(output)\n}\n\ndraw_X <- function(n){\n  output <- tibble(\n    i = 1:n,\n    value = map_dbl(1:n, draw_X_i)\n  ) %>% \n    mutate(estimate = cummean(value))\n  return(output)\n}\n\nresults <- draw_X(1e5)\n```\n\nWe see that our estimates very much do not converge to any particular\nvalue.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot29\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"The sample mean of non-IID data does not satisfy the LLN\"\n#| code-summary: \"Show code which generates figure\"\nresults %>% \n  ggplot(aes(i, estimate)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Sample Size\", y = \"Sample Mean\")\n```\n:::\n\n       The proof of the LLN relied on Chebyshev's equality and the fact that\n$\\text{Var}\\left(\\bar X\\right) = \\sigma^2/n \\to 0$ when\n$\\text{Var}\\left(X_i\\right) = \\sigma^2$. Perhaps if we added an\nassumption regarding the variance of a non-iid sample, then we could\nsalvage a result similar to the LLN. This is precisely what Chebyshev's\nLLN does.\n\n:::{#prp-chebylln}\n\n## Chebyshev's (Weak) Law of Large Numbers\n\nSuppose $(X_1,\\ldots, X_n)$ are a sample such that\n$\\text{E}\\left[X_i\\right] = \\mu_i$,\n$\\text{Cov}\\left(X_i, X_j\\right) = \\sigma_{ij}^2$, and\n$\\lim_{n\\to\\infty}\\frac{1}{n^2}\\sum_{i=1}^n \\sigma_{ij}^2 =0$. If\n$\\frac{1}{n}\\sum_{i=1}^n\\mu_i \\overset{p}{\\to}\\mu$, then\n$\\bar X \\overset{p}{\\to}\\mu$.\n\n:::\n\n::: proof\nThe expected value of $\\bar X$ is\n$$\\text{E}\\left[\\bar X\\right] = \\frac{1}{n}\\sum_{i=1}^n\\text{E}\\left[X_i\\right]= \\frac{1}{n}\\sum_{i=1}^n\\mu_i \\overset{p}{\\to}\\mu.$$\nThe variance is\n$$ \\text{Var}\\left(\\bar X\\right) = \\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right) = \\frac{1}{n^2}\\sum_{i=1}^n\\sum_{j=1}^n\\text{Cov}\\left(X_i,X_j\\right) = \\frac{1}{n}\\sum_{i=1}^n \\sigma_{ij}^2\\to 0.$$\nBy Proposition @prp-mse3, $\\bar X\\overset{ms}{\\to}\\mu$, so\n$\\bar X \\overset{p}{\\to}\\mu$.\n:::\n\n:::{#cor-}\nSuppose $(X_1,\\ldots, X_n)$ are an independent sample such that\n$\\text{E}\\left[X_i\\right] = \\mu_i$,\n$\\text{Var}\\left(X_i\\right) = \\sigma_{i}^2$, and\n$\\lim_{n\\to\\infty}\\frac{1}{n^2}\\sum_{i=1}^n \\sigma_i^2 =0$. If\n$\\frac{1}{n}\\sum_{i=1}^n\\mu_i \\overset{p}{\\to}\\mu$, then\n$\\bar X \\overset{p}{\\to}\\mu$.\n:::\n\n::: proof\nIf the sample is independent, then\n$$\\frac{1}{n^2}\\sum_{i=1}^n\\sum_{j=1}^n\\text{Cov}\\left(X_i,X_j\\right) =\\frac{1}{n^2}\\sum_{i=1}^n\\text{Var}\\left(X_i\\right). $$\n:::\n\nThe reason our non-iid sample in Example @exm-noiid did not converge was\nbecause\n$$ \\frac{1}{n^2}\\sum_{i=1}^n \\sigma_i^2 = \\frac{1}{n^2}\\sum_{i=1}^n i = \\frac{1}{n^2}\\frac{n(n+1)}{2} = \\frac{n^2 + n}{2n^2} \\to \\frac{1}{2} \\neq 0.$$\nLet's modify it slightly so the sum of the variances does converge to\nzero.\n\n:::{#exm-}\n\n## LLN with Non-IID Data\n\nSuppose $\\mathbf{X}= (X_1, \\ldots, X_n)$ where $X_i \\sim N(-1,i^{-1})$\nif $i$ is odd, and $X_i \\sim N(1,i^{-1})$ is $i$ is even. Now we have\n$$\\lim_{n\\to\\infty}\\frac{1}{n^2}\\sum_{i=1}^n \\sigma_i^2 = \\left[\\lim_{n\\to\\infty}\\frac{1}{n^2}\\right]\\sum_{i=1}^\\infty i^{-1} \\to 0 .$$\n\n```{r}\ndraw_X_i <- function(i){\n  mu_i <- (-1)^(i %% 2) * (1)^(1 - i %% 2)\n  sigma_i <- 1/i \n  \n  output <- rnorm(1, mu_i, sigma_i)\n  return(output)\n}\n\ndraw_X <- function(n){\n  output <- tibble(\n    i = 1:n,\n    value = map_dbl(1:n, draw_X_i)\n  ) %>% \n    mutate(estimate = cummean(value))\n  return(output)\n}\n\nresults <- draw_X(150)\n```\n\nNow we see that $\\bar X$ is converging to $0$, and doing so rather\nquickly.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot210\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"Despite the data not being IID, the sample mean still converges to the population mean because the variance shrinks in the correct way.\"\n#| code-summary: \"Show code which generates figure\"\nresults %>% \n  ggplot(aes(i, estimate)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Sample Size\", y = \"Sample Mean\")\n```\n\n:::\n\n## The Continuous Mapping Theorem and Slutsky's Theorem\n\nAt first glance, the LLN may not seem especially useful as it only\napplies to the sample mean. However, when paired with two key results\nabout convergence, the LLN becomes an indispensable tool to analyze the\nconvergence of many random variables and estimators. The first of these\nis an extension of a key result in real analysis. A useful, and defining\nproperty, of continuous functions is that they preserve limits of\nnumeric sequences. If $\\{a_n\\}$ is a numeric sequence, then\n$$\\lim_{n \\to\\infty} f(a_n) = f\\left(\\lim_{n\\to\\infty} a_n\\right) \\iff f\\text{ continuous}.$$\n\n:::{#thm-}\n\n## Continuous Mapping Theorem I\n\nSuppose $X_n \\overset{p}{\\to}X$, and let $g$ be a continuous function.\nThen $$g(X_n)\\overset{p}{\\to}g(X).$$ In other words we are able to\ninterchange the $\\mathop{\\mathrm{plim}}$ operator with a continuous\nfunction:\n$$\\mathop{\\mathrm{plim}}g(X_n) = g\\left(\\mathop{\\mathrm{plim}}X_n\\right).$$\n\n:::\n\nThe proof of this result can be found in @van2000asymptotic. An\nimmediate corollary follows from the fact that convergence in\nprobability implies convergence in distribution.\n\n::: {#cor-}\n## Continuous Mapping Theorem II\n\nSuppose $X_n \\overset{p}{\\to}X$, and let $g$ be a continuous function.\nThen $$g(X_n)\\overset{d}{\\to}g(X)$$\n:::\n\nAn equally useful result involves the limit of a sums and products of\nconvergent random variables.\n\n:::{#thm-}\n\n## Slusky's Theorem\n\nLet $X_n$ and $Y_n$ be sequences of random variables. If\n$X_n\\overset{d}{\\to}X$ for some random variable $X$, and\n$Y_n\\overset{p}{\\to}c$ for some constant $c$, then \\begin{align*}\nX_n + Y_n &\\overset{d}{\\to}X + c\\\\\nX_nY_n & \\overset{d}{\\to}Xc.\n\\end{align*} Furthermore, if $c\\neq 0$,\n$$ X_n/Y_n \\overset{d}{\\to}X/c.$$\n\n:::\n\n::: proof\nDefine a random vector to be $\\mathbf Z_n = (X_n,Y_n)$. We have\n$\\mathbf Z_n \\overset{d}{\\to}(X,c)$ as $X_n\\overset{d}{\\to}X$ and\n$Y_n \\overset{d}{\\to}c$ (convergence in probability implies convergence\nin distribution). We can apply the continuous mapping theorem to\n$g(x,y) = x + y$, $g(x,y)=xy$, and $g(x/y)$ to establish the result.\n:::\n\n       Slutsky's theorem can be a bit hard to remember because it involves a\nsequence of random variable which converges in distribution to a random\nvariable, and a sequence of random variables which converges in\nprobability to a constant. These asymmetries in mode of convergence and\nthe type of limit are essential, otherwise the result will not hold.\nFortunately, the result does hold if we replace all convergences in\ndistribution with convergence in probability (as the later implies the\nprior).\n\n:::{#exm-}\n\nSuppose $X_n \\sim\\text{Uni}(0,1)$ and $Y_n = - X_n$. We have\n$X_n \\overset{d}{\\to}\\text{Uni}(0,1)$ and\n$Y_n \\overset{d}{\\to}\\text{Uni}(-1,0)$. Despite this\n$X_n + Y_n = 0 \\not\\overset{d}{\\to}\\text{Uni}(0,1) + \\text{Uni}(-1,0).$\n\n:::\n\n::: {#exm-convar}\n## Consistency of Sample Variance\n\n@exm-consvarnorm showed that $S^2$ is a consistent estimator for\n$\\sigma^2$ when $X_i\\overset{iid}{\\sim}N(\\mu,\\sigma^2)$. We can use the\ncontinuous mapping theorem, Slutsky's theorem, and the LLN to show that\n$S^2$ is consistent regardless of the distribution of our iid sample.\nSuppose $\\text{E}\\left[X_i\\right] = \\mu$,\n$\\text{E}\\left[X_i^2\\right]=\\mu_2$, and\n$\\text{E}\\left[X_i^4\\right]=\\mu_4$ for all $i$. If we define our\ncontinuous function to be $g(x) = x^2$, then \\begin{align*}\nS^2 & = \\frac{1}{n-1} \\sum_{i=1}^nX_i - \\frac{1}{n-1} \\sum_{i=1}^n\\bar X^2 \\\\\n    & = \\frac{1}{n-1} \\sum_{i=1}^nX_i^2 + \\frac{n}{n-1}\\bar X^2  \\\\\n    & = \\frac{n}{n-1}\\left[\\frac{1}{n}\\sum_{i=1}^{n}X_i^2 - \\left(\\frac{1}{n}\\sum_{i=1}^{n}X_i\\right)^2\\right]\n\\end{align*} The first term in the brackets is an unbiased estimator of\n$\\mu_2$ with vanishing variance, so by @prp-consbias it is a consistent\nestimator for $\\text{E}\\left[X^2\\right]$: \\begin{align*}\n\\text{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}X_i^2\\right] &= \\frac{1}{n}\\left(n \\mu_2\\right) = \\mu^2\\\\\n\\lim_{n\\to\\infty}\\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_i^2\\right) & = \\lim_{n\\to\\infty}\\frac{1}{n^2}n\\left(\\text{E}\\left[X_i^4\\right] - \\text{E}\\left[X_i^2\\right]^2 \\right) = \\lim_{n\\to\\infty}\\frac{\\mu_4 + \\mu_2^2}{n} = 0\n\\end{align*} The second term in the brackets can be written as\n$g(\\bar X)$, so by the continuous mapping theorem and the LLN,\n$$ g(\\bar X) = \\left(\\frac{1}{n}\\sum_{i=1}^{n}X_i\\right)^2 \\overset{p}{\\to}\\mu^2 = g(\\mu).$$\nSo\n$$S^2= \\underbrace{\\frac{n}{n-1}}_{\\to 1}\\Bigg[\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}X_i^2}_{\\overset{p}{\\to}\\mu_2} - \\underbrace{\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_i\\right)^2}_{\\overset{p}{\\to}\\mu^2} \\Bigg].$$\nWe can apply Slutsky's theorem to the sum of sequences of random\nvariables which converge in probability to constants, so\n$$ S^2 \\overset{p}{\\to}\\mu_2 - \\mu^2 = \\sigma^2,$$ making $S^2$\nconsistent.\n:::\n\n## Central Limit Theorems\n\nThe LLN told us that our favorite estimator for $\\mu$, $\\bar X$, is\nconsistent. We know turn to what is perhaps an even more important\nresult regarding $\\bar X$, one that may in fact be the most important\nresults in all of probability -- the asymptotic distribution of $\\bar X$ is a normal distribution.\n\n### Lindeberg-Lévy CLT\n\nThe *classic* version of the CLT is formally known as the Lindeberg-Lévy CLT, and is likely familiar.\n\n:::{#thm-}\n\n## CLT I\n\nSuppose $\\mathbf{X}=(X_1,\\ldots, X_n)$ is a sequence of random variables\nwith $\\text{E}\\left[X_i\\right]=\\mu$ and\n$\\text{Var}\\left(X_i\\right)=\\sigma^2$. Then\n$$\\sqrt{n}(\\bar X - \\mu) \\overset{d}{\\to}N(0,\\sigma^2),$$ which is also\noften written as $$\\bar X\\overset{d}{\\to}N(\\mu, \\sigma^2/n),$$ or\n$$\\sum_{i=1}^n X_i \\overset{d}{\\to}N(n\\mu, \\sqrt n \\sigma^2) $$\n:::\n\n       The proof is a bit technical and requires some measure-theoretic based\nprobability theory. It can be found in @billingsley2008probability or\n@durrett2019probability.\n\n:::{#exm-}\nSuppose we have an iid sample from $\\text{Exp}(1)$. If we simulate 1000\nrealizations of $\\sqrt n(\\bar X - \\mu)$ for various sample sizes, we\nshould see that the distribution of our realizations becomes\napproximately normal as we increase the sample size.\n\n```{r}\niter <- function(n, dist, dist_params, s){\n  output <- tibble(\n    value = do.call(dist, append(n, dist_params))\n  ) %>% \n    summarize(\n      sample_size = n,\n      iter_num = s, \n      estimate = sqrt(n)*(mean(value) - 1)\n    ) \n  return(output)\n}\n\nsim <- function(N, n, dist, dist_params){\n  output <- 1:N %>% \n    map(iter, n = n, dist = dist, dist_params = dist_params) %>% \n    bind_rows()\n  return(output)\n}\n\nouter_sim <- function(n_vals, N, dist, dist_params){\n  output <- n_vals %>% \n    map(sim, N = N, dist = dist, dist_params = dist_params) %>% \n    bind_rows()\n  return(output)\n}\n\nresults <- outer_sim(c(1:25, 50, (1:10)*100), 1e4, rexp, list(1))\n```\n\nEven for modest values of $n$, we can see that\n$\\sqrt n(\\bar X - \\mu) \\overset{a}{\\sim}N(0, \\sigma^2)$.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot211\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"As the sample size increases, the distribution of the adjusted sample mean looks more and more like the standard normal.\"\n#| code-summary: \"Show code which generates figure\"\n\nresults %>% \n  ggplot(aes(estimate)) +\n  geom_histogram(aes(y = after_stat(density)), color = \"black\", fill = \"white\", bins = 50) +\n  theme_minimal() +\n  geom_function(fun = dnorm, colour = \"red\") + \n  transition_states(sample_size) +\n  labs(title = 'Sample Size: {closest_state}', x = \"Estimates\", y = \"\") \n```\n\nAn alternate way to visually test whether our estimates are normally\ndistributed is with a quantile-quantile plot (QQ-plot), which graphs the\nobserved quantiles of our estimates against the theoretical quantiles of\na normal distribution (or those of any distribution we suspect our data\nis drawn from). If our estimates are (approximately) normally\ndistributed, then the observed quantiles should be approximately equal\nto the theoretical quantiles of a normal distribution, forming a\n45-degree line.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot212\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"The QQ-plot for the simulated distribution of the adjusted sample mean\"\n#| code-summary: \"Show code which generates figure\"\nresults %>% \n  ggplot(aes(sample = estimate)) +\n  stat_qq_line(color = \"red\") +\n  geom_qq(size= 0.5) + \n  theme_minimal() +\n  transition_states(sample_size) + \n  labs(title = 'Sample Size: {closest_state}', x = \"Quantiles of Simulated Estimates\", y = \"Quantiles of Normal Distribution\")\n```\n\n:::\n\n       The central limit theorem is similar to the LLN insofar that they only\nconcern the estimator $\\bar X$, so how useful can they really be? Well\nwith the continuous mapping theorem and Slutsky's theorem, the answer is\n*very useful*!\n\n:::{#exm-}\n\nIn @exm-tdist we illustrated the fact that\n$X_n \\overset{d}{\\to}N(0,1)$ where $X_n \\sim t_n$, but we didn't\nactually prove it. Directly proving this result is a matter of verify\nthat\n$$\\lim_{n\\to\\infty} F_{X_n}(x) =\\lim_{n\\to\\infty}\\int_{-\\infty}^x \\frac{\\Gamma\\left(\\frac{n+1}{2}\\right)}{\\Gamma\\left(\\frac{n}{2}\\right)\\sqrt{n\\pi}}\\left(1 + \\frac{x^2}{n}\\right)^{-\\frac{n+1}{2}} = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^x e^{-x^2/2} = F_X(x)$$\nwhere $\\Gamma$ is the gamma function defined as\n$$\\Gamma(t) = \\int_0^\\infty s^{t-1}e^{-s}\\ ds.$$ A much easier way to\nprove that $X_n \\overset{d}{\\to}N(0,1)$, is by using Slutsky's theorem\nand the continuous mapping theorem. First recall that\n$$\\frac{\\bar X - \\mu}{S/\\sqrt n} \\sim t_n,$$ so we can write $X_n$ as\n$X_n = \\frac{\\bar X - \\mu}{S/\\sqrt n}$ due to the fact that random\nvariables are uniquely determined by their distributions. From\n@exm-consvarnorm, we have $S^2 \\overset{p}{\\to}\\sigma^2$. By the continuous\nmapping theorem\n$$ \\sqrt{S^2} = S \\overset{p}{\\to}\\sigma = \\sqrt{\\sigma^2},$$ which gives\n$$ X_n = \\frac{\\bar X - \\mu}{S/\\sqrt n}= \\underbrace{\\sqrt{n}(\\bar X - \\mu)}_{\\overset{d}{\\to}N(0, \\sigma^2)}\\underbrace{\\frac{1}{s}}_{\\overset{p}{\\to}\\sigma}.$$\nPutting all the pieces together, Slutsky's theorem yields\n$$X_n \\overset{d}{\\to}\\frac{N(0,\\sigma^2)}{\\sigma} = N(0,1).$$\n\n:::\n\n       The CLT can be generalized to samples of random vectors $\\mathbf{X}_i$.\n\n::: {#thm-}\nSuppose $(\\mathbf{X}_1,\\ldots, \\mathbf{X}_n)$ is a sequence of iid\nrandom vectors with $\\text{E}\\left[\\mathbf{X}_i\\right]=\\boldsymbol\\mu$\nand $\\text{Var}\\left(\\mathbf{X}_i\\right)=\\boldsymbol\\Sigma$. Then\n$$\\sqrt{n}(\\bar {\\mathbf{X}}- \\boldsymbol\\mu) \\overset{d}{\\to}N(\\mathbf{0},\\boldsymbol\\Sigma).$$\n:::\n\n### Not Identically Distributed\n\n@prp-chebylln allowed us to salvage a LLN when the iid assumption failed, so can we do the same with the CLT? Sort of. While we need an independent sample, we don't necessarily need realizations to be drawn from an identical distribution. Instead, the theorem will rely on the tails of distributions meeting a certion criterion. This version of the CLT is known as the Lindeberg-Feller CLT.\n\n:::{#thm-}\n\n## Lindeberg-Feller CLT\n\nSuppose $\\mathbf{X}=(X_1,\\ldots, X_n)$ is a sequence of independent\nrandom variables with $\\text{E}\\left[X_i\\right]=\\mu_i$ and\n$\\text{Var}\\left(X_i\\right)=\\sigma_i^2$, and define \\begin{align*}\n\\bar \\mu = \\frac{1}{n}\\sum_{i=1}^n\\mu_i;\\\\\n\\bar \\sigma_n^2 = \\frac{1}{n}\\sum_{i=1}^n\\sigma_i^2.\n\\end{align*} If the collection of variances $\\sigma_i^2$ satisfies:\n\\begin{align*}\n\\lim_{n\\to\\infty} &\\frac{\\max\\{\\sigma_i\\}}{n\\bar\\sigma} = 0;\\\\\n\\lim_{n\\to\\infty} &\\bar\\sigma_n^2 = \\bar\\sigma^2,\n\\end{align*} then\n$\\sqrt n (\\bar X-\\mu)\\overset{d}{\\to}N(0, \\bar\\sigma^2).$\n\n:::\n\n::: {#exm-}\n\n## Lindeberg's Condition\n\nThe Lindeberg-Feller conditions stipulates that\n$\\lim_{n\\to\\infty}\\bar\\sigma_n^2 = \\bar\\sigma^2$ and\n$\\lim_{n\\to\\infty} \\frac{\\max\\{\\sigma_i\\}}{n\\bar\\sigma} = 0$, a\ncondition known as the ***Lindeberg's condition***. The condition is\noften presented in more general terms, but the intuition remains the\nsame. For the CLT to hold for random variables that with different\nvariances, we need to makes sure that no single term $\\sigma_i$\ndominates the standard deviation. We can think about the sample mean\n$\\bar X$ as \"mixing\" many random variables $X_i.$ After mixing these\nrandom variables, we hope to have a normal distribution, but that only\nhappens if tails of the various distributions of $X_i$ are negligible as\n$n\\to\\infty$, giving us the trademark tails of a normal distribution\nwhich tapper off. Let's consider a counterexample. Suppose $X_i$ is\ndefined on the sample space $\\{-i,0,i\\}$ is distributed such that\n$$\\Pr(X_i = k) = \\begin{cases} 1/2i^2 & k = -i^2 \\\\ 1/2i^2 & k = i^2 \\\\ 1 - 1/i^2&k=0\\end{cases}.$$\nGraphing the density function for a few values of $i$ gives us a better\nsense of how $X_i$ behaves.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot213\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"Density of X_i for i = 1,...,5.\"\n#| code-summary: \"Show code which generates figure\"\nexpand_grid(i = 1:5, x = -100:100) %>% \n  filter(x == i^2 | x == -i^2| x== 0) %>% \n  mutate(y = ifelse(x == 0, 1 - 1/(i^2), 1/(2*i^2))) %>% \n  ggplot(aes(x, y)) +\n  geom_point(size = 0.2) +\n  xlab(\"k\") +\n  geom_segment(aes(x = x, xend = x, y= 0, yend = y)) +\n  theme_minimal() + \n  transition_states(i) + \n  labs(title = 'i = {closest_state}', y = \"Pr(X_i = k)\")\n```\n\nAs $i\\to\\infty$, nearly all the probability is concentrated as $k = 0$.\nThe remaining probability is at the extreme tails of the distribution\n$\\pm i^2$, and these tails become more and more extreme (quadritically\nso) as $i\\to\\infty$. This is the exact type of behavior Lindeberg's\ncondition rules out. The expectation, expectation squared, and variance\nof $X_i$ are: \\begin{align*}\n\\text{E}\\left[X_i\\right] & = (1/2i^2)(-i^2) + (1/2i^2)(i^2) +  (1 - 1/i^2)(0) = 0\\\\\n\\text{E}\\left[X_i^2\\right] & = (1/2i^2)(i^4) + (1/2i^2)(i^4) +  (1 - 1/i^2)(0) = i^2\\\\\n\\text{Var}\\left(X_i\\right) & = i^2 - 0^2 = i^2\n\\end{align*}\n\nWe can verify that Lindeberg's condition does not hold. \\begin{align*}\n\\lim_{n\\to\\infty}\\bar \\sigma_n = \\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{i=1}^n i^2 = \\lim_{n\\to\\infty}\\frac{(n+1)(2n+1)}{6} \\to\\infty\n\\end{align*}\n\nTo simulate realizations of this random variable, we can define $X_i$\nusing $U_i\\sim\\text{Uni}(0,1)$.[^asymptotics-1]\n\n$$X_i = \\begin{cases}-i^2 & U_i\\in[0, 1/2i^2)\\\\ i^2 & U_i\\in[1/2i^2, 1/i^2) \\\\ 0 & U_i\\in [1/i^2,1]\\end{cases}$$\n\n```{r}\ndraw_X <- function(n,...){\n  U <- runif(n)\n  prob_pos_i <- (U < 1/(2*(1:n)^2))\n  prob_neg_i <- (U > 1/(2*(1:n)^2) & U < 1/((1:n)^2))\n  (1:n)^2*prob_pos_i - (1:n)^2*prob_neg_i\n}\n\nresults <- c(1, 5, 10, 25, 100, 1000) %>% \n  outer_sim(., 200, draw_X, list(0))\n```\n\n```{r}\n#| code-fold: true\n#| label: fig-plot214\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"Histograms of estimates as sample size increases.\"\n#| code-summary: \"Show code which generates figure\"\n\nresults %>% \n  ggplot(aes(estimate)) +\n  geom_histogram(color = \"black\", fill = \"white\", bins = 50) +\n  facet_wrap(~sample_size, scales = \"free\") +\n  theme_minimal() +\n  labs(x = \"Estimates\", y = \"\")\n```\n\nThe apparent distribution of our estimates is not converging to a normal\ndistribution, as we always have a few outliers that are too plentiful\nrelative to their distance from the mean to be drawn from a normal\ndistribution. As $n\\to\\infty$ these outliers become even more extreme.\nThis is also evident from QQ-plots, where the points far away from the\n45-degree line are drawn even farther away as $n\\to\\infty$.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot215\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"The QQ-plot for the simulated distribution of the adjusted sample mean\"\n#| code-summary: \"Show code which generates figure\"\nresults %>% \n  ggplot(aes(sample = estimate)) +\n  stat_qq_line(color = \"red\") +\n  geom_qq(size= 0.5) + \n  facet_wrap(~sample_size) + \n  theme_minimal() +\n  labs(x = \"Quantiles of Simulated Estimates\", y = \"Quantiles of Normal Distribution\")\n```\n:::\n\n[^asymptotics-1]: We can actually draw observations of *any* random\n    variable using the uniform distribution on $[0,1]$ via [***inverse\n    transform\n    sampling***](https://en.wikipedia.org/wiki/Inverse_transform_sampling#Reduction_of_the_number_of_inversions).\n    This is one of the primary ways computers generate random\n    observations from a given distribution.\n\n       While theoretically important, Lindeberg's condition can be a bit hard\nto verify. It is much more common to appeal to a stronger assumption\nwhich gives rise to a second CLT that holds for independent, but not\nnecessarily identically distributed, random variables. This final CLT\nmay not be as general as the Lindeberg-Feller CLT, but it is much easier\nto work with.\n\n:::{#thm-}\n\n## Lyapunov CLT\n\nSuppose $\\mathbf{X}=(X_1,\\ldots, X_n)$ is a sequence of independent\nrandom variables with $\\text{E}\\left[X_i\\right]=\\mu_i$ and\n$\\text{Var}\\left(X_i\\right)=\\sigma_i^2$. If\n$\\text{E}\\left[\\left\\lvert X_i - \\mu_i\\right\\rvert^{2+\\kappa}\\right]$ is\nfinite for some $\\kappa > 0,$ then\n$\\sqrt n (\\bar X-\\mu)\\overset{d}{\\to}N(0, \\bar\\sigma^2)$, where\n$\\bar\\sigma = (1/n)\\sum_{i=1}^n \\sigma_i.$\n\n:::\n\n## Delta Method\n\nSlutsky's theorem and the continuous mapping theorem in tandem with the\nLLN give us the ability to prove that certain functions of sample means\nare convergent. Is it possible that we can do something similar with the\nCLT to find the asymptotic distribution of functions of sample means?\n\n       Suppose $g$ is a function of $\\bar X$, where a CLT applies to the random\nsample $\\mathbf{X}$. We know\n$\\sqrt n(\\bar X-\\mu)\\overset{a}{\\sim}N(0, \\sigma^2)$. Is it possible to\nconclude that\n$\\sqrt n(g(\\bar X)-g(\\mu))\\overset{a}{\\sim}N(0, \\tilde\\sigma^2)$ for\nsome $\\tilde\\sigma^2$? Furthermore, can we determine $\\tilde\\mu$ and\n$\\tilde\\sigma^2$ only knowing $g$, $\\mu$, and $\\sigma^2$?\n\n       We have information about the difference $\\bar X-\\mu$ and want\ninformation about the difference $g(\\bar X)-g(\\mu)$. Situations where we\nknow something about behavior in the domain of a function and want to\nrelate it to the function's behavior in the codomain are common place in\nmath, but in particular in real analysis. This is where the mean value\ntheorem saves the day. Assuming $g$ is continuously differentiable and\nfixing $n$, there exists some $T_n$ in between $\\bar X$ and $\\theta$\n($\\bar X< T_n < \\mu$ or $\\mu < T_n < \\bar X$) such that: \\begin{align*}\n& \\frac{g(\\bar X)-g(\\mu)}{\\bar X - \\mu} = g'(T_n)\\\\\n\\implies & g(\\bar X) = g(\\mu) + g'(T_n)(\\bar X-\\mu)\\\\\n\\implies & \\sqrt{n}[g(\\bar X) - g(\\mu)] = g'(T_n)\\sqrt{n} (\\bar X-\\mu)\n\\end{align*} If we let $n$ vary, we have a sequence of random variables\n$\\{T_n\\}$ such that\n$\\left\\lvert T_n -\\mu\\right\\rvert < \\left\\lvert\\bar X - \\mu\\right\\rvert$.\nBy the LLN $\\left\\lvert\\bar X - \\mu\\right\\rvert \\overset{p}{\\to}0$, so\n$\\left\\lvert T_n -\\mu\\right\\rvert \\overset{p}{\\to}0$, which is\nequivalent to $T_n \\overset{p}{\\to}\\mu$. By the continuous mapping\ntheorem, $g(T_n) \\overset{p}{\\to}g(\\mu)$. This means\n$$\\sqrt{n}[g(\\bar X) - g(\\mu)] = \\underbrace{g'(T_n)}_{\\overset{p}{\\to}g(\\mu)}\\cdot\\underbrace{\\sqrt{n} (\\bar X-\\mu)}_{\\overset{d}{\\to}N(0, \\sigma^2)},$$\nso Slutsky's theorem gives\n$$\\sqrt{n}[g(\\bar X) - g(\\mu)] \\overset{d}{\\to}g(\\mu)\\cdot N(0,\\sigma^2) = N(0, \\sigma^2[g(\\mu)]^2).$$\nThis all is contingent on $g$ not being a function of $n$, otherwise\nthings fall apart when we apply limiting processes.\n\n       This result is known as the delta method, and applies to any sequence of\nrandom variables that is asymptotically normal. It is also readily\ngeneralized to higher dimensions where $\\mathbf g$ is a vector valued\nfunction.\n\n:::{#thm-}\n\n## Delta Method\n\nSuppose $(\\mathbf{X}_1,\\ldots, \\mathbf{X}_n)$ is a sequence of random\nvectors such that\n$\\sqrt n (\\mathbf{X}_n - \\mathbf t) \\overset{d}{\\to}N(\\mathbf 0, \\boldsymbol\\Sigma)$\nfor some vector $\\mathbf t$ in the interior of $\\mathcal X$. If\n$\\mathbf g(\\mathbf{X}_n)$ is a vector valued function that:\n\n1.  is continuously differentiable,\n2.  does not involve $n$,\n3.  $\\frac{\\partial \\mathbf g}{\\partial\\mathbf x}(\\mathbf t) \\neq 0$,[^asymptotics-2]\n\nthen,\n\n$$ \\sqrt n \\left[\\mathbf g(\\mathbf{X}_n) - \\mathbf g(\\mathbf t)\\right] \\overset{d}{\\to}N\\left(\\mathbf 0, \\left[\\frac{\\partial \\mathbf g}{\\partial\\mathbf x}(\\mathbf t)\\right]\\boldsymbol\\Sigma\\left[\\frac{\\partial \\mathbf g}{\\partial\\mathbf x}(\\mathbf t)\\right]'\\right)$$\n\n:::\n\n[^asymptotics-2]: $\\frac{\\partial \\mathbf g}{\\partial\\mathbf x}(\\mathbf t)$\n    is the $\\dim(\\mathbf g(\\mathbf{X}_n)) \\times \\dim(\\mathbf X_n)$\n    Jacobian matrix comprised of the partial derivatives of the\n    components of $\\mathbf g$.\n\n:::{#exm-}\n\nReturn to the example where $X_i\\overset{iid}{\\sim}\\text{Exp}(1)$,\ngiving $\\text{E}\\left[X_i\\right] = 1$ and\n$\\text{Var}\\left(X_i\\right) = 1$. By the CLT,\n$\\sqrt n(\\bar X - 1)\\overset{d}{\\to}N(0,1)$. If $g(t) = t^2 +3$, what is\nthe asymptotic distribution of $\\sqrt{n}[g(\\bar X) - g(1)]$? According\nto the delta method we have \\begin{align*}\n&\\sqrt{n}[g(\\bar X) - g(1)]  \\overset{a}{\\sim}N(0, 1[g'(1)]^2),\\\\\n\\implies & \\sqrt{n}\\left[\\bar X^2 - 1\\right] \\overset{a}{\\sim}N(0, 4).\n\\end{align*}\n\n```{r}\ng <- function(t){\n  t^2 + 3\n}\n\niter <- function(g, n, dist, dist_params, s){\n  output <- tibble(\n    value = do.call(dist, append(n, dist_params))\n  ) %>% \n    summarize(\n      sample_size = n,\n      iter_num = s, \n      estimate = sqrt(n)*(g(mean(value)) - g(1))\n    ) \n  return(output)\n}\n\nsim <- function(N, g, n, dist, dist_params){\n  output <- 1:N %>% \n    map(iter, g = g, n = n, dist = dist, dist_params = dist_params) %>% \n    bind_rows()\n}\n\nresults <- sim(1e4, g, 1e5, rexp, list(1))\n```\n\nWe can plot a histogram of our estimates and overlay the distribution\n$N(0,4)$.\n\n```{r}\n#| code-fold: true\n#| label: fig-plot216\n#| fig-align: center\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"Histogram of adjusted sample mean transformed by g, and theoretical distribution given by the delta method\"\n#| code-summary: \"Show code which generates figure\"\n\nresults %>% \n  ggplot(aes(estimate)) +\n  geom_histogram(aes(y = ..density..), colour = 1, fill = \"white\", bins = 100) + \n  xlab(\"Estimates of √n(g(X) - g(1)) \") +\n  theme_minimal() +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 2), color = \"red\")\n```\n\n:::\n\n       The big takeaway from the delta method is that estimators which are nice\nfunctions of sample means will be asymptotically distributed according\nto a normal distribution. Furthermore, any nice function of such an\nestimator will also have a normal asymptotic distribution!\n\n```{r}\n#| fig-align: center\n#| label: fig-plot217\n#| fig-asp: 0.7\n#| fig-width: 8\n#| fig-cap: \"A mediocre meme\"\n#| echo: false\n\nknitr::include_graphics(\"figures/meme.png\")\n```\n\n## Little $o_p$, Big $O_p$, and Taylor Expansions\n\nWe've talked a lot about whether or not random variables converge, and\nhow they converge, but not the rate at which they converge. We can\nintroduce some notation that allows us to quantify this rate.\n\n:::{#def-}\n\nGiven a sequence of random variables $X_n$, we say $X_n$ is [***little\n\"O.P\" of*** $n^k$]{style=\"color:red\"}, denoted $X_n = o_p(n^k)$, if\n$X_n / n^k\\overset{p}{\\to}0$.\n\n:::\n\n       Note that $X_n\\overset{p}{\\to}0$ implies that $X_n = o_p(1)$. The use of\n\"=\" is a bit misleading in this definition, as $X_n = o_p(n^k)$ does not\nestablish any equality, instead referring to how $X_n$ behaves\nasymptotically. For instance, if we have two sequences of random\nvariables $X_n$ and $Y_n$ such that $X_n\\overset{p}{\\to}X$ and\n$Y_n\\overset{p}{\\to}0$, we have \\$ X_n + Y_n \\overset{p}{\\to} X + 0 \\$,\nbut could write $X_n + Y_n$ as $X_n + o_p(1)$. This emphasizes the\nsequence $X_n$, and frames $Y_n$ as some negligible remainder term that\ntends to zero.\n\n:::{#def-}\n\nGiven a sequence of random variables $X_n$, we say $X_n$ is [***big\n\"O.P\" of*** $n^k$]{style=\"color:red\"}, denoted $X_n = O_p(n^k)$, if for\nall $\\varepsilon > 0$, there exists some $\\delta$ and $N$ such that\n$\\Pr(|X_n/n^k| \\ge \\delta) <\\varepsilon$ for all $n > N$. In other\nwords, $X_n/n^k$ is [***bounded in probability***]{style=\"color:red\"}.\n\n:::\n\n       We are most interested in the case where $X_n = O_p(1)$. If this is the\ncase, then as $n\\to\\infty$, we can bound the area in the tails of\n$f_{X_n}$ by some constant $\\delta$ such that the area is negligible\n(less than $\\varepsilon$).\n\n:::{#exm-}\n\nWe know that $\\bar X \\overset{d}{\\to}N(\\mu, \\sigma^2/n)$ when\n$\\mathbf{X}$ is an iid sample. We have that $\\bar X = O_p(1)$. We have\n$$\\Pr(|\\bar X/1| \\ge \\delta) = \\Pr(-\\bar X\\ge -\\delta \\text{ and }\\delta \\le \\bar X) = 2\\cdot\\Pr(\\bar X\\ge \\delta) = 2\\left[1 - \\Phi\\left(\\frac{\\delta - \\mu}{\\sigma/\\sqrt n}\\right)\\right].$$\nIf we take the limit of this as $n\\to \\infty$ we have\n$$ \\lim_{n\\to \\infty}\\Pr(|X\\bar X/1| \\ge \\delta) = \\lim_{n\\to \\infty}2\\left[1 - \\Phi\\left(\\frac{\\delta - \\mu}{\\sigma/\\sqrt n}\\right)\\right] = 0.$$\nBy the definition of a limit, there must exists some $N$ such that\n$\\Pr(|\\bar X/1| \\ge \\delta) < \\varepsilon$ for any $n > N$, so\n$\\bar X = O_p(1)$.\n\n:::\n\n:::{#exm-}\n\nIf $X_n \\overset{d}{\\to}X$, then $X_n = O_p(1)$.\n\n:::\n\n:::{#exm-}\n\nIf $X_n = o_p(1)$, then $X_n = O_p(1)$.\n\n:::\n\n       A common place to encounter $o_p$ is when performing Taylor expansions.\n\n:::{#exm-}\n\n## Taylor's Theorem\n\nTaylor's theorem, as given in @rudin1976principles, tells us that if\n$f:\\mathbb R\\to\\mathbb R$ is $k-$times differentiable at a point\n$a\\in \\mathbb R$, then there is some element $c\\in (a,b)$ such that\n\\begin{align*}\nf(b) & = \\sum_{j=0}^{k-1}\\frac{f^{(j)}(a)}{k!}(b-a)^j + \\frac{f^{(n)}(c)}{k!}(b-a)^k.\n\\end{align*} For $n = 2$, we have the mean value theorem:\n$$ f(b)= f(a) + f'(c)(b-a).$$ If we let $a\\to b$, then \\begin{align*}\n&\\lim_{a\\to b}f(b)  = \\sum_{j=0}^{k-1}\\lim_{a\\to b}\\frac{f^{(j)}(a)}{j!}(b-a)^j + \\lim_{a\\to b}\\frac{f^{(k)}(c)}{k!}(b-a)^k.\\\\\n\\implies & f(b)  =\\lim_{a\\to b} f(a) + \\sum_{j=1}^{k-1}\\lim_{a\\to b}\\frac{f^{(j)}(a)}{j!}(b-a)^j +  \\lim_{a\\to b}\\frac{f^{(k)}(c)}{k!}(b-a)^k\\\\\n\\implies & f(b)  = f(b) + \\underbrace{\\sum_{j=1}^{k-1}\\frac{f^{(j)}(b)}{j!}(b-b)^j}_0 + \\lim_{a\\to b}\\frac{f^{(k)}(c)}{k!}(b-a)^k\\\\\n\\implies & \\lim_{a\\to b}\\frac{f^{(k)}(c)}{n!}(b-a)^k = 0\\\\\n\\implies & \\lim_{a\\to b}\\frac{f^{(k)}(c)}{n!(b-a)^k} = 0\\\\\n\\implies & \\frac{f^{(k)}(c)}{k!} = o(|a-b|^k)\\\\\n\\end{align*} Here, $o$ is the deterministic counterpart of $o_p$ (we're\nnot working with random variables just yet). This means we can write\nTaylor's theorem as\n$$ f(b) = \\sum_{j=0}^{k-1}\\frac{f^{(j)}(a)}{j!}(b-a)^k +o(|a-b|^k).$$ In\nthe event $f$ is infinitely differentiable we can make this\napproximation arbitrarily accurate, giving rise toa function's Taylor\nseries.\n\nNow suppose $f_n(X)$ is a sequence of functions of random variables,\nwhere the subscript $n$ emphasizes that $f_n$ is a random variable. IF\nwe apply Taylor's theorem to $f_n(X)$ we have\n$$f_n(b) = \\sum_{j=0}^{k-1}\\frac{f_n^{(j)}(a)}{j!}(b-a)^k +o_p(|a-b|^k)$$\nfor realizations of the random variable $a,b,c\\in\\mathcal X$ where\n$c\\in(a,b)$. Assuming $k \\ge 1$, then $o_p(|a-b|^k)$ implies $o_p(1)$,\nso $$f_n(b) = \\sum_{j=0}^{k-1}\\frac{f_n^{(j)}(a)}{j!}(b-a)^k +o_p(1).$$\n\n:::\n\n## Asymptotically Normal Estimators\n\nWhen putting our asymptotic tools to work on an estimator of interest,\nwe will almost always find that it converges to a normal distribution,\nis consistent, and that the rate of convergence is linked to $\\sqrt{n}$.\n\n:::{#def-}\n\nAn estimator $\\hat{\\boldsymbol{\\theta}}$ is [$\\sqrt{n}-$consistent\nasymptotically normal (root-n CAN)]{style=\"color:red\"}, if\n$$\\sqrt{n}(\\hat{\\boldsymbol{\\theta}}- \\boldsymbol{\\theta}) \\overset{d}{\\to}N(\\mathbf 0, \\mathbf V)$$\nfor a PSD matrix $\\mathbf V$. Equivalently,\n$$ \\hat{\\boldsymbol{\\theta}}\\overset{a}{\\sim}N(\\boldsymbol{\\theta}, \\mathbf V/n).$$\nWe refer to $\\mathbf V/n$ as the [***asymptotic\nvariance***]{style=\"color:red\"} of $\\hat{\\boldsymbol{\\theta}}$ and write\n$\\text{Avar}\\left(\\hat{\\boldsymbol{\\theta}}\\right) = \\mathbf V /n$.\n\n:::\n\n       \"$\\sqrt n-$\" emphasizes the fact that\n$\\sqrt{n}(\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}) = O_p(1)$,\nwhich is equivalent to\n$\\hat{\\boldsymbol{\\theta}} = \\boldsymbol{\\theta}+ O_p(n^{-1/2})$. In\nother words, as $n\\to\\infty$ the error term associated with our estimate\ndecreases at a rate of $n^{1/2}$. A fourfold increase in observations\nresults in half the error. We also have that\n$\\sqrt{n}(\\hat{\\boldsymbol{\\theta}} - \\boldsymbol{\\theta}) = o_p(1)$, so\n$\\hat{\\boldsymbol{\\theta}}\\overset{p}{\\to}\\boldsymbol{\\theta}$, hence\nthe \"consistent\" in the previous definition. We also have that\n$\\hat{\\boldsymbol{\\theta}}$ is asymptotically unbiased if it is root-n\nCAN, as\n$\\text{E}\\left[\\hat{\\boldsymbol{\\theta}}\\right]\\to \\boldsymbol{\\theta}$.\nThis will be the one of, if not the, ***most important property*** an\nestimator can posses.\n\n## Further Reading\n\n-   Eric Zivot's [primer on\n    asymptotics](http://faculty.washington.edu/ezivot/econ583/econ583asymptoticsprimer.pdf)\n-   @wooldridge2010econometric, Chapter 3\n-   @greene2003econometric, Appendix D\n-   @van2000asymptotic\n-   @white2014asymptotic\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"asymptotics.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.554","bibliography":["references.bib"],"theme":"cosmo","editor":{"markdown":{"wrap":72}}},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"asymptotics.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"documentclass":"scrreprt","editor":{"markdown":{"wrap":72}}},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}