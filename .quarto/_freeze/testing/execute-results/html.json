{
  "hash": "3437dba89510d2f7efd24e93a436376c",
  "result": {
    "engine": "knitr",
    "markdown": "\\DeclareMathOperator{\\plim}{plim}\n\\DeclareMathOperator{\\argmin}{argmin}\n\\DeclareMathOperator{\\argmax}{argmax}\n\\newcommand{\\var}[1]{\\text{Var}\\left(#1\\right)}\n\\newcommand{\\avar}[1]{\\text{Avar}\\left(#1\\right)}\n\\newcommand{\\E}[1]{\\text{E}\\left[#1\\right]}\n\\newcommand{\\cov}[1]{\\text{Cov}\\left(#1\\right)}\n\\newcommand{\\mse}[1]{\\text{MSE}\\left(#1\\right)}\n\\newcommand{\\se}[1]{\\text{se}\\left(#1\\right)}\n\\newcommand{\\limfunc}{lim} \n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\Xm}{\\mathbb{X}}\n\\newcommand{\\EER}{\\bar{\\thet}_\\text{EE}}\n\\newcommand{\\NLS}{\\hat{\\bet}_\\text{NLLS}}\n\\newcommand{\\z}{\\mathbf{z}}\n\\newcommand{\\rr}{\\mathbf{r}}\n\\newcommand{\\C}{\\mathbf{C}}\n\\newcommand{\\Pe}{\\mathbf{P}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\uu}{\\mathbf{u}}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\xm}{\\mathbb{x}}\n\\newcommand{\\Zm}{\\mathbb{Z}}\n\\newcommand{\\Wm}{\\mathbb{W}}\n\\newcommand{\\Hm}{\\mathbb{H}}\n\\newcommand{\\W}{\\mathbf{W}}\n\\newcommand{\\Z}{\\mathbf{Z}}\n\\newcommand{\\Hess}{\\mathbf{H}(\\mathbf{\\Z\\mid\\thet})}\n\\newcommand{\\Score}{\\mathbf{S}(\\mathbf{\\Z\\mid\\thet})}\n\\newcommand{\\A}{\\mathbf{A}}\n\\newcommand{\\h}{\\mathbf{h}}\n\\newcommand{\\Q}{\\mathbf{Q}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\G}{\\mathbf{G}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\renewcommand{\\D}{\\mathbf{D}}\n\\renewcommand{\\C}{\\mathbf{C}}\n\\newcommand{\\zer}{\\mathbf{0}}\n\\newcommand{\\OLS}{\\hat{\\boldsymbol\\beta}_\\text{OLS} }\n\\newcommand{\\OLSOV}{\\hat{\\boldsymbol\\beta}_\\text{OLS,OV} }\n\\newcommand{\\OLSME}{\\hat{\\boldsymbol\\beta}_\\text{OLS,ME} }\n\\newcommand{\\EE}{\\hat{\\boldsymbol\\theta}_\\text{EX} }\n\\newcommand{\\ME}{\\hat{\\boldsymbol\\theta}_\\text{M} }\n\\newcommand{\\MDE}{\\hat{\\boldsymbol\\theta}_\\text{MDE} }\n\\newcommand{\\IV}{\\hat{\\boldsymbol\\beta}_\\text{IV} }\n\\newcommand{\\TSLS}{\\hat{\\boldsymbol\\beta}_\\text{2SLS} }\n\\newcommand{\\thet}{\\boldsymbol{\\theta}}\n\\newcommand{\\et}{\\boldsymbol{\\eta}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Sig}{\\boldsymbol{\\Sigma}}\n\\newcommand{\\ep}{\\boldsymbol{\\varepsilon}}\n\\newcommand{\\Omeg}{\\boldsymbol{\\Omega}}\n\\newcommand{\\Thet}{\\boldsymbol{\\Theta}}\n\\newcommand{\\bet}{\\boldsymbol{\\beta}}\n\\newcommand{\\rk}{rank}\n\\newcommand{\\tsum}{\\sum}\n\\newcommand{\\tr}{tr}\n\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n\\newcommand{\\abs}[1]{\\left\\lvert#1\\right\\rvert}\n\\newcommand{\\ms}{\\overset{ms}{\\to}}\n\\newcommand{\\pto}{\\overset{p}{\\to}}\n\\newcommand{\\iid}{\\overset{iid}{\\sim}}\n\\newcommand{\\dto}{\\overset{d}{\\to}}\n\\newcommand{\\asim}{\\overset{a}{\\sim}}\n\n# Hypothesis Testing {#sec-testing}\n\n\n::: {.cell}\n\n:::\n\n\nUntil now, we've focused on point estimation, but that's only half the picture when it comes to statistics. The other half is inference. How do we test hypotheses about the true $\\thet_0$, and make inferences about the underling data generating process $P_{\\thet_0}\\in \\mathcal P$? An exhaustive treatment of inference is due to @lehmann2005testing, while @bickel2015mathematical offer an equally technical, yet briefer, treatment. \n\n## Decision Theory\n\nIn @sec-est we defined a (point) estimator as a function from a sample space $\\mathcal X$ to the parameter space $\\Theta$. For some specified model $\\mathcal P$, we observe a realization of the random vector $\\X\\sim P_{\\thet_0}$ for $P_{\\thet_0}\\in \\mathcal P$, and then calculate an estimate $\\hat{\\thet}$. This process is a special case of a more general framework that unifies point estimation and hypothesis testing. \n\nConsider an **_action space_** $\\mathcal A$. A **_decision process/rule_** $\\delta:\\mathcal X\\to \\mathcal A$ prescribes an action given an observation of a random vector $\\X$ defined on $\\mathcal X$. The set of all decision rules is $\\mathcal D$. If the true data generating process is $P_\\thet\\in\\mathcal P$, the cost of taking the action $a$ is given by the **_loss function_** $l(P_\\thet, a)$ where $l:\\mathcal P\\times \\mathcal A\\to\\mathbb R^+$. The loss associated with a decision rule is $l(P_\\thet, \\delta(\\X))$. We cannot calculate this loss, as we do not know $P_\\thet$. Instead we average the loss over $\\Theta$ (which is the same as over all $P_\\theta$ if $\\mathcal P$ is identified), giving a **_risk function_** $R:\\mathcal P\\to \\mathbb R^+$ defined as $\\E{l(P_\\thet, \\delta(\\X))}$. \n\n:::{#exm- name=\"Point Estimation\"}\nIn the case of Section @sec-est, we took $\\mathcal A = \\Theta$. Our space of actions where simply parameter values. We also defined a quadratic loss function which resulted in the risk function taking the form of the MSE of an estimator.\n:::\n\nWe can also define hypothesis testing using decision theory. \n\n:::{#def-}\nLet $\\mathcal X$ and $\\mathcal P$ be the sample space and model, respectively, and partition $\\mathcal P$ into $\\mathcal P_0$ and $\\mathcal P_1$.^[That is $\\mathcal P = \\mathcal P_0 \\cup \\mathcal P_1$ and $\\mathcal P_0 \\cap \\mathcal P_1 = \\emptyset$.] A <span style=\"color:red\">**_test function_**</span> is a decision rule defined on $\\mathcal A =\\{\\mathcal P_0,\\mathcal P_1\\}$ given as $$\\delta(\\X) = \\begin{cases}\\mathcal P_1 & T(\\X) \\in C \\\\ \\mathcal P_0 & T(\\X)\\notin C\\end{cases}$$ for some <span style=\"color:red\">**_critical region_**</span> $C\\subseteq \\mathcal X$ and <span style=\"color:red\">**_test statistic_**</span> $T:\\mathcal X\\to\\mathcal X$.\n:::\n\nOf the set of models $\\mathcal P_0$ and $\\mathcal P_1$, one is often easier to specify. For example, suppose $\\X = (X_1,\\ldots,X_n)$ captures the effectiveness of a drug on a series of patients $i=1,\\ldots,n$, and $\\X\\sim P_\\thet\\in \\mathcal P$. If we want to test whether this drug has an effect on patients' health, then we want to partition $\\mathcal P$ into two groups: one group corresponding to the drug having no effect, and one where the drug has an effect. It's *much* easier to specify the models which correspond to no effect than the models that correspond to the drug having an effect, as there are nearly infinite possibilities when it comes to the type and degree of effectiveness. The easier of the two groups to specify is traditionally denoted $\\mathcal P_0$, and is often associated with some well formed **_(null) hypothesis_**. This hypothesis is often written as $H_0:P_\\theta\\in \\mathcal P_0$. Our decision $\\delta(\\X)$ prescribed whether we **_fail to reject/reject_** $H_0:P_\\thet\\in\\mathcal P_0$. If we reject of the hypothesis $H_0:P_\\thet \\in\\mathcal P_0$, we conclude that $P_\\thet$ belongs to the **_class of alternative_** $H_1:P_\\thet \\in \\mathcal P_1$. We often think of $H_0:P_\\thet \\in\\mathcal P_0$ as a statement we assume to be true, with the burden of proof being on $H_1:P_\\thet \\in \\mathcal P_1$.  \n\n::: {.hypothesis name=\"Different Conventions\"}\nThere exist two other popular ways of writing the decision problem associated with testing a hypothesis:\n\n1. Assuming $\\mathcal P$ is identified and each $P_\\thet$ is uniquely determined by a $\\thet \\in \\Theta$, then we can partition $\\Theta$ into $\\Theta_1$ and $\\Theta_0$, and define \n$\\delta(\\X) = \\begin{cases}\\Theta_1 & T(\\X) \\in C \\\\ \\Theta_0 & T(\\X)\\notin C\\end{cases},$  where $\\mathcal A = \\{\\Theta_0, \\Theta_1\\}$ The hypothesis and class of alternatives are now written as $H_0: \\thet \\in\\Theta_0$ and $H_1: \\thet \\in\\Theta_1$, respectively.\n\n2. We could define $\\mathcal A = \\{0,1\\}$, where $1$ corresponds to rejecting $H_0:P_\\thet \\in\\mathcal P_0$ and $0$ failing to reject $H_0:P_\\thet \\in\\mathcal P_0$.  \n\nVirtually all concrete examples of hypothesis test use notation similar to 1. \n:::\n\n\n:::{#exm-normtest}\n\n## One-Sided Z-Test\n\nSuppose $\\mathcal P$ is the collection of normal distributions with known variance $\\sigma^2$. This model is parameterized by mean $\\mu$. We want to test the following hypothesis:\n\\begin{align*}\nH_0:&\\mu \\le \\mu_0\\\\\nH_1:&\\mu > \\mu_0\n\\end{align*} In this case, the null hypothesis is often abbreviated as $H_0: \\mu = \\mu_0$. Our hypothesis has partitions our parameter space $\\Theta = \\mathbb R$ into $\\Theta_0 = (-\\infty,\\mu_0]$ and $\\Theta_1 = (\\mu_0,\\infty)$. Define a statistic $$T(\\X) = \\frac{\\bar X - \\mu_0}{\\sigma/\\sqrt n} = \\frac{\\frac{1}{n}\\sum_{i=1}^nX_i - \\mu_0}{\\sigma/\\sqrt n},$$ and a critical region $C = [1.645,\\infty)$. Our decision rule $\\delta$ takes the form \n$$\\delta(\\X) = \\begin{cases} (-\\infty,\\mu_0] & T(\\X) < 1.645 \\\\ (\\mu_0,\\infty) & T(\\X) \\ge 1.645 \\end{cases},$$ which is  written succinctly as $\\delta(\\X) = I[T(\\X) \\ge 1.645 ]$ if we take $\\mathcal A = \\{0,1\\}$\n::: \n\n\nThe binary nature of hypothesis testing makes defining a loss function quite simple. Our decision is either correct or incorrect. We can take the loss function to be 0 if we are correct, and 1 if we are incorrect.\n$$l(P_\\theta, \\delta(\\X))=\\begin{cases}1 & \\delta(\\X) = \\mathcal P_0 \\text{ and }P_\\thet\\in\\mathcal P_0 \\\\\n0 & \\delta(\\X) = \\mathcal P_1 \\text{ and }P_\\thet\\in\\mathcal P_0\\\\\n1 & \\delta(\\X) = \\mathcal P_1 \\text{ and }P_\\thet\\in\\mathcal P_1\\\\\n0 & \\delta(\\X) = \\mathcal P_0 \\text{ and }P_\\thet\\in\\mathcal P_1\\end{cases}$$\nThe associated risk function becomes \n\\begin{align*}\nR(P_\\thet, \\delta(\\X)) & = \\E{l(P_\\theta, \\delta(\\X))} = l(P_\\theta, \\mathcal P_0)\\cdot \\Pr(\\delta(\\X) = \\mathcal P_0) +l(P_\\theta, \\mathcal P_1)\\cdot \\Pr(\\delta(\\X) = \\mathcal P_1).\n\\end{align*}\nWe can simplify $R(P_\\thet, \\delta(\\X))$ if we condition one either $P_\\theta \\in \\mathcal P_0$, or $P_\\theta \\in \\mathcal P_1$.\n\n\\begin{align*}\nR(P_\\thet, \\delta(\\X) \\mid P_\\thet\\in\\mathcal P_0)& =  \\underbrace{l(P_\\theta, \\mathcal P_0)}_0\\cdot \\Pr(\\delta(\\X) = \\mathcal P_0 \\mid P_\\thet\\in\\mathcal P_0) +\\underbrace{l(P_\\theta, \\mathcal P_1)}_1\\cdot \\Pr(\\delta(\\X) = \\mathcal P_1 \\mid P_\\thet\\in\\mathcal P_0) =\\Pr(\\delta(\\X) = \\mathcal P_1 \\mid P_\\thet\\in\\mathcal P_0)\\\\ \nR(P_\\thet, \\delta(\\X) \\mid P_\\thet\\in\\mathcal P_1)& =  \\underbrace{l(P_\\theta, \\mathcal P_0)}_1\\cdot \\Pr(\\delta(\\X) = \\mathcal P_0 \\mid P_\\thet\\in\\mathcal P_1) +\\underbrace{l(P_\\theta, \\mathcal P_1)}_0\\cdot \\Pr(\\delta(\\X) = \\mathcal P_1 \\mid P_\\thet\\in\\mathcal P_1) =\\Pr(\\delta(\\X) = \\mathcal P_0 \\mid P_\\thet\\in\\mathcal P_1)\n\\end{align*}\n\nIn both cases, the risk function is the probability of making an erroneous decision. These two types of errors are likely familiar.\n\n:::{#def-}\nSuppose we have a null hypothesis $H_0:P_\\thet \\in\\mathcal P_0$. If $P_\\thet \\in \\mathcal P_0$, but $\\delta(\\X) = \\mathcal P_1$ (the null hypothesis is true but we reject it), then we have committed a <span style=\"color:red\">**_type I error_**</span>. On the other hand, if $P_\\thet \\in \\mathcal P_1$, but $\\delta(\\X) = \\mathcal P_0$ (the null hypothesis is false but we fail to reject it), then we have committed a <span style=\"color:red\">**_type II error_**</span>.\n:::\n\n|                      | $H_0$ True       | $H_0$  False     |\n|----------------------|------------------|------------------|\n| Reject $H_0$         | type I error | correct decision    |\n| Fail to Reject $H_0$ | correct decision     | type II error |\n\nConsidering we have two types of errors, which is more important? How do we construct optimal test? @neyman1933 provide a solution to these problems.\n\n## Size and Power\n\nIn order to assess tests, we will need to define probabilities related to type I and type II error.\n\n:::{#def-}\nThe <span style=\"color:red\">**_level_**</span> $\\alpha \\in [0,1]$ is a specified number such that $\\Pr(\\text{type I error})>\\alpha$ is unacceptable. In other words, $$\\Pr(\\delta(\\X) = \\mathcal P_1 \\mid P_\\thet\\in P_0) = \\Pr(T(\\X)\\in C \\mid P_\\thet\\in P_0) \\le \\alpha \\ \\ \\forall P_\\thet\\in \\mathcal P_0.$$\n:::\n\nNote that $\\alpha$ must hold for all $P_\\thet \\in \\mathcal P_0$. A test may have a level of 0.01 for one $P_{\\thet} \\in \\mathcal P_0$, but could have a level of 0.10 for $P_{\\thet'} \\in \\mathcal P_0$. How then do we assess the \"aggregate\" level of a test across all $\\mathcal P_0$? We will do so by considering the worst case scenario, and associating a test with the largest level $\\alpha$ possible, where the maximum is taken over all $P_\\thet\\in \\mathcal P_0$. This will be known as the size of the test.\n\n:::{#def-}\nThe <span style=\"color:red\">**_size of a test_**</span> defined with a statistic $T(\\X)$ and critical value $c$ is $$\\alpha(\\delta) = \\sup_{P_\\thet \\in \\mathcal P_0} \\Pr(T(\\X)\\in C \\mid P_\\thet \\in \\mathcal P_0).$$\n:::\n\nThe size is nothing more than the maximum probability of committing a type I error permitted by a test $\\delta$. If we want to avoid type I errors, we want $\\alpha$ to be very small. We also need to consider type II errors. Note that the probability of a type II error is\n\n$$\\Pr(\\text{type II error}) = \\Pr(T(\\X)\\in C \\mid P_\\thet \\in\\mathcal P_0) = 1 - \\Pr(T(\\X)\\in C \\mid P_\\thet \\in\\mathcal P_1).$$\nA small chance of committing a type II error is the same as the probability of our test correctly identifying $P_\\thet\\in \\mathcal P_1$ being high. This ability is the power of our test.\n\n:::{#def-}\nThe <span style=\"color:red\">**_power of a test_**</span> defined with a statistic $T(\\X)$ and critical region $C$ is $$\\beta(\\delta, P_\\theta) = \\Pr(T(\\X) \\in C \\mid P_\\thet).$$\n:::\n\nThe power function is simply the probability of rejecting the null hypothesis for any $P_\\thet\\in \\mathcal P$. It can be thought of as a test power detect that $H_1$ is true ($H_0$ is false). Note that $\\beta(\\delta, P_\\theta)$ is redundant on the subset $\\mathcal P_0 \\subset \\mathcal P$. For any $P_\\theta\\in \\mathcal P_0$,\n$$ \\beta(\\delta, P_\\theta\\mid P_\\theta\\in \\mathcal P_0) = \\Pr(T(\\X) \\in C \\mid   P_\\thet\\in \\mathcal P_0) \\le \\sup_{P_\\thet\\in P_0} \\Pr(T(\\X)\\in C \\mid P_\\thet\\in\\mathcal P_0) = \\alpha(\\delta).$$ By construction $\\beta(\\delta, P_\\theta) \\le \\alpha(\\delta)$ on $\\mathcal P_0$, the power on $\\mathcal P_0$ doesn't provide any new information. On the other hand it gives us another way of writing the size of a test:\n$$ \\alpha(\\delta) = \\sup_{P_\\theta \\in\\mathcal P_0} \\beta(P_\\thet, \\delta).$$We're interested in the power of our test when $\\mathcal P\\in P_1$, which corresponds to the probability of correctly detecting $\\mathcal P\\in P_1$.   \n$$\\beta(\\delta, P_\\theta\\mid P_\\theta\\in \\mathcal P_1) = \\Pr(T(\\X) \\in C \\mid   P_\\thet\\in \\mathcal P_1) = 1 - \\Pr(\\text{type II error})$$ For this reason, you will very often see $\\beta$ only defined on $\\mathcal P_1$.\n\n:::{#exm-}\n\n## Z-Test\n\nReconsider @exm-normtest. We have \n\\begin{align*}\nH_0:&\\mu \\le \\mu_0\\\\\nH_1:&\\mu > \\mu_0\\\\\nT(\\X) &= \\frac{\\bar X - \\mu_0}{\\sigma/\\sqrt n }\\\\\n\\delta(\\X) &= \\begin{cases} (-\\infty,\\mu_0] & T(\\X) < 1.645 \\\\ (\\mu_0,\\infty) & T(\\X) \\ge 1.645 \\end{cases}\n\\end{align*}\n\nNote that an equivalent test for our null hypothesis is \n$$\\delta(\\X) = \\begin{cases} (-\\infty,\\mu_0] & \\bar X < \\mu_0 +1.645\\left(\\frac{\\sigma}{\\sqrt n}\\right) \\\\ (\\mu_0,\\infty) & \\bar X  \\ge \\mu_0 +1.645\\left(\\frac{\\sigma}{\\sqrt n}\\right) \\end{cases}$$\nWe can calculate the size of our test using the fact that $T(\\X) \\sim N(0,1)$. Any such $\\mu$ can be written as $\\mu_0 + t(\\sigma/\\sqrt n)$ for $t \\le 0$. The level of a test for some $\\mu \\in (-\\infty,\\mu_0]$ is \n\\begin{align*}\n\\Pr(\\bar X \\ge \\mu_0 +1.645(\\sigma/\\sqrt n)) &= \\Pr(\\bar X \\ge [\\mu - t(\\sigma/\\sqrt n)] +1.645(\\sigma/\\sqrt n)) & (\\mu_0 =\\mu - t(\\sigma/\\sqrt n))\\\\\n&= \\Pr(\\bar X \\ge \\mu+ (1.645-t)(\\sigma/\\sqrt n))\\\\\n& = \\Pr \\left(\\frac{\\bar X - \\mu}{(\\sigma/\\sqrt n)} \\ge 1.645 - t\\right)\\\\\n& = \\Phi(-1.645 + t) \n\\end{align*}\nBefore we take the supremum over all such probabilities, note that $\\mu \\le \\mu_0$ is equivalent to $t\\le 0$ where $\\mu = \\mu_0 + t(\\sigma/\\sqrt n)$.^[In order to keep this problem a bit more general and avoid defining an explicit $\\mu_0$, we express $\\mu$ in terms of its standardized distance from $\\mu_0$.] Therefore the size of our test is \n\\begin{align*}\n\\alpha &= \\sup_{\\mu \\le \\mu_0} \\Pr(T(\\X)\\in C \\mid \\mu < \\mu_0)\\\\\n& = \\sup_{t \\le 0} \\Phi(-1.645 + t)\n\\end{align*}\n\nPlotting this probability makes it clear that $\\alpha(\\delta) \\approx 0.05$, and the supremum is achieved when $\\mu = \\mu_0$ ($t=0$).\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code which generates figure\"}\ntibble(t = -300:0/100) %>% \n  mutate(y = pnorm(-1.645 + t)) %>% \n  ggplot(aes(t,y)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"t =  (μ - μ0)/(σ√n)\", y = \"Pr(Reject H0 | H0 True)\") +\n  geom_hline(yintercept = 0.05, linetype = \"dashed\", color = \"red\")\n```\n\n::: {.cell-output-display}\n![The size of the test is given by the red dashed line corresponding to the supremum of the probability of rejecting a true null hypothesis.\"](testing_files/figure-html/fig-plot31-1.png){#fig-plot31 fig-align='center' width=768}\n:::\n:::\n\n\nNow we can consider the power $\\beta(\\mu)$. We've in fact already done nearly all the calculations required for this. Consider $\\mu = \\mu_0 + t(\\sigma/\\sqrt n)$ for $t\\in \\R$. If $t \\le 0$, then $\\mu \\le \\mu_0$, and the null hypothesis is true. If $t > 0$, then $\\mu >\\mu_0$ and the null hypothesis is false. The power $\\beta(\\mu)$ is $\\Phi(-1.645 + t)$, but on the domain $t\\in \\R$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code which generates figure\"}\ntibble(\n  t = seq(-2, 4, length = 1000), \n  group = \"Power Curve\"\n) %>% \n  mutate(y = pnorm(-1.645 + t)) %>% \n  bind_rows(data.frame(t =c(-2,0), y = c(0,0), group = \"H0 True, μ < μ0\")) %>%\n  bind_rows(data.frame(t =c(0,4), y = c(0,0), group = \"H1 True, μ > μ0\")) %>%\n  ggplot(aes(t,y, color = group)) +\n  geom_line() +\n  theme_minimal() +\n  labs(\"t =  (μ - μ0)/(σ√n)\", y = \"Pr(Reject H0)\" , color= \"\") +\n  scale_color_manual(values = c(\"red\", \"green\", \"black\")) + \n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Power curve of test.](testing_files/figure-html/fig-power-1.png){#fig-power fig-align='center' width=768}\n:::\n:::\n\nTo make this more concrete, we can perform a Monte Carlo simulation for fixed values of $\\mu_0$, $\\mu$, $\\sigma^2$, and $n$. For all simulations, let's fix $\\mu_0 = 2$, $n = 100$ and $\\sigma^2 =1$. First, assume $\\mu = \\mu_0 = 2$, and record $\\delta(\\X)$ for 100,000 simulations. In this case we should expect to make the correct decision (fail to reject the null hypothesis) about 95% of the time, as the size of our test is $\\alpha = 0.05$ which corresponds to the maximum level which occurs at $\\mu = \\mu_0$. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# draw on realization of a test stat and report decision\ndraw_test <- function(delta, test_stat, critical_region, mu_0, sigma, n, dist, dist_params, s){\n  X <- do.call(dist, append(n, dist_params))\n  output <- delta(test_stat, critical_region, X, mu_0, sigma) %>% \n    mutate(iter = s)\n  return(output)\n}\n\n# draw N realizations of a test stat\ndraw_N_tests <- function(N, delta, test_stat, critical_region, mu_0, sigma, n, dist, dist_params){\n  output <- 1:N %>% \n    map_df(\\(s) draw_test(delta, test_stat, critical_region, mu_0, sigma, n, dist, dist_params, s))\n  return(output)\n}\n\n# define test stat for this particular example\ntest_stat <- function(X, mu_0, sigma){\n  n <- length(X)\n  output <- (mean(X)-mu_0)/(sigma/sqrt(n))\n  return(output)\n}\n\n# Define decision function, supply critical region as a list of bounds, ex: list(c(-Inf, -3), c(0, 1), c(3, Inf))\ndelta <- function(test_stat, critical_region, X, mu_0, sigma){\n  stat <- test_stat(X, mu_0, sigma)\n  # determine if the test stat falls within any of the interval comprising the critical region\n  decision <- critical_region %>% \n    map_lgl(\\(bounds) between(stat, bounds[1], bounds[2])) %>% \n    sum() %>% \n    as.logical()\n  \n  output <- tibble(\n    stat = stat,\n    decision = decision,\n    H_0 = mu_0\n  )\n  return(output)\n}\n\nresults <- draw_N_tests(\n  N = 1e5,\n  delta = delta, \n  test_stat = test_stat, \n  critical_region = list(c(1.645, Inf)), \n  mu_0 = 2, \n  sigma = 1, \n  n = 100, \n  dist = rnorm, \n  dist_params = list(\n    mean = 2, \n    sd= 1\n  )\n)\n\nresults %>% \n  group_by(decision) %>% \n  count()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n# Groups:   decision [2]\n  decision     n\n  <lgl>    <int>\n1 FALSE    94986\n2 TRUE      5014\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(results$decision)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05014\n```\n\n\n:::\n:::\n\n\nThis simulation calculated the power of our test given $\\mu$. Let's define the power function using this simulation, and calculate the power for $\\mu\\in[1.8,2.4]$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Approximate the power of a test for a given μ using N simulations, assume normal distribution\npower <- function(mu, N, delta, test_stat, critical_region, mu_0, sigma, n){\n  output <- draw_N_tests(N, delta, test_stat, critical_region, mu_0, sigma, n, rnorm, list(mu, sigma)) %>% \n    summarize(\n      simulated_power = mean(decision),\n      t = mu\n    )\n  return(output)\n}\n\npower_curve <- function(domain, N, delta, test_stat, critical_region, mu_0, sigma, n){\n  output <- domain %>%\n      map_df(\\(mu) power(mu, N, delta, test_stat, critical_region, mu_0, sigma, n))\n  return(output)\n}\n\nresults <- power_curve(\n  domain = seq(1.8, 2.4, length = 25), \n  N = 1e4,\n  delta = delta, \n  test_stat = test_stat, \n  critical_region = list(c(1.645, Inf)), \n  mu_0 = 2, \n  sigma = 1, \n  n = 100\n)\n```\n:::\n\n\nWe can plot this simulated power curve over the theoretical curve we calculated for a general $\\mu_0$ (see @fig-power).^[This requires us to reparameterize the domain of the theoretical curve in terms of $\\mu$ instead of $t$.]\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code which generates figure\"}\ndf <- results %>% \n  mutate(\n    y = simulated_power,\n    group = \"Simulated Power\",\n  ) \n\ntibble(\n  t = seq(1.8, 2.4, length = 1000),\n  group = \"Power Curve\"\n) %>%\n  mutate(y = pnorm(-1.645 + (t-2)/(1/10) )) %>%\n  bind_rows(data.frame(t =c(1.8,2), y = c(0,0), group = \"H0 True, μ < μ0\")) %>%\n  bind_rows(data.frame(t =c(2,2.4), y = c(0,0), group = \"H1 True, μ > μ0\")) %>%\n  ggplot(aes(t, y, color = group)) +\n  geom_line() +\n  geom_point(data = df, size = 1.5) +\n  theme_minimal() +\n  labs(\"True μ\", \"Pr(Reject H0)\", color = \"\") +\n  scale_color_manual(values = c(\"red\", \"green\", \"black\", \"blue\")) +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Simulated power function on the interval [1.8, 2.4] using 10,000 simulations for each point.](testing_files/figure-html/fig-plot33-1.png){#fig-plot33 fig-align='center' width=768}\n:::\n:::\n\n\nOur simulated probabilities are virtually identical to the theoretical probabilities calculated!\n\n:::\n\nThis example is special for a few reason. Note that we have: \n\\begin{align*}\n\\alpha & = \\sup_{\\mu \\le \\mu_0}\\beta(\\mu) = \\beta(\\mu_0). \n\\end{align*}\nThis equality holds because $\\beta$ is monotonically increasing:\n\\begin{align*}\n \\frac{\\partial}{\\partial \\mu} \\beta(\\mu) &= \\frac{\\partial }{\\partial \\mu}\\Phi(-1.645 + t)  \\\\ \n & = \\frac{\\partial t}{\\partial \\mu}\\varphi(-1.645 - t) & (\\varphi \\text{ standard normal pdf})\\\\\n & = \\frac{\\partial}{\\partial \\mu}\\left(\\frac{\\mu - \\mu_0}{\\sigma/\\sqrt n}\\right)\\varphi\\left(-1.645 + \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt n}\\right) &(t = (\\mu - \\mu_0)/(\\sigma/\\sqrt n))\\\\ & = \n\\frac{1}{\\sigma/\\sqrt n}\\varphi\\left(-1.645 + \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt n}\\right) \\\\\n& > 0 &(\\varphi(\\cdot) >0 , n >0, \\sigma > 0).\n\\end{align*}\n\nIf the probability we reject the null hypothesis grows with $\\mu\\in(-\\infty,\\mu_0]$, then of course the supremum of these probabilities is the probability at the boundary $\\mu_0$. Equivalently, \n$$ \\alpha = \\inf_{\\mu_0 < \\mu}\\beta(\\mu) = \\inf_\\mu \\beta(\\mu \\mid \\mu_0 < \\mu) = \\inf_\\mu \\beta(\\mu \\mid H_0 \\text{ false}).$$ Many people would define power $\\beta(\\mu)$ only on $(\\mu_0,\\infty)$ (when $H_0$ is false), so in this case the size is in the infimum of the power. Finally because $$\\beta(\\mu) = \\Phi(-1.645 + t) = \\Phi\\left(-1.645 + \\frac{\\mu - \\mu_0}{\\sigma/\\sqrt n}\\right),$$\nwe have $\\alpha = \\beta(\\mu_0) = \\Phi(-1.645)$. This means if we desire a size of $\\alpha$, we can use the standard quantile function to calculate the critical value required -- $c = -\\Phi^{-1}(\\alpha)$. Just to reiterate, these nice properties hold because $\\beta$ is monotonic!   \n\n::: {.hypothesis name=\"p-Values\"}\nOur decision function $\\delta(\\X)$ only tells us whether we reject the null hypothesis or not. It doesn't directly give us any information on how confident we should be in our decision, or by what degree we reject the null hypothesis. We can capture this by looking at the distribution of the test statistics $T(\\X)$ in relation to the critical region $C(\\alpha)$ for size $\\alpha$. We can define the  <span style=\"color:red\">**_$p$-value_**</span>, denoted $p$, as $$p = \\inf\\{\\alpha \\mid T(\\X)\\in C(\\alpha)\\}.$$ The $p-$value is the minimum level of the test such that we still reject the null hypothesis (because $T(\\X)$ is in the critical region). Suppose in the case of the one sided $Z-$test we had $T(\\X) = 2$. We reject the null hypothesis because $T(\\X) \\ge  2$. We would also reject the null hypothesis for any size $\\alpha$ such that $2 \\ge -\\Phi^{-1}(\\alpha)$. Because $-\\Phi^{-1}$ is monotonic, \n$$p = \\inf\\{\\alpha\\mid 2 \\ge -\\Phi^{-1}(\\alpha)\\} = \\{\\alpha\\mid 2= -\\Phi^{-1}(\\alpha)\\} = \\Phi(-2) \\approx 0.02275.$$ One important result which follows immediately from the definition of of $p$:\n$$T(\\X) \\in C\\iff \\alpha(\\delta) < p.$$ We reject $H_0$ *if and only if* the $p-$value associated with $T(\\X)$ is less than the size of the test. One interpretation related to this is that the $p-$value tells you the degree to which you reject the null hypothesis. If $p\\approx0.02275$, not only do we reject the null hypothesis for $\\alpha(\\delta) = 0.05$, but we also reject it for more stringent tests with smaller sizes.\n:::\n\nSo if we can pick $\\alpha$ by virtue of the critical value $c$, then why don't we simply pick $\\alpha\\approx 0$. This would mean that we almost always fail to reject the null hypothesis, and in doing so we're bound to fail to reject the null hypothesis even when it is false, increasing the probability of a type II error \n\n:::{#exm-}\n\n## Power vs. Size\n\nAgain consider\n\\begin{align*}\nH_0:&\\mu \\le \\mu_0\\\\\nH_1:&\\mu > \\mu_0\n\\end{align*}\nwhere $X_i\\iid N(\\mu,\\sigma^2)$ for a known $\\sigma^2$, and $\\delta(\\X) = 1[T(\\X) \\ge c]$ for some critical value $c$. If we desire a test of size $\\alpha$ we set our critical value as $c = -\\Phi^{-1}(\\alpha)$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code which generates figure\"}\ntibble(x = 0:1000/1000) %>% \n  mutate(y = -qnorm(x)) %>% \n  ggplot(aes(x,y)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Desired Size, α\", y = \"Required Critical Value, c\")\n```\n\n::: {.cell-output-display}\n![The relationship between critical value and size.](testing_files/figure-html/fig-plot34-1.png){#fig-plot34 fig-align='center' width=768}\n:::\n:::\n\n\nDefine $t = (\\mu - \\mu_0)/(\\sigma/\\sqrt n)$ to be the standardized distance between $\\mu_0$ and the true $\\mu$. The power of the test is $$\\beta(\\mu)= \\Phi(- c + t) = \\Phi(\\Phi^{-1}(\\alpha) + t),$$ which is increasing in $\\alpha$:\n\\begin{align*}\n\\frac{\\partial \\beta}{\\partial \\alpha} & = \\frac{\\partial \\Phi^{-1}(\\alpha)}{\\partial \\alpha} \\varphi(\\Phi^{-1}(\\alpha) + t) \\\\\n& = \\frac{\\varphi(\\Phi^{-1}(\\alpha) + t)}{\\varphi(\\Phi^{-1}(\\alpha))} & (\\text{inverse function theorem}) \\\\\n& > 0 & (\\varphi(\\cdot) > 0).\n\\end{align*}\n\nAs we let $\\alpha \\to 0$, we have $\\beta \\to 0$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code which generates figure\"}\nexpand_grid(\n  a = c(0.01, 0.05, 0.10, 0.25), \n  t = -2000:4000/1000\n) %>% \n  mutate(power = pnorm(-(-qnorm(a)) + t)) %>% \n  ggplot(aes(t, power, color = as.factor(a))) + \n  geom_line() +\n  theme_minimal() +\n  labs(x = \"t =  (μ - μ0)/(σ√n)\", y = \"Power\", color = \"size\") +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![There is an inherent tradeoff between the power and size of a test, as small values of α result in less power.](testing_files/figure-html/fig-plot35-1.png){#fig-plot35 fig-align='center' width=768}\n:::\n:::\n\n\n:::\n\nSo how do we select $\\alpha$? Do we care more about type I error or type II error? Furthermore, how do we even construct test statistics?  We can begin to answer this question with the guidance of @neyman1933.\n\n\n## Neyman-Pearson Lemma and UMP Tests\n\nOne of the key ideas presented proposed by @neyman1933 is that type II errors are more erroneous than their type I counterparts, so we should minimize the probability of committing a type II error subject to a predetermined test size $\\alpha$. If someone if getting tested for a disease, a false positive (type I) is much better than a false negative (type II). On the other hand, many would argue it is worse to sentence an innocent person to jail (type I error) than let a guilty person go free, so whether this assumption holds depends on the context of our test. \n\nThe Neyman-Pearson lemma solves this problem by maximizing the power of a test subject to a specified $\\alpha$:\n$$ \\max_{\\delta\\in \\mathcal D}\\{\\beta(\\delta, P_\\thet) \\mid \\alpha(\\delta) < \\alpha\\},$$ *fixing* $P_\\thet$. The test which solves this maximization problem may vary across $P_\\thet$. As such, the Neyman-Pearson lemma solves our problem in the context of simplified hypotheses. \n\n:::{#def-}\nA null hypothesis $H_0$ is <span style=\"color:red\">**_simple_**</span> if $\\mathcal P_0$ is a singleton, $\\mathcal P_0 = \\{P_{\\thet_0}\\}$. Similarly, an alternative hypothesis $H_1$ is simple if $\\mathcal P_1 = \\{P_{\\thet_1}\\}$.\n:::\n\nThe Neyman-Pearson Lemma will only apply directly to hypotheses of the form:\n\\begin{align*}\nH_0:P_\\thet &= P_{\\thet_0}\\\\\nH_1:P_\\thet &=P_{\\thet_1}\n\\end{align*}\nNote that for simple hypotheses $$ \\alpha(\\delta) = \\sup_{P_\\thet \\in \\mathcal P_0} \\Pr(T(\\X)\\in C \\mid P_\\thet \\in \\mathcal P_0) = \\Pr(T(\\X)\\in C \\mid P_\\thet =P_{\\thet_0}) $$\n\n:::{#thm-  #NPlemma name=\"Neyman-Pearson Lemma\"}\nConsider a test with simple hypotheses $H_0:P_\\thet = P_{\\thet_0}$ and $H_1:P_\\thet=P_{\\thet_1}$. If $\\delta(\\X)$ is test with size $\\alpha$ and defined as \n$$\\delta(\\X)=\\begin{cases} P_{\\thet_0} & \\frac{f_{\\X}(\\x \\mid \\thet_1)}{f_{\\X}(\\x \\mid \\thet_0)} > \\eta\\\\  P_{\\thet_1} & \\frac{f_{\\X}(\\x \\mid \\thet_1)}{f_{\\X}(\\x \\mid \\thet_0)} < \\eta\\end{cases}$$ for $\\eta > 0$, then $\\beta(\\delta, \\theta_1) \\ge \\beta(\\delta', \\theta_1)$ for all $\\delta' \\in \\mathcal D$ with a size less than or equal to $\\alpha$. In this case we refer to $\\delta(\\X)$ as the <span style=\"color:red\">**_most powerful (MP) test_**</span> for our hypotheses.\n:::\n\n:::{.proof}\nLet $\\delta(\\X)$ be some arbitrary decision rule in $\\mathcal D$ with size $\\alpha$, and let $\\delta'\\in\\mathcal D$ be some other decision rule with size less than or equal to $\\alpha$. \n\\begin{align*}\n\\delta(\\X) &= \\begin{cases}P_{\\thet_1} & \\X \\in C \\\\ P_{\\thet_0} & \\X \\notin C\\end{cases}\\\\\n\\delta'(\\X)& = \\begin{cases}P_{\\thet_1} & \\X \\in C' \\\\ P_{\\thet_0} & \\X \\notin C'\\end{cases}\\\\\n \\Pr(\\delta'(\\X) = P_{\\thet_1} \\mid P_{\\thet} = P_{\\thet_0} ) &\\le \\alpha = \\Pr(\\delta(\\X) = P_{\\thet_1} \\mid P_{\\thet} = P_{\\thet_0} )\n\\end{align*}\nWithout loss of generality, we've taken $T(\\X) = \\X$.^[How is this WLOG? If $\\delta(\\X) = 1[T(\\X) \\in C]$, we can always write it as $\\delta(\\X) = 1[\\X \\in T^{-1}(C)]$ where $T^{-1}$ is the preimage of the test statistic. This is convenient for two reasons. Firstly we're comparing arbitrary elements from an infinite set of decision rules $\\mathcal D$, so it's much easier just to assume they both have the trivial test statistic $T(\\X)=\\X$, because in this case decision rules are determined only by critical regions. Comparing decision rules now amounts to comparing critical regions. Secondly, we'll want to calculate the probability of errors, which requires us to know the distribution of $T(\\X)$. If $T(\\X) = \\X$, then we have $T(\\X) \\sim F_\\X(\\x \\mid \\thet)$ where $\\thet \\in \\{\\thet_1, \\thet_2\\}$. We know this distribution, so we can easily calculate the probability of errors. The proof is exactly the same without taking this step, but it looks a bit gnarlier with the additional notation.] The only assumption we have made about our decision rules is,\n\\begin{align}\n& \\Pr(\\delta'(\\X) = P_{\\thet_1} \\mid P_{\\thet} = P_{\\thet_0} ) \\le \\alpha = \\Pr(\\delta(\\X) = P_{\\thet_1} \\mid P_{\\thet} = P_{\\thet_0} )\\\\\n\\implies & \\Pr(\\X\\in C' \\mid \\thet_0 ) \\le \\Pr(\\X\\in C \\mid \\thet_0 ) (\\#eq:npa).\n\\end{align}\n\nIn order to compare $\\delta$ and $\\delta'$, we'll want to write their powers in term of the critical regions of both tests. We can write $C$ in terms of the disjoint union of its intersection with the disjoint sets $C'$ and $(C')^c$, as $C'$ and $(C')^c$ partition the sample space $\\mathcal X$. We can also do the same with $C'$ and the disjoint sets $C$ and $C^c$. \n\\begin{align*}\nC & = (C\\cap C') \\cup (C\\cap (C')^c)\\\\\nC' & = (C'\\cap C) \\cup (C'\\cap C^c)\n\\end{align*}\nAccounting for these being disjoint unions, we have the following conditional probabilities:\n\\begin{align}\n\\Pr(\\X \\in C\\mid \\thet) & = \\Pr(\\X \\in C\\cap C' \\mid \\thet) + \\Pr(\\X \\in C\\cap (C')^c \\mid \\thet)& \\text{for }\\thet\\in\\{\\thet_0, \\thet_1\\} (\\#eq:npa2)\\\\\n\\Pr(\\X \\in C'\\mid\\thet) & = \\Pr(\\X \\in C'\\cap C \\mid \\thet) + \\Pr(\\X \\in C'\\cap C^c \\mid \\thet) & \\text{for }\\thet\\in\\{\\thet_0, \\thet_1\\} (\\#eq:npa3)\n\\end{align}\nWe can use these two equations to rewrite Equation \\@ref(eq:npa). \n\\begin{align}\n&  \\Pr(\\X\\in C' \\mid \\thet_0 ) \\le \\Pr(\\X\\in C \\mid \\thet_0 )\\\\\n\\implies & \\Pr(\\X \\in C'\\cap C \\mid \\thet_0) + \\Pr(\\X \\in C'\\cap C^c \\mid \\thet_0) \\le \\Pr(\\X \\in C\\cap C' \\mid \\thet_0) + \\Pr(\\X \\in C\\cap (C')^c \\mid \\thet_0)\\\\\n\\implies & \\Pr(\\X \\in C'\\cap C^c \\mid \\thet_0) \\le\\Pr(\\X \\in C\\cap (C')^c \\mid \\thet_0) (\\#eq:npa4)\n\\end{align}\nSimilarly, \n\\begin{align}\n\\beta(\\delta,\\thet_1) & = \\Pr(\\X\\in C \\mid \\theta_1) &(\\text{definition of }\\beta)  (\\#eq:npa5)\\\\\n& = \\Pr(\\X \\in C\\cap C' \\mid \\thet_1) + \\Pr(\\X \\in C\\cap (C')^c \\mid \\thet_1) & (\\text{Equation }(4.2))\\\\\n\\beta(\\delta',\\thet_1) & =\\Pr(\\X \\in C'\\cap C \\mid \\thet_1) + \\Pr(\\X \\in C'\\cap C^c \\mid \\thet_1) & (\\text{Equation }(4.3))  (\\#eq:npa6)\n\\end{align}\n\n\nWe want to construct $\\delta(\\X)$ such that \n\\begin{align*}\n&\\beta(\\delta,\\thet_1)  \\ge\\beta(\\delta',\\thet_1),\\\\\n\\implies&\\Pr(\\X \\in C\\cap C' \\mid \\thet_1) + \\Pr(\\X \\in C\\cap (C')^c \\mid \\thet_1) \\le  \\Pr(\\X \\in C'\\cap C \\mid \\thet_1) + \\Pr(\\X \\in C'\\cap C^c \\mid \\thet_1) &(\\text{Equation }(4.5)\\text{ and }(4.6)),\\\\\n \\implies &\\Pr(\\X \\in C\\cap (C')^c \\mid \\thet_1)  \\ge \\Pr(\\X \\in C'\\cap C^c \\mid \\thet_1),\\\\\n \\implies & \\int 1[\\X \\in C\\cap (C')^c]\\ dF_\\X(\\x\\mid \\thet_1) \\ge \\int 1[\\X \\in C'\\cap C^c]\\ dF_\\X(\\x\\mid \\thet_1),\\\\\n \\implies & \\int_{C\\cap (C')^c} f_\\X(\\x\\mid \\thet_1) \\ d\\x \\ge \\int_{C'\\cap C^c} f_\\X(\\x\\mid \\thet_1) \\ d\\x\n\\end{align*} In other words, we need to define $C$ such that \n$$ \\int_{C\\cap (C')^c} f_\\X(\\x\\mid \\thet_1)\\stackrel{?}{\\ge}\\int_{C'\\cap C^c} f_\\X(\\x\\mid \\thet_1) \\ d\\x$$ using the fact that \n\\begin{align*}\n\\int_{C'\\cap C^c }f_\\X(\\x\\mid \\thet_0)\\ d\\x & \\le \\int_{C\\cap (C')^c }f_\\X(\\x\\mid \\thet_0)\\ d\\x & (\\text{Equation (4.4)}).\n\\end{align*}\nWe can write the left hand side of this known inequality as \n\\begin{align*}\n\\int_{C'\\cap C^c }f_\\X(\\x\\mid \\thet_0)\\ d\\x = \\int_{C'\\cap C^c}f_\\X(\\x\\mid \\thet_1) \\cdot \\frac{f_\\X(\\x\\mid \\thet_0)}{f_\\X(\\x\\mid \\thet_1)}\\ d\\x.\n\\end{align*}\nWe want to somehow relate this to the integral of $f_\\X(\\x\\mid\\thet_1)$, but in general cannot \"remove\" $f_\\X(\\x\\mid\\thet_0)/f_\\X(\\x\\mid\\thet_1)$ from the integrand without an assumption about $f_\\X(\\x\\mid\\thet_0)/f_\\X(\\x\\mid\\thet_1)$. Namely, if we assume $f_\\X(\\x\\mid\\thet_0)/f_\\X(\\x\\mid\\thet_1) >\\eta^{-1}$ on $C'\\cap C^c$ for some constant $\\eta$,^[Recall that $\\int f(x)g(x)\\ dx \\le M \\int f(x) $ if $g(x)\\le M$ on the domain of integration.] we have \n\\begin{align*}\n\\int_{C'\\cap C^c }f_\\X(\\x\\mid \\thet_0)\\ d\\x= \\int_{C'\\cap C^c }f_\\X(\\x\\mid \\thet_1) \\cdot \\frac{f_\\X(\\x\\mid \\thet_0)}{f_\\X(\\x\\mid \\thet_1)}\\ d\\x\n \\ge \\eta^{-1}\\int_{C'\\cap C^c}f_\\X(\\x\\mid \\thet_0) \\ d\\x.\n\\end{align*}\nSimilarly, the right hand side of Equation 4.4 in integral form can be written as\n$$\\int_{C\\cap (C')^c }f_\\X(\\x\\mid \\thet_0)\\ d\\x = \\int_{C\\cap (C')^c }f_\\X(\\x\\mid \\thet_1) \\cdot \\frac{f_\\X(\\x\\mid \\thet_0)}{f_\\X(\\x\\mid \\thet_1)}\\ d\\x\\le \\eta^{-1}\\int_{C\\cap (C')^c }f_\\X(\\x\\mid \\thet_0) \\ d\\x,$$ assuming $f_\\X(\\x\\mid\\thet_0)/f_\\X(\\x\\mid\\thet_1) < \\eta^{-1}$ on $C\\cap (C')^c$. Therefore, \n\\begin{align*}\n\\int_{C\\cap (C')^c} f_\\X(\\x\\mid \\thet_1)\\ d\\x & = \\int_{C\\cap (C')^c}f_\\X(\\x\\mid \\thet_1)  \\cdot \\frac{f_\\X(\\x\\mid \\thet_0)}{f_\\X(\\x\\mid \\thet_1)}\\ d\\x \\\\\n& \\ge \\eta^{-1}\\int_{C\\cap (C')^c}f_\\X(\\x\\mid \\thet_0)\\ d\\x & (f_\\X(\\x\\mid\\thet_0)/f_\\X(\\x\\mid\\thet_1) < \\eta^{-1} \\text{ on } C\\cap (C')^c)\\\\\n& =\\eta^{-1}\\Pr(\\X \\in C\\cap (C')^c \\mid \\thet_0)\\\\\n& \\ge  \\eta^{-1}\\Pr(\\X \\in C'\\cap C^c \\mid \\thet_0) & (\\text{Equation }(4.4))\\\\\n& =\\eta^{-1}\\int_{ C'\\cap C^c}f_\\X(\\x\\mid \\thet_0)\\ d\\x\\\\\n& \\ge \\int_{C'\\cap C^c }f_\\X(\\x\\mid \\thet_1) \\cdot \\frac{f_\\X(\\x\\mid \\thet_0)}{f_\\X(\\x\\mid \\thet_1)}\\ d\\x & (f_\\X(\\x\\mid\\thet_0)/f_\\X(\\x\\mid\\thet_1) > \\eta^{-1} \\text{ on } C'\\cap C^c)\\\\\n& = \\int_{C'\\cap C^c} f_\\X(\\x\\mid \\thet_1)\\ d\\x, \n\\end{align*}\nwhich is the desired result for a fixed $\\delta'$. If we extend $f_\\X(\\x\\mid\\thet_0)/f_\\X(\\x\\mid\\thet_1) > \\eta^{-1}$ to all of $C^c$ and $f_\\X(\\x\\mid\\thet_0)/f_\\X(\\x\\mid\\thet_1) < \\eta^{-1}$ to all of $C$, then this will hold *for all* $\\delta'\\in \\mathcal D$ (with a size of at least $\\alpha$, otherwise Equation \\@ref(eq:npa4) needn't hold). What then is the explicit form of $\\delta(\\X)$? It is defined using the bounds which allowed us to establish the desired inequality using properties of integrals. The critical region is,\n\\begin{align*}\nC &= \\left\\{\\x \\ \\bigg|\\ \\frac{f_{\\X}(\\x \\mid \\thet_1)}{f_{\\X}(\\x \\mid \\thet_0)} > \\eta \\right\\}\n\\end{align*}\nand $\\delta(\\X) = 1[\\x \\in C]$.\n:::\n\nBefore discussing the intuition behind Theorem \\@ref(thm:NPlemma), here are some technical points:\n\n1. In the event $f_{\\X}(\\x \\mid \\thet_1)/f_{\\X}(\\x \\mid \\thet_0) = \\eta$, then it doesn't matter if $\\delta(\\X)$ rejects the null hypothesis or not.^[For the measure theory fans, this equality holds with measure zero, so we can just sweep it under the rug.] \n2. A more general version of the result can be found in @bickel2015mathematical or @lehmann2005testing, and concerns non-deterministic decision rules which the reject the null hypothesis with some probability when $T(\\X) \\in C$ (instead of with probability one)\n3. The result says nothing about the uniqueness of the MP test.\n\nTheorem \\@ref(thm:NPlemma) tells us that for simple hypotheses, our test statistic should be $T(\\X) = \\frac{f_\\X(\\x\\mid\\thet_1)}{f_\\X(\\x\\mid\\thet_0)}$, which makes a fair bit of sense. If we observe $\\x$, then $f_\\X(\\x\\mid\\thet_1)$ and $f_\\X(\\x\\mid\\thet_0)$ are the probabilities we observe $\\x$ given $\\thet_1$ and $\\thet_0$, respectively. In the event that $f_\\X(\\x\\mid\\thet_1) \\gg f_\\X(\\x\\mid\\thet_0)$, it's so likely that $\\thet = \\thet_1$, that we should reject the null hypothesis. In other words, we reject the null hypothesis is the ratio of these probabilities is high enough. This ratio actually pops up elsewhere in statistics and has its own name.\n\n:::{#def-}\nThe <span style=\"color:red\">**_likelihood ratio_**</span> associated with densities $f_\\X(\\x\\mid\\thet_1)$ and  $f_\\X(\\x\\mid\\thet_0)$ is defined as \n$$ L(\\thet_1, \\thet_0 \\mid \\x) = \\frac{f_\\X(\\x\\mid\\thet_1)}{f_\\X(\\x\\mid\\thet_0)}.$$ \n:::\n\nSo how large does this ratio need to be such that we reject the null hypothesis? The Neyman-Pearson Lemma seems a bit vague here, as it only says that it needs to exceed *some* $\\eta$. The proof gives us some mathematical context on $\\eta$, but also fails to explicitly define it, so can we really pick *any* constant? Of course not, because we assume that $\\delta(\\X)$ has size $\\alpha$. The actual value $\\eta$ is implicitly given when we assume the size of $\\delta(\\X)$, but we can define is explicitly.\n\\begin{align*}\n& \\alpha = \\Pr(T(\\X) > \\eta \\mid P_\\thet = P_{\\thet_0})\\\\\n\\implies & \\alpha = 1 - \\Pr(T(\\X) \\le \\eta \\mid P_\\thet = P_{\\thet_0})\\\\\n\\implies & \\alpha = 1 - F_{T(\\X)}(\\eta \\mid P_{\\thet_0})\\\\\n\\implies & \\eta = F_{T(\\X)}^{-1}(1-\\alpha \\mid P_{\\thet_0})\n\\end{align*}\n\nLet's see the Neyman-Pearson Lemma in action. \n\n:::{#exm-npnorm}\nSuppose $X_i \\iid N(\\mu,\\sigma^2)$ for a known $\\sigma^2$, and \n\\begin{align*}\nH_0:\\mu=\\mu_0,\\\\\nH_1:\\mu=\\mu_1,\n\\end{align*}\nwhere $\\mu_1 > \\mu_0$. Our likelihood ratio is \n\\begin{align*}\n\\frac{f_\\X(\\x\\mid\\mu_1)}{f_\\X(\\x\\mid\\mu_0)} & = \\frac{\\prod_{i=1}^nf_{X_i}(x\\mid\\mu_1)}{\\prod_{i=1}^nf_{X_i}(x\\mid\\mu_0)} \\\\\n& = \\frac{\\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{(x-\\mu_1)^2}{2\\sigma^2}\\right]}{\\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{(x-\\mu_0)^2}{2\\sigma^2}\\right]}\\\\\n& = \\frac{\\prod_{i=1}^n\\exp\\left[-\\frac{(x-\\mu_1)^2}{2\\sigma^2}\\right]}{\\prod_{i=1}^n\\exp\\left[-\\frac{(x-\\mu_0)^2}{2\\sigma^2}\\right]}\\\\\n& = \\frac{\\exp\\left[-\\sum_{i=1}^n\\frac{(x-\\mu_1)^2}{2\\sigma^2}\\right]}{\\exp\\left[-\\sum_{i=1}^n\\frac{(x-\\mu_0)^2}{2\\sigma^2}\\right]}\\\\ \n& = \\exp\\left[\\frac{1}{2\\sigma^2}\\left(\\sum_{i=1}^n(x_i - \\mu_0)^2 - \\sum_{i=1}^n(x_i - \\mu_1)^2\\right)\\right]\\\\ \n& = \\exp\\left[\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i^2 - x_i\\mu_0 + \\mu_0^2 - x_i^2  - x_i\\mu_1 + \\mu_1^2)\\right]\\\\\n& = \\exp\\left[\\frac{1}{2\\sigma^2}[n(\\mu_0^2 - \\mu_1^2) - 2n\\bar x(\\mu_0 - \\mu_1)]\\right].\n\\end{align*}\nThe Neyman-Pearson lemma says the critical region of our test should take the form\n\\begin{align*}\nC & = \\left\\{\\x\\ \\Big| \\  \\exp\\left[\\frac{1}{2\\sigma^2}[n(\\mu_0^2 - \\mu_1^2) - 2n\\bar x(\\mu_0 - \\mu_1)]\\right] > \\eta\\right\\}\\\\\n  & = \\left\\{\\x\\ \\Big| \\ \\bar x > \\frac{\\mu_0+\\mu_1}{2} - \\frac{\\sigma^2 \\ln \\eta}{n(\\mu_0-\\mu_1)}\\right\\}\n\\end{align*}\n...okay so this looks like a monstrosity. Let's define the constant $\\eta^*$ to as\n$$\\eta^* = \\frac{\\mu_0+\\mu_1}{2} - \\frac{\\sigma^2 \\ln \\eta}{n(\\mu_0-\\mu_1)},$$ as to give us \n$$ C = \\{\\x \\mid \\bar x > \\eta^*\\}.$$\nIf we want our test to have a size of $\\alpha$, we let $\\eta^* = \\mu_0 +\\Phi^{-1}(1-\\alpha) \\frac{\\sigma}{\\sqrt n}$, which is the same one sided test we've been exploring in this section! Therefore the most powerful test for $H_0 : \\mu = \\mu_1$ versus $H_1 : \\mu = \\mu_1$ is: \n\n\\begin{align*}\n\\delta(\\X) & = \\begin{cases}\\mu_0 & \\bar X < \\mu_0 +\\Phi^{-1}(1-\\alpha) \\frac{\\sigma}{\\sqrt n} \\\\ \\mu_1 & \\bar X > \\mu_0 +\\Phi^{-1}(1-\\alpha) \\frac{\\sigma}{\\sqrt n}\\end{cases} & (T(\\X) = \\bar X)\\\\\n& = \\begin{cases}\\mu_0 & \\frac{\\bar X - \\mu_0}{\\sigma/\\sqrt n} < \\Phi^{-1}(1-\\alpha) \\\\ \\mu_1 & \\frac{\\bar X - \\mu_0}{\\sigma/\\sqrt n} > \\Phi^{-1}(1-\\alpha) \\end{cases}& (T(\\X) = (\\bar X - \\mu_0)/(\\sigma/\\sqrt n))\n\\end{align*}\n\nThese decision rules are equivalent to the likelihood ratio test given by the Neyman-Pearson lemma. If we want to keep with the spirit of the lemma and insist on using the test statistic $T(\\X) = \\frac{f_\\X(\\x\\mid\\mu_1)}{f_\\X(\\x\\mid\\mu_0)}$, we need to solve for $\\eta$.\n\n\\begin{align*}\n&\\eta^* = \\mu_0 +\\Phi^{-1}(1-\\alpha) \\frac{\\sigma}{\\sqrt n}\\\\\n\\implies & \\frac{\\mu_0+\\mu_1}{2} - \\frac{\\sigma^2 \\ln \\eta}{n(\\mu_0-\\mu_1)} = \\mu_0 +\\Phi^{-1}(1-\\alpha) \\frac{\\sigma}{\\sqrt n}\\\\\n\\implies & \\eta = \\exp\\left[-\\left(\\mu_0 +\\Phi^{-1}(1-\\alpha) \\frac{\\sigma}{\\sqrt n}\\right)\\left(\\frac{n(\\mu_0-\\mu_1)}{\\sigma^2}\\right)\\right]\n\\end{align*}\n\nThis gives the decision rule \n\\begin{align*}\n\\delta(\\X) &= \\begin{cases}\\mu_0 & \\frac{f_\\X(\\x\\mid\\mu_1)}{f_\\X(\\x\\mid\\mu_0)} < \\exp\\left[-\\left(\\mu_0 +\\Phi^{-1}(1-\\alpha) \\frac{\\sigma}{\\sqrt n}\\right)\\left(\\frac{n(\\mu_0-\\mu_1)}{\\sigma^2}\\right)\\right] \\\\ \\mu_1 & \\frac{f_\\X(\\x\\mid\\mu_1)}{f_\\X(\\x\\mid\\mu_0)} >\\exp\\left[-\\left(\\mu_0 +\\Phi^{-1}(1-\\alpha) \\frac{\\sigma}{\\sqrt n}\\right)\\left(\\frac{n(\\mu_0-\\mu_1)}{\\sigma^2}\\right)\\right]\\end{cases} & (T(\\X) = f_\\X(\\x\\mid\\mu_1)/f_\\X(\\x\\mid\\mu_0))\n\\end{align*}\n\nTo confirm these three test are equivalent, let's simulate the decision rule for $H_0:\\mu = 0$ versus $H_1: \\mu = 3$, where $\\sigma^2 = 1$, $n = 100$, $\\alpha = 0.05$, and null hypothesis $\\mu = 0$ is true. Not only should the tests agree for each simulation, but we should also see that we reject the null hypothesis (commit a type I error) with probability $\\alpha$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_mean_test <- function(X, mu0, alpha, sigma){\n  n <- length(X)\n  test_stat <- function(X) {\n    mean(X)\n  }\n  test_stat(X) > mu0 + qnorm(1-alpha)*(sigma/sqrt(n))    \n}\n\nz_score_test <- function(X, mu0, alpha, sigma){\n  n <- length(X)\n  test_stat <- function(X) {\n    (mean(X) - mu0)/(sigma/sqrt(n))\n  }\n  test_stat(X) > qnorm(1-alpha)   \n}\n\nlikelihood_ratio_test <- function(X, mu0, mu1, alpha, sigma){\n  n <- length(X)\n  test_stat <- function(X) {\n    prod(dnorm(X, mu1, sigma)) / prod(dnorm(X, mu0, sigma))\n  }\n  eta <- exp(-(mu0 + qnorm(1-alpha)*(sigma/sqrt(n)) - (mu1+mu0)/2)*((n*(mu0-mu1)) /(sigma^2)))\n  test_stat(X) > eta\n}\n\niter <- function(mu0, mu1, alpha, sigma, n, dist, dist_params, s){\n  X <- do.call(dist, append(n, dist_params))\n  output <- tibble(\n    smt = sample_mean_test(X, mu0, alpha, sigma),\n    zst = z_score_test(X, mu0, alpha, sigma),\n    lrt = likelihood_ratio_test(X, mu0, mu1, alpha, sigma),\n    iter_num = s\n  )\n  return(output)\n}\n\nsim <- function(N, mu0, mu1, alpha, sigma, n, dist, dist_params){\n  output <- 1:N %>% \n    map(\n      iter, \n      mu0 = mu0, \n      mu1 = mu1, \n      alpha = alpha, \n      sigma = sigma, \n      n = n, \n      dist = dist, \n      dist_params = dist_params\n    ) %>% \n    bind_rows()\n  return(output)\n}\n\nresults <- sim(1e5, 0, 3, 0.05, 1, 100, rnorm, list(0,1))\n\n#If all decisions were the same, this should be 1\nresults %>% \n  summarize(prob = mean(smt == zst & zst == lrt))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n   prob\n  <dbl>\n1     1\n```\n\n\n:::\n\n```{.r .cell-code}\n#Check number of rejections of the true null hypothesis\nmean(results$smt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05058\n```\n\n\n:::\n:::\n\nThis example is particularly special, because we were able to write the most powerful test in a form that did not depend on $\\mu_1$:\n$$\\delta(\\X) =\\begin{cases}\\mu_0 & \\frac{\\bar X - \\mu_0}{\\sigma/\\sqrt n} > \\Phi^{-1}(1-\\alpha) \\\\ \\mu_1 & \\frac{\\bar X - \\mu_0}{\\sigma/\\sqrt n} < \\Phi^{-1}(1-\\alpha) \\end{cases}$$ The test statistic and critical region do not depend on $\\mu_1$, so this test is the most powerful test for any alternative $\\mu_1 > \\mu_0$. This means that this test is the most powerful test for any hypothesis of the form $H_0:\\mu = \\mu_0$ versus $\\mu \\ge \\mu_0$. In fact, we can even go one step further -- this test is the most powerful test for any hypotheses of the form $H_0:\\mu\\le \\mu_0$ versus $H_1:\\mu > \\mu_0$! Consider testing $H_0:\\mu = \\mu_0$ versus $H_1:\\mu \\ge \\mu_0$, and testing the modified hypothesis $H_0':\\mu = \\mu_0'$ versus $H_1':\\mu \\ge \\mu_0'$, where $\\mu_0'<\\mu_0$.\nAll we've done is slightly tweaked $H_0 : \\mu \\le \\mu_0$ by lowering the value of $\\mu_0'$. What happens if we attempt to test $H_0'$ versus $H_1'$ using the test statistic for $\\delta(\\X)$ (as given above) instead of the test statistic of its modified counterpart $\\delta'(\\X)$? \n\\begin{align*}\nT(\\X)&=\\frac{\\bar X - \\mu_0}{\\sigma/\\sqrt n}\\\\\nT'(\\X)&=\\frac{\\bar X - \\mu_0'}{\\sigma/\\sqrt n}\n\\end{align*}\nUnder $H_0'$: \n\\begin{align*}\nT'(\\X) & \\sim N(0,1)\\\\\nT(\\X) & = \\frac{\\bar X - \\mu_0'}{\\sigma/\\sqrt n} + \\frac{\\mu_0' - \\mu_0}{\\sigma/\\sqrt n} = T'(\\X) + \\frac{\\mu_0' - \\mu_0}{\\sigma/\\sqrt n} \\sim N\\left(\\frac{\\mu_0' - \\mu_0}{\\sigma/\\sqrt n}, 1\\right)\n\\end{align*}\n\nIf we calculate the power of $\\delta'$, we find that\n\\begin{align*}\nc' & = -\\Phi^{-1}(\\alpha),\\\\\nc & = -\\left[\\Phi^{-1}\\left(\\alpha\\right) - \\frac{\\mu_0' - \\mu_0}{\\sigma/\\sqrt n}\\right] = \\frac{\\mu_0' - \\mu_0}{\\sigma/\\sqrt n}- \\Phi^{-1}(\\alpha).\n\\end{align*}\nThe power of the tests are \n\\begin{align*}\n\\beta(\\delta, \\mu) & = \\Pr\\left(T(\\X) > \\frac{\\mu_0' - \\mu_0}{\\sigma/\\sqrt n}- \\Phi^{-1}(\\alpha)\\ \\bigg| \\ \\mu\\right)\\\\\n& = \\Pr\\left(T(\\X) - \\frac{\\mu_0' - \\mu_0}{\\sigma/\\sqrt n} >  \\Phi^{-1}(\\alpha)\\ \\bigg| \\ \\mu\\right)\\\\\n& = \\Pr\\left(T'(\\X) > - \\Phi^{-1}(\\alpha)  \\mid \\mu\\right)\\\\\n& = \\beta(\\delta', \\mu).\n\\end{align*}\nBoth tests have the same power, and same size, so $\\delta$ is the most powerful test for any $H_0':\\mu \\neq \\mu_0'$ versus $H_1':\\mu > \\mu_0'$ where $\\mu_0'<\\mu_0$. We can confirm this with simulations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncurve <- power_curve(\n  domain = seq(2.6, 3.3, length = 10), ## update to 40\n  N = 1e3, ## update to 1e4\n  delta = delta, \n  test_stat = test_stat,\n  critical_region = list(c(-qnorm(0.05), Inf)),\n  mu_0 = 2.7,\n  sigma = 1, \n  n = 100\n) %>% \n  mutate(decision = \"δ\")\n\ncurve_prime <- power_curve(\n  domain = seq(2.6, 3.3, length = 10), ## update to 40\n  N = 1e3 , ## update to 1e4\n  delta = delta, \n  test_stat = test_stat,\n  critical_region = list(c((2.7 - 3)/(1 / sqrt(100)) - qnorm(0.05), Inf)),\n  mu_0 = 3,\n  sigma = 1, \n  n = 100\n) %>% \n  mutate(decision = \"δ'\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code which generates figure\"}\ncurve %>% \n  bind_rows(curve_prime) %>% \n  ggplot(aes(t, simulated_power, color = decision)) +\n  geom_line() +\n  scale_color_manual(values = c(\"blue\", \"red\")) +\n  theme_minimal() +\n  labs(x = \"μ\", y = \"Simulated Power\", color = \"\") +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Simulated power curve for the two null hypotheses](testing_files/figure-html/fig-plot36-1.png){#fig-plot36 fig-align='center' width=768}\n:::\n:::\n\n\n\nThis will hold for all hypotheses $H_1':\\mu\\le \\mu_0'$ where $\\mu_0'$, as $\\mu_0'$ was arbitrary. This means that if we are testing $H_0:\\mu \\le \\mu_0$ versus $H_1:\\mu>\\mu_0$, the most powerful test is the z-test, regardless of the specified $\\mu_0$ or alternative $\\mu$. Because we pick the test that was derived using $H_0:\\mu = \\mu_0$, we often write \n\\begin{align*}\nH_0&:\\mu=\\mu_0,\\\\\nH_1&:\\mu>\\mu_0.\n\\end{align*}\n:::\n\nWhile this example started as an application of the Neyman-Pearson lemma, it took several interesting turns. Firstly, while the Neyman-Pearson lemma gives the most powerful test in terms of the likelihood ratio, we were able to find several equivalent tests. Secondly, the most powerful test for the simple hypothesis $H_0: \\mu = \\mu_0$ versus $H_1:\\mu = \\mu_1$ (where $\\mu_1 > \\mu_0$), as given by the Neyman-Pearson lemma, did not depend on $\\mu_1$. This meant it was the most powerful test for $H_0: \\mu = \\mu_0$ versus $H_1:\\mu > \\mu_0$. It is uniform in its status as the most powerful test.\n\n:::{#def-}\nA decision rule $\\delta(\\X)$, with size $\\alpha$, is the  <span style=\"color:red\">**_uniformly most powerful (UMP)_**</span> test for $H_0: P_{\\thet}\\in\\mathcal P_0$ versus $H_1: P_{\\thet}\\in\\mathcal P_1$ if \n$$ \\beta(\\delta,P_\\thet) \\ge \\beta(\\delta',P_\\thet) \\ \\ \\ \\text{for all }P_\\thet\\in\\mathcal P_1$$\nfor all other decision rules $\\delta'$ with a size of at least $\\alpha$.\n:::\n\nWe also were able to establish that the UMP test for $H_0: \\mu = \\mu_0$ versus $H_1:\\mu > \\mu_0$, is the UMP for any test of the form $H_0:\\mu \\le \\mu_0$ versus $H_1:\\mu >\\mu_0$. Despite the Neyman-Pearson lemma only holding for simple hypotheses, we were able to derive the optimal (in the sense of power) test for composite hypotheses in this case. Is this always the case, or was there something special at work? Let's look at another example to get a better lay of the land. \n\n:::{#exm-}\nSuppose we draw a single observation of $X$ where $X\\sim \\text{Cauchy}(\\theta,1)$ and want to test $H_0:\\theta = 0$ versus $H_1:\\theta = \\theta_1$. In this case\n$$f_X(x\\mid\\theta)= \\frac{1}{\\pi(1+(x-\\theta)^2)},$$ so the Neyman-Pearson lemma tells us the most powerful test is \n$$\\delta(X) = \\begin{cases}0 & \\frac{1+x^2}{1+(x-\\theta_1)^2} < \\eta\\\\ \\theta_1 & \\frac{1+x^2}{1+(x-\\theta_1)^2} > \\eta\\end{cases}.$$\nTo get a better sense of what the critical region for this test is, let's plot the likelihood ratio. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code which generates figure\"}\nexpand_grid(\n  x = seq(-5, 10, length = 3000), \n  alt_par = c(1, 1.2, 1.4,1.6,1.8,2)\n) %>% \n  mutate(y = dcauchy(x, alt_par, 1)/dcauchy(x, 0, 1)) %>% \n  ggplot(aes(x,y, color = as.factor(alt_par))) + \n  geom_line() +\n  ylab(\"Likelihood Ratio\") +\n  xlab(\"Value of Single Observation\") +\n  theme_minimal() +\n  geom_hline(yintercept = 2, color = \"black\", linetype = \"dashed\") +\n  labs(color = \"Alternative θ\") +\n  theme(legend.position = \"bottom\") +\n  annotate(\"text\", x = -3, y = 2.3, label = \"η\")\n```\n\n::: {.cell-output-display}\n![The liklihood ratio associated with the Cauchy distribution. The liklihood ratio test tells us to reject the null hypothesis whenever the ratio exceeds η](testing_files/figure-html/fig-plot37-1.png){#fig-plot37 fig-align='center' width=768}\n:::\n:::\n\n\nOur rejection region will be a bounded interval. After some quick algebra, we find that the critical region is \n$$C = \\left\\{x\\ \\Bigg| \\ \\frac{\\eta\\theta_1-\\sqrt{\\eta\\theta_1^2+2\\eta-\\eta^2-1}}{\\eta-1}< x <\\frac{\\eta\\theta_1+\\sqrt{\\eta\\theta_1^2+2\\eta-\\eta^2-1}}{\\eta-1} \\right\\}.$$ Fixing $\\eta$ to be some value, say $\\eta = 2$, we can plot these critical regions across values of $\\theta_1$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code which generates figure\"}\neta <- 2\nexpand_grid(par = c(1,1.2,1.4,1.6,1.8,2)) %>% \n  mutate(\n    lower = (eta*par - sqrt(eta*par^2+2*eta-eta^2-1)) / (eta - 1),\n    upper = (eta*par + sqrt(eta*par^2+2*eta-eta^2-1)) / (eta - 1),\n    val = (upper + lower)/2\n  ) %>% \n  ggplot(aes(par, val)) +\n  geom_linerange(aes(ymin = lower, ymax = upper)) +\n  theme_minimal() +\n  labs(x = \"Alternative θ\", y = \"Critical Region\")\n```\n\n::: {.cell-output-display}\n![The critical regions associated with the liklihood ratio test for varying alternatives. None of the critial regions are subsets of others.](testing_files/figure-html/fig-plot38-1.png){#fig-plot38 fig-align='center' width=768}\n:::\n:::\n\nThe upper and lower bounds depend on the alternate $\\theta_1$ so the most powerful test for one choice of $\\theta_1$ fails to be the most powerful test for another $\\theta_1'$.\n:::\n\nSo what makes this different from the example with the normal distribution? Fixing $H_0:\\mu_0 = 0$, $\\sigma^2 = 1$, and $n=1$, Let's see if the likelihood ratio for normally distributed data has any clues.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code which generates figure\"}\nexpand_grid(\n  x = seq(0, 3, length = 3000), \n  alt_par = c(1, 1.2, 1.4,1.6,1.8,2)\n) %>% \n  mutate(y = dnorm(x, alt_par, 1)/dnorm(x, 0, 1)) %>% \n  ggplot(aes(x,y, color = as.factor(alt_par))) + \n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Value of Single Observation\", y = \"Likelihood Ratio\", color = \"Alternative μ\") +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![The liklihood ratio associated with the standard normal distribution](testing_files/figure-html/fig-plot39-1.png){#fig-plot39 fig-align='center' width=768}\n:::\n:::\n\n\nNow let's plot the critical region \n$$ C = \\left\\{x \\mid x > \\mu_1/2 + \\ln \\eta / \\mu_1\\right\\}$$\nfor varying alternatives $\\mu_1$, fixing $\\eta = 2$ (which is equivalent to fixing the size $\\alpha$).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code which generates figure\"}\nexpand_grid(par = c(1,1.2,1.4,1.6,1.8,2)) %>% \n  mutate(\n    lower = par/2 - (log(2))/(par),\n    upper = Inf,\n    val = (upper + lower)/2\n  ) %>% \n  ggplot(aes(par, val)) +\n  geom_linerange(aes(ymin = lower, ymax = upper)) +\n  theme_minimal() +\n  labs(x = \"Alternative μ\", y = \"Critical Region\")\n```\n\n::: {.cell-output-display}\n![ The critical regions associated with the liklihood ratio test for varying alternatives. As we increase the alternative μ, each critical region is a subset of the prior one.](testing_files/figure-html/fig-plot310-1.png){#fig-plot310 fig-align='center' width=768}\n:::\n:::\n\nAs $\\mu_1$ increases, each critical region is a subset of the prior. In other words, for $\\mu_1' > \\mu_1$,\n$$ x > \\frac{\\mu_1'}{2} + \\frac{\\ln \\eta}{\\mu_1'} \\implies x > \\frac{\\mu_1}{2} + \\frac{\\ln \\eta}{\\mu_1}.$$ If we reject the null hypothesis for $\\mu_1'$, we will reject it for $\\mu_1$. If we want to pick the critical region that gives us the most power out of these options (subject to the fixed size $\\alpha$), we should pick the largest one, as it maximizes our chance of rejecting the null hypothesis, and is is a superset of the other choices of critical region. It is essential that the critical regions nest in one another like this as. Such sets are often called a monotonic sequence of sets, which is interesting, as the corresponding likelihood ratio is monotonically increasing for any $\\theta_1>\\theta_0$. This monotonicity turns out to be the key to extending the Neyman-Pearson lemma to composite hypotheses.\n\n:::{#def-}\nA density $f_{\\X}(\\x\\mid \\theta)$ has a <span style=\"color:red\">**_monotone likelihood ratio (MLR) with respect to a statistic $T$_**</span> if $f(\\x \\mid \\theta_1)/f(\\x \\mid \\theta_0)$ can be written as $f_{\\X}(T(\\x) \\mid \\theta_1)/f_{\\X}(T(\\x) \\mid \\theta_0)$, and $f_{\\X}(T(\\x) \\mid \\theta_1)/f_{\\X}(T(\\x) \\mid \\theta_0)$ is a monotonically increasing function in $T(\\x)$ for $\\theta_1 > \\theta_0$.\n:::\n\nIn Example \\@ref(exm:npnorm), the normal distribution has an increasing MLR in the statistic $T(\\X) = \\bar X$.\n\n:::{#thm-KR}\n\n## Karlin-Rubin Theorem\n\nSuppose $f_{\\X}(\\x\\mid \\theta)$ has an increasing MLR in the statistic $T$. The decision rule $\\delta(\\X)$ defined as \n$$ \\delta(\\X) = \\begin{cases} \\mathcal P_1 & T(\\X) > \\eta \\\\\n\\mathcal P_0  & T(\\X) < \\eta\n\\end{cases}$$ is the UMP test for $H_0: \\theta \\le \\theta_0$ versus $H_1:\\theta > \\theta_0$. Additionally, the power function $\\beta(\\delta, \\theta)$ is monotonically increasing. Analogously, \n$$ \\delta(\\X) = \\begin{cases} \\mathcal P_1 & T(\\X) < \\eta \\\\\n\\mathcal P_0  & T(\\X) > \\eta\n\\end{cases}$$ is the UMP test for $H_0: \\theta \\ge \\theta_0$ versus $H_1:\\theta < \\theta_0$, and the power function is monotonically decreasing.\n:::\n\n:::{.proof}\nSee @degroot2012probability.\n:::\n\n\n\n:::{#exm- name=\"Testing Variance\"}\nSuppose we want to test $H_0:\\sigma^2 \\ge \\sigma_0^2$ versus $H_1:\\sigma^2 < \\sigma_0^2$ where $X_i \\iid N(\\mu,\\sigma^2)$ for a known $\\mu$. For $\\sigma_1^2 < \\sigma_0^2$, the likelihood ratio of the joint distribution of our data is \\begin{align*}\n\\frac{f_\\X(\\x\\mid\\sigma_1^2)}{f_\\X(\\x\\mid\\sigma_0^2)} & = \\frac{\\prod_{i=1}^nf_{X_i}(x\\mid\\sigma_1^2)}{\\prod_{i=1}^nf_{X_i}(x\\mid\\sigma_0^2)} \\\\ \n& = \\frac{\\left(\\frac{1}{\\sigma_1\\sqrt{2\\pi}}\\right)^n}{\\left(\\frac{1}{\\sigma_0\\sqrt{2\\pi}}\\right)^n}\\frac{\\exp\\left[-\\sum_{i=1}^n\\frac{(x_i-\\mu)^2}{2\\sigma_1^2}\\right]}{\\exp\\left[-\\sum_{i=1}^n\\frac{(x_i-\\mu)^2}{2\\sigma_0^2}\\right]}\\\\ & = \\left(\\frac{\\sigma_0}{\\sigma_1}\\right)^{n}\n\\exp \\left[\\sum_{i=1}^n\\frac{(x_i-\\mu)^2}{2\\sigma_0^2}-\\sum_{i=1}^n\\frac{(x_i-\\mu)^2}{2\\sigma_1^2}  \\right] \\\\\n& = \\left(\\frac{\\sigma_1^2}{\\sigma_0^2}\\right)^{-n/2}\n\\exp \\left[-\\frac{1}{2(\\sigma_1^2 - \\sigma_0^2)}\\sum_{i=1}^n(x_i-\\mu)^2 \\right]  \\\\\n& = \\left(\\frac{\\sigma_1^2}{\\sigma_0^2}\\right)^{-n/2}\n\\exp \\left[-\\frac{T(\\x)}{2(\\sigma_1^2 - \\sigma_0^2)} \\right] \n\\end{align*}\nIf we define $T(\\X) = \\sum_{i=1}^n(X_i-\\mu)^2$, then the likelihood ratio is monotonically decreasing for $\\sigma_1^2 < \\sigma_0^2$ (meaning $\\sigma_1^2 - \\sigma_0^2 < 0$).  \n$$ \\frac{\\partial}{\\partial T(\\x)}\\left[\\frac{f_\\X(\\x\\mid\\sigma_1^2)}{f_\\X(\\x\\mid\\sigma_0^2)}\\right] = -\\frac{1}{2(\\sigma_1^2 - \\sigma_0^2)}\\cdot \\left[\\frac{f_\\X(\\x\\mid\\sigma_1^2)}{f_\\X(\\x\\mid\\sigma_0^2)}\\right] > 0.$$\nThis means the UMP test takes the form \n$$\\delta(\\X) = \\begin{cases} \\sigma^2 < \\sigma_0^2 & \\sum_{i=1}^n(X_i-\\mu)^2 < \\eta \\\\\n\\mathcal \\sigma^2 \\ge \\sigma_0^2 & \\sum_{i=1}^n(X_i-\\mu)^2 > \\eta\n\\end{cases}.$$ If we define $\\eta^* = 1/\\sigma_0^2$, then \n$$\\delta(\\X) = \\begin{cases} \\sigma^2 < \\sigma_0^2 & \\sum_{i=1}^n\\left(\\frac{X_i-\\mu}{\\sigma_0}\\right)^2 < \\eta^* \\\\\n\\mathcal \\sigma^2 \\ge \\sigma_0^2 & \\sum_{i=1}^n\\left(\\frac{X_i-\\mu}{\\sigma_0}\\right)^2 > \\eta^*\n\\end{cases}.$$ Writing our test like this is far more useful, because $T(\\X)$ is now the sum of $n$ random variables which are distributed according to a standard normal distribution, i.e $T(\\X) \\sim \\chi^2_n$. This allows us to easily calculate $\\eta^*$ given a desired size $\\alpha$ using the quantile function for $T(\\X) \\sim \\chi^2_n$.\n$$\\delta(\\X) = \\begin{cases} \\sigma^2 < \\sigma_0^2 & \\sum_{i=1}^n\\left(\\frac{X_i-\\mu}{\\sigma_0}\\right)^2 < (\\chi^2_n)^{-1}(\\alpha) \\\\\n\\mathcal \\sigma^2 \\ge \\sigma_0^2 & \\sum_{i=1}^n\\left(\\frac{X_i-\\mu}{\\sigma_0}\\right)^2 > (\\chi^2_n)^{-1}(\\alpha)\n\\end{cases}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_stat <- function(X, mu, sigma_0){\n  sum(((X - mu)/sigma_0)^2)\n}\n\ndelta <- function(test_stat, critical_region, X, mu, sigma_0){\n  stat <- test_stat(X, mu, sigma_0)\n  decision <- critical_region %>% \n    map_lgl(\\(bounds) between(stat, bounds[1], bounds[2])) %>% \n    sum() %>% \n    as.logical()\n  \n  output <- tibble(\n    stat = stat,\n    decision = decision,\n    H_0 = sigma_0\n  )\n  return(output)\n}\n\ntest_iter <- function(delta, test_stat, critical_region, mu, sigma_0, n, dist, dist_params, s){\n  X <- do.call(dist, append(n, dist_params))\n  output <- delta(test_stat, critical_region, X, mu, sigma_0) %>% \n    mutate(iter = s)\n  return(output)\n}\n\ntest_sim <- function(N, delta, test_stat, critical_region, mu, sigma_0, n, dist, dist_params){\n  output <- 1:N %>% \n    map(\n      test_iter, \n      delta = delta, \n      test_stat = test_stat, \n      critical_region = critical_region, \n      mu = mu, \n      sigma_0 = sigma_0,\n      n = n, \n      dist = dist, \n      dist_params = dist_params\n    ) %>% \n    bind_rows()\n  return(output)\n}\n\n# This function assumes dist = rnorm and dist_params = list(0, sigma)\npower <- function(sigma, N, delta, test_stat, critical_region, mu, sigma_0, n){\n  dist <- rnorm\n  dist_params <- list(mu, sigma)\n  output <- test_sim(N, delta, test_stat, critical_region, mu, sigma_0, n, dist, dist_params) %>% \n    summarize(\n      simulated_power = mean(decision),\n      t = sigma\n    )\n  return(output)\n}\n\npower_curve <- function(domain, N, delta, test_stat, critical_region, mu, sigma_0, n){\n  output <- domain %>% \n    map(\n      power,\n      N = N,\n      delta = delta,\n      test_stat = test_stat,\n      critical_region = critical_region,\n      mu = mu,\n      sigma_0 = sigma_0,\n      n = n\n    ) %>% \n    bind_rows()\n  return(output)\n}\n\nresults <- power_curve(\n  domain = seq(0.65, 1.2, length = 10), ## change length to 40\n  N = 1e3, ## change to 1e4\n  delta = delta,\n  test_stat = test_stat,\n  critical_region = list(c(0, qchisq(0.05, 100))),\n  mu = 0,\n  sigma_0 = 1,\n  n = 100\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code which generates figure\"}\nresults %>% \n  ggplot(aes(t, simulated_power)) + \n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Alternate σ\", y = \"Power\")\n```\n\n::: {.cell-output-display}\n![Power curve associated with the liklihood ratio for variance](testing_files/figure-html/fig-plot311-1.png){#fig-plot311 fig-align='center' width=768}\n:::\n:::\n\n\n:::\n\n\n\nThe Karlin-Rubin Theorem is very powerful, but is limited. It only works for scalar parameters $\\theta_0$. The second we consider situations with a vector of parameters $\\thet_0$, or a situation where are model is semiparametric or nonparametric, UMP tests usually do not exist. The theorem also does not directly apply to two-sided tests. There is some disagreement among sources about whether UMP tests even exist for two-sided tests (@degroot2012probability and @casella2021statistical argue that they do not exist, while @lehmann2005testing say otherwise).\n\n:::{#exm- name=\"Two-Sided Z Test\"}\nSuppose we want to test $H_0:\\mu = \\mu_0$ versus $H_1:\\mu\\neq \\mu_0$ where $X_i \\iid N(\\mu,\\sigma^2)$ for a known $\\sigma^2$. To test this for some significance level $\\alpha$, we usually use a two-sided form of the $z-$test:\n\\begin{align*}\n\\delta(\\X) & = \\begin{cases}\\mu = \\mu_0 & \\Phi^{-1}(\\alpha/2)< \\frac{\\bar X-\\mu_0}{\\sigma/\\sqrt{n}}<\\Phi^{-1}(1-\\alpha/2)\\\\ \n\\mu \\neq \\mu_0 & \\frac{\\bar X-\\mu_0}{\\sigma/\\sqrt{n}}\\le \\Phi^{-1}(1-\\alpha/2) \\text{ or }\\frac{\\bar X-\\mu_0}{\\sigma/\\sqrt{n}}\\ge\\Phi^{-1}(1-\\alpha/2)\\end{cases}  \n = \\begin{cases}\\mu = \\mu_0 & \\abs{\\frac{\\bar X-\\mu_0}{\\sigma/\\sqrt{n}}}<\\Phi^{-1}(1-\\alpha/2)\\\\ \n\\mu \\neq \\mu_0 & \\abs{\\frac{\\bar X-\\mu_0}{\\sigma/\\sqrt{n}}}\\ge \\Phi^{-1}(1-\\alpha/2)\\end{cases}\n\\end{align*}\nConsider the \"right-handed\" and \"left-handed\" alternatives $\\delta_R(\\X)$ and $\\delta_L(\\X)$, respectively. Let's graph the power functions of these three tests for $\\alpha = 0.05$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code which generates figure\"}\ntibble(x = (-500:500)/100) %>% \n  mutate(\n    'Right Handed' = pnorm(-1.645 + x), \n    'Left Handed' = pnorm(-1.645 - x),\n    'Two Sided' = pnorm(-1.956 + abs(x))\n  ) %>% \n  gather(key = \"test\", value = \"power\", -x) %>% \n  ggplot(aes(x, power, color = test)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"(μ - μ0)/(σ√n)\", y = \"Power\", color = \"\") +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Power for three versions of the Z-test](testing_files/figure-html/fig-plot312-1.png){#fig-plot312 fig-align='center' width=768}\n:::\n:::\n\n\nWe see that $\\beta(\\delta_R) > \\beta(\\delta)$ when $\\mu > \\mu_0$ and $\\beta(\\delta_L) > \\beta(\\delta)$ when $\\mu < \\mu_0$, so clearly $\\delta$ is not a UMP. Despite this @lehmann2005testing would argue this test is UMP, because it actually has significance level $\\alpha/2$. For two sided tests in this setting, @lehmann2005testing define a size $\\alpha$ test to be one where \n$$ \\alpha(\\delta \\mid \\mu < \\mu_0) =  \\alpha(\\delta \\mid \\mu > \\mu_0) = \\alpha.$$\n:::\n\n\n:::{.remark}\nThe approach to testing proposed by @neyman1933 was/is not without controversy. Ronald Fisher, one of the central figures in the formalization of statistics, took issue with approach of Neyman and Pearson. A debate raged between the scientists as to the proper way of testing statistical hypotheses. The full details of this dispute is summarized by @lehmann1993fisher and @lehmann2011fisher. \n:::\n\n## Asymptotics\n\nUntil now, all of our examples have assumed $X_i \\iid N(\\mu,\\sigma^2)$. In this case, we were able to find the exact distribution of test statistics used to test hypotheses about $\\mu$ and $\\sigma$. This will usually not be possible, and we will need to use the tools developed in Section @sec-asy to determine the asymptotic distribution of test statistics. To highlight this, we will consider a situation where we can derive the distribution of a test statistic, but as $n\\to\\infty$, it converges in distribution such that there is virtually no harm done from using the asymptotic distribution instead.\n\n:::{#exm- name=\"t-Test\"}\n\nFor one final time, assume $X_i \\iid N(\\mu,\\sigma^2)$, *but* drop the assumption that $\\sigma^2$ is known. Assuming that we knew $\\sigma^2$ made little to no sense. If we don't know $\\mu$, then how would we possibly know $\\sigma^2$ (which is calculated using $\\mu$)?! If we want to test $H_0:\\mu = \\mu_0$ versus $H_1: \\mu \\neq \\mu_0$, which decision rule do we pick? We can no longer calculate the statistic $Z = \\frac{\\bar X - \\mu_0}{\\sigma/\\sqrt n}$, as we do not know $\\sigma$. What if we simply replaced $\\sigma$ with the (unbiased) sample standard deviation?\n$$T(\\X) = \\frac{\\bar X - \\mu_0}{s/\\sqrt n}$$\nBy replacing $\\sigma$ with $s$, we now have $T(\\X)\\not\\sim N(0,1)$, so we can no longer define the critical region using $\\Phi^{-1}$. Instead we have $T(\\X)\\sim t_{n-1}$,^[This follows from the definition of the student's $t$-distribution.] so we can still test $H_0$ versus $H_1$. This new test is the classic **_(student's) $t-$test_**, and we usually write the test statistic as $t = T(\\X)$. \n$$\\delta(\\X) = \\begin{cases} \\mu \\neq \\mu_0& \\abs{\\frac{\\bar X - \\mu_0}{s/\\sqrt n}} \\ge t_{n-1}^{-1}(1-\\alpha/2)\\\\ \\mu = \\mu_0 & \\abs{\\frac{\\bar X - \\mu_0}{s/\\sqrt n}} < t_{n-1}^{-1}(1-\\alpha/2)\\end{cases} $$\n\nBut does this difference in distribution really matter? In Section \\@ref(asymptotic-properties-of-estimators), one example highlighted that if $Y \\sim t_{n}$, then $Y\\dto N(0,1)$. This means that $t\\dto Z$, and the $t-$test is asymptotically equivalent to the $z-$test. To illustrate this, suppose $X_i \\iid N(4,1)$, $H_0:\\mu = 2$, and $H_1:\\mu \\neq 2$. For $\\alpha = 0.01$, we can compare the results of the two tests as $n$ increases.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Define t-test\nt_test <- function(X, mu_0, alpha){\n  n <- length(X)\n  T_stat <- (mean(X) - mu_0)/(sd(X)/sqrt(n))\n  abs(T_stat) > qt(1 - alpha/2, df = n - 1)\n}\n\n#Define an \"incorrect\" z-test using the sample standard deviation\nz_test <- function(X, mu_0, alpha){\n  n <- length(X)\n  Z <- (mean(X) - mu_0)/(sd(X)/sqrt(n))\n  abs(Z) > qnorm(1 - alpha/2)\n}\n\niter <- function(alpha, mu_0, n, mu, sigma, t){\n  X <- rnorm(n, mu, sigma)\n  output <- tibble(\n    iter_num = t,\n    agree = (t_test(X, mu_0, alpha) == z_test(X, mu_0, alpha))\n  )\n  return(output)\n}\n\nsim <- function(N, alpha, mu_0, n, mu, sigma){\n  output <- 1:N %>% \n    map(iter, alpha = alpha, mu_0 = mu_0, n = n, mu = mu, sigma = sigma) %>% \n    bind_rows() %>% \n    mutate(sample_size = n)\n  return(output)\n}\n\nouter_sim <- function(n_vals, N, alpha, mu_0, mu, sigma){\n  output <- n_vals %>% \n    map(sim, N = N, alpha = alpha, mu_0 = mu_0, mu = mu, sigma = sigma) %>% \n    bind_rows()\n  return(output)\n}\n\nresults <- outer_sim(2:15, 1e4, 0.01, 2, 4, 1)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code which generates figure\"}\nresults %>% \n  group_by(sample_size) %>% \n  summarize(prop = sum(agree)/n()) %>% \n  ggplot(aes(sample_size, prop)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Sample Size, n\", y = \"Frequency of Tests Agreeing over 10000 Simulations\")\n```\n\n::: {.cell-output-display}\n![As the sample size increases, the frequency at which the Z-test and t-test agree increase](testing_files/figure-html/fig-plot313-1.png){#fig-plot313 fig-align='center' width=768}\n:::\n:::\n\n\nEven for modest values of $n$, the tests return the same results for 10,000 simulations. \n:::\n\nWhile this example illustrates how we can leverage asymptotics when testing hypotheses, we weren't \"required\" by any means to take advantage of the fact that $t\\dto N(0,1)$, because we knew that $t\\sim t_{n-1}$. Unfortunately, $t\\sim t_{n-1}$ will only hold if $X_i\\iid N(\\mu,\\sigma^2)$. Once we drop this assumption we must rely on the fact that $t \\dto N(0,1)$ \n\n\n:::{#exm- name=\"Non-normal Data\"}\n\nSuppose $X_i\\iid F_X$, and we want to test $H_0:\\mu = \\mu_0$ versus $H_1: \\mu \\neq \\mu_0$. The (irregular) model $\\mathcal P$ is a collection of sets, each comprised of all the distributions with a common expected value $\\mu$.\n$$P_\\mu = \\left\\{f_\\X(\\x)\\ \\Bigg|\\ f_\\X(\\x) = \\prod_{i=1}^n f_{X_i}(x) \\text{ and } f_{X_i} = f_{X_j}\\ \\forall i,j \\text{ and }\\E{X_i}=\\mu\\ \\forall i\\right\\}$$In this case, the CLT tells us that $\\sqrt{n}(\\bar X - \\mu_0) \\dto N(0,1)$, so\n$$ t = \\frac{(\\bar X - \\mu_0)}{S/\\sqrt{n}} = \\frac{\\sqrt{n}(\\bar X - \\mu_0)}{S}  \\dto  \\frac{N(0,\\sigma^2)}{S} = t_{n-1}.$$ Even though we do not know the actual distribution of $T$, we know the asymptotic distribution, and can calculate critical regions when $n$ is sufficiently large. To demonstrate this, let's simulate $\\Pr(T\\in C\\mid \\mu=\\mu_0)$ for $$\\delta(\\X) = \\begin{cases} \\mu \\neq \\mu_0& \\abs{\\frac{\\bar X - \\mu_0}{s/\\sqrt n}} \\ge t_{n-1}^{-1}(1-\\alpha/2)\\\\ \\mu = \\mu_0 & \\abs{\\frac{\\bar X - \\mu_0}{s/\\sqrt n}} < t_{n-1}^{-1}(1-\\alpha/2)\\end{cases}$$ when $X_i \\iid \\text{Exp}(1/\\mu)$ (such that $\\E{X_i}=\\mu$). As $n\\to\\infty$, we should see $\\Pr(T\\in C\\mid \\mu=\\mu_0)\\to \\alpha$, indicating that $t_{n-1}$ does indeed provide a sufficient approximation to calculate critical values. We will test $H_0:\\mu=2$ versus $H_0:\\mu=8$ where $\\alpha = 0.05$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_test <- function(alpha, mu_0, n, dist, dist_params, t){\n  X <- do.call(dist, append(n, dist_params))\n  output <- tibble(\n    decision = t_test(X, mu_0, alpha),\n    iter_num = t\n  )\n}\n\nsimulate_alpha_at_n <- function(N, alpha, mu_0, n, dist, dist_params){\n  output <- 1:N %>% \n    map(simulate_test, alpha = alpha, mu_0 = mu_0, n = n, dist = dist, dist_params = dist_params) %>% \n    bind_rows() %>% \n    summarize(\n      alpha = mean(decision),\n      sample_size = n\n    )\n  return(output)\n}\n\nsimulate_alpha_over_n <- function(n_vals, N, alpha, mu_0, dist, dist_params){\n  output <- n_vals %>% \n    map(simulate_alpha_at_n, N = N, alpha = alpha, mu_0 = mu_0, dist = dist, dist_params) %>% \n    bind_rows()\n  return(output)\n}\n\nresults <- simulate_alpha_over_n(\n  n_vals = (1:50)*10, \n  N = 1e2,  # increase to 1e5\n  alpha = 0.05, \n  mu_0 = 2, \n  dist = rexp, \n  dist_params = list(1/2)\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code which generates figure\"}\nresults %>% \n  ggplot(aes(sample_size, alpha)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Sample Size, n\", y = \"Simulated α\") +\n  geom_hline(yintercept = 0.05, color = \"red\", linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![Despite our data being drawn from an exponential distribution, the test statistic is asympotically distributed according to the t-distribution, so the simulated size approaches the theoretical size of 0.05](testing_files/figure-html/fig-plot314-1.png){#fig-plot314 fig-align='center' width=768}\n:::\n:::\n\n\nWe also have that $t\\dto N(0,1)$ when $X_i\\iid F_X$. We have $S\\to\\sigma$, so by Slutsky's theorem \n$$ t = \\underbrace{\\sqrt{n}(\\bar X - \\mu_0)}_{\\dto N(0,\\sigma^2)} / \\underbrace{S}_{\\pto \\sigma} \\dto N(0,1).$$ If we wanted to, we could still calculate critical values using the standard normal distribution, even with non-normal data, but those calculated with $t_{n-1}$ would be more slightly more accurate (with this advantage quickly diminishing as $n\\to\\infty$). \n\n:::\n\nIn practice, the overwhelming majority of our tests will rely on asymptotic distributions of test statistics. What does this mean when considering the power of a test? The Neyman-Pearson lemma and Karlin-Rubin theorem assumed a fixed sample size. In this sense, we were consider finite sample properties of tests, just like how we considered finite sample properties of estimators in \\@ref(finite-sample-properties-of-estimators). This was a bit shortsighted considering the effect sample size has on power. \n\n:::{#exm- name=\"Power and Sample Size\"}\nConsider testing $H_0:\\mu \\le \\mu_0$ versus $H_1:\\mu > \\mu_0$ using the $z-$test. where $X_i\\iid N(\\mu,\\sigma^2)$. The power of this test is $$\\beta(\\mu) = \\Phi\\left(-1.645 + \\frac{\\mu-\\mu_0}{\\sigma / \\sqrt{n}} \\right).$$ Power is increasing in $n$ as \n$$ \\frac{\\partial \\beta}{\\partial n} = \\frac{\\mu-\\mu_0}{2\\sigma\\sqrt n}\\varphi\\left(-1.645 + \\frac{\\mu-\\mu_0}{\\sigma / \\sqrt{n}} \\right) > 0.$$\n\nIf we plot $\\beta(\\mu)$ for various sample sizes, it appears that that $\\beta(\\mu \\mid \\mu > \\mu_0) \\to 1$, and $\\beta(\\mu \\mid \\mu \\le \\mu_0) \\to 0$. For example if $\\mu_0 = 2$ and $\\sigma = 1$, we have:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code which generates figure\"}\nmu_0 <- 2\nsigma <- 1\nexpand_grid(\n  mu = seq(1.8, 2.2, length = 1000), \n  n = c((1:9)*10, (1:9)*100, (1:9)*1000, (1:9)*10000)\n) %>% \n  mutate(power = pnorm(-1.645 + (mu - mu_0)/(sigma/sqrt(n)))) %>% \n  ggplot(aes(mu, power)) +\n  geom_line() +\n  theme_minimal() +\n  geom_segment(aes(x = 1.8, y = 0, xend = 2, yend = 0), color = \"red\") +\n  geom_segment(aes(x = 2, y = 1, xend = 2.2, yend = 1), color = \"red\") +\n  geom_segment(aes(x = 2, y = 0, xend = 2, yend = 1), color = \"red\", linetype = \"dashed\") +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"Alternate μ\", y = \"Power\", color = \"Sample Size\") +\n  guides(colour = guide_legend(nrow = 1)) +\n  transition_states(n) + \n  labs(title = 'n = {closest_state}')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in geom_segment(aes(x = 1.8, y = 0, xend = 2, yend = 0), color = \"red\"): All aesthetics have length 1, but the data has 36000 rows.\nℹ Did you mean to use `annotate()`?\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in geom_segment(aes(x = 2, y = 1, xend = 2.2, yend = 1), color = \"red\"): All aesthetics have length 1, but the data has 36000 rows.\nℹ Did you mean to use `annotate()`?\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in geom_segment(aes(x = 2, y = 0, xend = 2, yend = 1), color = \"red\", : All aesthetics have length 1, but the data has 36000 rows.\nℹ Did you mean to use `annotate()`?\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Cannot get dimensions of plot table. Plot region might not be fixed\nCaused by error in `setup_params()`:\n! `direction` must be a string or character vector.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Failed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nFailed to plot frame\nCaused by error in `setup_params()`:\n! `direction` must be a string or character vector.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![As the sample size increases, the power curve approaches a step function that is 0 where the null hypothesis is true, and 1 where is is false](testing_files/figure-html/fig-plot315-1.gif){#fig-plot315 fig-align='center'}\n:::\n:::\n\n:::\n\nThis example highlights that as $n\\to\\infty$, tests become optimal in the sense that the probability of committing any error (whether it be type I or type II) approaches zero. \n\n::: {#def-}\nSuppose $\\delta(\\X)$ is a decision rule for the hypotheses $H_0:P_{\\thet}\\in\\mathcal P_0$ versus $H_1:P_{\\thet}\\in\\mathcal P_0$. The decision rule has <span style=\"color:red\">**_asymptotic size $\\alpha$_**</span> if \n$$ \\lim_{n\\to\\infty} \\beta(\\delta, P_{\\thet} \\mid P_{\\thet}\\in\\mathcal P_0) = \\alpha.$$\nThe decision rule has <span style=\"color:red\">**_consistent_**</span> if \n$$ \\lim_{n\\to\\infty} \\beta(\\delta, P_{\\thet} \\mid P_{\\thet}\\in\\mathcal P_1) = 1.$$\n:::\n\nThese two definitions are somewhat informal, and barely scratch the surface of asymptotic properties of hypothesis testing. For a detailed treatment, see @lehmann2005testing. \n\n## Confidence Intervals\n\nAn alternate way to think about hypothesis testing is in terms of confidence intervals. An inherent flaw in point estimation is that is necessarily provides a single estimate. We may want to provide a set of plausible estimates in an effort to address the inherent uncertainty that arises from point estimation. \n\n::: {#def-}\nA statistic $\\underline\\theta(\\X)$ is a <span style=\"color:red\">**_level $(1-\\alpha)$ lower confidence bound for $\\theta$_**</span> if $\\Pr[\\underline\\theta(\\X) \\le \\theta] \\ge 1 - \\alpha$ for all $\\theta \\in \\Theta$. A statistic $\\bar\\theta(\\X)$ is a <span style=\"color:red\">**_level $(1-\\alpha)$ upper confidence bound for $\\theta$_**</span> if $\\Pr[\\bar\\theta(\\X) \\ge \\theta] \\ge 1 - \\alpha$ for all $\\theta \\in \\Theta$. Together these statistics form an interval $[\\underline\\theta(\\X),\\bar\\theta(\\X)]$ known as a level <span style=\"color:red\">**_level $(1-\\alpha)$ confidence interval for $\\theta$_**</span> when \n$$ \\Pr[\\underline\\theta(\\X) \\le \\theta \\le \\bar\\theta(\\X)] \\ge 1-\\alpha$$ The <span style=\"color:red\">**_confidence coefficient_**</span> for a confidence interval is the largest possible confidence level given as \n$$ \\inf_{\\theta \\in \\Theta}\\{\\Pr[\\underline\\theta(\\X) \\le \\theta \\le \\bar\\theta(\\X)]\\}.$$ A confidence interval with confidence coefficient $1-\\alpha$ is called a <span style=\"color:red\">**_$100(1-\\alpha)$% confidence interval_**</span>.\n:::\n\nThe level of a confidence interval is not unique. If $\\Pr[\\underline\\theta(\\X) \\le \\theta \\le \\bar\\theta(\\X)] \\ge 1-\\alpha$, then $\\Pr[\\underline\\theta(\\X) \\le \\theta \\le \\bar\\theta(\\X)] \\ge 1-\\alpha'$ for all $\\alpha'>\\alpha$, so a confidence interval of level $(1-\\alpha)$ will be a confidence interval of level $(1-\\alpha')$ for all $\\alpha'>\\alpha$. The confidence coefficient eliminates this ambiguity by taking the maximum level of confidence $(1-\\alpha)$.\n\nIt's important to emphasize that the probability $\\Pr[\\underline\\theta(\\X) \\le \\theta \\le \\bar\\theta(\\X)]$ does *not* correspond to the probability that $\\theta$ is contained in $[\\underline\\theta,\\bar\\theta]$. The parameter $\\theta$ is a constant, and not a random variable. It is either in the interval or not. Instead $\\Pr[\\underline\\theta(\\X) \\le \\theta \\le \\bar\\theta(\\X)]$ is related to the calculation of the statistics $\\underline\\theta$ and $\\bar\\theta$, both of which are random variables. If we construct a 95% confidence interval for $\\theta$, then there is a 95% chance that the interval $[\\underline\\theta,\\bar\\theta]$ contains $\\theta$, where probability is taken over all possible samples $\\X$.  \n\n\n:::{#exm-}\nSuppose we want to construct a confidence interval for $\\mu$, where $X_i\\iid N(\\mu,\\sigma^2)$ for an unknown $\\sigma$. In order to construct some confidence interval $[\\underline\\mu(\\X),\\bar \\mu(\\X)]$, we need to define $\\underline\\mu$ and $\\bar \\mu$ such that we know their respective probability distributions, thereby allowing us to calculate $\\Pr[\\underline\\mu(\\X) \\le \\mu \\le \\bar\\mu(\\X)]$. One statistic for which we know the distribution is $t = \\frac{\\bar X - \\mu}{s/\\sqrt n}$, as $t \\sim t_{n-1}$. The distribution $t_{n-1}$ is symmetric, so we can use the quantile function $t_{n-1}^{-1}$ to determine a confidence interval for $t$.\n$$\\Pr[-t_{n-1}^{-1}(1-\\alpha/2)\\le t\\le t_{n-1}^{-1}(1-\\alpha/2)] = 1-\\alpha $$ This is a key step in the direction of finding a confidence interval for $\\mu$, as $t$ is a function of $\\mu$.\n\\begin{align*}\n&\\Pr[-t_{n-1}^{-1}(1-\\alpha/2)\\le t\\le t_{n-1}^{-1}(1-\\alpha/2)] = 1-\\alpha \\\\\n\\implies &\\Pr\\left[-t_{n-1}^{-1}(1-\\alpha/2)\\le \\frac{\\bar X - \\mu}{s/\\sqrt n}\\le t_{n-1}^{-1}(1-\\alpha/2)\\right] = 1-\\alpha \\\\\n\\implies &\\Pr\\left[ \\bar X - (t_{n-1}^{-1}(1-\\alpha/2))\\frac{s}{\\sqrt{n}} \\le \\mu \\le \\bar X + (t_{n-1}^{-1}(1-\\alpha/2))\\frac{s}{\\sqrt{n}}\\right] = 1-\\alpha\n\\end{align*}\nTherefore we have a $(1-\\alpha)$ level confidence interval for $\\mu$ in the form of \n$$ \\left[ \\bar X - (t_{n-1}^{-1}(1-\\alpha/2))\\frac{s}{\\sqrt{n}} , \\bar X + (t_{n-1}^{-1}(1-\\alpha/2))\\frac{s}{\\sqrt{n}}\\right].$$\nIf we let $\\alpha = 0.05$, $\\mu = 0$, $\\sigma = 1$, and $n = 10$, we can generate confidence intervals for a large number of simulated samples. About 95% of the constructed intervals will contain the true mean $\\mu = 0$. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfidence_interval <- function(alpha, n, dist, dist_params, t){\n  X <- do.call(dist, append(n, dist_params))\n  output <- tibble(\n    lower = mean(X) - qt(0.975, n-1)*(sd(X)/sqrt(n)),\n    upper = mean(X) + qt(0.975, n-1)*(sd(X)/sqrt(n)),\n    alpha = alpha,\n    iter_num = t\n  )\n  return(output)\n}\n\ndraw_confidence_intervals <- function(N, alpha, n, dist, dist_params){\n  output <- 1:N %>% \n    map(confidence_interval, alpha = alpha, n = n, dist = dist, dist_params = dist_params) %>% \n    bind_rows()\n  return(output)\n}\n\nresults <- draw_confidence_intervals(1e5, 0.05, 10, rnorm, list(0, 1))\n\nresults %>% \n  summarize(prob_contains = mean((lower <= 0)*(0 <= upper)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  prob_contains\n          <dbl>\n1         0.951\n```\n\n\n:::\n:::\n\n\nLet's plot 100 of our constructed 95% confidence intervals (drawn at random).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code which generates figure\"}\nresults %>% \n  slice(1:100) %>% \n  mutate(contains = (lower <= 0)*(0 <= upper)) %>% \n  ggplot(aes(iter_num, color = as.factor(contains))) +\n  geom_linerange(aes(ymin = lower, ymax = upper)) +\n  labs(color = \"Interval Contains   μ = 0:\", x = \"Sample\", y = \"Parameter Space\") +\n  scale_color_manual(values = c(\"red\", \"green\")) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![100 of the 10,000 simulated confidence intervals drawn at random. Approximately 5% of the 100 do not contain the true parameter value](testing_files/figure-html/fig-plot316-1.png){#fig-plot316 fig-align='center' width=768}\n:::\n:::\n\n:::\n\nWe can also construct approximate confidence intervals using asymptotic theory. If we modified the previous example such that $X_i$ had some arbitrary distribution $F_X$ with mean $\\mu$, then $t\\asim t_{n-1}$, so \n$$ \\lim_{n\\to\\infty}\\Pr\\left[ \\bar X - (t_{n-1}^{-1}(1-\\alpha/2))\\frac{s}{\\sqrt{n}} \\le \\mu \\le \\bar X + (t_{n-1}^{-1}(1-\\alpha/2))\\frac{s}{\\sqrt{n}}\\right] = 1- \\alpha. $$ In other words, for a sufficiently large $n$, we will have \n$$ \\Pr\\left[ \\bar X - (t_{n-1}^{-1}(1-\\alpha/2))\\frac{s}{\\sqrt{n}} \\le \\mu \\le \\bar X + (t_{n-1}^{-1}(1-\\alpha/2))\\frac{s}{\\sqrt{n}}\\right] \\approx 1- \\alpha.$$\n\nThe previous example also suggests that we may be able to use confidence intervals to test hypotheses. We derived an interval $[\\underline\\mu(\\X),\\bar \\mu(\\X)]$ such that $\\Pr(0 \\notin [\\underline\\mu(\\X),\\bar \\mu(\\X)]) = \\alpha$. If we wanted to test $H_0:\\mu=0$ versus $H_1:\\mu\\neq0$ with a significance level of $\\alpha$, then perhaps we define a decision rule $\\delta$ such that we fail to reject $H_0$ if and only if $\\bar X\\in[\\underline\\mu(\\X),\\bar \\mu(\\X)]$. Not only does this work, but it highlights how confidence intervals and hypothesis tests are two sides of the same coin.\n\n:::{#thm-}\nSuppose $\\delta(\\X)$ is a decision rule with level $\\alpha$ for the hypothesis $H_0:\\thet = \\thet_0$. If $A(\\thet_0)=\\{\\x \\mid \\delta(\\x) = 0\\}$ is the set of observations for which we fail to reject $H_0$, then the set \n$$S(\\x) = \\{\\thet \\mid \\x\\in A(\\thet)\\} $$ is a family of confidence intervals for $\\thet$ with level $1-\\alpha$.\n:::\n\n:::{.proof}\nWe've defined $S(\\x)$ such that $\\thet\\in S(\\x)$ holds if and only if $\\x\\in A(\\thet)$, so \n$$ \\Pr[\\thet \\in S(\\X)] = \\Pr[\\X \\in A(\\thet)] .$$ Since $\\delta$ is a level $\\alpha$ test, \n$\\Pr[\\X \\in A(\\thet)] \\ge 1-\\alpha$, so $$\\Pr[\\thet \\in S(\\X)] \\ge 1-\\alpha.$$\n::: \n\n## Wald Test and t-Test\n\nWhile we've talked a fair amount about the properties tests can have, and considered very specific examples of tests, we have only defined one test in general -- the likelihood ratio test given by Theorem \\@ref(thm:NPlemma). This test has appealing properties in finite samples, especially if \\@ref(thm:KR) holds. Unfortunately, the distribution of the test statistic in finite samples depends on the model which generates the data, so the likelihood ratio is not very robust. Instead, we will consider a large-sample alternative. \n\nSuppose we are testing $H_0: P_{\\thet} \\in \\mathcal P_0$ versus $H_1: P_{\\thet} \\in \\mathcal P_1$ where \n\\begin{align*}\n\\mathcal P_0 = \\{P_{\\thet}\\in\\mathcal P \\mid \\mathbf h(\\thet) = \\mathbf 0\\},\\\\\n\\mathcal P_1 = \\{P_{\\thet}\\in\\mathcal P \\mid \\mathbf h(\\thet) \\neq \\mathbf 0\\},\n\\end{align*}\nfor some function $\\mathbf h:\\boldsymbol\\Theta \\to \\mathbb R^q$. In other words, we are testing $H_0:\\mathbf h(\\thet) = \\mathbf 0$ versus $H_1: \\mathbf h(\\thet) \\neq \\mathbf 0$. See @wolak1989local for the case where $\\mathbf h(\\thet) \\ge \\zer$ or $\\mathbf h(\\thet) \\le \\zer$. The function $\\mathbf h:\\boldsymbol\\Theta \\to \\mathbb R^q$ describes the $q$ relationships we are testing between the $k=\\dim(\\boldsymbol\\Theta)$ components of $\\thet = (\\theta_1,\\ldots,\\theta_k)$. For example, if $\\boldsymbol\\Theta = \\mathbb R^4$ and our null hypothesis was comprised of the following equations:\n\\begin{align*}\n\\theta_1 &= 2\\theta_3\\\\\n\\theta_2 &= \\theta_1\\ln \\theta_4\\\\\n\\theta_3 &= 3\n\\end{align*}\nthen $\\mathbf h: \\mathbb R^4 \\to \\mathbb R^3$ is \n$$ \\mathbf h(\\thet) = \\begin{bmatrix} \\theta_1 - 2\\theta_3\\\\ \\theta_2 -\\theta_1\\ln \\theta_4\\\\\\theta_3-3\\end{bmatrix}.$$ There are two special cases of hypotheses we should consider. \n\n:::{#exm- name=\"Linear Hypotheses\"}\nIn the event our null hypothesis postulates that the components of $\\thet$ are linear combinations of each other, we can write $\\mathbf h(\\thet) = \\mathbf H\\thet$ for some $q\\times k$ matrix of constants. \n$$\\begin{cases}\\sum_{i=1}^k c_{1i}\\theta_i = 0\\\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\vdots\\\\ \\sum_{i=1}^k c_{qi}\\theta_i = 0\\end{cases} \\implies \\mathbf H = \\begin{bmatrix} c_{11} & \\cdots &c_{1k} \\\\ \\vdots & \\ddots & \\vdots \\\\ c_{q1} & \\cdots &c_{qk} \\end{bmatrix}$$ Our null hypothesis is $\\mathbf H\\thet = \\mathbf 0$. \n:::\n\n:::{#exm-}\nIn many settings, the default null hypothesis considered is $H_0:\\thet = \\mathbf 0$. In this case, $\\mathbf h(\\thet) = \\thet$. If we are only interested in one parameter $\\theta_j$, then $\\mathbf h(\\thet) = \\theta_j$. \n:::\n\nIn order to determine the asymptotic distribution of the test statistics prescribed by each test, we need to adopt a distributional assumption about our estimator.\n\n::: {.exercise #assone}\nWe have an estimator $\\hat {\\thet}$ satisfying $\\sqrt{n}(\\hat{\\thet} - \\thet) \\dto N(\\zer, \\mathbf V)$ for a PSD matrix $\\mathbf V$. That is, $\\hat {\\thet}$ is root-$n$ CAN and $\\hat  {\\thet}\\asim N(\\thet, \\mathbf V/n)$ where $\\avar{\\hat{\\thet}}= \\mathbf V/n$\n:::\n\nWe are making no assumptions about the distribution of $\\X$, the asymptotic distribution of $\\X$, or the distribution of $\\hat{\\thet}$. We are only making an assumption about the asymptotic distribution of some $\\hat{\\thet}$. Because of the LLN and CLT, this assumption turns out to be fairly weak, as nearly all common estimators are  root-$n$ CAN. \n\nThe first test we consider will be the most familiar, and will not involve the likelihood function in our general setting. Suppose $\\theta$ and $h(\\theta)=\\theta_0$ are scalars. If we want to test $h(\\theta)=0 $, we may want to decide whether or not to reject the null hypothesis based on the discrepancy between $h(\\theta)$ and $h(\\hat\\theta)$. If the true parameter value is $\\theta_0$, then we would reject the null hypothesis if $\\hat\\theta- \\theta_0 \\gg 0$ or  $\\hat\\theta- \\theta_0 \\ll 0$. We can consider these cases simultaneously by rejecting the null hypothesis if $(\\hat\\theta-\\theta_0)^2 \\gg 0$. This is a bit ambiguous though, because it isn't clear what distance $\\hat\\theta- \\theta_0$ is surprising enough to merit rejecting the null hypothesis. Fortunately, we've assumed that $\\avar{\\hat{\\theta}}=  V/n$ so we can standardize the distance using the standard deviation of $\\hat\\theta$. This standard deviation has a special name.\n\n:::{#def-}\nThe <span style=\"color:red\">**_standard error_**</span> of an estimator $\\hat{\\thet}$ is its standard deviation. $$\\se{\\hat{\\thet}}=\\text{diag}\\left[\\var{\\hat{\\thet}}\\right]^{1/2} $$ \n:::\n\nUsing the standard error we can measure the non-negative distance as \n$$ \\left(\\frac{\\hat\\theta-\\theta_0}{\\se{\\hat{\\theta}}}\\right)^2 = \\frac{(\\hat\\theta-\\theta_0)^2}{\\var{\\hat\\theta}}.$$ This still doesn't do though, because we do not know $\\var{\\hat\\theta}$. Perhaps instead we can use $\\avar{\\hat\\theta}$, giving \n$$ \\frac{(\\hat\\theta-\\theta_0)^2}{\\avar{\\hat\\theta}} = \\frac{(\\hat\\theta-\\theta_0)^2}{V/n} .$$ Unfortunately, if $\\avar{\\hat\\theta}$ is a function of $\\theta$ (which is more often than not the case), then we must estimate $\\avar{\\hat\\theta}$, giving the statistic \n$$ \\frac{(\\hat\\theta-\\theta_0)^2}{\\widehat{\\text{Avar}}(\\hat\\theta)} = \\frac{(\\hat\\theta-\\theta_0)^2}{\\hat V/n}.$$ This should look *very familiar*. For example, if we have $X_i\\iid N(\\mu,\\sigma^2)$ and want to test $H_0:\\mu\\neq \\mu_0$, a situation where $\\sqrt n(\\bar X-\\mu_0) \\dto N(0,\\sigma^2)$ and $\\avar(\\bar X) = \\sigma^2/n$, we have \n$$ \\frac{(\\hat\\theta-\\theta_0)^2}{\\widehat{\\text{Avar}}(\\hat\\theta)} = \\frac{(\\bar X-\\mu_0)^2}{\\widehat{(\\sigma^2/n)}} = \\frac{(\\bar X-\\mu_0)^2}{S^2/n}.$$ This is just the squared test statistic for the $t-$test!\n$$ \\left[\\frac{(\\bar X-\\mu_0)^2}{S^2/n}\\right]^{1/2} = \\frac{\\bar X -\\mu_0}{S/\\sqrt n}.$$ In the event we know $\\sigma^2$, we don't even need to estimate $\\avar{\\bar X}$, and this statistic becomes the squared version of test statistic from the $z-$test. In general this test statistic simply reports how many estimated standard deviations $\\theta$ is from $\\theta_0$, squared. This statistic is due to @wald1943tests, and we will now define it in higher dimensions for possibly nonlinear hypotheses.\n\n:::{#def-}\nGiven the hypotheses $H_0:\\mathbf h(\\thet) = \\mathbf 0$ versus $H_1: \\mathbf h(\\thet) \\neq \\mathbf 0$, the <span style=\"color:red\">**_Wald statistic_**</span> is defined as \n$$ W(\\X) = \\mathbf h(\\hat{\\thet})'\\left[\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})\\widehat{\\text{Avar}}(\\hat{\\thet})\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})'\\right]^{-1}\\mathbf h(\\hat{\\thet}).$$\n:::\n\nWhile this looks fairly complex, the following example shows that it does indeed simplify to statistic we used to build intuition.\n\n:::{#exm-}\nSuppose $\\dim(\\boldsymbol\\Theta)=1$, and $h(\\theta) = \\theta - \\theta_0$. The null hypothesis is $H_0: \\theta - \\theta_0 = 0$, which is $H_0: \\theta  = \\theta_0$. We have \n$$\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet}) = \\frac{\\partial }{\\partial \\theta}[\\theta - \\theta_0]_{\\theta=\\hat\\theta} = 1.$$ Because we have a single parameter, transposes are trivial:\n\\begin{align*}\n\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})' & = [1]' = 1 = \\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet}).\\\\ \n\\mathbf h(\\hat{\\thet})' &= [\\hat\\theta - \\theta_0]'=\\hat\\theta - \\theta_0 = \\mathbf h(\\hat{\\thet}).\n\\end{align*}\nOur statistic is \n\\begin{align*}\nW &= \\mathbf h(\\hat{\\thet})'\\left[\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})\\widehat{\\text{Avar}}(\\hat{\\thet})\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})'\\right]^{-1}\\mathbf h(\\hat{\\thet})\\\\\n& = (\\hat\\theta - \\theta_0)[1\\cdot\\widehat{\\text{Avar}}(\\hat{\\theta})\\cdot 1 ]^{-1}(\\hat\\theta - \\theta_0)\\\\\n& = \\frac{(\\hat\\theta - \\theta_0)^2}{\\widehat{\\text{Avar}}(\\hat{\\theta})}\n\\end{align*}\n:::\n\nTo use the Wald statistic in a test, we need to be able to construct critical regions given a choice of size $\\alpha$. When $n$ is sufficiently large, this requires knowing the asymptotic  distribution of $W$. It's possible to make an informed guess about this distribution if we think about the Wald statistic as a squared $t-$statistic. We established that \n$$ t = \\frac{(\\bar X-\\mu_0)^2}{S^2/n} \\dto N(0,1).$$ If we apply the continuous mapping theorem to $t^2 = W$, we have \n$$ W = t^2 \\dto [N(0,1)]^2 = \\chi^2_1.$$ Not only is $W$ asymptotically distributed according to a chi-square distribution in this simple setting, but it is in the general setting. The only thing we need to account for when showing this in higher dimensions is that we are testing $q$ one dimensional hypotheses simultaneously when $\\mathbf h:\\boldsymbol \\Theta \\to \\mathbb R^q$.  \n\n\n:::{.lemma #quadchi}\nIf $\\x\\sim N(\\zer,\\mathbf I)$ where $\\x = (x_1,\\ldots,x_n)$, then $\\x'\\x\\sim \\chi_n^2$. \n:::\n\n:::{.proof}\nWe have $\\x'\\x = \\sum_{i=1}^n x_i^2$ where $x_i\\sim N(0,1)$, so $\\x'\\x$ is the sum of $n$ random variables with a standard normal distribution. This means $\\x'\\x\\sim \\chi_n^2$. \n:::\n\n:::{#thm-wald}\n\nSuppose:\n\n1. Assumption \\@ref(exr:assone) holds;\n2. $\\mathbf h:\\boldsymbol \\Theta \\to \\mathbb R^q$ is continuously differentiable on $\\boldsymbol\\Theta\\subset\\mathbb R^k$ and $\\frac{\\partial \\mathbf h}{\\partial \\thet}$ is invertible;\n3. $\\hat {\\mathbf V}$ is a consistent estimator for $\\mathbf V$;\n4. $\\thet$ is in the interior of $\\boldsymbol\\Theta$  .\n\nThen $W \\dto \\chi_q^2$ under $H_0$. As a result, the <span style=\"color:red\">**_Wald test_**</span> with size $\\alpha$ takes the form \n$$\\delta(\\X) = \\begin{cases} \\mathbf h(\\thet) = \\zer & W < (\\chi_q^2)^{-1}(1-\\alpha)\\\\\n\\mathbf h(\\thet) \\neq \\zer & W \\ge (\\chi_q^2)^{-1}(1-\\alpha)\\end{cases}.$$\n:::\n\n:::{.proof}\nWe are given that $\\sqrt{n}(\\hat{\\thet} - \\thet) \\dto N(\\zer, \\mathbf V)$. Under assumptions 2 and 4, we can apply the delta method as follows:\n$$ \\sqrt{n}(\\mathbf h(\\hat{\\thet}) - \\mathbf h(\\thet)) \\dto N\\left(\\zer, \\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})\\mathbf V\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})'\\right).$$ We are assuming $H_0$ holds, so $\\mathbf h(\\thet) = \\zer$, giving \n\\begin{align*}\n&\\sqrt{n}(\\mathbf h(\\hat{\\thet}) - \\mathbf h(\\thet)) \\dto N\\left(\\zer, \\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})\\mathbf V\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})'\\right)\\\\\n\\implies & \\sqrt{n}\\mathbf h(\\hat{\\thet}) \\dto N\\left(\\zer, \\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})\\mathbf V\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})'\\right)\\\\\n\\implies & \\left[\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})\\mathbf V\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})'\\right]^{-1/2}\\sqrt{n}\\mathbf h(\\hat{\\thet}) \\dto N(\\zer, \\mathbf I)\n\\end{align*}\nUsing the previous lemma we have \n\\begin{align*}\n&\\sqrt{n}\\mathbf h(\\hat{\\thet})'\\left[\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})\\mathbf V\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})'\\right]^{-1/2}\\left[\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})\\mathbf V\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})'\\right]^{-1/2}\\sqrt{n}\\mathbf h(\\hat{\\thet})\\dto \\chi_q^2\\\\\n\\implies & \nn\\mathbf h(\\hat{\\thet})'\\left[\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})\\mathbf V\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})'\\right]^{-1}\\mathbf h(\\hat{\\thet})\\dto \\chi_q^2\\\\\n\\implies & \\mathbf h(\\hat{\\thet})'\\left[\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})(\\mathbf V/n)\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})'\\right]^{-1}\\mathbf h(\\hat{\\thet})\\dto \\chi_q^2\n\\end{align*}\nIf we replace $\\mathbf V$ with our estimator $\\hat {\\mathbf V}$ which satisfies $\\hat {\\mathbf V}\\pto \\mathbf V$, then we can apply Slutky's theorem and conclude \n$$\\mathbf h(\\hat{\\thet})'\\left[\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})(\\hat {\\mathbf V}/n)\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})'\\right]^{-1}\\mathbf h(\\hat{\\thet})\\dto \\chi_q^2$$\n:::\n\nThe intuition behind the Wald test is identical to that of the $z-$test, the $t$-test, and many other familiar tests -- we know the distribution of $W(\\X)$ under $H_0$, so if we observe a value of $W(\\x)$ that is very unlikely then we should reject $H_0$.\n\n\n:::{#exm-}\nWe can write a simple function which implements the Wald test.\n\n::: {.cell}\n\n```{.r .cell-code}\nWald_test <- function(alpha, h, n, theta_hat, V_hat){\n  #Calculate test stat\n  h_prime <- jacobian(h, theta_hat)\n  W <- as.numeric(t(h(theta_hat)) %*% solve(h_prime %*% (V_hat/n) %*% t(h_prime)) %*% h(theta_hat))\n  \n  #determine if we reject\n  dof <- length(h(theta_hat))\n  c <- qchisq(1 - alpha, dof)\n  reject <- (W >= c)\n  \n  #Output information\n  output <- tibble(\n    statistic = W,\n    critical_value = c,\n    decision = reject\n  )\n  \n  return(output)\n}\n```\n:::\n\n\nIn order to calculate $\\frac{\\partial \\mathbf h}{\\partial \\thet}(\\hat{\\thet})$, we made use of the `jacobian()` function from the `numDeriv` package instead of requiring an additional argument where we supply the calculated Jacobian. Let's put our function to use. Suppose that $\\X \\iid N(\\boldsymbol\\mu, \\boldsymbol\\Sigma)$ where \n\\begin{align*}\n\\boldsymbol\\mu &= \\begin{bmatrix}2&2&2\\end{bmatrix}',\\\\\n\\boldsymbol\\Sigma&= \\begin{bmatrix}1&0.4&0.2\\\\0.4& 2& 0.1\\\\0.2&0.1&1 \\end{bmatrix}.\n\\end{align*}\nThe sample mean satisfies $\\sqrt{n}(\\bar {\\X} - \\boldsymbol\\mu)\\dto N(\\zer, \\mathbf V)$ trivially, as $\\sqrt{n}(\\bar {\\X} - \\boldsymbol\\mu)\\sim N(\\zer, \\boldsymbol\\Sigma)$. We can use the Wald test to test the hypothesis that $\\boldsymbol\\mu = (2,2,2)$ with $\\alpha = 0.05$ and $n = 100$. In this case we have \n$$ \\mathbf h(\\boldsymbol\\mu) = \\begin{bmatrix} \\mu_1 - 2\\\\ \\mu_2 -2\\\\\\mu_3-2\\end{bmatrix}.$$ We do need a consistent estimate of $\\mathbf V = \\boldsymbol \\Sigma$, but the most intuitive candidate will do. The sample covariance, calculated using `cov()`, of our observed data is a consistent estimator of  $\\boldsymbol \\Sigma$ just as the sample variance is a consistent estimator of the actual variance. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_wald <- function(alpha, h, n, dist, dist_params, t){\n  X <- do.call(dist, append(n, dist_params))\n  output <- Wald_test(alpha, h, n, colMeans(X), cov(X)) %>% \n    mutate(iter_num = t)\n  return(output)\n}\n\ndraw_N_wald <- function(N, alpha, h, n, dist, dist_params){\n  output <- 1:N %>% \n    map(sim_wald, alpha = alpha, h = h, n = n, dist = dist, dist_params = dist_params) %>% \n    bind_rows()\n  return(output)\n}\n\n#Define hypothesis\nh <- function(t){\n  c(t[1], t[2], t[3]) - c(2, 2, 2)\n}\n\nresults <- draw_N_wald(\n  N = 1e5,\n  alpha = 0.05, \n  h = h,\n  n = 1e3, \n  dist = rmvnorm,\n  dist_params = list(\n      c(2, 2, 2), \n      matrix(c(1,.4,.2,.4,2,.1,.2,.1,1), nrow = 3, ncol = 3)\n  )\n)\n\n\n#Should be ≈ 0.05\nresults %>% \n  summarize(prob_reject = mean(decision))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  prob_reject\n        <dbl>\n1      0.0517\n```\n\n\n:::\n:::\n\n\nIf we plot the test statistics from our 10,000 simulations, we can illustrate that $W\\asim \\chi_3^2$. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code which generates figure\"}\nresults %>% \n  ggplot(aes(statistic)) + \n  geom_histogram(aes(y = ..density..), colour = 1, fill = \"white\", bins = 50) +\n    xlab(\"Simulated Wald Test Statistics\") +\n  theme_minimal() +\n  stat_function(fun = dchisq, args = list(df = 3), color = \"red\") +\n  xlim(0,15)\n```\n\n::: {.cell-output-display}\n![The hisotrgram of simulated wald test statistics (calculated using a sample size of 100) coincides with the asymptotic distribution given in Theorem 3.2](testing_files/figure-html/fig-plot317-1.png){#fig-plot317 fig-align='center' width=768}\n:::\n:::\n\n:::\n\nIf we are only testing $\\theta_j = \\theta_0$ for one component of $\\thet$, then we can use a general version of the $t-$test, which is a modified Wald test.\n\n:::{#def-}\nGiven the hypotheses $H_0:\\theta_j = \\theta_{0}$ versus $H_1: \\theta_j \\neq \\theta_{0}$, the <span style=\"color:red\">**_$t-$statistic_**</span> is defined as \n$$ t = \\frac{\\hat\\theta_j-\\theta_0}{\\widehat{\\text{se}}(\\hat\\theta_j)} = [W(\\X)]^{1/2}.$$\nAssuming \\@ref(exr:assone) holds, the <span style=\"color:red\">**_$t-$test_**</span> with size $\\alpha$ takes the form \n$$\\delta(\\X) = \\begin{cases} \\theta_j = \\theta_0 & \\abs{t} \\ge t_{n-1}^{-1}(1-\\alpha/2)\\\\\n\\theta_j \\neq \\theta_0 & \\abs{t} < t_{n-1}^{-1}(1-\\alpha/2)\\end{cases}.$$\n:::\n\nConsidering the $t-$test in the broader context of the Wald test highlights an important distinction -- there is a difference between the one hypothesis $\\thet = \\thet_0$, and the separate hypotheses $\\theta_j = \\theta_{0,j}$ for $j=1,\\ldots,k$. The prior requires that $\\theta_j = \\theta_{0,j}$ for all $j$ *simultaneously*, while the latter hypotheses are completely independent. We can highlight this by comparing the $t-$test and the Wald test.\n\n\n:::{#exm- name=\"t-test versus Wald test\"}\n\nWe will start by writing a function which performs the $t-$test just like we did with the Wald test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt_test <- function(alpha, theta0, n, theta_hat, se_hat){\n  #calculate test stat\n  t <- (theta_hat - theta0)/se_hat\n  \n  #determine if we reject\n  dof <- n - 1\n  c <- qt(1 - alpha/2, dof)\n  reject <- (abs(t) >= c)\n  \n  #Output information\n  output <- tibble(\n    parameter = paste0(\"θ\", 1:length(t)),\n    statistic = t,\n    critical_value = c,\n    decision = reject\n  ) \n  \n  return(output)\n}\n```\n:::\n\n\nAs defined, the function is able to perform multiple $t-$tests simultaneously. If we supply a matrix for `X` along with vectors for `theta0`, `theta_hat`, and `se_hat`, then`t` will be a vector and `decision` will record whether the components of `t` exceed the critical value.  \n\nTo compare `t_test()` and `Wald_test()`, suppose $\\X \\iid N(\\boldsymbol\\mu, \\boldsymbol\\Sigma)$ for \n\\begin{align*}\n\\boldsymbol\\mu &=/ \\begin{bmatrix}0&0.5\\end{bmatrix}',\\\\\n\\boldsymbol\\Sigma&= \\begin{bmatrix}1&0\\\\0&2 \\end{bmatrix},\n\\end{align*}\nand $n=2$. We will test $H_0:\\boldsymbol\\mu = \\zer$ with `Wald_test()` and the separate hypotheses $H_0:\\mu_1 = 0$ (which is true) and $H_0:\\mu_2 = 0$, all with $\\alpha = 0.05$. We should reject $\\mu_1 = 0$ with an approximate probability $\\alpha$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwald_vs_t <- function(alpha, h, mu0, n, dist, dist_params, t){\n  X <- do.call(dist, append(n, dist_params))\n  t_test_results <- t_test(alpha, mu0, n, colMeans(X), sqrt(diag(var(X)/n))) %>% \n    mutate(test = \"t-test\")\n  wald_test_results <- Wald_test(alpha, h, n, colMeans(X), cov(X)) %>% \n    mutate(\n      parameter = \"θ\",\n      test = \"Wald test\"\n    )\n  output <- t_test_results %>% \n    bind_rows(wald_test_results) %>% \n    mutate(iter_num = t)\n  return(output)\n}\n\ndraw_N_wald_vs_t <- function(N, alpha, h, mu0, n, dist, dist_params){\n  output <- 1:N %>% \n    map(wald_vs_t, alpha = alpha, h = h, mu0 = mu0, n = n, dist = dist, dist_params = dist_params) %>% \n    bind_rows() \n  return(output)\n}\n\nh <- function(t){\n  c(t[1], t[2]) - c(0, 0)\n}\n\nresults <- draw_N_wald_vs_t(\n  N = 1e5,\n  alpha = 0.05,\n  h = h,\n  mu0 = c(0, 0),\n  n = 100,\n  dist = rmvnorm,\n  dist_params = list(\n    mean = c(0, 0.5),\n    sigma = matrix(c(1,0,0,2), nrow = 2, ncol = 2)\n  )\n)\n\nresults %>% \n  group_by(test, parameter) %>% \n  summarize(prob_reject = mean(decision)) %>% \n  knitr::kable()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'test'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n::: {.cell-output-display}\n\n\n|test      |parameter | prob_reject|\n|:---------|:---------|-----------:|\n|Wald test |θ         |     0.89695|\n|t-test    |θ1        |     0.04834|\n|t-test    |θ2        |     0.93781|\n\n\n:::\n:::\n\n\nWe end up rejecting the null hypothesis $H_0:\\boldsymbol\\mu = \\zer$ less than the null hypothesis $H_0:\\mu_2 = 0$, as the Wald test also must incorporate evidence about $\\mu_1$ in the form of $\\bar X_1$. A large value of $\\bar X_2$ (which is sufficient to reject $H_0:\\mu_2 = 0$ with the $t-$test), is only one part of the story for the Wald test, as it also must consider $\\bar X_1$. That being said, $\\bar X_2$ is usually *so large* that the Wald test will still reject $H_0:\\boldsymbol\\mu = \\zer$ even if $\\bar X_1$ is not large enough for the $t-$test to reject $H_0:\\mu_1= 0$. To see this, we can breakdown the simulated probabilities that we reject $H_0:\\mu_1= 0$ and/or $H_0:\\mu_2 = 0$.  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code which generates figure\"}\nresults %>%\n  filter(test == \"t-test\") %>% \n  select(\n    parameter,\n    decision,\n    iter_num\n  ) %>% \n  group_by(iter_num) %>% \n  pivot_wider(names_from = parameter, values_from = decision) %>% \n  ungroup() %>% \n  add_count() %>% \n  group_by(θ1, θ2) %>% \n  summarize(prob = n()/max(n)) %>% \n  group_by(θ1) %>% \n  pivot_wider(names_from = θ2, values_from = prob) %>% \n  rename(\n    ` ` = 1,\n    `Fail to Reject θ2 = 0` = 2,\n    `Reject θ2 = 0` = 3\n  ) %>% \n  mutate(\n    ` ` = ifelse(` `, \"Reject θ1 = 0\", \"Fail to Reject θ1 = 0\")\n  ) %>% \n  knitr::kable()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'θ1'. You can override using the `.groups`\nargument.\n```\n\n\n:::\n\n::: {.cell-output-display}\n\n\n|                      | Fail to Reject θ2 = 0| Reject θ2 = 0|\n|:---------------------|---------------------:|-------------:|\n|Fail to Reject θ1 = 0 |               0.05919|       0.89247|\n|Reject θ1 = 0         |               0.00300|       0.04534|\n\n\n\nThe QQ-plot for the simulated distribution of the adjusted sample mean\n:::\n:::\n\n\nWe reject $H_0:\\mu_1= 0$ and $H_0:\\mu_2 = 0$ (simultaneously) with an approximate probability of $0.047$, far less than that calculated using the Wald test. Why does this happen? For our example, we reject $H_0:\\boldsymbol\\mu = \\zer$ if $W \\ge (\\chi_2^2)^{-1}(1-0.05) \\approx 6$. We reject $H_0:\\mu_1 = 0$ if $|t_1| \\ge  1.98$ and $H_0:\\mu_2 = 0$ if $|t_2| \\ge  1.98$. In terms of $\\bar X$, we reject $H_0:\\mu_1 = 0$ if $$\\abs{\\bar X_1} \\ge 0 + 1.98\\cdot\\widehat{\\text{se}}(\\bar X_1) \\approx 1.98\\cdot\\text{se}(\\bar X_1) = 1.98\\cdot(1/\\sqrt{ 100}) = 0.198,$$ and we reject $H_0:\\mu_2 = 0$ if $$\\abs{\\bar X_2} \\ge 0 + 1.98\\cdot\\widehat{\\text{se}}(\\bar X_2) \\approx 1.98\\cdot\\text{se}(\\bar X_1) = 1.98\\cdot(2/\\sqrt{ 100}) \\approx 0.28.$$ Because the Wald statistic is a function of $\\bar X_1$ and $\\bar X_2$, we can plot the values of $W$ over values of $\\{\\bar X_1,\\bar X_2\\}$ and compare this value to the rejection regions of the $t-$tests.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code which generates figure\"}\nexpand_grid(\n  x = seq(-.40, .40, length = 500),\n  y = seq(-.40, .40, length = 500)\n  ) %>% \n  mutate(\n    map2_vec(x, y, \\(x, y) Wald_test(alpha = 0.5, h = h, n = 100, theta_hat = c(x, y), V_hat = matrix(c(1,0,0,2), nrow = 2, ncol = 2)))\n  )  %>% \n  ggplot(aes(x, y, fill = statistic)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"red\", mid = \"white\", high = \"green\", midpoint = 5.991465) +\n  labs(x = \"Estimate of μ1\", y = \"Estimate of μ2\", fill = \"Wald Statistic\") +\n  theme_classic() +\n  geom_vline(xintercept = c(-.198,.198), linetype = \"dashed\", size = 0.1) +\n  geom_hline(yintercept = c(-(sqrt(2)/10)*1.98,(sqrt(2)/10)*1.98), linetype = \"dashed\", size = 0.1) +\n  theme(legend.position = \"bottom\") +\n  annotate(\"text\", x = 0, y = 0, label = \"Don't Reject μ1 = 0, Don't Reject μ2 = 0\", size = 2.5) +\n  annotate(\"text\", x = -.3, y = 0, label = \"Reject μ1 = 0, Don't Reject μ2 = 0\", size = 2.5) +\n  annotate(\"text\", x = .3, y = 0, label = \"Reject μ1 = 0, Don't Reject μ2 = 0\", size = 2.5) +\n  annotate(\"text\", x = 0, y = .35, label = \"Don't Reject μ1 = 0, Reject μ2 = 0\", size = 2.5) +\n  annotate(\"text\", x = 0, y = -.35, label = \"Don't Reject μ1 = 0, Reject μ2 = 0\", size = 2.5) +\n  annotate(\"text\", x = -.3, y = -.35, label = \"Reject μ1 = 0, Reject μ2 = 0\", size = 2.5) +\n  annotate(\"text\", x = .3, y = .35, label = \"Reject μ1 = 0, Reject μ2 = 0\", size = 2.5) +\n  annotate(\"text\", x = -.3, y = .35, label = \"Reject μ1 = 0, Reject μ2 = 0\", size = 2.5) +\n  annotate(\"text\", x = .3, y = -.35, label = \"Reject μ1 = 0, Reject μ2 = 0\", size = 2.5) +\n  scale_y_continuous(breaks = (-4:4)/10, expand = c(0,0)) +\n  scale_x_continuous(breaks = (-4:4)/10, expand = c(0,0))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![The values of the Wald test statistic in relation to the decisions seperate t-tests make regarding the null hypotheses when considering each in one dimmension](testing_files/figure-html/fig-plot318-1.png){#fig-plot318 fig-align='center' width=768}\n:::\n:::\n\n\nThis illustrates that it's possible to reject $H_0:\\boldsymbol\\mu = \\zer$ using the Wald test while not rejecting $H_0:\\mu_1 = 0$ or not rejecting $H_1:\\mu_1 = 0$. On the other hand, if we reject $H_0:\\mu_1 = 0$ and $H_1:\\mu_1 = 0$ using separate $t-$tests, we are guaranteed to reject $H_0:\\boldsymbol\\mu = \\zer$ using the Wald test. This plot omits an important consideration -- the true distribution of $\\{\\bar X_1,\\bar X_2\\}$ which will determine how frequently our estimates fall in the various rejection regions. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show code which generates figure\"}\nexpand_grid(\n  x = seq(-.35, .35, length = 500),\n  y = seq(-.40, 1, length = 500)\n  ) %>% \n  mutate(\n    map2_vec(x, y, \\(x, y) Wald_test(alpha = 0.05, h = h, n = 100, theta_hat = c(x, y), V_hat = matrix(c(1,0,0,2), nrow = 2, ncol = 2))),\n    density = map2_dbl(x, y, \\(x, y) dmvnorm(c(x,y), c(0,0.5), sqrt(matrix(c(1,0,0,2), nrow = 2, ncol = 2))/10)),\n    decision = ifelse(decision, \"Wald Test Rejects μ = 0\", \"Wald Test Fails to Reject μ = 0\")\n  ) %>% \n  ggplot(aes(x, y, fill = density)) +\n  geom_tile() +\n  labs(x = \"Estimate of μ1\", y = \"Estimate of μ2\", fill = \"Density\") +\n  theme_classic() +\n  geom_vline(xintercept = c(-.198,.198), linetype = \"dashed\", linewidth = 0.2, color = \"red\") +\n  geom_hline(yintercept = c(-(sqrt(2)/10)*1.98,(sqrt(2)/10)*1.98), linetype = \"dashed\", linewidth = 0.2, color = \"red\") +\n  theme(legend.position = \"bottom\", panel.spacing.x = unit(2, \"lines\")) +\n  scale_fill_gradient(low = \"white\", high = \"black\") +\n  facet_wrap(~decision) +\n  scale_y_continuous(breaks = (-4:10)/5, expand = c(0,0)) +\n  scale_x_continuous(breaks = (-35:35)/4, expand = c(0,0))\n```\n\n::: {.cell-output-display}\n![The joint density of the sample means](testing_files/figure-html/fig-plot319-1.png){#fig-plot319 fig-align='center' width=768}\n:::\n:::\n\nThe majority of the density is concentrated in a region where the Wald test rejects $H_0:\\boldsymbol\\mu = \\zer$, the $t-$test does not reject $H_0:\\mu_1 = 0$, and the $t-$test rejects $H_0:\\mu_2 = 0$. This is why our Wald test rejected $H_0:\\boldsymbol\\mu = \\zer$ so often, despite $\\mu_1 = 0$ being true. Finally, note that we've assumed $\\cov{X_1,X_2} = 0$. In the event that $\\cov{X_1,X_2} \\neq 0$, things become even more interesting, as the Wald test considers this covariance, whereas the $t-$tests do not. If you play around with this example by changing $\\boldsymbol\\Sigma =\\var{\\X}$, the elliptical rejection region of the Wald test will rotate/shrink/expand, thereby affecting how its results relate to those of the separate $t-$tests.\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}