<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Noah Jussila">
<meta name="dcterms.date" content="2022-03-07">

<title>Advanced Econometrics with Examples</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./estimators.html" rel="next">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./index.html">Preliminaries</a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Advanced Econometrics with Examples</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Preliminaries</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Statistical Theory</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimators.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Finite Sample Properties of Estimators</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Asymptotic Properties of Estimators</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exp_fam.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exponential Families</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stochastic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./adv_asymptotics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Advanced Asymptotics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Linear Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./endog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Endogeniety I: IV and 2SLS</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gls.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Generalized Least Squares</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./simul.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Endogeniety II: Simultaneity and Multiple Regressions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Estimation Frameworks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./extremum.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Extremum Estimators</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gmm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">The Generalized Method of Moments</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mle.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Basic Microeconometrics and Time Series</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./binary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Binary Choice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./panel.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Panel Data I: The Basics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./time_series.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Basic Time Series</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nonpar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Nonparametrics I: Distribution and Density Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nonpar_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Nonparametrics II: Functional Forms</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./semipar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Semiparametrics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sieve.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Sieve Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./partial_id.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Partial Identification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./endog_nonlinear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Endogeniety III: Nonlinear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./weak_inst.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Endogeniety IV: Weak Instruments</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Microeconometrics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./discrete_choice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Discrete Choice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tobit.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Tobit</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./survival.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Survival Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./count.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Count Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mixed.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Mixed and Multilevel Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./GLM.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./panel2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Panel Methods II</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
 <span class="menu-text">Time Series</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./time_math.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Math Review</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
 <span class="menu-text">Causal Inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./causal1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Potential Outcomes Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./matching.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Matching Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rdd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Regression Discontinuity Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./did.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Difference in Differences</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./LATE.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Non-Compliance and LATE</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hetero.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Heterogeneous Treatment Effects</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true">
 <span class="menu-text">More Statistical Theory</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayes1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Bayesian Estimation I: Theory</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayes2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Bayesian Estimation II: Computation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayes3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Bayesian Estimation III: Applications</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./modelselection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./robust.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Robust Statistics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="true">
 <span class="menu-text">Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./learning_theory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Statistical Learning Theory</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ML1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Supervised Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ML2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Dimmension Reduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ML3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Unsupervised Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ML4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Text Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ML5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">ML meets Causal Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#preliminaries" id="toc-preliminaries" class="nav-link active" data-scroll-target="#preliminaries">Preliminaries</a>
  <ul class="collapse">
  <li><a href="#probablitiy-theory" id="toc-probablitiy-theory" class="nav-link" data-scroll-target="#probablitiy-theory">Probablitiy Theory</a></li>
  <li><a href="#random-matrices-and-vectors" id="toc-random-matrices-and-vectors" class="nav-link" data-scroll-target="#random-matrices-and-vectors">Random Matrices and Vectors</a></li>
  <li><a href="#multivariate-normal-distribution" id="toc-multivariate-normal-distribution" class="nav-link" data-scroll-target="#multivariate-normal-distribution">Multivariate Normal Distribution</a></li>
  <li><a href="#conditional-expectation-and-independence" id="toc-conditional-expectation-and-independence" class="nav-link" data-scroll-target="#conditional-expectation-and-independence">Conditional Expectation and Independence</a></li>
  <li><a href="#existence-of-expectation" id="toc-existence-of-expectation" class="nav-link" data-scroll-target="#existence-of-expectation">Existence of Expectation</a></li>
  <li><a href="#code" id="toc-code" class="nav-link" data-scroll-target="#code">Code</a></li>
  <li><a href="#organization" id="toc-organization" class="nav-link" data-scroll-target="#organization">Organization</a>
  <ul class="collapse">
  <li><a href="#part-i---statistics" id="toc-part-i---statistics" class="nav-link" data-scroll-target="#part-i---statistics">Part I - Statistics</a></li>
  <li><a href="#part-ii---linear-models" id="toc-part-ii---linear-models" class="nav-link" data-scroll-target="#part-ii---linear-models">Part II - Linear Models</a></li>
  <li><a href="#part-iii---estimation-framework" id="toc-part-iii---estimation-framework" class="nav-link" data-scroll-target="#part-iii---estimation-framework">Part III - Estimation Framework</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Advanced Econometrics with Examples</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Noah Jussila </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 7, 2022</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="preliminaries" class="level1 unnumbered">
<h1 class="unnumbered">Preliminaries</h1>
<ul>
<li>Multivariable Calculus
<ul>
<li>partial derivatives, the gradient, Jacobian matrix, Hessian matrix</li>
<li>optimization</li>
</ul></li>
<li>Linear Algebra
<ul>
<li>matrices and vectors</li>
<li>linear transformations</li>
<li>projections</li>
<li>PSD matrices</li>
</ul></li>
<li>Probability
<ul>
<li>random variables</li>
<li>distribution and density of RVs</li>
<li>expectation and variance</li>
<li>moments</li>
<li>common distributions
<ul>
<li>normal distribution</li>
<li>“friends” of the normal distribution: chi-squared, student’s t, F distribution</li>
</ul></li>
</ul></li>
<li>Mathematical Statistics
<ul>
<li>Estimation</li>
<li>Hypothesis testing</li>
</ul></li>
<li>Basic <a href="https://github.com/noahjussila/analysis_notes/blob/master/Analysis%20Notes.pdf">Real Analysis</a>
<ul>
<li>infimum and supremum</li>
<li>metric spaces</li>
<li>compact sets in <span class="math inline">\(\mathbb R\)</span></li>
<li>mean value theorem</li>
<li>convergence of sequences</li>
<li>pointwise versus uniform convergence</li>
<li>Taylor series</li>
</ul></li>
<li>Basic Numerical Optimization
<ul>
<li>“numerical” vs.&nbsp;“analytic”</li>
</ul></li>
</ul>
<section id="probablitiy-theory" class="level2">
<h2 class="anchored" data-anchor-id="probablitiy-theory">Probablitiy Theory</h2>
<p>We’ll briefly go over the basics of probability theory. A rigorous treatment can be found in <span class="citation" data-cites="durrett2019probability">Durrett (<a href="#ref-durrett2019probability" role="doc-biblioref">2019</a>)</span> or <span class="citation" data-cites="billingsley2008probability">Billingsley (<a href="#ref-billingsley2008probability" role="doc-biblioref">2008</a>)</span>. For an even more general discussion, see <span class="citation" data-cites="folland1999real">Folland (<a href="#ref-folland1999real" role="doc-biblioref">1999</a>)</span>, <span class="citation" data-cites="royden1988real">Royden and Fitzpatrick (<a href="#ref-royden1988real" role="doc-biblioref">1988</a>)</span>, and/or <span class="citation" data-cites="rudin">Rudin (<a href="#ref-rudin" role="doc-biblioref">1987</a>)</span>.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1</strong></span> A <span style="color:red"><strong><em>measure space</em></strong></span> is a triple <span class="math inline">\((X, \mathcal F, \mu)\)</span> comprised of:</p>
<ol type="1">
<li>A set <span class="math inline">\(\mathcal X\)</span>.</li>
<li>A collection of subsets of <span class="math inline">\(X\)</span>, <span class="math inline">\(\mathcal F\subseteq 2^{X}\)</span>. This collection of sets satisfies the following properties: <span class="math inline">\(A^c\in \mathcal F\)</span> if <span class="math inline">\(A\in \mathcal F\)</span>, and <span class="math inline">\(\cup_{\alpha} A_\alpha \subseteq \mathcal F\)</span> for all countable collections of sets <span class="math inline">\(\{A_\alpha\}\subseteq \mathcal F\)</span>. Such a collection of sets is known as a <span style="color:red"><strong><em>sigma-algebra</em></strong></span>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></li>
<li>A <span style="color:red"><strong><em>measure</em></strong></span> <span class="math inline">\(\mu:\mathcal F\to [0,\infty]\)</span> satisfying <span class="math inline">\(\mu(\emptyset) = 0\)</span>, and <span class="math inline">\(\mu(\sum_\alpha A_\alpha)= \sum_\alpha \mu(A_\alpha)\)</span> for all countable collections of <em>disjoint</em> sets <span class="math inline">\(\{A_\alpha\}\subseteq \mathcal F\)</span>.</li>
</ol>
<p>We also refer to just the pair <span class="math inline">\((X,\mathcal F)\)</span> as a <span style="color:red"><strong><em>measurable space</em></strong></span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 1 (Lebesgue Measure)</strong></span> The most important such space is used to measure subsets of the real line (and more generally euclidean spaces). If we want to measure subsets of <span class="math inline">\(\mathbb R\)</span>, we define the associated sigma-algebra as the collection of all compliments and countable unions of the open intervals (along with the subsequent sets generated), denoted <span class="math inline">\(\mathcal B(\mathbb R) = \{(a,b)\subset \mathbb R\mid a,b\in\mathbb R\}\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> The measure of some interval <span class="math inline">\(I = (a,b)\subset \mathbb R\)</span> is defined as <span class="math display">\[m(I)=b -a,\]</span> which is fairly reasonable. This measure is known as the Lebesgue measure.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 2 (Counting Measure)</strong></span> Another nice example of a measure space is <span class="math inline">\((X, 2^{X}, \mu)\)</span> where <span class="math display">\[\mu(A) = \begin{cases}|A|&amp; A\text{ finite}\\ \infty&amp; A\text{ infinite}\end{cases}.\]</span> This measure simply assigns each set its cardinality, and is known as the counting measure.</p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; One of the main motivations of measure theory is to make the operation of integration more robust. We can do this by defining a property of functions that will make integration with respect to a measure possible.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2</strong></span> Let <span class="math inline">\((X, \mathcal F)\)</span> and <span class="math inline">\((Y, \mathcal G)\)</span> be two measurable spaces. A function <span class="math inline">\(f:X\to Y\)</span> is <span style="color:red"><strong><em>measurable</em></strong></span> if <span class="math display">\[\begin{align*}
&amp; f^{-1}(G) \in \mathcal F &amp; (\forall G\in\mathcal G)
\end{align*}\]</span> where <span class="math inline">\(f^{-1}(G)\)</span> is the preimage of a set <span class="math inline">\(G\in \mathcal G\)</span>.</p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; It turns out that any function that is Riemann integrable is measurable.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> Defining the integral of a measurable function with respect to a measure <span class="math inline">\(\mu\)</span> requires some intermediate steps that make for a verbose definition. Instead of presenting the technical definition, we’ll just introduce the notation: <span class="math display">\[\int_{[a,b]} f\ d\mu\]</span> This is the <span style="color:red"><strong><em>Lebesgue integral</em></strong></span> of <span class="math inline">\(f\)</span> on <span class="math inline">\([a,b]\subseteq \mathcal F\)</span> with respect to the measure <span class="math inline">\(\mu\)</span>.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 3 (Integration and Lebesgue Measure)</strong></span> Suppose we have the measure space <span class="math inline">\((\mathbb{R}, \mathcal B(\mathbb{R}), m)\)</span>. For a measurable function <span class="math inline">\(f:[a,b]\to\mathbb{R}\)</span> we have <span class="math display">\[\int_{[a,b]}f\ dm = \int_a^b f(x)\ dx\]</span> where the later integral is the Riemann integral. Since they’re equal, it’s rare that we need to turn to tools beyond basic calculus to integrate functions.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4 (Integration and Counting Measure)</strong></span> Suppose we have a measure space <span class="math inline">\((\mathbb N, 2^{\mathbb N}, \mu)\)</span> where <span class="math inline">\(\mu\)</span> is the counting measure. If we have some function <span class="math inline">\(f:\mathbb N\to\mathbb{R}\)</span><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> and a set <span class="math inline">\(A\subset \mathbb N\)</span> on which we want to integrate <span class="math inline">\(f\)</span>, then <span class="math display">\[\int_A f\ d\mu = \sum_{x\in A}f(x).\]</span> Proving why this is the case requires the formal definition of the Lebesgue integral, but the intuition shouldn’t be anything new. An integral can be thought of as a continuous version of summation, so if we integrate on a discrete set we should just end up with a summation.</p>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The fact that the Lebesgue integral reduces to summation demonstrates why it’s such a powerful tool.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Given a probability space <span class="math inline">\((\mathcal X, \mathcal F, P)\)</span>, we can define a function <span class="math inline">\(X:\mathcal X\to E\)</span> where <span class="math inline">\(E\)</span> is some set equipped with a sigma-algebra <span class="math inline">\(\mathcal E\)</span>. This function is a <strong><em>random variable</em></strong> if the preimage of every set in <span class="math inline">\(\mathcal E\)</span> is in <span class="math inline">\(\mathcal F\)</span>.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> <span class="math display">\[ X^{-1}(I) \in \mathcal F \ \ \ \forall I\in\mathcal E.\]</span> In most cases, we’re interested in random variables that take on real values (equipped with the <span class="math inline">\(\mathcal B(\mathbb R)\)</span>) in which case <span class="math inline">\(X:\mathcal X\to \mathbb{R}\)</span> and <span class="math display">\[ X^{-1}(I) \in \mathcal F \ \ \ \forall I\in\mathcal B(\mathbb{R}).\]</span> In other words, if we can measure <span class="math inline">\(I\)</span> using the Lebesgue measure <span class="math inline">\(\mu\)</span>, we are able to measure <span class="math inline">\(X^{-1}(I)\)</span> using the probability measure <span class="math inline">\(P\)</span>. This allows us to define a probability measure <span class="math inline">\(P(X^{-1}(I))\)</span> on <span class="math inline">\(\mathcal B(\mathbb R)\)</span> which gives us the probability that <span class="math inline">\(X\)</span> is in the set <span class="math inline">\(I\subset \mathcal B(\mathbb R)\)</span>. This probability measure is the <strong><em>distribution</em></strong> of <span class="math inline">\(X\)</span>. If <span class="math inline">\(I = (-\infty,x)\)</span> for <span class="math inline">\(x\in\mathcal X\)</span>, then the distribution takes the form <span class="math inline">\(P(X^{-1}(I)) = P(X \in (-\infty,x)) = P(X\le x)\)</span>. We usually work with the distribution in this form and call it the <strong><em>distribution function</em></strong> <span class="math inline">\(F_X(x) = P(X\le x)\)</span>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The chief motivation for defining measure spaces and measurable functions is a general theory of integration. The Riemann integral is achieved by taking the limit of a process which partitions the area under a function into rectangles. The width of these rectangles is the length of their base, which can be thought of as the measure of an interval. When performing Riemann integration, if <span class="math inline">\(I=(a,b)\)</span> is the base of a rectangle, we take the width to be <span class="math inline">\(m(I)=b -a\)</span>. That is, we use the Lebesgue measure. This is why you will sometimes see integrals written as <span class="math display">\[ \int f(x)\ dx = \int f(x) \ dm.\]</span> Writing the integral this way emphasize that we are integrating with respect to some measure (notion of length/volume), but it is more than just a notational difference. It is the result of defining integration in an entirely different way giving the <strong><em>Lebesgue integral (with respect to <span class="math inline">\(m\)</span>)</em></strong>. In practice this won’t matter a whole lot, because if we can calculate the Riemann integral of a function than we can calculate the Lebesgue integral, and both integrals are equal.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> What’s important is that the idea of integration with respect to a measure can be generalized to any measure space <span class="math inline">\((X, \mathcal N,\mu)\)</span>. In general, the <strong><em>Lebesgue integral</em></strong> of <span class="math inline">\(f\)</span> over <span class="math inline">\(A\in \mathcal N\)</span> is written as <span class="math display">\[\int_A f \ d\mu.\]</span> The rigorous definition of this integral, and how it is calculated using that definition, aren’t essential at the moment. That being said, one useful property to know is that if we integrate the constant <span class="math inline">\(1\)</span> on <span class="math inline">\(A\)</span> with respect to a measure <span class="math inline">\(\mu\)</span>, we get <span class="math inline">\(\mu(A)\)</span>. <span class="math display">\[\int_A 1\ d\mu = \int_A d\mu =\mu(A)\]</span> This should seem familiar from calculus. If we have a set <span class="math inline">\(I=(a,b)\)</span>, then <span class="math display">\[\int_a^b 1\ dx = b - a\]</span> using Riemann integration. Lebesgue integration gives <span class="math display">\[ \int_I 1\ dm = m(I) = b-a.\]</span> Interestingly, if we look at counting measure on the set of real numbers (our measure space is <span class="math inline">\((\mathbb R, 2^{\mathbb R}, \mu)\)</span>), and some set <span class="math inline">\(A = \{a_1,\ldots, a_n\}\in 2^{\mathbb R}\)</span> <span class="math display">\[ \int_Af\ d\mu = \int_A 1\ d\mu = \mu(A) = |A| = \sum_{i=1}^n 1=\sum_{i=1}^n f(a_i).\]</span> In general, integration with respect to the counting measure is a simple summation. This illustrates one nice consequence of the generality that Lebesgue integration provides – summation is a special case of integration.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;What happens if we apply Lebesgue integration to the distribution of a random variable <span class="math inline">\(X\)</span> defined on the sample space <span class="math inline">\(\mathcal X\)</span>? Well if we integrate <span class="math inline">\(1\)</span> with respect to some real interval <span class="math inline">\(I=(a,b)\subset \mathbb R\)</span>, then <span class="math display">\[ \int _I 1\ dP = P(I) = F(b) - F(a).\]</span> If we instead integrate over all <span class="math inline">\(x\in\mathcal X\)</span>, we can think of the integral over the measure <span class="math inline">\(P\)</span> as a weighted sum of all <span class="math inline">\(x\)</span>, where weights are given by <span class="math inline">\(P\)</span>. This is just the <strong><em>expected value</em></strong> of <span class="math inline">\(X\)</span>. <span class="math display">\[\text{E}\left[X\right] = \int_{\mathcal X}x \ dP = \int_{\mathcal X}x\ dF_X\]</span> But how do we calculate this expectation? Even though this is a measure space over real numbers, we really don’t know how to calculate integrals with respect to any measure besides Lebesgue measure. In the event that the distribution <span class="math inline">\(F\)</span> satisfies certain standard conditions, we can find some function <span class="math inline">\(f_X\)</span> such that <span class="math display">\[ \text{E}\left[X\right] = \int_{\mathcal X}x\ dF_X = \int_{\mathcal X} xf_X(x)\ d\mu = \int_{\mathcal X} x f_X(x)\ dx.\]</span> This function is called the <strong><em>density function</em></strong> <span class="math inline">\(f(x)\)</span> of <span class="math inline">\(X\)</span>, and is actually given as <span class="math display">\[f_X = \frac{dF_X}{dx}.\]</span> In a sense, the density function is a “conversion rate” between the measure <span class="math inline">\(P\)</span> and the measure <span class="math inline">\(\mu\)</span>.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;But what about “discrete” random variables? The expectation of a discrete random variable is not an integral…or is it? Just like how the integral with respect to the counting measure reduces to a sum, if our sample space <span class="math inline">\(\mathcal X\)</span> is countably infinite such that a random variable <span class="math inline">\(X:\mathcal X \to \mathbb R\)</span> takes on one of a discrete number of values, then <span class="math display">\[ \text{E}\left[X\right] = \int_{\mathcal X} x\ dF_X = \sum_{\mathcal x\in \mathcal X}x f(x).\]</span></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Definitions are readily extended to higher dimensions. $$</p>
<p>If you couldn’t care less about measure theory,<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> then all this boils down to two facts:</p>
<ol type="1">
<li>All results will be stated in terms of continuous random variables. If you want to show them for discrete random variables, just replace integrals with sums.</li>
<li>Sometimes we will write expectation as <span class="math inline">\(\int x\ dF_X\)</span>, which is the same as <span class="math display">\[\int xf(x)\ dx.\]</span></li>
</ol>
</section>
<section id="random-matrices-and-vectors" class="level2">
<h2 class="anchored" data-anchor-id="random-matrices-and-vectors">Random Matrices and Vectors</h2>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3</strong></span> A <span class="math inline">\(m\)</span> by <span class="math inline">\(n\)</span> <span style="color:red"><strong><em>random matrix</em></strong></span> <span class="math inline">\(\mathbb{X}\)</span> is a matrix whose entries are <span class="math inline">\(m\times n\)</span> random variables <span class="math inline">\(X_{i,j}\)</span>, where <span class="math inline">\(\mathbb{X}_{i,j} = X_{i,j}\)</span>. <span class="math display">\[\begin{align*}
\mathbb{X}= \begin{bmatrix}
X_{11}&amp;\cdots&amp; X_{1n}\\\vdots&amp;\ddots&amp;\vdots\\X_{m1}&amp;\cdots&amp; X_{mn}
\end{bmatrix}.
\end{align*}\]</span> In the event that either <span class="math inline">\(m=1\)</span> or <span class="math inline">\(n=1\)</span> we have a <span style="color:red"><strong><em>random vector</em></strong></span> <span class="math inline">\(\mathbf{X}\)</span>.</p>
</div>
<p>We will often want to write a random matrix <span class="math inline">\(\mathbb{X}\)</span> as a collection of random vector <span class="math inline">\(\mathbf{X}\)</span>. Whether these be column vectors or row vectors depends on the context. If <span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(m\times n\)</span> random matrix where columns are indexed by <span class="math inline">\(i\)</span> and rows by <span class="math inline">\(j\)</span>, we will take <span class="math inline">\(\mathbf{X}_j\)</span> to be a column vector and <span class="math inline">\(\mathbf{X}_i\)</span> to be a row vector. <span class="math display">\[\begin{align*}
\mathbb{X}&amp;= \begin{bmatrix}\mathbf{X}_1 &amp; \cdots &amp; \mathbf{X}_i &amp;\cdots &amp; \mathbf{X}_n\end{bmatrix}\\
\mathbb{X}&amp;= \begin{bmatrix}\mathbf{X}_1 \\ \vdots \\ \mathbf{X}_j \\ \vdots \\ \mathbf{X}_m\end{bmatrix}
\end{align*}\]</span></p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4</strong></span> The <span style="color:red"><strong><em>expectation</em></strong></span> of a random matrix <span class="math inline">\(\mathbb{X}\)</span> is defined as</p>
<p><span class="math display">\[\begin{align*}
\text{E}\left[\mathbb{X}\right] = \begin{bmatrix}
\text{E}[X_{11}]&amp;\cdots&amp; \text{E}[X_{1n}]\\\vdots&amp;\ddots&amp;\vdots\\\text{E}[X_{m1}]&amp;\cdots&amp; \text{E}[X_{mn}]
\end{bmatrix}.
\end{align*}\]</span></p>
</div>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1 (Properties of Expectation)</strong></span> Suppose <span class="math inline">\(\mathbb{X}\)</span> and <span class="math inline">\(\mathbb Y\)</span> are <span class="math inline">\(m\times n\)</span> random matrices, <span class="math inline">\(\mathbf A\)</span> is a <span class="math inline">\(\ell\times m\)</span> matrix, <span class="math inline">\(\mathbf B\)</span> is a <span class="math inline">\(n\times k\)</span> matrix, and <span class="math inline">\(c\)</span> is a scalar. Then:</p>
<ol type="1">
<li><span class="math inline">\(\text{E}\left[\mathbb{X}'\right]= \text{E}\left[\mathbb{X}\right]'\)</span></li>
<li><span class="math inline">\(\text{E}\left[c\mathbb{X}\right]=c\text{E}\left[\mathbb{X}\right]\)</span></li>
<li><span class="math inline">\(\text{E}\left[\mathbf A \mathbb{X}\mathbf B\right] = \mathbf A \text{E}\left[\mathbb{X}\right] \mathbf B\)</span></li>
<li><span class="math inline">\(\text{E}\left[\mathbb{X}+ \mathbb Y\right] = \text{E}\left[\mathbb{X}\right] +\text{E}\left[\mathbb Y\right]\)</span></li>
</ol>
</div>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5</strong></span> The <span style="color:red"><strong><em>variance/covariance matrix</em></strong></span> of a random column vector <span class="math inline">\(\mathbf{X}= [X_1, \ldots, X_n]'\)</span> is defined as <span class="math display">\[ \text{Var}\left(\mathbf{X}\right) = \text{E}\left[(\mathbf{X}- \text{E}\left[\mathbf{X}\right])(\mathbf{X}- \text{E}\left[\mathbf{X}\right])'\right]. \]</span></p>
</div>
<p>The definition of <span class="math inline">\(\text{Var}\left(\mathbf{X}\right)\)</span> can be rewritten in a much more approachable form:</p>
<p><span class="math display">\[\begin{align*}
\text{Var}\left(\mathbf{X}\right) &amp;= \text{E}\left[(\mathbf{X}- \text{E}\left[\mathbf{X}\right])(\mathbf{X}- \text{E}\left[\mathbf{X}\right])'\right]\\\\
  &amp; = \text{E}\begin{bmatrix}
(X_1 - \text{E}\left[X_1\right])(X_1 - \text{E}\left[X_1\right])&amp;\cdots&amp; (X_1 - \text{E}\left[X_1\right])(X_n - \text{E}\left[X_n\right])\\\vdots&amp;\ddots&amp;\vdots\\(X_n - \text{E}\left[X_n\right])(X_1 - \text{E}\left[X_1\right])&amp;\cdots&amp; (X_n - \text{E}\left[X_n\right])(X_n - \text{E}\left[X_n\right])\end{bmatrix}\\\\
  &amp; = \begin{bmatrix}
\text{E}\left[(X_1 - \text{E}\left[X_1\right])^2\right]&amp;\cdots&amp; \text{E}\left[(X_1 - \text{E}\left[X_1\right])(X_n - \text{E}\left[X_n\right])\right]\\\vdots&amp;\ddots&amp;\vdots\\\text{E}\left[(X_n - \text{E}\left[X_n\right])(X_1 - \text{E}\left[X_1\right])\right]&amp;\cdots&amp; \text{E}{(X_n - \text{E}\left[X_n\right])^2}
\end{bmatrix}\\\\&amp; = \begin{bmatrix}
\text{Var}\left(X_1\right)&amp;\cdots&amp; \text{Cov}\left(X_1, X_n\right)\\\vdots&amp;\ddots&amp;\vdots\\\text{Cov}\left(X_n, X_1\right)&amp;\cdots&amp; \text{Var}\left(X_n\right)
\end{bmatrix}
\end{align*}\]</span></p>
<p>The diagonal entries capture the dispersion of each random variable <span class="math inline">\(X_i\)</span> while the off diagonal entries correspond to the joint dispersion of all possible pairs <span class="math inline">\((X_i,X_j)\)</span>.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6</strong></span> The <span style="color:red"><strong><em>covariance matrix</em></strong></span> of random column vectors <span class="math inline">\(\mathbf{X}= [X_1, \ldots, X_n]'\)</span> and <span class="math inline">\(\mathbf{Y}= [Y_1, \ldots, Y_n]'\)</span><br>
<span class="math display">\[ \text{Cov}\left(\mathbf{X},\mathbf{Y}\right) = \text{E}\left[(\mathbf{X}- \text{E}\left[\mathbf{X}\right])(\mathbf{Y}- \text{E}\left[\mathbf{Y}\right])'\right]. \]</span></p>
</div>
<p>Alternatively, we can write the covariance matrix between two random vectors as:</p>
<p><span class="math display">\[\begin{align*}
\text{Cov}\left(\mathbf{X},\mathbf{Y}\right)=\begin{bmatrix}
\text{Cov}\left(X_1, Y_1\right)&amp;\cdots&amp; \text{Cov}\left(X_1, Y_n\right)\\\vdots&amp;\ddots&amp;\vdots\\\text{Cov}\left(X_n, Y_1\right)&amp;\cdots&amp; \text{Cov}\left(X_n,Y_n\right)
\end{bmatrix}
\end{align*}\]</span></p>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 2 (Properties of Variance/Covariance)</strong></span> Suppose <span class="math inline">\(\mathbf{X}_1\)</span> and <span class="math inline">\(\mathbf{X}_2\)</span> are random column vectors of length <span class="math inline">\(n\)</span>, <span class="math inline">\(\mathbf A\)</span> and <span class="math inline">\(\mathbf B\)</span> are matrices with <span class="math inline">\(n\)</span> columns, and <span class="math inline">\(\mathbf c\)</span> and <span class="math inline">\(\mathbf d\)</span> are column vectors of length <span class="math inline">\(n\)</span>. Then:</p>
<ol type="1">
<li><span class="math inline">\(\text{Var}\left(\mathbf A\mathbf{X}+\mathbf c\right)= \mathbf A \text{Var}\left(\mathbf{X}\right) \mathbf A'\)</span></li>
<li><span class="math inline">\(\text{Cov}\left(\mathbf A\mathbf{X}+\mathbf b, \mathbf B\mathbf{y}+\mathbf c\right) = \mathbf A \text{Cov}\left(\mathbf{X},\mathbf{Y}\right)\mathbf B'\)</span></li>
</ol>
</div>
<p>The first part of this proposition is a generalization of the familiar result that <span class="math inline">\(\text{Var}\left(aX + b\right)= a^2\text{Var}\left(x\right)\)</span>.</p>
</section>
<section id="multivariate-normal-distribution" class="level2">
<h2 class="anchored" data-anchor-id="multivariate-normal-distribution">Multivariate Normal Distribution</h2>
<p>Recall that if <span class="math inline">\(X\sim N(\mu,\sigma^2)\)</span>, then any linear function of <span class="math inline">\(X\)</span> is also normally distributed. To be precise, if <span class="math inline">\(Y=aX +b\)</span>, then <span class="math inline">\(Y\sim N(a\mu +b,a^2 \sigma^2)\)</span>. This useful properties generalizes to random vectors.</p>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 3</strong></span> Suppose <span class="math inline">\(\mathbf{X}\sim N(\boldsymbol \mu, \boldsymbol{\Sigma})\)</span>. Then <span class="math display">\[ \mathbf A \mathbf{X}+ \mathbf b \sim N(\mathbf A\boldsymbol\mu +\mathbf b, \mathbf A\boldsymbol{\Sigma}\mathbf A')\]</span> where <span class="math inline">\(\mathbf A\)</span> and <span class="math inline">\(\mathbf b\)</span> are a matrix and vector with compatible dimensions.</p>
</div>
</section>
<section id="conditional-expectation-and-independence" class="level2">
<h2 class="anchored" data-anchor-id="conditional-expectation-and-independence">Conditional Expectation and Independence</h2>
<p>As put by <span class="citation" data-cites="wooldridge2010econometric">Wooldridge (<a href="#ref-wooldridge2010econometric" role="doc-biblioref">2010</a>)</span>:</p>
<blockquote class="blockquote">
<p>A substantial portion of research in econometric methodology can be interpreted as finding ways to estimate conditional expectations in the numerous settings that arise in economic applications.</p>
</blockquote>
<p>For this reason, properties related to conditional expectation will prove useful time and time again. The first of these is the law of iterated expectation.</p>
<div class="theorem">
<p>Suppose <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables, then <span class="math display">\[\text{E}\left[\text{E}\left[X\mid Y\right]\right]= \text{E}\left[X\right]\]</span></p>
</div>
<p>Conditional expectation is related to the idea of independence. As one may remember from a probability course, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables, then <span class="math inline">\(\text{E}\left[X\mid Y\right]= \text{E}\left[\mathbf{X}\right]\)</span>. But is the converse true? As it turns out, the converse does not hold, so we have multiple notions of “independence”.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7</strong></span> Suppose <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables with densities <span class="math inline">\(f_X(x)\)</span> and <span class="math inline">\(f_Y(y)\)</span>, respectively, and a joint density of <span class="math inline">\(f_{X,Y}(x,y)\)</span>. <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <span style="color:red"><strong><em>independent</em></strong></span>, written as <span class="math inline">\(X\perp Y\)</span>, if <span class="math display">\[f_{X,Y}(x,y)=f_X(x)f_Y(y).\]</span></p>
</div>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8</strong></span> Suppose <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables. <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <span style="color:red"><strong><em>mean independent</em></strong></span> if <span class="math display">\[\text{E}\left[X\mid Y\right]= \text{E}\left[X\right].\]</span></p>
</div>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9</strong></span> Suppose <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables. <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <span style="color:red"><strong><em>uncorrelated</em></strong></span> if <span class="math display">\[\text{E}\left[XY\right]= \text{E}\left[Y\right]\text{E}\left[X\right],\]</span> which is equivalent to <span class="math inline">\(\text{Cov}\left(X,Y\right) = 0\)</span>.</p>
</div>
<p>Our final result in this brief section of review related these three definitions.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1</strong></span> Suppose <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables. If <span class="math inline">\(X \perp Y\)</span>, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are mean independent. In turn, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are mean independent, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are uncorrelated.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If <span class="math inline">\(X \perp Y\)</span>, then <span class="math display">\[\begin{align*}
\text{E}\left[X\mid Y\right] &amp; = \int x f_{X\mid Y}(s\mid t)\ dx \\
              &amp; = \int x \frac{f_{X,Y}(x,y)}{f_{Y}(y)}\ dx &amp; (\text{def. of conditional probability}) \\
              &amp; = \int x \frac{f_X(x)f_Y(y)}{f_{Y}(y)}\ dx &amp; (X \perp Y) \\
              &amp; = \int x f_X(x)\ dx \\
              &amp; = \text{E}\left[X\right].
\end{align*}\]</span> In turn, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are mean independent, then <span class="math display">\[\begin{align*}
\text{E}\left[XY\right] &amp; = \text{E}\left[\text{E}\left[XY\mid Y\right]\right] &amp; (\text{Law of Iterated Expectations})\\
       &amp; = \text{E}\left[YE\left[X\mid Y\right]\right] &amp; (Y\text{ is constant}) \\
       &amp; = \text{E}\left[YE\left[X\right]\right] &amp; (\text{mean independence}) \\
       &amp; = \text{E}\left[Y\right]\text{E}\left[X\right] &amp; (\text{E}\left[X\right]\text{ is a constant})
\end{align*}\]</span></p>
</div>
</section>
<section id="existence-of-expectation" class="level2">
<h2 class="anchored" data-anchor-id="existence-of-expectation">Existence of Expectation</h2>
<p>It’s possible that the expectation of a random variable does not exist. It may be the case that <span class="math display">\[ \int_{\mathcal X}x\ dF_X \not&lt; \infty,\]</span> in which case we cannot assign an expected value to <span class="math inline">\(X\)</span>. While this is theoretically interesting, the applications where we need to be careful assuming <span class="math inline">\(\text{E}\left[X\right]\)</span> exists are few and far between. <strong><em>We will always assume the expectation, and relevant higher moments, of random variables exist</em></strong>. This is done for expositional ease, and to emphasize other assumptions that tend to be much more important.</p>
</section>
<section id="code" class="level2">
<h2 class="anchored" data-anchor-id="code">Code</h2>
<p>The majority of examples and simulations will be done in R, mostly because I like R, writing code in RStudio, and the <code>tidyverse</code>. That being said, once we get to topics in machine learning, I’ll switch over to Python. Python’s <code>scikit-learn</code> is much more comprehensive than its counterparts in R, and it seems that most machine learning tutorials use Python. I suspect this is because machine learning is applied very often in “industry” where code needs to be production ready and implementable by engineers, something Python affords that R does not. Machine learning tools like TensorFlow and Keras are also built with Python in mind. Eventually I may include a section on scientific computing and optimization, in which case I may use Julia. For <em>excellent</em> examples of economic modelling in Julia (and Python), see <a href="https://quantecon.org/">QuantEcon</a>.</p>
<p>Two other softwares/languages that are popular in (academic) economics are Stata and MATLAB. These tools share in one major drawback – they are not open source. While not as flexible as R or Python, Stata is particularly well-suited for basic econometrics. For example, I find it much easier to work with panel data and basic time series in Stata than in R. A great overview of econometric theory using Stata is provided by <span class="citation" data-cites="cameron2010microeconometrics">Adrian Colin Cameron, Trivedi, et al. (<a href="#ref-cameron2010microeconometrics" role="doc-biblioref">2010</a>)</span> (which is a less technical companion to <span class="citation" data-cites="cameron2005microeconometrics">A. Colin Cameron and Trivedi (<a href="#ref-cameron2005microeconometrics" role="doc-biblioref">2005</a>)</span>).</p>
</section>
<section id="organization" class="level2">
<h2 class="anchored" data-anchor-id="organization">Organization</h2>
<p>The organization of sections roughly follows a handful of econometrics courses I took while at Boston College.</p>
<section id="part-i---statistics" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="part-i---statistics">Part I - Statistics</h3>
<p>We’ll start with a review of mathematical statistics presented at the level of <span class="citation" data-cites="lehmann2006theory">Lehmann and Casella (<a href="#ref-lehmann2006theory" role="doc-biblioref">1998</a>)</span> of <span class="citation" data-cites="lehmann2005testing">Romano and Lehmann (<a href="#ref-lehmann2005testing" role="doc-biblioref">2005</a>)</span>.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> While the concepts will likely be familiar, they may seem a bit more technical when defined in the context of statistical decision theory. The focus is entirely on frequentist statistics, as Bayesian statistics will be covered later on.</p>
<ol start="2" type="1">
<li><strong>Finite Sample Properties of Estimators</strong>: We consider the problem of estimation, and give formal definitions and related notation to nebulous concepts such as: models, parametrizations, identification, statistics, parametric, semi-parametric, non-parametric, and estimators. Then we consider how to assess estimators for a fixed sample size.</li>
<li><strong>Asymptotic Properties of Estimators</strong>: In most cases, we won’t be able to determine finite sample properties of estimators, so we consider the situation where <span class="math inline">\(n\to \infty\)</span>. Familiar results like the central limit theorem (CLT) and law of large numbers (LLN) will be discussed, but we’ll also introduce a handful of essential results (the continuous mapping theorem, Slutsky’s theorem, the delta method) that will enable us to use the CLT and LLN to determine the asymptotic behavior of almost every estimator we will consider.<br>
</li>
<li><strong>Hypothesis Testing</strong>: The problem of estimation is only one part of statistics. The other is inference. We give an overview of inference and hypothesis testing using the Neyman-Pearson framework, and then consider how inference in relation to asymptotics. Two large sample tests (the Wald test and <span class="math inline">\(t-\)</span>test) are covered.</li>
<li><strong>Exponential Families</strong>: A quick treatment of a special type of probability distribution is given. Familiarity with these distributions is not necessary as far as econometrics is concerned, but a cursory understanding will make it possible to illuminate some cool connections between statistics and econometrics. In particular, we’ll see exponential families come up when considering maximum likelihood estimation (MLE), generalized linear models (GLMs), and Bayesian estimation.</li>
</ol>
</section>
<section id="part-ii---linear-models" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="part-ii---linear-models">Part II - Linear Models</h3>
<p>The foundation of econometrics is <em>the</em> linear model. In this section, we build the classical linear model from scratch adding assumptions as needed until arriving at the Gauss-Markov theorem. Then we “take the model apart” by dropping assumptions, and determine how to suitably estimate the subsequent linear models.</p>
<ol start="6" type="1">
<li><strong>Classical Regression Model</strong>: This section is a lengthy treatment of the classic linear model and estimation via ordinary least squares (OLS)</li>
<li><strong>Endogeneity I</strong></li>
<li><strong>Endogeneity II</strong></li>
<li><strong>Generalized Least Squares</strong></li>
</ol>
</section>
<section id="part-iii---estimation-framework" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="part-iii---estimation-framework">Part III - Estimation Framework</h3>
<ol start="11" type="1">
<li><strong>Extremum Estimators</strong></li>
<li><strong>The Generalized Method of Moments</strong></li>
<li><strong>Maximum Likelihood Estimation</strong></li>
</ol>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-billingsley2008probability" class="csl-entry" role="listitem">
Billingsley, Patrick. 2008. <em>Probability and Measure</em>. John Wiley &amp; Sons.
</div>
<div id="ref-cameron2005microeconometrics" class="csl-entry" role="listitem">
Cameron, A Colin, and Pravin K Trivedi. 2005. <em>Microeconometrics: Methods and Applications</em>. Cambridge university press.
</div>
<div id="ref-cameron2010microeconometrics" class="csl-entry" role="listitem">
Cameron, Adrian Colin, Pravin K Trivedi, et al. 2010. <em>Microeconometrics Using Stata</em>. Vol. 2. Stata press College Station, TX.
</div>
<div id="ref-durrett2019probability" class="csl-entry" role="listitem">
Durrett, Rick. 2019. <em>Probability: Theory and Examples</em>. Vol. 49. Cambridge university press.
</div>
<div id="ref-folland1999real" class="csl-entry" role="listitem">
Folland, Gerald B. 1999. <em>Real Analysis: Modern Techniques and Their Applications</em>. Vol. 40. John Wiley &amp; Sons.
</div>
<div id="ref-lehmann2006theory" class="csl-entry" role="listitem">
Lehmann, Erich L, and George Casella. 1998. <em>Theory of Point Estimation</em>. 2nd ed. Springer.
</div>
<div id="ref-lehmann2005testing" class="csl-entry" role="listitem">
Romano, Joseph P, and EL Lehmann. 2005. <em>Testing Statistical Hypotheses</em>. Vol. 3. Springer.
</div>
<div id="ref-royden1988real" class="csl-entry" role="listitem">
Royden, Halsey Lawrence, and Patrick Fitzpatrick. 1988. <em>Real Analysis</em>. Vol. 32. Macmillan New York.
</div>
<div id="ref-rudin" class="csl-entry" role="listitem">
Rudin, Walter. 1987. <em>Real and Complex Analysis</em>. McGraw-hill New York.
</div>
<div id="ref-wooldridge2010econometric" class="csl-entry" role="listitem">
Wooldridge, Jeffrey M. 2010. <em>Econometric Analysis of Cross Section and Panel Data</em>. MIT press.
</div>
</div>
</section>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>We cannot define measure over the entire power set <span class="math inline">\(2^{\mathcal X}\)</span>, or else we run into some technical problems.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Anytime a set is endowed with a topology (some notion of open sets), we can define a sigma-algebra as the collection of open subsets. This is known as the Borel sigma-algebra<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Examples include continuous functions, monotonic functions, and functions that are discontinuous at a countable number of points.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Technically this function is a real sequence, as a real sequence is just a mapping from the natural numbers to <span class="math inline">\(\mathbb{R}\)</span>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>More generally, such functions are called measurable.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>The converse is not true, which is why the Lebesgue integral is of more theoretical interest. It turns out there are <em>many</em> functions which are not Riemann integrable but are Lebesgue integrable.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>But is this really new? The Riemann integral is just the limit of a sum.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>In general, this type of construction is called the <strong><em>Radon–Nikodym derivative</em></strong>, and its existence is outlined by the <strong><em>Radon–Nikodym theorem</em></strong>.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>It won’t save your life in emergency situations.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>These are the two standard references used for a PhD-level statistical theory course.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
  </div>
  <div class="nav-page nav-page-next">
      <a href="./estimators.html" class="pagination-link" aria-label="Finite Sample Properties of Estimators">
        <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Finite Sample Properties of Estimators</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\plim}{plim}</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\argmin}{argmin}</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\argmax}{argmax}</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>\newcommand{\var}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Var}\left(#1\right)}</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>\newcommand{\avar}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Avar}\left(#1\right)}</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>\newcommand{\E}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{E}\left<span class="co">[</span><span class="ot">#1\right</span><span class="co">]</span>}</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>\newcommand{\cov}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Cov}\left(#1\right)}</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>\newcommand{\mse}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{MSE}\left(#1\right)}</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>\newcommand{\se}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{se}\left(#1\right)}</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>\newcommand{\limfunc}{lim} </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>\newcommand{\X}{\mathbf{X}}</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>\newcommand{\Xm}{\mathbb{X}}</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>\newcommand{\EER}{\bar{\thet}_\text{EE}}</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>\newcommand{\NLS}{\hat{\bet}_\text{NLLS}}</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>\newcommand{\z}{\mathbf{z}}</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>\newcommand{\rr}{\mathbf{r}}</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>\newcommand{\C}{\mathbf{C}}</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>\newcommand{\Pe}{\mathbf{P}}</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>\newcommand{\y}{\mathbf{y}}</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>\newcommand{\Y}{\mathbf{Y}}</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>\newcommand{\uu}{\mathbf{u}}</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>\newcommand{\e}{\mathbf{e}}</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>\newcommand{\D}{\mathbf{D}}</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>\newcommand{\x}{\mathbf{x}}</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>\newcommand{\xm}{\mathbb{x}}</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>\newcommand{\Zm}{\mathbb{Z}}</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>\newcommand{\Wm}{\mathbb{W}}</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>\newcommand{\Hm}{\mathbb{H}}</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>\newcommand{\W}{\mathbf{W}}</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>\newcommand{\Z}{\mathbf{Z}}</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>\newcommand{\Hess}{\mathbf{H}(\mathbf{\Z\mid\thet})}</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>\newcommand{\Score}{\mathbf{S}(\mathbf{\Z\mid\thet})}</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>\newcommand{\A}{\mathbf{A}}</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>\newcommand{\h}{\mathbf{h}}</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>\newcommand{\Q}{\mathbf{Q}}</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>\newcommand{\F}{\mathbf{F}}</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>\newcommand{\G}{\mathbf{G}}</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>\newcommand{\I}{\mathbf{I}}</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>\renewcommand{\D}{\mathbf{D}}</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>\renewcommand{\C}{\mathbf{C}}</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>\newcommand{\zer}{\mathbf{0}}</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLS}{\hat{\boldsymbol\beta}_\text{OLS} }</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLSOV}{\hat{\boldsymbol\beta}_\text{OLS,OV} }</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLSME}{\hat{\boldsymbol\beta}_\text{OLS,ME} }</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>\newcommand{\EE}{\hat{\boldsymbol\theta}_\text{EX} }</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>\newcommand{\ME}{\hat{\boldsymbol\theta}_\text{M} }</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>\newcommand{\MDE}{\hat{\boldsymbol\theta}_\text{MDE} }</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>\newcommand{\IV}{\hat{\boldsymbol\beta}_\text{IV} }</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>\newcommand{\TSLS}{\hat{\boldsymbol\beta}_\text{2SLS} }</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>\newcommand{\thet}{\boldsymbol{\theta}}</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>\newcommand{\et}{\boldsymbol{\eta}}</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>\newcommand{\R}{\mathbb{R}}</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>\newcommand{\Sig}{\boldsymbol{\Sigma}}</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>\newcommand{\ep}{\boldsymbol{\varepsilon}}</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>\newcommand{\Omeg}{\boldsymbol{\Omega}}</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>\newcommand{\Thet}{\boldsymbol{\Theta}}</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>\newcommand{\bet}{\boldsymbol{\beta}}</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>\newcommand{\rk}{rank}</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>\newcommand{\tsum}{\sum}</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>\newcommand{\tr}{tr}</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>\newcommand{\norm}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\left\lVert#1\right\rVert}</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>\newcommand{\abs}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\left\lvert#1\right\rvert}</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>\newcommand{\ms}{\overset{ms}{\to}}</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>\newcommand{\pto}{\overset{p}{\to}}</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>\newcommand{\iid}{\overset{iid}{\sim}}</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>\newcommand{\dto}{\overset{d}{\to}}</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>\newcommand{\asim}{\overset{a}{\sim}}</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="fu"># Preliminaries {.unnumbered}</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Multivariable Calculus</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>partial derivatives, the gradient, Jacobian matrix, Hessian matrix</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>optimization</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Linear Algebra</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>matrices and vectors</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>linear transformations</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>projections</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>PSD matrices</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Probability</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>random variables</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>distribution and density of RVs</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>expectation and variance </span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>moments </span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>common distributions</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a><span class="ss">        -   </span>normal distribution</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a><span class="ss">        -   </span>"friends" of the normal distribution: chi-squared, student's</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>            t, F distribution</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Mathematical Statistics</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>Estimation</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>Hypothesis testing</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Basic <span class="co">[</span><span class="ot">Real Analysis</span><span class="co">](https://github.com/noahjussila/analysis_notes/blob/master/Analysis%20Notes.pdf)</span></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>infimum and supremum </span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>metric spaces </span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>compact sets in $\mathbb R$</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>mean value theorem</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>convergence of sequences</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>pointwise versus uniform convergence</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>Taylor series</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Basic Numerical Optimization </span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>"numerical" vs. "analytic"</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a><span class="fu">## Probablitiy Theory</span></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>\indent We'll briefly go over the basics of probability theory. A rigorous treatment can be found in @durrett2019probability or @billingsley2008probability. For an even more general discussion, see @folland1999real, @royden1988real, and/or @rudin.</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>::: {#def-}</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>A <span class="co">[</span><span class="ot">***measure space***</span><span class="co">]</span>{style="color:red"} is a triple $(X, \mathcal F, \mu)$ comprised of:</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>A set $\mathcal X$.</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>A collection of subsets of $X$, $\mathcal F\subseteq 2^{X}$. This collection of sets satisfies the following properties: $A^c\in \mathcal F$ if $A\in \mathcal F$, and $\cup_{\alpha} A_\alpha \subseteq \mathcal F$ for all countable collections of sets $\{A_\alpha<span class="sc">\}</span>\subseteq \mathcal F$. Such a collection of sets is known as a <span class="co">[</span><span class="ot">***sigma-algebra***</span><span class="co">]</span>{style="color:red"}.^<span class="co">[</span><span class="ot">We cannot define measure over the entire power set $2^{\mathcal X}$, or else we run into some technical problems.</span><span class="co">]</span> </span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>A <span class="co">[</span><span class="ot">***measure***</span><span class="co">]</span>{style="color:red"} $\mu:\mathcal F\to <span class="co">[</span><span class="ot">0,\infty</span><span class="co">]</span>$ satisfying $\mu(\emptyset) = 0$, and $\mu(\sum_\alpha A_\alpha)= \sum_\alpha \mu(A_\alpha)$ for all countable collections of *disjoint* sets $\{A_\alpha<span class="sc">\}</span>\subseteq \mathcal F$.</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>We also refer to just the pair $(X,\mathcal F)$ as a <span class="co">[</span><span class="ot">***measurable space***</span><span class="co">]</span>{style="color:red"}.</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a><span class="fu">## Lebesgue Measure</span></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>The most important such space is used to measure subsets of the real line (and more generally euclidean spaces). If we want to measure subsets of $\mathbb R$, we define the associated sigma-algebra as the collection of all compliments and countable unions of the open intervals (along with the subsequent sets generated), denoted $\mathcal B(\mathbb R) = <span class="sc">\{</span>(a,b)\subset \mathbb R\mid a,b\in\mathbb R<span class="sc">\}</span>$.^<span class="co">[</span><span class="ot">Anytime a set is endowed with a topology (some notion of open sets), we can define a sigma-algebra as the collection of open subsets. This is known as the Borel sigma-algebra</span><span class="co">]</span> The measure of some interval $I = (a,b)\subset \mathbb R$ is defined as $$m(I)=b -a,$$ which is fairly reasonable. This measure is known as the Lebesgue measure.</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a><span class="fu">## Counting Measure</span></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>Another nice example of a measure space is $(X, 2^{X}, \mu)$ where $$\mu(A) = \begin{cases}|A|&amp; A\text{ finite}<span class="sc">\\</span> \infty&amp; A\text{ infinite}\end{cases}.$$ This measure simply assigns each set its cardinality, and is known as the counting measure.  </span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; One of the main motivations of measure theory is to make the operation of integration more robust. We can do this by defining a property of functions that will make integration with respect to a measure possible. </span></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>::: {#def-}</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>Let $(X, \mathcal F)$ and $(Y, \mathcal G)$ be two measurable spaces. A function $f:X\to Y$ is <span class="co">[</span><span class="ot">***measurable***</span><span class="co">]</span>{style="color:red"} if </span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>&amp; f^{-1}(G) \in \mathcal F &amp; (\forall G\in\mathcal G)</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>where $f^{-1}(G)$ is the preimage of a set $G\in \mathcal G$.</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; It turns out that any function that is Riemann integrable is measurable.^[Examples include continuous functions, monotonic functions, and functions that are discontinuous at a countable number of points.] Defining the integral of a measurable function with respect to a measure $\mu$ requires some intermediate steps that make for a verbose definition. Instead of presenting the technical definition, we'll just introduce the notation:</span></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>$$\int_{<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>} f\ d\mu$$ This is the <span class="co">[</span><span class="ot">***Lebesgue integral***</span><span class="co">]</span>{style="color:red"} of $f$ on $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>\subseteq \mathcal F$ with respect to the measure $\mu$. </span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a><span class="fu">## Integration and Lebesgue Measure</span></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>Suppose we have the measure space $(\R, \mathcal B(\R), m)$. For a measurable function $f:<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>\to\R$ we have </span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>$$\int_{<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>}f\ dm = \int_a^b f(x)\ dx$$ where the later integral is the Riemann integral. Since they're equal, it's rare that we need to turn to tools beyond basic calculus to integrate functions.</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a><span class="fu">## Integration and Counting Measure</span></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>Suppose we have a measure space $(\mathbb N, 2^{\mathbb N}, \mu)$ where $\mu$ is the counting measure. If we have some function $f:\mathbb N\to\R$^<span class="co">[</span><span class="ot">Technically this function is a real sequence, as a real sequence is just a mapping from the natural numbers to $\R$.</span><span class="co">]</span> and a set $A\subset \mathbb N$ on which we want to integrate $f$, then </span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>$$\int_A f\ d\mu = \sum_{x\in A}f(x).$$ Proving why this is the case requires the formal definition of the Lebesgue integral, but the intuition shouldn't be anything new. An integral can be thought of as a continuous version of summation, so if we integrate on a discrete set we should just end up with a summation.</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The fact that the Lebesgue integral reduces to summation demonstrates why it's such a powerful tool. </span></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Given a probability space $(\mathcal X, \mathcal F, P)$, we can define a function $X:\mathcal X\to E$ where $E$ is some set equipped with a sigma-algebra $\mathcal E$. This function is a **_random variable_** if the preimage of every set in $\mathcal E$ is in $\mathcal F$.^[More generally, such functions are called measurable.] </span></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>$$ X^{-1}(I) \in \mathcal F \ \ \ \forall I\in\mathcal E.$$ In most cases, we're interested in random variables that take on real values (equipped with the $\mathcal B(\mathbb R)$) in which case $X:\mathcal X\to \R$ and </span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>$$ X^{-1}(I) \in \mathcal F \ \ \ \forall I\in\mathcal B(\R).$$ In other words, if we can measure $I$ using the Lebesgue measure $\mu$, we are able to measure $X^{-1}(I)$ using the probability measure $P$. This allows us to define a probability measure $P(X^{-1}(I))$ on $\mathcal B(\mathbb R)$ which gives us the probability that $X$ is in the set $I\subset \mathcal B(\mathbb R)$. This probability measure is the **_distribution_** of $X$. If $I = (-\infty,x)$ for $x\in\mathcal X$, then the distribution takes the form $P(X^{-1}(I)) = P(X \in (-\infty,x)) = P(X\le x)$. We usually work with the distribution in this form and call it the  **_distribution function_** $F_X(x) = P(X\le x)$.</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The chief motivation for defining measure spaces and measurable functions is a general theory of integration. The Riemann integral is achieved by taking the limit of a process which partitions the area under a function into rectangles. The width of these rectangles is the length of their base, which can be thought of as the measure of an interval. When performing Riemann integration, if $I=(a,b)$ is the base of a rectangle, we take the width to be $m(I)=b -a$. That is, we use the Lebesgue measure. This is why you will sometimes see integrals written as </span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>$$ \int f(x)\ dx = \int f(x) \ dm.$$ Writing the integral this way emphasize that we are integrating with respect to some measure (notion of length/volume), but it is more than just a notational difference. It is the result of defining integration in an entirely different way giving the **_Lebesgue integral (with respect to $m$)_**. In practice this won't matter a whole lot, because if we can calculate the Riemann integral of a function than we can calculate the Lebesgue integral, and both integrals are equal.^[The converse is not true, which is why the Lebesgue integral is of more theoretical interest. It turns out there are *many* functions which are not Riemann integrable but are Lebesgue integrable.] What's important is that the idea of integration with respect to a measure can be generalized to any measure space $(X, \mathcal N,\mu)$. In general, the **_Lebesgue integral_** of $f$ over $A\in \mathcal N$ is written as $$\int_A f \ d\mu.$$ The rigorous definition of this integral, and how it is calculated using that definition, aren't essential at the moment. That being said, one useful property to know is that if we integrate the constant $1$ on $A$ with respect to a measure $\mu$, we get $\mu(A)$. </span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>$$\int_A 1\ d\mu = \int_A d\mu =\mu(A)$$ This should seem familiar from calculus. If we have a set $I=(a,b)$, then $$\int_a^b 1\ dx = b - a$$ using Riemann integration. Lebesgue integration gives </span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>$$ \int_I 1\ dm = m(I) = b-a.$$ Interestingly, if we look at counting measure on the set of real numbers (our measure space is $(\mathbb R, 2^{\mathbb R}, \mu)$), and some set $A = <span class="sc">\{</span>a_1,\ldots, a_n<span class="sc">\}</span>\in 2^{\mathbb R}$ </span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>$$ \int_Af\ d\mu = \int_A 1\ d\mu = \mu(A) = |A| = \sum_{i=1}^n 1=\sum_{i=1}^n f(a_i).$$ In general, integration with respect to the counting measure is a simple summation. This illustrates one nice consequence of the generality that Lebesgue integration provides -- summation is a special case of integration.^<span class="co">[</span><span class="ot">But is this really new? The Riemann integral is just the limit of a sum.</span><span class="co">]</span> </span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;What happens if we apply Lebesgue integration to the distribution of a random variable $X$ defined on the sample space $\mathcal X$? Well if we integrate $1$ with respect to some real interval $I=(a,b)\subset \mathbb R$, then </span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>$$ \int _I 1\ dP = P(I) = F(b) - F(a).$$ If we instead integrate over all $x\in\mathcal X$, we can think of the integral over the measure $P$ as a weighted sum of all $x$, where weights are given by $P$. This is just the **_expected value_** of $X$. $$\E{X} = \int_{\mathcal X}x \ dP = \int_{\mathcal X}x\ dF_X$$ But how do we calculate this expectation? Even though this is a measure space over real numbers, we really don't know how to calculate integrals with respect to any measure besides Lebesgue measure. In the event that the distribution $F$ satisfies certain standard conditions, we can find some function $f_X$ such that </span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>$$ \E{X} = \int_{\mathcal X}x\ dF_X = \int_{\mathcal X} xf_X(x)\ d\mu = \int_{\mathcal X} x f_X(x)\ dx.$$ This function is called the **_density function_** $f(x)$ of $X$, and is actually given as $$f_X = \frac{dF_X}{dx}.$$ In a sense, the density function is a "conversion rate" between the measure $P$ and the measure $\mu$.^[In general, this type of construction is called the **_Radon–Nikodym derivative_**, and its existence is outlined by the **_Radon–Nikodym theorem_**.] </span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;But what about "discrete" random variables? The expectation of a discrete random variable is not an integral...or is it? Just like how the integral with respect to the counting measure reduces to a sum, if our sample space $\mathcal X$ is countably infinite such that a random variable $X:\mathcal X \to \mathbb R$ takes on one of a discrete number of values, then </span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>$$ \E{X} = \int_{\mathcal X} x\ dF_X = \sum_{\mathcal x\in \mathcal X}x f(x).$$</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a><span class="in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Definitions are readily extended to higher dimensions.  $$</span></span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>If you couldn't care less about measure theory,^<span class="co">[</span><span class="ot">It won't save your life in emergency situations.</span><span class="co">]</span> then all this boils down to two facts:</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>All results will be stated in terms of continuous random variables. If you want to show them for discrete random variables, just replace integrals with sums.</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Sometimes we will write expectation as $\int x\ dF_X$, which is the same as $$\int xf(x)\ dx.$$</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a><span class="fu">## Random Matrices and Vectors</span></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>::: {#def-} </span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>A $m$ by $n$ &lt;span style="color:red"&gt;**_random matrix_**&lt;/span&gt; $\mathbb{X}$ is a matrix whose entries</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>are $m\times n$ random variables $X_{i,j}$, where</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>$\mathbb{X}_{i,j} = X_{i,j}$.</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>\mathbb{X}= \begin{bmatrix}</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>X_{11}&amp;\cdots&amp; X_{1n}<span class="sc">\\</span>\vdots&amp;\ddots&amp;\vdots<span class="sc">\\</span>X_{m1}&amp;\cdots&amp; X_{mn}</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}.</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>In the event that either $m=1$ or $n=1$ we have a</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>&lt;span style="color:red"&gt;**_random vector_**&lt;/span&gt; $\mathbf{X}$. </span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>We will often want to write a random matrix $\Xm$ as a collection of random vector $\X$. Whether</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>these be column vectors or row vectors depends on the context. If $\X$ is an $m\times n$ random matrix</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a>where columns are indexed by $i$ and rows by $j$, we will take $\X_j$ to be a column vector and $\X_i$ to be a row vector.</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>\Xm &amp;= \begin{bmatrix}\X_1 &amp; \cdots &amp; \X_i &amp;\cdots &amp; \X_n\end{bmatrix}<span class="sc">\\</span></span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>\Xm &amp;= \begin{bmatrix}\X_1 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \X_j <span class="sc">\\</span> \vdots <span class="sc">\\</span> \X_m\end{bmatrix}</span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>::: {#def-} </span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>The &lt;span style="color:red"&gt;**_expectation_**&lt;/span&gt; of a random matrix $\mathbb{X}$ is defined as</span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a>\E{\Xm} = \begin{bmatrix}</span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>\text{E}<span class="co">[</span><span class="ot">X_{11}</span><span class="co">]</span>&amp;\cdots&amp; \text{E}<span class="co">[</span><span class="ot">X_{1n}</span><span class="co">]</span><span class="sc">\\</span>\vdots&amp;\ddots&amp;\vdots<span class="sc">\\</span>\text{E}<span class="co">[</span><span class="ot">X_{m1}</span><span class="co">]</span>&amp;\cdots&amp; \text{E}<span class="co">[</span><span class="ot">X_{mn}</span><span class="co">]</span></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}.</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>::: {#prp-}</span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a><span class="fu">## Properties of Expectation</span></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>Suppose $\mathbb{X}$ and $\mathbb Y$ are $m\times n$ random matrices,</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a>$\mathbf A$ is a $\ell\times m$ matrix, $\mathbf B$ is a $n\times k$</span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>matrix, and $c$ is a scalar. Then:</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>$\text{E}\left<span class="co">[</span><span class="ot">\mathbb{X}'\right</span><span class="co">]</span>= \text{E}\left<span class="co">[</span><span class="ot">\mathbb{X}\right</span><span class="co">]</span>'$</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>$\text{E}\left<span class="co">[</span><span class="ot">c\mathbb{X}\right</span><span class="co">]</span>=c\text{E}\left<span class="co">[</span><span class="ot">\mathbb{X}\right</span><span class="co">]</span>$</span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>$\text{E}\left<span class="co">[</span><span class="ot">\mathbf A \mathbb{X}\mathbf B\right</span><span class="co">]</span> = \mathbf A \text{E}\left<span class="co">[</span><span class="ot">\mathbb{X}\right</span><span class="co">]</span> \mathbf B$</span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>$\text{E}\left<span class="co">[</span><span class="ot">\mathbb{X}+ \mathbb Y\right</span><span class="co">]</span> = \text{E}\left<span class="co">[</span><span class="ot">\mathbb{X}\right</span><span class="co">]</span> +\text{E}\left<span class="co">[</span><span class="ot">\mathbb Y\right</span><span class="co">]</span>$</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>::: {#def-} </span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>The &lt;span style="color:red"&gt;**_variance/covariance matrix_**&lt;/span&gt; of a random column vector</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>$\mathbf{X}= <span class="co">[</span><span class="ot">X_1, \ldots, X_n</span><span class="co">]</span>'$ is defined as</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>$$ \text{Var}\left(\mathbf{X}\right) = \text{E}\left<span class="co">[</span><span class="ot">(\mathbf{X}- \text{E}\left[\mathbf{X}\right])(\mathbf{X}- \text{E}\left[\mathbf{X}\right])'\right</span><span class="co">]</span>. $$</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>The definition of $\text{Var}\left(\mathbf{X}\right)$ can be rewritten</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a>in a much more approachable form:</span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>\text{Var}\left(\mathbf{X}\right) &amp;= \text{E}\left<span class="co">[</span><span class="ot">(\mathbf{X}- \text{E}\left[\mathbf{X}\right])(\mathbf{X}- \text{E}\left[\mathbf{X}\right])'\right</span><span class="co">]</span><span class="sc">\\\\</span></span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>  &amp; = \text{E}\begin{bmatrix}</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>(X_1 - \text{E}\left<span class="co">[</span><span class="ot">X_1\right</span><span class="co">]</span>)(X_1 - \text{E}\left<span class="co">[</span><span class="ot">X_1\right</span><span class="co">]</span>)&amp;\cdots&amp; (X_1 - \text{E}\left<span class="co">[</span><span class="ot">X_1\right</span><span class="co">]</span>)(X_n - \text{E}\left<span class="co">[</span><span class="ot">X_n\right</span><span class="co">]</span>)<span class="sc">\\</span>\vdots&amp;\ddots&amp;\vdots<span class="sc">\\</span>(X_n - \text{E}\left<span class="co">[</span><span class="ot">X_n\right</span><span class="co">]</span>)(X_1 - \text{E}\left<span class="co">[</span><span class="ot">X_1\right</span><span class="co">]</span>)&amp;\cdots&amp; (X_n - \text{E}\left<span class="co">[</span><span class="ot">X_n\right</span><span class="co">]</span>)(X_n - \text{E}\left<span class="co">[</span><span class="ot">X_n\right</span><span class="co">]</span>)\end{bmatrix}<span class="sc">\\\\</span></span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a>  &amp; = \begin{bmatrix}</span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>\text{E}\left<span class="co">[</span><span class="ot">(X_1 - \text{E}\left[X_1\right])^2\right</span><span class="co">]</span>&amp;\cdots&amp; \text{E}\left<span class="co">[</span><span class="ot">(X_1 - \text{E}\left[X_1\right])(X_n - \text{E}\left[X_n\right])\right</span><span class="co">]</span><span class="sc">\\</span>\vdots&amp;\ddots&amp;\vdots<span class="sc">\\</span>\text{E}\left<span class="co">[</span><span class="ot">(X_n - \text{E}\left[X_n\right])(X_1 - \text{E}\left[X_1\right])\right</span><span class="co">]</span>&amp;\cdots&amp; \text{E}{(X_n - \text{E}\left<span class="co">[</span><span class="ot">X_n\right</span><span class="co">]</span>)^2}</span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}<span class="sc">\\\\</span>&amp; = \begin{bmatrix}</span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a>\text{Var}\left(X_1\right)&amp;\cdots&amp; \text{Cov}\left(X_1, X_n\right)<span class="sc">\\</span>\vdots&amp;\ddots&amp;\vdots<span class="sc">\\</span>\text{Cov}\left(X_n, X_1\right)&amp;\cdots&amp; \text{Var}\left(X_n\right)</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a>The diagonal entries capture the dispersion of each random variable</span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>$X_i$ while the off diagonal entries correspond to the joint dispersion</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a>of all possible pairs $(X_i,X_j)$.</span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>::: {#def-} </span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a>The &lt;span style="color:red"&gt;**_covariance matrix_**&lt;/span&gt; of random column vectors</span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a>$\mathbf{X}= <span class="co">[</span><span class="ot">X_1, \ldots, X_n</span><span class="co">]</span>'$ and $\mathbf{Y}= <span class="co">[</span><span class="ot">Y_1, \ldots, Y_n</span><span class="co">]</span>'$\</span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a>$$ \text{Cov}\left(\mathbf{X},\mathbf{Y}\right) = \text{E}\left<span class="co">[</span><span class="ot">(\mathbf{X}- \text{E}\left[\mathbf{X}\right])(\mathbf{Y}- \text{E}\left[\mathbf{Y}\right])'\right</span><span class="co">]</span>. $$</span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>Alternatively, we can write the covariance matrix between two random</span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>vectors as:</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a>\text{Cov}\left(\mathbf{X},\mathbf{Y}\right)=\begin{bmatrix}</span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a>\text{Cov}\left(X_1, Y_1\right)&amp;\cdots&amp; \text{Cov}\left(X_1, Y_n\right)<span class="sc">\\</span>\vdots&amp;\ddots&amp;\vdots<span class="sc">\\</span>\text{Cov}\left(X_n, Y_1\right)&amp;\cdots&amp; \text{Cov}\left(X_n,Y_n\right)</span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a>\end{bmatrix} </span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a>::: {#prp-}</span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a><span class="fu">## Properties of Variance/Covariance</span></span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a>Suppose $\mathbf{X}_1$ and $\mathbf{X}_2$ are random column vectors of length $n$, $\mathbf A$ and $\mathbf B$ are matrices with $n$ columns, and $\mathbf c$ and $\mathbf d$ are column vectors of length $n$. Then:</span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>$\text{Var}\left(\mathbf A\mathbf{X}+\mathbf c\right)= \mathbf A \text{Var}\left(\mathbf{X}\right) \mathbf A'$</span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>$\text{Cov}\left(\mathbf A\mathbf{X}+\mathbf b, \mathbf B\mathbf{y}+\mathbf c\right) = \mathbf A \text{Cov}\left(\mathbf{X},\mathbf{Y}\right)\mathbf B'$</span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a>The first part of this proposition is a generalization of the familiar result that</span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a>$\text{Var}\left(aX + b\right)= a^2\text{Var}\left(x\right)$.</span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multivariate Normal Distribution</span></span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>Recall that if $X\sim N(\mu,\sigma^2)$, then any linear function of $X$</span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>is also normally distributed. To be precise, if $Y=aX +b$, then</span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a>$Y\sim N(a\mu +b,a^2 \sigma^2)$. This useful properties generalizes to</span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a>random vectors.</span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a>::: {#prp-}</span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a>Suppose $\mathbf{X}\sim N(\boldsymbol \mu, \boldsymbol{\Sigma})$. Then</span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a>$$ \mathbf A \mathbf{X}+ \mathbf b \sim N(\mathbf A\boldsymbol\mu +\mathbf b, \mathbf A\boldsymbol{\Sigma}\mathbf A')$$</span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>where $\mathbf A$ and $\mathbf b$ are a matrix and vector with</span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>compatible dimensions.</span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conditional Expectation and Independence</span></span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a>As put by @wooldridge2010econometric:</span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; A substantial portion of research in econometric methodology can be</span></span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; interpreted as finding ways to estimate conditional expectations in</span></span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; the numerous settings that arise in economic applications.</span></span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a>For this reason, properties related to conditional expectation will</span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a>prove useful time and time again. The first of these is the law of</span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a>iterated expectation.</span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a>::: theorem</span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a>Suppose $X$ and $Y$ are random variables, then</span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a>$$\text{E}\left<span class="co">[</span><span class="ot">\text{E}\left[X\mid Y\right]\right</span><span class="co">]</span>= \text{E}\left<span class="co">[</span><span class="ot">X\right</span><span class="co">]</span>$$</span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a>Conditional expectation is related to the idea of independence. As one</span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a>may remember from a probability course, if $X$ and $Y$ are independent</span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a>random variables, then</span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a>$\text{E}\left<span class="co">[</span><span class="ot">X\mid Y\right</span><span class="co">]</span>= \text{E}\left<span class="co">[</span><span class="ot">\mathbf{X}\right</span><span class="co">]</span>$. But is the converse</span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a>true? As it turns out, the converse does not hold, so we have multiple</span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a>notions of "independence".</span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a>::: {#def-} </span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a>Suppose $X$ and $Y$ are random variables with densities $f_X(x)$ and</span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a>$f_Y(y)$, respectively, and a joint density of $f_{X,Y}(x,y)$. $X$ and</span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a>$Y$ are &lt;span style="color:red"&gt;**_independent_**&lt;/span&gt;, written as $X\perp Y$, if</span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a>$$f_{X,Y}(x,y)=f_X(x)f_Y(y).$$</span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a>::: {#def-} </span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a>Suppose $X$ and $Y$ are random variables. $X$ and $Y$ are &lt;span style="color:red"&gt;**_mean</span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a>independent_**&lt;/span&gt; if $$\text{E}\left<span class="co">[</span><span class="ot">X\mid Y\right</span><span class="co">]</span>= \text{E}\left<span class="co">[</span><span class="ot">X\right</span><span class="co">]</span>.$$</span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a>::: {#def-} </span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a>Suppose $X$ and $Y$ are random variables. $X$ and $Y$ are</span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a>&lt;span style="color:red"&gt;**_uncorrelated_**&lt;/span&gt; if</span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a>$$\text{E}\left<span class="co">[</span><span class="ot">XY\right</span><span class="co">]</span>= \text{E}\left<span class="co">[</span><span class="ot">Y\right</span><span class="co">]</span>\text{E}\left<span class="co">[</span><span class="ot">X\right</span><span class="co">]</span>,$$ which is equivalent to $\text{Cov}\left(X,Y\right) = 0$.</span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a>Our final result in this brief section of review related these three</span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a>definitions.</span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a>:::{#thm-}</span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a>Suppose $X$ and $Y$ are random variables. If $X \perp Y$, then $X$ and $Y$ are mean independent. In turn, if $X$ and $Y$ are mean independent, then $X$ and $Y$ are uncorrelated.</span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a>If $X \perp Y$, then \begin{align*}</span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a>\text{E}\left<span class="co">[</span><span class="ot">X\mid Y\right</span><span class="co">]</span> &amp; = \int x f_{X\mid Y}(s\mid t)\ dx <span class="sc">\\</span></span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a>              &amp; = \int x \frac{f_{X,Y}(x,y)}{f_{Y}(y)}\ dx &amp; (\text{def. of conditional probability}) <span class="sc">\\</span></span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a>              &amp; = \int x \frac{f_X(x)f_Y(y)}{f_{Y}(y)}\ dx &amp; (X \perp Y) <span class="sc">\\</span></span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a>              &amp; = \int x f_X(x)\ dx <span class="sc">\\</span></span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a>              &amp; = \text{E}\left<span class="co">[</span><span class="ot">X\right</span><span class="co">]</span>.</span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a>\end{align*} In turn, if $X$ and $Y$ are mean independent, then</span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a>\text{E}\left<span class="co">[</span><span class="ot">XY\right</span><span class="co">]</span> &amp; = \text{E}\left<span class="co">[</span><span class="ot">\text{E}\left[XY\mid Y\right]\right</span><span class="co">]</span> &amp; (\text{Law of Iterated Expectations})<span class="sc">\\</span></span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a>       &amp; = \text{E}\left<span class="co">[</span><span class="ot">YE\left[X\mid Y\right]\right</span><span class="co">]</span> &amp; (Y\text{ is constant}) <span class="sc">\\</span></span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a>       &amp; = \text{E}\left<span class="co">[</span><span class="ot">YE\left[X\right]\right</span><span class="co">]</span> &amp; (\text{mean independence}) <span class="sc">\\</span></span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a>       &amp; = \text{E}\left<span class="co">[</span><span class="ot">Y\right</span><span class="co">]</span>\text{E}\left<span class="co">[</span><span class="ot">X\right</span><span class="co">]</span> &amp; (\text{E}\left<span class="co">[</span><span class="ot">X\right</span><span class="co">]</span>\text{ is a constant})</span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-363"><a href="#cb1-363" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-364"><a href="#cb1-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-365"><a href="#cb1-365" aria-hidden="true" tabindex="-1"></a><span class="fu">## Existence of Expectation</span></span>
<span id="cb1-366"><a href="#cb1-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-367"><a href="#cb1-367" aria-hidden="true" tabindex="-1"></a>It's possible that the expectation of a random variable does not exist. It may be the case that </span>
<span id="cb1-368"><a href="#cb1-368" aria-hidden="true" tabindex="-1"></a>$$ \int_{\mathcal X}x\ dF_X \not&lt; \infty,$$ in which case we cannot assign an expected value to $X$. While this is theoretically interesting, the applications where we need to be careful assuming $\E{X}$ exists are few and far between. **_We will always assume the expectation, and relevant higher moments, of random variables exist_**. This is done for expositional ease, and to emphasize other assumptions that tend to be much more important.  </span>
<span id="cb1-369"><a href="#cb1-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a><span class="fu">## Code</span></span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a>The majority of examples and simulations will be done in R, mostly because I like R, writing code in RStudio, and the ```tidyverse```. That being said, once we get to topics in machine learning, I'll switch over to Python. Python's ```scikit-learn``` is much more comprehensive than its counterparts in R, and it seems that most machine learning tutorials use Python. I suspect this is because machine learning is applied very often in "industry" where code needs to be production ready and implementable by engineers, something Python affords that R does not. Machine learning tools like TensorFlow and Keras are also built with Python in mind. Eventually I may include a section on scientific computing and optimization, in which case I may use Julia. For *excellent* examples of economic modelling in Julia (and Python), see <span class="co">[</span><span class="ot">QuantEcon</span><span class="co">](https://quantecon.org/)</span>. </span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a>Two other softwares/languages that are popular in (academic) economics are Stata and MATLAB. These tools share in one major drawback -- they are not open source. While not as flexible as R or Python, Stata is particularly well-suited for basic econometrics. For example, I find it much easier to work with panel data and basic time series in Stata than in R. A great overview of econometric theory using Stata is provided by @cameron2010microeconometrics (which is a less technical companion to @cameron2005microeconometrics).</span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a><span class="fu">## Organization </span></span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a>The organization of sections roughly follows a handful of econometrics courses I took while at Boston College. </span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a><span class="fu">### Part I - Statistics  {-}</span></span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a>We'll start with a review of mathematical statistics presented at the level of @lehmann2006theory of @lehmann2005testing.^<span class="co">[</span><span class="ot">These are the two standard references used for a PhD-level statistical theory course.</span><span class="co">]</span> While the concepts will likely be familiar, they may seem a bit more technical when defined in the context of statistical decision theory. The focus is entirely on frequentist statistics, as Bayesian statistics will be covered later on.</span>
<span id="cb1-383"><a href="#cb1-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-384"><a href="#cb1-384" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Finite Sample Properties of Estimators**: We consider the problem of estimation, and give formal definitions and related notation to nebulous concepts such as: models, parametrizations, identification, statistics, parametric, semi-parametric, non-parametric, and estimators. Then we consider how to assess estimators for a fixed sample size. </span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Asymptotic Properties of Estimators**: In most cases, we won't be able to determine finite sample properties of estimators, so we consider the situation where $n\to \infty$. Familiar results like the central limit theorem (CLT) and law of large numbers (LLN) will be discussed, but we'll also introduce a handful of essential results (the continuous mapping theorem, Slutsky's theorem, the delta method) that will enable us to use the CLT and LLN to determine the asymptotic behavior of almost every estimator we will consider.   </span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Hypothesis Testing**: The problem of estimation is only one part of statistics. The other is inference. We give an overview of inference and hypothesis testing using the Neyman-Pearson framework, and then consider how inference in relation to asymptotics. Two large sample tests (the Wald test and $t-$test) are covered. </span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Exponential Families**: A quick treatment of a special type of probability distribution is given. Familiarity with these distributions is not necessary as far as econometrics is concerned, but a cursory understanding will make it possible to illuminate some cool connections between statistics and econometrics. In particular, we'll see exponential families come up when considering maximum likelihood estimation (MLE), generalized linear models (GLMs), and Bayesian estimation. </span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a><span class="fu">### Part II - Linear Models  {-}</span></span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a>The foundation of econometrics is *the* linear model. In this section, we build the classical linear model from scratch adding assumptions as needed until arriving at the Gauss-Markov theorem. Then we "take the model apart" by dropping assumptions, and determine how to suitably estimate the subsequent linear models.</span>
<span id="cb1-392"><a href="#cb1-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-393"><a href="#cb1-393" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**Classical Regression Model**: This section is a lengthy treatment of the classic linear model and estimation via ordinary least squares (OLS)</span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>**Endogeneity I**</span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a><span class="ss">8. </span>**Endogeneity II**</span>
<span id="cb1-396"><a href="#cb1-396" aria-hidden="true" tabindex="-1"></a><span class="ss">9. </span>**Generalized Least Squares**</span>
<span id="cb1-397"><a href="#cb1-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a><span class="fu">### Part III - Estimation Framework  {-}</span></span>
<span id="cb1-399"><a href="#cb1-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-400"><a href="#cb1-400" aria-hidden="true" tabindex="-1"></a><span class="ss">11. </span>**Extremum Estimators** </span>
<span id="cb1-401"><a href="#cb1-401" aria-hidden="true" tabindex="-1"></a><span class="ss">12. </span>**The Generalized Method of Moments** </span>
<span id="cb1-402"><a href="#cb1-402" aria-hidden="true" tabindex="-1"></a><span class="ss">13. </span>**Maximum Likelihood Estimation**</span>
<span id="cb1-403"><a href="#cb1-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-404"><a href="#cb1-404" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>