<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Noah Jussila">
<meta name="dcterms.date" content="2022-10-29">

<title>Advanved Econometrics with Examples</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./estimators.html" rel="next">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Advanved Econometrics with Examples</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Advanved Econometrics with Examples</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link active">Preliminaries</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Statistical Theory</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimators.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Finite Sample Properties of Estimators</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Asymptotic Properties of Estimators</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./testing.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exp_fam.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exponential Families</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Linear Models</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ols.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./endog.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Endogeniety I: IV and 2SLS</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Estimation Frameworks</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./extremum.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Extremum Estimators</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Basic Microeconometrics and Time Series</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./binary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Binary Choice</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">Advanced Topics</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nonpar.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Nonparametrics I: Distribution and Density Estimation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#preliminaries" id="toc-preliminaries" class="nav-link active" data-scroll-target="#preliminaries">Preliminaries</a>
  <ul class="collapse">
  <li><a href="#probablitiy-theory" id="toc-probablitiy-theory" class="nav-link" data-scroll-target="#probablitiy-theory">Probablitiy Theory</a></li>
  <li><a href="#random-matrices-and-vectors" id="toc-random-matrices-and-vectors" class="nav-link" data-scroll-target="#random-matrices-and-vectors">Random Matrices and Vectors</a></li>
  <li><a href="#multivariate-normal-distribution" id="toc-multivariate-normal-distribution" class="nav-link" data-scroll-target="#multivariate-normal-distribution">Multivariate Normal Distribution</a></li>
  <li><a href="#conditional-expectation-and-independence" id="toc-conditional-expectation-and-independence" class="nav-link" data-scroll-target="#conditional-expectation-and-independence">Conditional Expectation and Independence</a></li>
  <li><a href="#existence-of-expectation" id="toc-existence-of-expectation" class="nav-link" data-scroll-target="#existence-of-expectation">Existence of Expectation</a></li>
  <li><a href="#code" id="toc-code" class="nav-link" data-scroll-target="#code">Code</a></li>
  <li><a href="#organization" id="toc-organization" class="nav-link" data-scroll-target="#organization">Organization</a>
  <ul class="collapse">
  <li><a href="#part-i---statistics" id="toc-part-i---statistics" class="nav-link" data-scroll-target="#part-i---statistics">Part I - Statistics</a></li>
  <li><a href="#part-ii---linear-models" id="toc-part-ii---linear-models" class="nav-link" data-scroll-target="#part-ii---linear-models">Part II - Linear Models</a></li>
  <li><a href="#part-iii---estimation-framework" id="toc-part-iii---estimation-framework" class="nav-link" data-scroll-target="#part-iii---estimation-framework">Part III - Estimation Framework</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title d-none d-lg-block">Advanved Econometrics with Examples</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Noah Jussila </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 29, 2022</p>
    </div>
  </div>
    
  </div>
  

</header>

<section id="preliminaries" class="level1 unnumbered">
<h1 class="unnumbered">Preliminaries</h1>
<ul>
<li>Multivariable Calculus
<ul>
<li>partial derivatives, the gradient, Jacobian matrix, Hessian matrix</li>
<li>optimization</li>
</ul></li>
<li>Linear Algebra
<ul>
<li>matrices and vectors</li>
<li>linear transformations</li>
<li>projections</li>
<li>PSD matrices</li>
</ul></li>
<li>Probability
<ul>
<li>random variables</li>
<li>distribution and density of RVs</li>
<li>expectation and variance</li>
<li>moments</li>
<li>common distributions
<ul>
<li>normal distribution</li>
<li>“friends” of the normal distribution: chi-squared, student’s t, F distribution</li>
</ul></li>
</ul></li>
<li>Mathematical Statistics
<ul>
<li>Estimation</li>
<li>Hypothesis testing</li>
</ul></li>
<li>Basic <a href="https://github.com/noahjussila/analysis_notes/blob/master/Analysis%20Notes.pdf">Real Analysis</a>
<ul>
<li>infimum and supremum</li>
<li>metric spaces</li>
<li>compact sets in <span class="math inline">\(\mathbb R\)</span></li>
<li>mean value theorem</li>
<li>convergence of sequences</li>
<li>pointwise versus uniform convergence</li>
<li>Taylor series</li>
</ul></li>
<li>Basic Numerical Optimization
<ul>
<li>“numerical” vs.&nbsp;“analytic”</li>
</ul></li>
</ul>
<section id="probablitiy-theory" class="level2">
<h2 class="anchored" data-anchor-id="probablitiy-theory">Probablitiy Theory</h2>
<p>We’ll briefly go over the basics of probability theory, <em>none of which is strictly required</em>. A rigorous treatment can be found in <span class="citation" data-cites="durrett2019probability">Durrett (<a href="references.html#ref-durrett2019probability" role="doc-biblioref">2019</a>)</span> or <span class="citation" data-cites="billingsley2008probability">Billingsley (<a href="references.html#ref-billingsley2008probability" role="doc-biblioref">2008</a>)</span>. For an even more general discussion, see <span class="citation" data-cites="folland1999real">Folland (<a href="references.html#ref-folland1999real" role="doc-biblioref">1999</a>)</span>, <span class="citation" data-cites="royden1988real">Royden and Fitzpatrick (<a href="references.html#ref-royden1988real" role="doc-biblioref">1988</a>)</span>, and/or <span class="citation" data-cites="rudin">Rudin (<a href="references.html#ref-rudin" role="doc-biblioref">1987</a>)</span>. If you have zero interest in the formal math behind probability (who would blame you?) then skip to the end of this section where all the math is reduced to two notational conventions.</p>
<p>A probability space is a triple <span class="math inline">\((\mathcal X, \mathcal F, P)\)</span> comprised of:</p>
<ol type="1">
<li>A <strong><em>sample space</em></strong> <span class="math inline">\(\mathcal X\)</span></li>
<li>A collection of events <span class="math inline">\(\mathcal F\)</span> which is a subset of the power set of events <span class="math inline">\(2^{\mathcal X}\)</span>. This collection of sets satisfies the following properties: <span class="math inline">\(\mathcal X\subset F\)</span>, <span class="math inline">\(A^c\in F\)</span> if <span class="math inline">\(A\in F\)</span>, and <span class="math inline">\(\cup_{i=1}^n A_i \subset F\)</span> if <span class="math inline">\(A_i\subset \mathcal F\)</span> for all <span class="math inline">\(i\)</span>. Such a collection of sets is known as a <strong><em>sigma-algebra</em></strong>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></li>
<li>A <strong><em>probability measure</em></strong> <span class="math inline">\(P:\mathcal F\to [0,1]\)</span> satisfying: <span class="math inline">\(P(\mathcal X) = 1\)</span>, <span class="math inline">\(P(\emptyset) = 0\)</span>, <span class="math inline">\(P(A) \le P(B)\)</span> for <span class="math inline">\(A\subset B\in \mathcal F\)</span>, and <span class="math inline">\(P\left(\cup_{i=1}^n A_i\right) = \sum_{i=1}^n P(A_i)\)</span>.</li>
</ol>
<p>A probability space is a special case of a measure space <span class="math inline">\((X, \mathcal N,\mu)\)</span>. This is just any set <span class="math inline">\(X\)</span>, an associated sigma-algebra <span class="math inline">\(\mathcal N\subseteq 2^X\)</span>, and some measure <span class="math inline">\(\mu\)</span> which assigns values to sets in <span class="math inline">\(\mathcal N\subseteq\)</span>. The most important such space is used to measure subsets of the real line (and more generally euclidean spaces). If we want to measure subsets of <span class="math inline">\(\mathbb R\)</span>, we define the associated sigma-algebra as the collection of all open intervals, denoted <span class="math inline">\(\mathcal B(\mathbb R) = \{(a,b)\subset \mathbb R\mid a,b\in\mathbb R\}\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> The measure of some interval <span class="math inline">\(I = (a,b)\subset \mathbb R\)</span> is defined as <span class="math display">\[m(I)=b -a,\]</span> which is fairly reasonable. This measure is known as the <strong><em>Lebesgue measure</em></strong>. Another nice example of a measure space is <span class="math inline">\((X, 2^{X}, \mu)\)</span> where <span class="math display">\[\mu(A) = \begin{cases}|A|&amp; A\text{ finite}\\ \infty&amp; A\text{ infinite}\end{cases}.\]</span> This measure simply assigns each set its cardinality, and is known as the <strong><em>counting measure</em></strong>.</p>
<p>Given a probability space <span class="math inline">\((\mathcal X, \mathcal F, P)\)</span>, we can define a <strong><em>random variable</em></strong> <span class="math inline">\(X:\mathcal X\to\mathbb R\)</span> to be a function such that the preimage of every set in <span class="math inline">\(\mathcal B(\mathbb R)\)</span> is in <span class="math inline">\(\mathcal F\)</span>.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> <span class="math display">\[ X^{-1}(I) \in \mathcal F \ \ \ \forall I\in\mathcal B(\mathbb R)\]</span> In other words, if we can measure <span class="math inline">\(I\)</span> using the Lebesgue measure <span class="math inline">\(\mu\)</span>, we are able to measure <span class="math inline">\(X^{-1}(I)\)</span> using the probability measure <span class="math inline">\(P\)</span>. This allows us to define a probability measure <span class="math inline">\(P(X^{-1}(I))\)</span> on <span class="math inline">\(\mathcal B(\mathbb R)\)</span> which gives us the probability that <span class="math inline">\(X\)</span> is in the set <span class="math inline">\(I\subset \mathcal B(\mathbb R)\)</span>. This probability measure is the <strong><em>distribution</em></strong> of <span class="math inline">\(X\)</span>. If <span class="math inline">\(I = (-\infty,x)\)</span> for <span class="math inline">\(x\in\mathcal X\)</span>, then the distribution takes the form <span class="math inline">\(P(X^{-1}(I)) = P(X \in (-\infty,x)) = P(X\le x)\)</span>. We usually work with the distribution in this form and call it the <strong><em>distribution function</em></strong> <span class="math inline">\(F_X(x) = P(X\le x)\)</span>.</p>
<p>The chief motivation for defining measure spaces and measurable functions is a general theory of integration. The Riemann integral is achieved by taking the limit of a process which partitions the area under a function into rectangles. The width of these rectangles is the length of their base, which can be thought of as the measure of an interval. When performing Riemann integration, if <span class="math inline">\(I=(a,b)\)</span> is the base of a rectangle, we take the width to be <span class="math inline">\(m(I)=b -a\)</span>. That is, we use the Lebesgue measure. This is why you will sometimes see integrals written as <span class="math display">\[ \int f(x)\ dx = \int f(x) \ dm.\]</span> Writing the integral this way emphasize that we are integrating with respect to some measure (notion of length/volume), but it is more than just a notational difference. It is the result of defining integration in an entirely different way giving the <strong><em>Lebesgue integral (with respect to <span class="math inline">\(m\)</span>)</em></strong>. In practice this won’t matter a whole lot, because if we can calculate the Riemann integral of a function than we can calculate the Lebesgue integral, and both integrals are equal.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> What’s important is that the idea of integration with respect to a measure can be generalized to any measure space <span class="math inline">\((X, \mathcal N,\mu)\)</span>. In general, the <strong><em>Lebesgue integral</em></strong> of <span class="math inline">\(f\)</span> over <span class="math inline">\(A\in \mathcal N\)</span> is written as <span class="math display">\[\int_A f \ d\mu.\]</span> The rigorous definition of this integral, and how it is calculated using that definition, aren’t essential at the moment. That being said, one useful property to know is that if we integrate the constant <span class="math inline">\(1\)</span> on <span class="math inline">\(A\)</span> with respect to a measure <span class="math inline">\(\mu\)</span>, we get <span class="math inline">\(\mu(A)\)</span>. <span class="math display">\[\int_A 1\ d\mu = \int_A d\mu =\mu(A)\]</span> This should seem familiar from calculus. If we have a set <span class="math inline">\(I=(a,b)\)</span>, then <span class="math display">\[\int_a^b 1\ dx = b - a\]</span> using Riemann integration. Lebesgue integration gives <span class="math display">\[ \int_I 1\ dm = m(I) = b-a.\]</span> Interestingly, if we look at counting measure on the set of real numbers (our measure space is <span class="math inline">\((\mathbb R, 2^{\mathbb R}, \mu)\)</span>), and some set <span class="math inline">\(A = \{a_1,\ldots, a_n\}\in 2^{\mathbb R}\)</span> <span class="math display">\[ \int_Af\ d\mu = \int_A 1\ d\mu = \mu(A) = |A| = \sum_{i=1}^n 1=\sum_{i=1}^n f(a_i).\]</span> In general, integration with respect to the counting measure is a simple summation. This illustrates one nice consequence of the generality that Lebesgue integration provides – summation is a special case of integration.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p>What happens if we apply Lebesgue integration to the distribution of a random variable <span class="math inline">\(X\)</span> defined on the sample space <span class="math inline">\(\mathcal X\)</span>? Well if we integrate <span class="math inline">\(1\)</span> with respect to some real interval <span class="math inline">\(I=(a,b)\subset \mathbb R\)</span>, then <span class="math display">\[ \int _I 1\ dP = P(I) = F(b) - F(a).\]</span> If we instead integrate over all <span class="math inline">\(x\in\mathcal X\)</span>, we can think of the integral over the measure <span class="math inline">\(P\)</span> as a weighted sum of all <span class="math inline">\(x\)</span>, where weights are given by <span class="math inline">\(P\)</span>. This is just the <strong><em>expected value</em></strong> of <span class="math inline">\(X\)</span>. <span class="math display">\[\text{E}\left[X\right] = \int_{\mathcal X}x \ dP = \int_{\mathcal X}x\ dF_X\]</span> But how do we calculate this expectation? Even though this is a measure space over real numbers, we really don’t know how to calculate integrals with respect to any measure besides Lebesgue measure. In the event that the distribution <span class="math inline">\(F\)</span> satisfies certain standard conditions, we can find some function <span class="math inline">\(f_X\)</span> such that <span class="math display">\[ \text{E}\left[X\right] = \int_{\mathcal X}x\ dF_X = \int_{\mathcal X} xf_X(x)\ d\mu = \int_{\mathcal X} x f_X(x)\ dx.\]</span> This function is called the <strong><em>density function</em></strong> <span class="math inline">\(f(x)\)</span> of <span class="math inline">\(X\)</span>, and is actually given as <span class="math display">\[f_X = \frac{dF_X}{dx}.\]</span> In a sense, the density function is a “conversion rate” between the measure <span class="math inline">\(P\)</span> and the measure <span class="math inline">\(\mu\)</span>.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p>But what about “discrete” random variables? The expectation of a discrete random variable is not an integral…or is it? Just like how the integral with respect to the counting measure reduces to a sum, if our sample space <span class="math inline">\(\mathcal X\)</span> is countably infinite such that a random variable <span class="math inline">\(X:\mathcal X \to \mathbb R\)</span> takes on one of a discrete number of values, then <span class="math display">\[ \text{E}\left[X\right] = \int_{\mathcal X} x\ dF_X = \sum_{\mathcal x\in \mathcal X}x f(x).\]</span></p>
<p>If you couldn’t care less about measure theory,<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> then all this boils down to two facts:</p>
<ol type="1">
<li>All results will be stated in terms of continuous random variables. If you want to show them for discrete random variables, just replace integrals with sums.</li>
<li>Sometimes we will write expectation as <span class="math inline">\(\int x\ dF_X\)</span>, which is the same as <span class="math display">\[\int xf(x)\ dx.\]</span></li>
</ol>
</section>
<section id="random-matrices-and-vectors" class="level2">
<h2 class="anchored" data-anchor-id="random-matrices-and-vectors">Random Matrices and Vectors</h2>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 </strong></span>A <span class="math inline">\(m\)</span> by <span class="math inline">\(n\)</span> <span style="color:red"><strong><em>random matrix</em></strong></span> <span class="math inline">\(\mathbb{X}\)</span> is a matrix whose entries are <span class="math inline">\(m\times n\)</span> random variables <span class="math inline">\(X_{i,j}\)</span>, where <span class="math inline">\(\mathbb{X}_{i,j} = X_{i,j}\)</span>. <span class="math display">\[\begin{align*}
\mathbb{X}= \begin{bmatrix}
X_{11}&amp;\cdots&amp; X_{1n}\\\vdots&amp;\ddots&amp;\vdots\\X_{m1}&amp;\cdots&amp; X_{mn}
\end{bmatrix}.
\end{align*}\]</span> In the event that either <span class="math inline">\(m=1\)</span> or <span class="math inline">\(n=1\)</span> we have a <span style="color:red"><strong><em>random vector</em></strong></span> <span class="math inline">\(\mathbf{X}\)</span>.</p>
</div>
<p>We will often want to write a random matrix <span class="math inline">\(\mathbb{X}\)</span> as a collection of random vector <span class="math inline">\(\mathbf{X}\)</span>. Whether these be column vectors or row vectors depends on the context. If <span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(m\times n\)</span> random matrix where columns are indexed by <span class="math inline">\(i\)</span> and rows by <span class="math inline">\(j\)</span>, we will take <span class="math inline">\(\mathbf{X}_j\)</span> to be a column vector and <span class="math inline">\(\mathbf{X}_i\)</span> to be a row vector. <span class="math display">\[\begin{align*}
\mathbb{X}&amp;= \begin{bmatrix}\mathbf{X}_1 &amp; \cdots &amp; \mathbf{X}_i &amp;\cdots &amp; \mathbf{X}_n\end{bmatrix}\\
\mathbb{X}&amp;= \begin{bmatrix}\mathbf{X}_1 \\ \vdots \\ \mathbf{X}_j \\ \vdots \\ \mathbf{X}_m\end{bmatrix}
\end{align*}\]</span></p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 </strong></span>The <span style="color:red"><strong><em>expectation</em></strong></span> of a random matrix <span class="math inline">\(\mathbb{X}\)</span> is defined as</p>
<p><span class="math display">\[\begin{align*}
\text{E}\left[\mathbb{X}\right] = \begin{bmatrix}
\text{E}[X_{11}]&amp;\cdots&amp; \text{E}[X_{1n}]\\\vdots&amp;\ddots&amp;\vdots\\\text{E}[X_{m1}]&amp;\cdots&amp; \text{E}[X_{mn}]
\end{bmatrix}.
\end{align*}\]</span></p>
</div>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1 (Properties of Expectation) </strong></span>Suppose <span class="math inline">\(\mathbb{X}\)</span> and <span class="math inline">\(\mathbb Y\)</span> are <span class="math inline">\(m\times n\)</span> random matrices, <span class="math inline">\(\mathbf A\)</span> is a <span class="math inline">\(\ell\times m\)</span> matrix, <span class="math inline">\(\mathbf B\)</span> is a <span class="math inline">\(n\times k\)</span> matrix, and <span class="math inline">\(c\)</span> is a scalar. Then:</p>
<ol type="1">
<li><span class="math inline">\(\text{E}\left[\mathbb{X}'\right]= \text{E}\left[\mathbb{X}\right]'\)</span></li>
<li><span class="math inline">\(\text{E}\left[c\mathbb{X}\right]=c\text{E}\left[\mathbb{X}\right]\)</span></li>
<li><span class="math inline">\(\text{E}\left[\mathbf A \mathbb{X}\mathbf B\right] = \mathbf A \text{E}\left[\mathbb{X}\right] \mathbf B\)</span></li>
<li><span class="math inline">\(\text{E}\left[\mathbb{X}+ \mathbb Y\right] = \text{E}\left[\mathbb{X}\right] +\text{E}\left[\mathbb Y\right]\)</span></li>
</ol>
</div>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 </strong></span>The <span style="color:red"><strong><em>variance/covariance matrix</em></strong></span> of a random column vector <span class="math inline">\(\mathbf{X}= [X_1, \ldots, X_n]'\)</span> is defined as <span class="math display">\[ \text{Var}\left(\mathbf{X}\right) = \text{E}\left[(\mathbf{X}- \text{E}\left[\mathbf{X}\right])(\mathbf{X}- \text{E}\left[\mathbf{X}\right])'\right]. \]</span></p>
</div>
<p>The definition of <span class="math inline">\(\text{Var}\left(\mathbf{X}\right)\)</span> can be rewritten in a much more approachable form:</p>
<p><span class="math display">\[\begin{align*}
\text{Var}\left(\mathbf{X}\right) &amp;= \text{E}\left[(\mathbf{X}- \text{E}\left[\mathbf{X}\right])(\mathbf{X}- \text{E}\left[\mathbf{X}\right])'\right]\\\\
  &amp; = \text{E}\begin{bmatrix}
(X_1 - \text{E}\left[X_1\right])(X_1 - \text{E}\left[X_1\right])&amp;\cdots&amp; (X_1 - \text{E}\left[X_1\right])(X_n - \text{E}\left[X_n\right])\\\vdots&amp;\ddots&amp;\vdots\\(X_n - \text{E}\left[X_n\right])(X_1 - \text{E}\left[X_1\right])&amp;\cdots&amp; (X_n - \text{E}\left[X_n\right])(X_n - \text{E}\left[X_n\right])\end{bmatrix}\\\\
  &amp; = \begin{bmatrix}
\text{E}\left[(X_1 - \text{E}\left[X_1\right])^2\right]&amp;\cdots&amp; \text{E}\left[(X_1 - \text{E}\left[X_1\right])(X_n - \text{E}\left[X_n\right])\right]\\\vdots&amp;\ddots&amp;\vdots\\\text{E}\left[(X_n - \text{E}\left[X_n\right])(X_1 - \text{E}\left[X_1\right])\right]&amp;\cdots&amp; \text{E}{(X_n - \text{E}\left[X_n\right])^2}
\end{bmatrix}\\\\&amp; = \begin{bmatrix}
\text{Var}\left(X_1\right)&amp;\cdots&amp; \text{Cov}\left(X_1, X_n\right)\\\vdots&amp;\ddots&amp;\vdots\\\text{Cov}\left(X_n, X_1\right)&amp;\cdots&amp; \text{Var}\left(X_n\right)
\end{bmatrix}
\end{align*}\]</span></p>
<p>The diagonal entries capture the dispersion of each random variable <span class="math inline">\(X_i\)</span> while the off diagonal entries correspond to the joint dispersion of all possible pairs <span class="math inline">\((X_i,X_j)\)</span>.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4 </strong></span>The <span style="color:red"><strong><em>covariance matrix</em></strong></span> of random column vectors <span class="math inline">\(\mathbf{X}= [X_1, \ldots, X_n]'\)</span> and <span class="math inline">\(\mathbf{Y}= [Y_1, \ldots, Y_n]'\)</span><br>
<span class="math display">\[ \text{Cov}\left(\mathbf{X},\mathbf{Y}\right) = \text{E}\left[(\mathbf{X}- \text{E}\left[\mathbf{X}\right])(\mathbf{Y}- \text{E}\left[\mathbf{Y}\right])'\right]. \]</span></p>
</div>
<p>Alternatively, we can write the covariance matrix between two random vectors as:</p>
<p><span class="math display">\[\begin{align*}
\text{Cov}\left(\mathbf{X},\mathbf{Y}\right)=\begin{bmatrix}
\text{Cov}\left(X_1, Y_1\right)&amp;\cdots&amp; \text{Cov}\left(X_1, Y_n\right)\\\vdots&amp;\ddots&amp;\vdots\\\text{Cov}\left(X_n, Y_1\right)&amp;\cdots&amp; \text{Cov}\left(X_n,Y_n\right)
\end{bmatrix}
\end{align*}\]</span></p>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 2 (Properties of Variance/Covariance) </strong></span>Suppose <span class="math inline">\(\mathbf{X}_1\)</span> and <span class="math inline">\(\mathbf{X}_2\)</span> are random column vectors of length <span class="math inline">\(n\)</span>, <span class="math inline">\(\mathbf A\)</span> and <span class="math inline">\(\mathbf B\)</span> are matrices with <span class="math inline">\(n\)</span> columns, and <span class="math inline">\(\mathbf c\)</span> and <span class="math inline">\(\mathbf d\)</span> are column vectors of length <span class="math inline">\(n\)</span>. Then:</p>
<ol type="1">
<li><span class="math inline">\(\text{Var}\left(\mathbf A\mathbf{X}+\mathbf c\right)= \mathbf A \text{Var}\left(\mathbf{X}\right) \mathbf A'\)</span></li>
<li><span class="math inline">\(\text{Cov}\left(\mathbf A\mathbf{X}+\mathbf b, \mathbf B\mathbf{y}+\mathbf c\right) = \mathbf A \text{Cov}\left(\mathbf{X},\mathbf{Y}\right)\mathbf B'\)</span></li>
</ol>
</div>
<p>The first part of this proposition is a generalization of the familiar result that <span class="math inline">\(\text{Var}\left(aX + b\right)= a^2\text{Var}\left(x\right)\)</span>.</p>
</section>
<section id="multivariate-normal-distribution" class="level2">
<h2 class="anchored" data-anchor-id="multivariate-normal-distribution">Multivariate Normal Distribution</h2>
<p>Recall that if <span class="math inline">\(X\sim N(\mu,\sigma^2)\)</span>, then any linear function of <span class="math inline">\(X\)</span> is also normally distributed. To be precise, if <span class="math inline">\(Y=aX +b\)</span>, then <span class="math inline">\(Y\sim N(a\mu +b,a^2 \sigma^2)\)</span>. This useful properties generalizes to random vectors.</p>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 3 </strong></span>Suppose <span class="math inline">\(\mathbf{X}\sim N(\boldsymbol \mu, \boldsymbol{\Sigma})\)</span>. Then <span class="math display">\[ \mathbf A \mathbf{X}+ \mathbf b \sim N(\mathbf A\boldsymbol\mu +\mathbf b, \mathbf A\boldsymbol{\Sigma}\mathbf A')\]</span> where <span class="math inline">\(\mathbf A\)</span> and <span class="math inline">\(\mathbf b\)</span> are a matrix and vector with compatible dimensions.</p>
</div>
</section>
<section id="conditional-expectation-and-independence" class="level2">
<h2 class="anchored" data-anchor-id="conditional-expectation-and-independence">Conditional Expectation and Independence</h2>
<p>As put by <span class="citation" data-cites="wooldridge2010econometric">Wooldridge (<a href="references.html#ref-wooldridge2010econometric" role="doc-biblioref">2010</a>)</span>:</p>
<blockquote class="blockquote">
<p>A substantial portion of research in econometric methodology can be interpreted as finding ways to estimate conditional expectations in the numerous settings that arise in economic applications.</p>
</blockquote>
<p>For this reason, properties related to conditional expectation will prove useful time and time again. The first of these is the law of iterated expectation.</p>
<div class="theorem">
<p>Suppose <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables, then <span class="math display">\[\text{E}\left[\text{E}\left[X\mid Y\right]\right]= \text{E}\left[X\right]\]</span></p>
</div>
<p>Conditional expectation is related to the idea of independence. As one may remember from a probability course, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables, then <span class="math inline">\(\text{E}\left[X\mid Y\right]= \text{E}\left[\mathbf{X}\right]\)</span>. But is the converse true? As it turns out, the converse does not hold, so we have multiple notions of “independence”.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5 </strong></span>Suppose <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables with densities <span class="math inline">\(f_X(x)\)</span> and <span class="math inline">\(f_Y(y)\)</span>, respectively, and a joint density of <span class="math inline">\(f_{X,Y}(x,y)\)</span>. <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <span style="color:red"><strong><em>independent</em></strong></span>, written as <span class="math inline">\(X\perp Y\)</span>, if <span class="math display">\[f_{X,Y}(x,y)=f_X(x)f_Y(y).\]</span></p>
</div>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6 </strong></span>Suppose <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables. <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <span style="color:red"><strong><em>mean independent</em></strong></span> if <span class="math display">\[\text{E}\left[X\mid Y\right]= \text{E}\left[X\right].\]</span></p>
</div>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7 </strong></span>Suppose <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables. <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <span style="color:red"><strong><em>uncorrelated</em></strong></span> if <span class="math display">\[\text{E}\left[XY\right]= \text{E}\left[Y\right]\text{E}\left[X\right],\]</span> which is equivalent to <span class="math inline">\(\text{Cov}\left(X,Y\right) = 0\)</span>.</p>
</div>
<p>Our final result in this brief section of review related these three definitions.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 </strong></span>Suppose <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables. If <span class="math inline">\(X \perp Y\)</span>, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are mean independent. In turn, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are mean independent, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are uncorrelated.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If <span class="math inline">\(X \perp Y\)</span>, then <span class="math display">\[\begin{align*}
\text{E}\left[X\mid Y\right] &amp; = \int x f_{X\mid Y}(s\mid t)\ dx \\
              &amp; = \int x \frac{f_{X,Y}(x,y)}{f_{Y}(y)}\ dx &amp; (\text{def. of conditional probability}) \\
              &amp; = \int x \frac{f_X(x)f_Y(y)}{f_{Y}(y)}\ dx &amp; (X \perp Y) \\
              &amp; = \int x f_X(x)\ dx \\
              &amp; = \text{E}\left[X\right].
\end{align*}\]</span> In turn, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are mean independent, then <span class="math display">\[\begin{align*}
\text{E}\left[XY\right] &amp; = \text{E}\left[\text{E}\left[XY\mid Y\right]\right] &amp; (\text{Law of Iterated Expectations})\\
       &amp; = \text{E}\left[YE\left[X\mid Y\right]\right] &amp; (Y\text{ is constant}) \\
       &amp; = \text{E}\left[YE\left[X\right]\right] &amp; (\text{mean independence}) \\
       &amp; = \text{E}\left[Y\right]\text{E}\left[X\right] &amp; (\text{E}\left[X\right]\text{ is a constant})
\end{align*}\]</span></p>
</div>
</section>
<section id="existence-of-expectation" class="level2">
<h2 class="anchored" data-anchor-id="existence-of-expectation">Existence of Expectation</h2>
<p>It’s possible that the expectation of a random variable does not exist. It may be the case that <span class="math display">\[ \int_{\mathcal X}x\ dF_X \not&lt; \infty,\]</span> in which case we cannot assign an expected value to <span class="math inline">\(X\)</span>. While this is theoretically interesting, the applications where we need to be careful assuming <span class="math inline">\(\text{E}\left[X\right]\)</span> exists are few and far between. <strong><em>We will always assume the expectation, and relevant higher moments, of random variables exist</em></strong>. This is done for expositional ease, and to emphasize other assumptions that tend to be much more important.</p>
</section>
<section id="code" class="level2">
<h2 class="anchored" data-anchor-id="code">Code</h2>
<p>The majority of examples and simulations will be done in R, mostly because I like R, writing code in RStudio, and the <code>tidyverse</code>. That being said, once we get to topics in machine learning, I’ll switch over to Python. Python’s <code>scikit-learn</code> is much more comprehensive than its counterparts in R, and it seems that most machine learning tutorials use Python. I suspect this is because machine learning is applied very often in “industry” where code needs to be production ready and implementable by engineers, something Python affords that R does not. Machine learning tools like TensorFlow and Keras are also built with Python in mind. Eventually I may include a section on scientific computing and optimization, in which case I may use Julia. For <em>excellent</em> examples of economic modelling in Julia (and Python), see <a href="https://quantecon.org/">QuantEcon</a>.</p>
<p>Two other softwares/languages that are popular in (academic) economics are Stata and MATLAB. These tools share in one major drawback – they are not open source. While not as flexible as R or Python, Stata is particularly well-suited for basic econometrics. For example, I find it much easier to work with panel data and basic time series in Stata than in R. A great overview of econometric theory using Stata is provided by <span class="citation" data-cites="cameron2010microeconometrics">Adrian Colin Cameron, Trivedi, et al. (<a href="references.html#ref-cameron2010microeconometrics" role="doc-biblioref">2010</a>)</span> (which is a less technical companion to <span class="citation" data-cites="cameron2005microeconometrics">A. Colin Cameron and Trivedi (<a href="references.html#ref-cameron2005microeconometrics" role="doc-biblioref">2005</a>)</span>).</p>
</section>
<section id="organization" class="level2">
<h2 class="anchored" data-anchor-id="organization">Organization</h2>
<p>The organization of sections roughly follows a handful of econometrics courses I took while at Boston College.</p>
<section id="part-i---statistics" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="part-i---statistics">Part I - Statistics</h3>
<p>We’ll start with a review of mathematical statistics presented at the level of <span class="citation" data-cites="lehmann2006theory">Lehmann and Casella (<a href="references.html#ref-lehmann2006theory" role="doc-biblioref">1998</a>)</span> of <span class="citation" data-cites="lehmann2005testing">Romano and Lehmann (<a href="references.html#ref-lehmann2005testing" role="doc-biblioref">2005</a>)</span>.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> While the concepts will likely be familiar, they may seem a bit more technical when defined in the context of statistical decision theory. The focus is entirely on frequentist statistics, as Bayesian statistics will be covered later on.</p>
<ol start="2" type="1">
<li><strong>Finite Sample Properties of Estimators</strong>: We consider the problem of estimation, and give formal definitions and related notation to nebulous concepts such as: models, parametrizations, identification, statistics, parametric, semi-parametric, non-parametric, and estimators. Then we consider how to assess estimators for a fixed sample size.</li>
<li><strong>Asymptotic Properties of Estimators</strong>: In most cases, we won’t be able to determine finite sample properties of estimators, so we consider the situation where <span class="math inline">\(n\to \infty\)</span>. Familiar results like the central limit theorem (CLT) and law of large numbers (LLN) will be discussed, but we’ll also introduce a handful of essential results (the continuous mapping theorem, Slutsky’s theorem, the delta method) that will enable us to use the CLT and LLN to determine the asymptotic behavior of almost every estimator we will consider.<br>
</li>
<li><strong>Hypothesis Testing</strong>: The problem of estimation is only one part of statistics. The other is inference. We give an overview of inference and hypothesis testing using the Neyman-Pearson framework, and then consider how inference in relation to asymptotics. Two large sample tests (the Wald test and <span class="math inline">\(t-\)</span>test) are covered.</li>
<li><strong>Exponential Families</strong>: A quick treatment of a special type of probability distribution is given. Familiarity with these distributions is not necessary as far as econometrics is concerned, but a cursory understanding will make it possible to illuminate some cool connections between statistics and econometrics. In particular, we’ll see exponential families come up when considering maximum likelihood estimation (MLE), generalized linear models (GLMs), and Bayesian estimation.</li>
</ol>
</section>
<section id="part-ii---linear-models" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="part-ii---linear-models">Part II - Linear Models</h3>
<p>The foundation of econometrics is <em>the</em> linear model. In this section, we build the classical linear model from scratch adding assumptions as needed until arriving at the Gauss-Markov theorem. Then we “take the model apart” by dropping assumptions, and determine how to suitably estimate the subsequent linear models.</p>
<ol start="6" type="1">
<li><strong>Classical Regression Model</strong>: This section is a lengthy treatment of the classic linear model and estimation via ordinary least squares (OLS)</li>
<li><strong>Endogeneity I</strong></li>
<li><strong>Endogeneity II</strong></li>
<li><strong>Generalized Least Squares</strong></li>
</ol>
</section>
<section id="part-iii---estimation-framework" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="part-iii---estimation-framework">Part III - Estimation Framework</h3>
<ol start="11" type="1">
<li><strong>Extremum Estimators</strong></li>
<li><strong>The Generalized Method of Moments</strong></li>
<li><strong>Maximum Likelihood Estimation</strong></li>
</ol>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-billingsley2008probability" class="csl-entry" role="doc-biblioentry">
Billingsley, Patrick. 2008. <em>Probability and Measure</em>. John Wiley &amp; Sons.
</div>
<div id="ref-cameron2005microeconometrics" class="csl-entry" role="doc-biblioentry">
Cameron, A Colin, and Pravin K Trivedi. 2005. <em>Microeconometrics: Methods and Applications</em>. Cambridge university press.
</div>
<div id="ref-cameron2010microeconometrics" class="csl-entry" role="doc-biblioentry">
Cameron, Adrian Colin, Pravin K Trivedi, et al. 2010. <em>Microeconometrics Using Stata</em>. Vol. 2. Stata press College Station, TX.
</div>
<div id="ref-durrett2019probability" class="csl-entry" role="doc-biblioentry">
Durrett, Rick. 2019. <em>Probability: Theory and Examples</em>. Vol. 49. Cambridge university press.
</div>
<div id="ref-folland1999real" class="csl-entry" role="doc-biblioentry">
Folland, Gerald B. 1999. <em>Real Analysis: Modern Techniques and Their Applications</em>. Vol. 40. John Wiley &amp; Sons.
</div>
<div id="ref-lehmann2006theory" class="csl-entry" role="doc-biblioentry">
Lehmann, Erich L, and George Casella. 1998. <em>Theory of Point Estimation</em>. 2nd ed. Springer.
</div>
<div id="ref-lehmann2005testing" class="csl-entry" role="doc-biblioentry">
Romano, Joseph P, and EL Lehmann. 2005. <em>Testing Statistical Hypotheses</em>. Vol. 3. Springer.
</div>
<div id="ref-royden1988real" class="csl-entry" role="doc-biblioentry">
Royden, Halsey Lawrence, and Patrick Fitzpatrick. 1988. <em>Real Analysis</em>. Vol. 32. Macmillan New York.
</div>
<div id="ref-rudin" class="csl-entry" role="doc-biblioentry">
Rudin, Walter. 1987. <em>Real and Complex Analysis</em>. McGraw-hill New York.
</div>
<div id="ref-wooldridge2010econometric" class="csl-entry" role="doc-biblioentry">
Wooldridge, Jeffrey M. 2010. <em>Econometric Analysis of Cross Section and Panel Data</em>. MIT press.
</div>
</div>
</section>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>We cannot define probability over the entire power set <span class="math inline">\(2^{\mathcal X}\)</span>, or else we run into some technical problems.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Anytime a set is endowed with a topology (some notion of open sets), we can define a sigma-algebra as the collection of open subsets. This is known as the Borel sigma-algebra<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>More generally, such functions are called measurable.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The converse is not true, which is why the Lebesgue integral is of more theoretical interest. It turns out there are <em>many</em> functions which are not Riemann integrable but are Lebesgue integrable.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>But is this really new? The Riemann integral is just the limit of a sum.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>In general, this type of construction is called the <strong><em>Radon–Nikodym derivative</em></strong>, and its existence is outlined by the <strong><em>Radon–Nikodym theorem</em></strong>.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>It won’t save your life in emergency situations.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>These are the two standard references used for a PhD-level statistical theory course.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
  </div>
  <div class="nav-page nav-page-next">
      <a href="./estimators.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Finite Sample Properties of Estimators</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\plim}{plim}</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\argmin}{argmin}</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\argmax}{argmax}</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>\newcommand{\var}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Var}\left(#1\right)}</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>\newcommand{\avar}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Avar}\left(#1\right)}</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>\newcommand{\E}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{E}\left<span class="co">[</span><span class="ot">#1\right</span><span class="co">]</span>}</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>\newcommand{\cov}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Cov}\left(#1\right)}</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>\newcommand{\mse}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{MSE}\left(#1\right)}</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>\newcommand{\se}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{se}\left(#1\right)}</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>\newcommand{\limfunc}{lim} </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>\newcommand{\X}{\mathbf{X}}</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>\newcommand{\Xm}{\mathbb{X}}</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>\newcommand{\EER}{\bar{\thet}_\text{EE}}</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>\newcommand{\NLS}{\hat{\bet}_\text{NLLS}}</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>\newcommand{\z}{\mathbf{z}}</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>\newcommand{\rr}{\mathbf{r}}</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>\newcommand{\C}{\mathbf{C}}</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>\newcommand{\Pe}{\mathbf{P}}</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>\newcommand{\y}{\mathbf{y}}</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>\newcommand{\Y}{\mathbf{Y}}</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>\newcommand{\uu}{\mathbf{u}}</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>\newcommand{\e}{\mathbf{e}}</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>\newcommand{\D}{\mathbf{D}}</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>\newcommand{\x}{\mathbf{x}}</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>\newcommand{\xm}{\mathbb{x}}</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>\newcommand{\Zm}{\mathbb{Z}}</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>\newcommand{\Wm}{\mathbb{W}}</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>\newcommand{\Hm}{\mathbb{H}}</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>\newcommand{\W}{\mathbf{W}}</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>\newcommand{\Z}{\mathbf{Z}}</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>\newcommand{\Hess}{\mathbf{H}(\mathbf{\Z\mid\thet})}</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>\newcommand{\Score}{\mathbf{S}(\mathbf{\Z\mid\thet})}</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>\newcommand{\A}{\mathbf{A}}</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>\newcommand{\h}{\mathbf{h}}</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>\newcommand{\Q}{\mathbf{Q}}</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>\newcommand{\F}{\mathbf{F}}</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>\newcommand{\G}{\mathbf{G}}</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>\newcommand{\I}{\mathbf{I}}</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>\renewcommand{\D}{\mathbf{D}}</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>\renewcommand{\C}{\mathbf{C}}</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>\newcommand{\zer}{\mathbf{0}}</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLS}{\hat{\boldsymbol\beta}_\text{OLS} }</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLSOV}{\hat{\boldsymbol\beta}_\text{OLS,OV} }</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLSME}{\hat{\boldsymbol\beta}_\text{OLS,ME} }</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>\newcommand{\EE}{\hat{\boldsymbol\theta}_\text{EX} }</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>\newcommand{\ME}{\hat{\boldsymbol\theta}_\text{M} }</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>\newcommand{\MDE}{\hat{\boldsymbol\theta}_\text{MDE} }</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>\newcommand{\IV}{\hat{\boldsymbol\beta}_\text{IV} }</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>\newcommand{\TSLS}{\hat{\boldsymbol\beta}_\text{2SLS} }</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>\newcommand{\thet}{\boldsymbol{\theta}}</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>\newcommand{\et}{\boldsymbol{\eta}}</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>\newcommand{\R}{\mathbb{R}}</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>\newcommand{\Sig}{\boldsymbol{\Sigma}}</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>\newcommand{\ep}{\boldsymbol{\varepsilon}}</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>\newcommand{\Omeg}{\boldsymbol{\Omega}}</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>\newcommand{\Thet}{\boldsymbol{\Theta}}</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>\newcommand{\bet}{\boldsymbol{\beta}}</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>\newcommand{\rk}{rank}</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>\newcommand{\tsum}{\sum}</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>\newcommand{\tr}{tr}</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>\newcommand{\norm}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\left\lVert#1\right\rVert}</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>\newcommand{\abs}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\left\lvert#1\right\rvert}</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>\newcommand{\ms}{\overset{ms}{\to}}</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>\newcommand{\pto}{\overset{p}{\to}}</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>\newcommand{\iid}{\overset{iid}{\sim}}</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>\newcommand{\dto}{\overset{d}{\to}}</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>\newcommand{\asim}{\overset{a}{\sim}}</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="fu"># Preliminaries {.unnumbered}</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Multivariable Calculus</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>partial derivatives, the gradient, Jacobian matrix, Hessian matrix</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>optimization</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Linear Algebra</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>matrices and vectors</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>linear transformations</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>projections</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>PSD matrices</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Probability</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>random variables</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>distribution and density of RVs</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>expectation and variance </span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>moments </span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>common distributions</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a><span class="ss">        -   </span>normal distribution</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a><span class="ss">        -   </span>"friends" of the normal distribution: chi-squared, student's</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>            t, F distribution</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Mathematical Statistics</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>Estimation</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>Hypothesis testing</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Basic <span class="co">[</span><span class="ot">Real Analysis</span><span class="co">](https://github.com/noahjussila/analysis_notes/blob/master/Analysis%20Notes.pdf)</span></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>infimum and supremum </span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>metric spaces </span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>compact sets in $\mathbb R$</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>mean value theorem</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>convergence of sequences</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>pointwise versus uniform convergence</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>Taylor series</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Basic Numerical Optimization </span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>"numerical" vs. "analytic"</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a><span class="fu">## Probablitiy Theory</span></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>We'll briefly go over the basics of probability theory, _none of which is strictly required_. A rigorous treatment can be found in @durrett2019probability or @billingsley2008probability. For an even more general discussion, see @folland1999real, @royden1988real, and/or @rudin. If you have zero interest in the formal math behind probability (who would blame you?) then skip to the end of this section where all the math is reduced to two notational conventions. </span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>A probability space is a triple $(\mathcal X, \mathcal F, P)$ comprised of:</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>A **_sample space_** $\mathcal X$</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>A collection of events $\mathcal F$ which is a subset of the power set of events $2^{\mathcal X}$. This collection of sets satisfies the following properties: $\mathcal X\subset F$, $A^c\in F$ if $A\in F$, and $\cup_{i=1}^n A_i \subset F$ if $A_i\subset \mathcal F$ for all $i$. Such a collection of sets is known as a **_sigma-algebra_**.^<span class="co">[</span><span class="ot">We cannot define probability over the entire power set $2^{\mathcal X}$, or else we run into some technical problems.</span><span class="co">]</span></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>A **_probability measure_** $P:\mathcal F\to <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ satisfying: $P(\mathcal X) = 1$, $P(\emptyset) = 0$, $P(A) \le P(B)$ for $A\subset B\in \mathcal F$, and $P\left(\cup_{i=1}^n A_i\right) = \sum_{i=1}^n P(A_i)$. </span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>A probability space is a special case of a measure space $(X, \mathcal N,\mu)$. This is just any set $X$, an associated sigma-algebra $\mathcal N\subseteq 2^X$, and some measure $\mu$ which assigns values to sets in $\mathcal N\subseteq$. The most important such space is used to measure subsets of the real line (and more generally euclidean spaces). If we want to measure subsets of $\mathbb R$, we define the associated sigma-algebra as the collection of all open intervals, denoted $\mathcal B(\mathbb R) = <span class="sc">\{</span>(a,b)\subset \mathbb R\mid a,b\in\mathbb R<span class="sc">\}</span>$.^<span class="co">[</span><span class="ot">Anytime a set is endowed with a topology (some notion of open sets), we can define a sigma-algebra as the collection of open subsets. This is known as the Borel sigma-algebra</span><span class="co">]</span> The measure of some interval $I = (a,b)\subset \mathbb R$ is defined as $$m(I)=b -a,$$ which is fairly reasonable. This measure is known as the **_Lebesgue measure_**. Another nice example of a measure space is $(X, 2^{X}, \mu)$ where $$\mu(A) = \begin{cases}|A|&amp; A\text{ finite}\\ \infty&amp; A\text{ infinite}\end{cases}.$$ This measure simply assigns each set its cardinality, and is known as the **_counting measure_**.  </span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>Given a probability space $(\mathcal X, \mathcal F, P)$, we can define a **_random variable_** $X:\mathcal X\to\mathbb R$ to be a function such that the preimage of every set in $\mathcal B(\mathbb R)$ is in $\mathcal F$.^<span class="co">[</span><span class="ot">More generally, such functions are called measurable.</span><span class="co">]</span> </span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>$$ X^{-1}(I) \in \mathcal F \ \ \ \forall I\in\mathcal B(\mathbb R)$$ In other words, if we can measure $I$ using the Lebesgue measure $\mu$, we are able to measure $X^{-1}(I)$ using the probability measure $P$. This allows us to define a probability measure $P(X^{-1}(I))$ on $\mathcal B(\mathbb R)$ which gives us the probability that $X$ is in the set $I\subset \mathcal B(\mathbb R)$. This probability measure is the **_distribution_** of $X$. If $I = (-\infty,x)$ for $x\in\mathcal X$, then the distribution takes the form $P(X^{-1}(I)) = P(X \in (-\infty,x)) = P(X\le x)$. We usually work with the distribution in this form and call it the  **_distribution function_** $F_X(x) = P(X\le x)$.</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>The chief motivation for defining measure spaces and measurable functions is a general theory of integration. The Riemann integral is achieved by taking the limit of a process which partitions the area under a function into rectangles. The width of these rectangles is the length of their base, which can be thought of as the measure of an interval. When performing Riemann integration, if $I=(a,b)$ is the base of a rectangle, we take the width to be $m(I)=b -a$. That is, we use the Lebesgue measure. This is why you will sometimes see integrals written as </span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>$$ \int f(x)\ dx = \int f(x) \ dm.$$ Writing the integral this way emphasize that we are integrating with respect to some measure (notion of length/volume), but it is more than just a notational difference. It is the result of defining integration in an entirely different way giving the **_Lebesgue integral (with respect to $m$)_**. In practice this won't matter a whole lot, because if we can calculate the Riemann integral of a function than we can calculate the Lebesgue integral, and both integrals are equal.^[The converse is not true, which is why the Lebesgue integral is of more theoretical interest. It turns out there are *many* functions which are not Riemann integrable but are Lebesgue integrable.] What's important is that the idea of integration with respect to a measure can be generalized to any measure space $(X, \mathcal N,\mu)$. In general, the **_Lebesgue integral_** of $f$ over $A\in \mathcal N$ is written as $$\int_A f \ d\mu.$$ The rigorous definition of this integral, and how it is calculated using that definition, aren't essential at the moment. That being said, one useful property to know is that if we integrate the constant $1$ on $A$ with respect to a measure $\mu$, we get $\mu(A)$. </span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>$$\int_A 1\ d\mu = \int_A d\mu =\mu(A)$$ This should seem familiar from calculus. If we have a set $I=(a,b)$, then $$\int_a^b 1\ dx = b - a$$ using Riemann integration. Lebesgue integration gives </span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>$$ \int_I 1\ dm = m(I) = b-a.$$ Interestingly, if we look at counting measure on the set of real numbers (our measure space is $(\mathbb R, 2^{\mathbb R}, \mu)$), and some set $A = <span class="sc">\{</span>a_1,\ldots, a_n<span class="sc">\}</span>\in 2^{\mathbb R}$ </span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>$$ \int_Af\ d\mu = \int_A 1\ d\mu = \mu(A) = |A| = \sum_{i=1}^n 1=\sum_{i=1}^n f(a_i).$$ In general, integration with respect to the counting measure is a simple summation. This illustrates one nice consequence of the generality that Lebesgue integration provides -- summation is a special case of integration.^<span class="co">[</span><span class="ot">But is this really new? The Riemann integral is just the limit of a sum.</span><span class="co">]</span> </span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>What happens if we apply Lebesgue integration to the distribution of a random variable $X$ defined on the sample space $\mathcal X$? Well if we integrate $1$ with respect to some real interval $I=(a,b)\subset \mathbb R$, then </span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>$$ \int _I 1\ dP = P(I) = F(b) - F(a).$$ If we instead integrate over all $x\in\mathcal X$, we can think of the integral over the measure $P$ as a weighted sum of all $x$, where weights are given by $P$. This is just the **_expected value_** of $X$. $$\E{X} = \int_{\mathcal X}x \ dP = \int_{\mathcal X}x\ dF_X$$ But how do we calculate this expectation? Even though this is a measure space over real numbers, we really don't know how to calculate integrals with respect to any measure besides Lebesgue measure. In the event that the distribution $F$ satisfies certain standard conditions, we can find some function $f_X$ such that </span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>$$ \E{X} = \int_{\mathcal X}x\ dF_X = \int_{\mathcal X} xf_X(x)\ d\mu = \int_{\mathcal X} x f_X(x)\ dx.$$ This function is called the **_density function_** $f(x)$ of $X$, and is actually given as $$f_X = \frac{dF_X}{dx}.$$ In a sense, the density function is a "conversion rate" between the measure $P$ and the measure $\mu$.^[In general, this type of construction is called the **_Radon–Nikodym derivative_**, and its existence is outlined by the **_Radon–Nikodym theorem_**.] </span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>But what about "discrete" random variables? The expectation of a discrete random variable is not an integral...or is it? Just like how the integral with respect to the counting measure reduces to a sum, if our sample space $\mathcal X$ is countably infinite such that a random variable $X:\mathcal X \to \mathbb R$ takes on one of a discrete number of values, then </span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>$$ \E{X} = \int_{\mathcal X} x\ dF_X = \sum_{\mathcal x\in \mathcal X}x f(x).$$</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>If you couldn't care less about measure theory,^<span class="co">[</span><span class="ot">It won't save your life in emergency situations.</span><span class="co">]</span> then all this boils down to two facts:</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>All results will be stated in terms of continuous random variables. If you want to show them for discrete random variables, just replace integrals with sums.</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Sometimes we will write expectation as $\int x\ dF_X$, which is the same as $$\int xf(x)\ dx.$$</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a><span class="fu">## Random Matrices and Vectors</span></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>::: {#def-} </span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>A $m$ by $n$ <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_random matrix_**<span class="kw">&lt;/span&gt;</span> $\mathbb{X}$ is a matrix whose entries</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>are $m\times n$ random variables $X_{i,j}$, where</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>$\mathbb{X}_{i,j} = X_{i,j}$.</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>\mathbb{X}= \begin{bmatrix}</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>X_{11}&amp;\cdots&amp; X_{1n}<span class="sc">\\</span>\vdots&amp;\ddots&amp;\vdots<span class="sc">\\</span>X_{m1}&amp;\cdots&amp; X_{mn}</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}.</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>In the event that either $m=1$ or $n=1$ we have a</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_random vector_**<span class="kw">&lt;/span&gt;</span> $\mathbf{X}$. </span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>We will often want to write a random matrix $\Xm$ as a collection of random vector $\X$. Whether</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>these be column vectors or row vectors depends on the context. If $\X$ is an $m\times n$ random matrix</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>where columns are indexed by $i$ and rows by $j$, we will take $\X_j$ to be a column vector and $\X_i$ to be a row vector.</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>\Xm &amp;= \begin{bmatrix}\X_1 &amp; \cdots &amp; \X_i &amp;\cdots &amp; \X_n\end{bmatrix}<span class="sc">\\</span></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>\Xm &amp;= \begin{bmatrix}\X_1 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \X_j <span class="sc">\\</span> \vdots <span class="sc">\\</span> \X_m\end{bmatrix}</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>::: {#def-} </span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>The <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_expectation_**<span class="kw">&lt;/span&gt;</span> of a random matrix $\mathbb{X}$ is defined as</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>\E{\Xm} = \begin{bmatrix}</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>\text{E}<span class="co">[</span><span class="ot">X_{11}</span><span class="co">]</span>&amp;\cdots&amp; \text{E}<span class="co">[</span><span class="ot">X_{1n}</span><span class="co">]</span><span class="sc">\\</span>\vdots&amp;\ddots&amp;\vdots<span class="sc">\\</span>\text{E}<span class="co">[</span><span class="ot">X_{m1}</span><span class="co">]</span>&amp;\cdots&amp; \text{E}<span class="co">[</span><span class="ot">X_{mn}</span><span class="co">]</span></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}.</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>::: {#prp-}</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a><span class="fu">## Properties of Expectation</span></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>Suppose $\mathbb{X}$ and $\mathbb Y$ are $m\times n$ random matrices,</span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>$\mathbf A$ is a $\ell\times m$ matrix, $\mathbf B$ is a $n\times k$</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>matrix, and $c$ is a scalar. Then:</span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>$\text{E}\left<span class="co">[</span><span class="ot">\mathbb{X}'\right</span><span class="co">]</span>= \text{E}\left<span class="co">[</span><span class="ot">\mathbb{X}\right</span><span class="co">]</span>'$</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>$\text{E}\left<span class="co">[</span><span class="ot">c\mathbb{X}\right</span><span class="co">]</span>=c\text{E}\left<span class="co">[</span><span class="ot">\mathbb{X}\right</span><span class="co">]</span>$</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>$\text{E}\left<span class="co">[</span><span class="ot">\mathbf A \mathbb{X}\mathbf B\right</span><span class="co">]</span> = \mathbf A \text{E}\left<span class="co">[</span><span class="ot">\mathbb{X}\right</span><span class="co">]</span> \mathbf B$</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>$\text{E}\left<span class="co">[</span><span class="ot">\mathbb{X}+ \mathbb Y\right</span><span class="co">]</span> = \text{E}\left<span class="co">[</span><span class="ot">\mathbb{X}\right</span><span class="co">]</span> +\text{E}\left<span class="co">[</span><span class="ot">\mathbb Y\right</span><span class="co">]</span>$</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>::: {#def-} </span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>The <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_variance/covariance matrix_**<span class="kw">&lt;/span&gt;</span> of a random column vector</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>$\mathbf{X}= <span class="co">[</span><span class="ot">X_1, \ldots, X_n</span><span class="co">]</span>'$ is defined as</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>$$ \text{Var}\left(\mathbf{X}\right) = \text{E}\left<span class="co">[</span><span class="ot">(\mathbf{X}- \text{E}\left[\mathbf{X}\right])(\mathbf{X}- \text{E}\left[\mathbf{X}\right])'\right</span><span class="co">]</span>. $$</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>The definition of $\text{Var}\left(\mathbf{X}\right)$ can be rewritten</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>in a much more approachable form:</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>\text{Var}\left(\mathbf{X}\right) &amp;= \text{E}\left<span class="co">[</span><span class="ot">(\mathbf{X}- \text{E}\left[\mathbf{X}\right])(\mathbf{X}- \text{E}\left[\mathbf{X}\right])'\right</span><span class="co">]</span><span class="sc">\\\\</span></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>  &amp; = \text{E}\begin{bmatrix}</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a>(X_1 - \text{E}\left<span class="co">[</span><span class="ot">X_1\right</span><span class="co">]</span>)(X_1 - \text{E}\left<span class="co">[</span><span class="ot">X_1\right</span><span class="co">]</span>)&amp;\cdots&amp; (X_1 - \text{E}\left<span class="co">[</span><span class="ot">X_1\right</span><span class="co">]</span>)(X_n - \text{E}\left<span class="co">[</span><span class="ot">X_n\right</span><span class="co">]</span>)<span class="sc">\\</span>\vdots&amp;\ddots&amp;\vdots<span class="sc">\\</span>(X_n - \text{E}\left<span class="co">[</span><span class="ot">X_n\right</span><span class="co">]</span>)(X_1 - \text{E}\left<span class="co">[</span><span class="ot">X_1\right</span><span class="co">]</span>)&amp;\cdots&amp; (X_n - \text{E}\left<span class="co">[</span><span class="ot">X_n\right</span><span class="co">]</span>)(X_n - \text{E}\left<span class="co">[</span><span class="ot">X_n\right</span><span class="co">]</span>)\end{bmatrix}<span class="sc">\\\\</span></span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>  &amp; = \begin{bmatrix}</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>\text{E}\left<span class="co">[</span><span class="ot">(X_1 - \text{E}\left[X_1\right])^2\right</span><span class="co">]</span>&amp;\cdots&amp; \text{E}\left<span class="co">[</span><span class="ot">(X_1 - \text{E}\left[X_1\right])(X_n - \text{E}\left[X_n\right])\right</span><span class="co">]</span><span class="sc">\\</span>\vdots&amp;\ddots&amp;\vdots<span class="sc">\\</span>\text{E}\left<span class="co">[</span><span class="ot">(X_n - \text{E}\left[X_n\right])(X_1 - \text{E}\left[X_1\right])\right</span><span class="co">]</span>&amp;\cdots&amp; \text{E}{(X_n - \text{E}\left<span class="co">[</span><span class="ot">X_n\right</span><span class="co">]</span>)^2}</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}<span class="sc">\\\\</span>&amp; = \begin{bmatrix}</span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>\text{Var}\left(X_1\right)&amp;\cdots&amp; \text{Cov}\left(X_1, X_n\right)<span class="sc">\\</span>\vdots&amp;\ddots&amp;\vdots<span class="sc">\\</span>\text{Cov}\left(X_n, X_1\right)&amp;\cdots&amp; \text{Var}\left(X_n\right)</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>The diagonal entries capture the dispersion of each random variable</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a>$X_i$ while the off diagonal entries correspond to the joint dispersion</span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a>of all possible pairs $(X_i,X_j)$.</span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>::: {#def-} </span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>The <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_covariance matrix_**<span class="kw">&lt;/span&gt;</span> of random column vectors</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>$\mathbf{X}= <span class="co">[</span><span class="ot">X_1, \ldots, X_n</span><span class="co">]</span>'$ and $\mathbf{Y}= <span class="co">[</span><span class="ot">Y_1, \ldots, Y_n</span><span class="co">]</span>'$\</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a>$$ \text{Cov}\left(\mathbf{X},\mathbf{Y}\right) = \text{E}\left<span class="co">[</span><span class="ot">(\mathbf{X}- \text{E}\left[\mathbf{X}\right])(\mathbf{Y}- \text{E}\left[\mathbf{Y}\right])'\right</span><span class="co">]</span>. $$</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>Alternatively, we can write the covariance matrix between two random</span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>vectors as:</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a>\text{Cov}\left(\mathbf{X},\mathbf{Y}\right)=\begin{bmatrix}</span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>\text{Cov}\left(X_1, Y_1\right)&amp;\cdots&amp; \text{Cov}\left(X_1, Y_n\right)<span class="sc">\\</span>\vdots&amp;\ddots&amp;\vdots<span class="sc">\\</span>\text{Cov}\left(X_n, Y_1\right)&amp;\cdots&amp; \text{Cov}\left(X_n,Y_n\right)</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a>\end{bmatrix} </span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>::: {#prp-}</span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a><span class="fu">## Properties of Variance/Covariance</span></span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>Suppose $\mathbf{X}_1$ and $\mathbf{X}_2$ are random column vectors of length $n$, $\mathbf A$ and $\mathbf B$ are matrices with $n$ columns, and $\mathbf c$ and $\mathbf d$ are column vectors of length $n$. Then:</span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>$\text{Var}\left(\mathbf A\mathbf{X}+\mathbf c\right)= \mathbf A \text{Var}\left(\mathbf{X}\right) \mathbf A'$</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>$\text{Cov}\left(\mathbf A\mathbf{X}+\mathbf b, \mathbf B\mathbf{y}+\mathbf c\right) = \mathbf A \text{Cov}\left(\mathbf{X},\mathbf{Y}\right)\mathbf B'$</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a>The first part of this proposition is a generalization of the familiar result that</span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>$\text{Var}\left(aX + b\right)= a^2\text{Var}\left(x\right)$.</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multivariate Normal Distribution</span></span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>Recall that if $X\sim N(\mu,\sigma^2)$, then any linear function of $X$</span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a>is also normally distributed. To be precise, if $Y=aX +b$, then</span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>$Y\sim N(a\mu +b,a^2 \sigma^2)$. This useful properties generalizes to</span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>random vectors.</span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>::: {#prp-}</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>Suppose $\mathbf{X}\sim N(\boldsymbol \mu, \boldsymbol{\Sigma})$. Then</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>$$ \mathbf A \mathbf{X}+ \mathbf b \sim N(\mathbf A\boldsymbol\mu +\mathbf b, \mathbf A\boldsymbol{\Sigma}\mathbf A')$$</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a>where $\mathbf A$ and $\mathbf b$ are a matrix and vector with</span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>compatible dimensions.</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conditional Expectation and Independence</span></span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a>As put by @wooldridge2010econometric:</span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; A substantial portion of research in econometric methodology can be</span></span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; interpreted as finding ways to estimate conditional expectations in</span></span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; the numerous settings that arise in economic applications.</span></span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a>For this reason, properties related to conditional expectation will</span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a>prove useful time and time again. The first of these is the law of</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a>iterated expectation.</span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a>::: theorem</span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a>Suppose $X$ and $Y$ are random variables, then</span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a>$$\text{E}\left<span class="co">[</span><span class="ot">\text{E}\left[X\mid Y\right]\right</span><span class="co">]</span>= \text{E}\left<span class="co">[</span><span class="ot">X\right</span><span class="co">]</span>$$</span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a>Conditional expectation is related to the idea of independence. As one</span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a>may remember from a probability course, if $X$ and $Y$ are independent</span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a>random variables, then</span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>$\text{E}\left<span class="co">[</span><span class="ot">X\mid Y\right</span><span class="co">]</span>= \text{E}\left<span class="co">[</span><span class="ot">\mathbf{X}\right</span><span class="co">]</span>$. But is the converse</span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a>true? As it turns out, the converse does not hold, so we have multiple</span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a>notions of "independence".</span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a>::: {#def-} </span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a>Suppose $X$ and $Y$ are random variables with densities $f_X(x)$ and</span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a>$f_Y(y)$, respectively, and a joint density of $f_{X,Y}(x,y)$. $X$ and</span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a>$Y$ are <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_independent_**<span class="kw">&lt;/span&gt;</span>, written as $X\perp Y$, if</span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a>$$f_{X,Y}(x,y)=f_X(x)f_Y(y).$$</span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>::: {#def-} </span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>Suppose $X$ and $Y$ are random variables. $X$ and $Y$ are <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_mean</span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a>independent_**<span class="kw">&lt;/span&gt;</span> if $$\text{E}\left<span class="co">[</span><span class="ot">X\mid Y\right</span><span class="co">]</span>= \text{E}\left<span class="co">[</span><span class="ot">X\right</span><span class="co">]</span>.$$</span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a>::: {#def-} </span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a>Suppose $X$ and $Y$ are random variables. $X$ and $Y$ are</span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>**_uncorrelated_**<span class="kw">&lt;/span&gt;</span> if</span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>$$\text{E}\left<span class="co">[</span><span class="ot">XY\right</span><span class="co">]</span>= \text{E}\left<span class="co">[</span><span class="ot">Y\right</span><span class="co">]</span>\text{E}\left<span class="co">[</span><span class="ot">X\right</span><span class="co">]</span>,$$ which is equivalent to $\text{Cov}\left(X,Y\right) = 0$.</span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a>Our final result in this brief section of review related these three</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a>definitions.</span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a>:::{#thm-}</span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a>Suppose $X$ and $Y$ are random variables. If $X \perp Y$, then $X$ and $Y$ are mean independent. In turn, if $X$ and $Y$ are mean independent, then $X$ and $Y$ are uncorrelated.</span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a>If $X \perp Y$, then \begin{align*}</span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a>\text{E}\left<span class="co">[</span><span class="ot">X\mid Y\right</span><span class="co">]</span> &amp; = \int x f_{X\mid Y}(s\mid t)\ dx <span class="sc">\\</span></span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a>              &amp; = \int x \frac{f_{X,Y}(x,y)}{f_{Y}(y)}\ dx &amp; (\text{def. of conditional probability}) <span class="sc">\\</span></span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a>              &amp; = \int x \frac{f_X(x)f_Y(y)}{f_{Y}(y)}\ dx &amp; (X \perp Y) <span class="sc">\\</span></span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a>              &amp; = \int x f_X(x)\ dx <span class="sc">\\</span></span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a>              &amp; = \text{E}\left<span class="co">[</span><span class="ot">X\right</span><span class="co">]</span>.</span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a>\end{align*} In turn, if $X$ and $Y$ are mean independent, then</span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a>\text{E}\left<span class="co">[</span><span class="ot">XY\right</span><span class="co">]</span> &amp; = \text{E}\left<span class="co">[</span><span class="ot">\text{E}\left[XY\mid Y\right]\right</span><span class="co">]</span> &amp; (\text{Law of Iterated Expectations})<span class="sc">\\</span></span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a>       &amp; = \text{E}\left<span class="co">[</span><span class="ot">YE\left[X\mid Y\right]\right</span><span class="co">]</span> &amp; (Y\text{ is constant}) <span class="sc">\\</span></span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a>       &amp; = \text{E}\left<span class="co">[</span><span class="ot">YE\left[X\right]\right</span><span class="co">]</span> &amp; (\text{mean independence}) <span class="sc">\\</span></span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a>       &amp; = \text{E}\left<span class="co">[</span><span class="ot">Y\right</span><span class="co">]</span>\text{E}\left<span class="co">[</span><span class="ot">X\right</span><span class="co">]</span> &amp; (\text{E}\left<span class="co">[</span><span class="ot">X\right</span><span class="co">]</span>\text{ is a constant})</span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a><span class="fu">## Existence of Expectation</span></span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a>It's possible that the expectation of a random variable does not exist. It may be the case that </span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a>$$ \int_{\mathcal X}x\ dF_X \not&lt; \infty,$$ in which case we cannot assign an expected value to $X$. While this is theoretically interesting, the applications where we need to be careful assuming $\E{X}$ exists are few and far between. **_We will always assume the expectation, and relevant higher moments, of random variables exist_**. This is done for expositional ease, and to emphasize other assumptions that tend to be much more important.  </span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a><span class="fu">## Code</span></span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a>The majority of examples and simulations will be done in R, mostly because I like R, writing code in RStudio, and the ```tidyverse```. That being said, once we get to topics in machine learning, I'll switch over to Python. Python's ```scikit-learn``` is much more comprehensive than its counterparts in R, and it seems that most machine learning tutorials use Python. I suspect this is because machine learning is applied very often in "industry" where code needs to be production ready and implementable by engineers, something Python affords that R does not. Machine learning tools like TensorFlow and Keras are also built with Python in mind. Eventually I may include a section on scientific computing and optimization, in which case I may use Julia. For *excellent* examples of economic modelling in Julia (and Python), see <span class="co">[</span><span class="ot">QuantEcon</span><span class="co">](https://quantecon.org/)</span>. </span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a>Two other softwares/languages that are popular in (academic) economics are Stata and MATLAB. These tools share in one major drawback -- they are not open source. While not as flexible as R or Python, Stata is particularly well-suited for basic econometrics. For example, I find it much easier to work with panel data and basic time series in Stata than in R. A great overview of econometric theory using Stata is provided by @cameron2010microeconometrics (which is a less technical companion to @cameron2005microeconometrics).</span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a><span class="fu">## Organization </span></span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a>The organization of sections roughly follows a handful of econometrics courses I took while at Boston College. </span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a><span class="fu">### Part I - Statistics  {-}</span></span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a>We'll start with a review of mathematical statistics presented at the level of @lehmann2006theory of @lehmann2005testing.^<span class="co">[</span><span class="ot">These are the two standard references used for a PhD-level statistical theory course.</span><span class="co">]</span> While the concepts will likely be familiar, they may seem a bit more technical when defined in the context of statistical decision theory. The focus is entirely on frequentist statistics, as Bayesian statistics will be covered later on.</span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Finite Sample Properties of Estimators**: We consider the problem of estimation, and give formal definitions and related notation to nebulous concepts such as: models, parametrizations, identification, statistics, parametric, semi-parametric, non-parametric, and estimators. Then we consider how to assess estimators for a fixed sample size. </span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Asymptotic Properties of Estimators**: In most cases, we won't be able to determine finite sample properties of estimators, so we consider the situation where $n\to \infty$. Familiar results like the central limit theorem (CLT) and law of large numbers (LLN) will be discussed, but we'll also introduce a handful of essential results (the continuous mapping theorem, Slutsky's theorem, the delta method) that will enable us to use the CLT and LLN to determine the asymptotic behavior of almost every estimator we will consider.   </span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Hypothesis Testing**: The problem of estimation is only one part of statistics. The other is inference. We give an overview of inference and hypothesis testing using the Neyman-Pearson framework, and then consider how inference in relation to asymptotics. Two large sample tests (the Wald test and $t-$test) are covered. </span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Exponential Families**: A quick treatment of a special type of probability distribution is given. Familiarity with these distributions is not necessary as far as econometrics is concerned, but a cursory understanding will make it possible to illuminate some cool connections between statistics and econometrics. In particular, we'll see exponential families come up when considering maximum likelihood estimation (MLE), generalized linear models (GLMs), and Bayesian estimation. </span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a><span class="fu">### Part II - Linear Models  {-}</span></span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a>The foundation of econometrics is *the* linear model. In this section, we build the classical linear model from scratch adding assumptions as needed until arriving at the Gauss-Markov theorem. Then we "take the model apart" by dropping assumptions, and determine how to suitably estimate the subsequent linear models.</span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**Classical Regression Model**: This section is a lengthy treatment of the classic linear model and estimation via ordinary least squares (OLS)</span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>**Endogeneity I**</span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a><span class="ss">8. </span>**Endogeneity II**</span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a><span class="ss">9. </span>**Generalized Least Squares**</span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a><span class="fu">### Part III - Estimation Framework  {-}</span></span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a><span class="ss">11. </span>**Extremum Estimators** </span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a><span class="ss">12. </span>**The Generalized Method of Moments** </span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a><span class="ss">13. </span>**Maximum Likelihood Estimation**</span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>