\DeclareMathOperator{\plim}{plim}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmax}
\newcommand{\var}[1]{\text{Var}\left(#1\right)}
\newcommand{\avar}[1]{\text{Avar}\left(#1\right)}
\newcommand{\E}[1]{\text{E}\left[#1\right]}
\newcommand{\cov}[1]{\text{Cov}\left(#1\right)}
\newcommand{\mse}[1]{\text{MSE}\left(#1\right)}
\newcommand{\se}[1]{\text{se}\left(#1\right)}
\newcommand{\limfunc}{lim} 
\newcommand{\X}{\mathbf{X}}
\newcommand{\Xm}{\mathbb{X}}
\newcommand{\EER}{\bar{\thet}_\text{EE}}
\newcommand{\NLS}{\hat{\bet}_\text{NLLS}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\rr}{\mathbf{r}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\Pe}{\mathbf{P}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\uu}{\mathbf{u}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\xm}{\mathbb{x}}
\newcommand{\Zm}{\mathbb{Z}}
\newcommand{\Wm}{\mathbb{W}}
\newcommand{\Hm}{\mathbb{H}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\Hess}{\mathbf{H}(\mathbf{\Z\mid\thet})}
\newcommand{\Score}{\mathbf{S}(\mathbf{\Z\mid\thet})}
\newcommand{\A}{\mathbf{A}}
\newcommand{\h}{\mathbf{h}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\F}{\mathbf{F}}
\newcommand{\G}{\mathbf{G}}
\newcommand{\I}{\mathbf{I}}
\renewcommand{\D}{\mathbf{D}}
\renewcommand{\C}{\mathbf{C}}
\newcommand{\zer}{\mathbf{0}}
\newcommand{\OLS}{\hat{\boldsymbol\beta}_\text{OLS} }
\newcommand{\OLSOV}{\hat{\boldsymbol\beta}_\text{OLS,OV} }
\newcommand{\OLSME}{\hat{\boldsymbol\beta}_\text{OLS,ME} }
\newcommand{\EE}{\hat{\boldsymbol\theta}_\text{EX} }
\newcommand{\ME}{\hat{\boldsymbol\theta}_\text{M} }
\newcommand{\MDE}{\hat{\boldsymbol\theta}_\text{MDE} }
\newcommand{\IV}{\hat{\boldsymbol\beta}_\text{IV} }
\newcommand{\TSLS}{\hat{\boldsymbol\beta}_\text{2SLS} }
\newcommand{\thet}{\boldsymbol{\theta}}
\newcommand{\et}{\boldsymbol{\eta}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Sig}{\boldsymbol{\Sigma}}
\newcommand{\ep}{\boldsymbol{\varepsilon}}
\newcommand{\Omeg}{\boldsymbol{\Omega}}
\newcommand{\Thet}{\boldsymbol{\Theta}}
\newcommand{\bet}{\boldsymbol{\beta}}
\newcommand{\rk}{rank}
\newcommand{\tsum}{\sum}
\newcommand{\tr}{tr}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\ms}{\overset{ms}{\to}}
\newcommand{\pto}{\overset{p}{\to}}
\newcommand{\iid}{\overset{iid}{\sim}}
\newcommand{\dto}{\overset{d}{\to}}
\newcommand{\asim}{\overset{a}{\sim}}

# Finite Sample Properties of Estimators {#sec-est}

```{r}
#| echo: false
#| output: false
library(tidyverse)
```

## Models and Parameterizations

The term "estimation" is used so often, that it's often easy to forget precisely what it means. It's worth briefly presenting the formal definition of (point) estimation in accordance with statistical theory and formulate the performance of an estimator using tools from decision theory. A comprehensive treatment of the theory of estimation, see @bickel2015mathematical or @lehmann2006theory. 

:::{#def-}
Given a random experiment with sample space $\mathcal X$ over which a random vector ${\X}=(X_1,\ldots. X_n)$ is defined, a <span style="color:red">**_model_**</span> $\mathcal P$ is the collection of probability distributions (or densities), or sets of distributions (or densities), from which $\X$ may be distributed. We call an element of a model $P\in\mathcal P$, <span style="color:red">**_model value_**</span>.
:::

In order to keep track of model values, we need to introduce labels for model values $P$. 

:::{#def-}
For every element of $\mathcal P$, we assign a label called a <span style="color:red">**_parameter_**</span> $\thet$ from a <span style="color:red">**_parameter space_**</span>  $\Thet = \Theta_1 \times\cdots\times\Theta_k$, denoting the labeled element as $P_{\thet}$. The function $\theta \mapsto P$ which assigns these labels is a <span style="color:red">**_parameter space_**</span>. 
:::

A parameterization enables to write the model as $\mathcal P =\{P_{\thet} \mid \thet\in \Thet\}$. 


::: {#exm-}

## Normally Distributed Random Variable

Assume that we know *a priori* that $X$ is a normally distributed random variable ($n=1$ here). The model $\mathcal P$ is the collection of all normal distributions. How do we label this entirely family of models? The most common way is via the mean and variance of the normal distribution. In this case, $\Thet = \mathbb R \times \mathbb R^+$, $\thet = (\mu, \sigma^2)$, and 
$$f_X(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right].$$ By parameterizing $\mathcal P$, we can now easily refer to each element of $\mathcal P$ using $\thet=(\mu,\sigma^2)$.

$$\mathcal P = \{f_X(x \mid \mu, \sigma^2) \mid  (\mu, \sigma^2) \in \mathbb R \times \mathbb R^+\}.$$

Alternatively, we could **_reparameterize_** the model using the standard deviation instead of the variance, $\thet = (\mu, \sigma)$. This gives 
$$f_X(x \mid \mu, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left[-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right].$$ If we wanted to really be crazy, we could parameterize the model using $\thet = (\mu/\sigma^2, -1/2\sigma)$, where $\Theta = \mathbb R\times \mathbb R^-$.

\begin{align*}
f_X(x \mid \thet) &= \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right] \\ & = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[\dfrac{x^2}{2\sigma^2} -\dfrac{2x\mu}{2\sigma^2} + \dfrac{\mu^2}{2\sigma^2}\right] \\
		 & =\exp\left[\log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)\right]\exp\left[-\dfrac{x^2}{2\sigma^2} + \dfrac{2x\mu}{2\sigma^2} - \dfrac{\mu^2}{2\sigma^2}\right]\\
		 & =\exp\left[\log\left( (2\pi\sigma^2)^{-1/2}\right)\right]\exp\left[-\dfrac{x^2}{2\sigma^2} + \dfrac{2x\mu}{2\sigma^2} - \dfrac{\mu^2}{2\sigma^2}\right] \\ 
		 & = \exp\left[-\frac{1}{2}\log\left( 2\pi\sigma^2\right)\right]\exp\left[-\dfrac{x^2}{2\sigma^2} + \dfrac{2x\mu}{2\sigma^2} - \dfrac{\mu^2}{2\sigma^2}\right] \\
		 & = \exp\left[-\dfrac{x^2}{2\sigma^2} + \dfrac{2x\mu}{2\sigma^2} - \dfrac{\mu^2}{2\sigma^2} -\frac{1}{2}\log\left( 2\pi\sigma^2\right)\right]\\
		 & = \exp\left[\dfrac{\mu}{\sigma^2}x-\dfrac{1}{2\sigma^2}x^2 -\frac{1}{2}\log\left(\frac{\mu^2}{\sigma^2}+\log\left( 2\pi\sigma^2\right)\right)\right]\\
		 & = \exp\left[\theta_1x+\theta_2x^2 -\frac{1}{2}\log\left(-\frac{\theta_1}{2\theta_2}+\log\left( \pi\theta_2^{-1}\right)\right)\right]
	\end{align*}
::: 

::: {#exm-}

## Exponentially Distributed Random Variable

If $X$ is distributed according to an exponential distribution, the most common parameterization has $\Theta = \mathbb R^+$, $\theta = \lambda$, and each element of $\mathcal P$ has a corresponding density of 
$$ f_X(x\mid\lambda) = \begin{cases}\lambda e^{-\lambda x}& x\ge 0 \\ 0 & x<0\end{cases}.$$ A common alternate parameterization is in terms of $\beta \in \mathbb R^+$ labels elements in $\mathcal P$ such that 
$$ f_X(x\mid\beta) = \begin{cases}\frac{1}{\beta} e^{-x/\beta}& x\ge 0 \\ 0 & x<0\end{cases}. $$
::: 


These last two examples were simple because each element $P_\thet \in \mathcal P$ is a single probability density/distribution. In this case we write ${\X} \sim P_{\thet}$, and the difference between $P_{\thet}$ and $f_{\X}({\X}\mid\thet)$ is purely a matter of notation. It is the same difference between writing a uniform distribution as $\text{Uni}(a,b)$ instead of $f_X(x\mid a,b) = 1/(b-a)$ or $F_X(x\mid a,b)=x/(b-a)$. We may also write the probability measure associated with the sample space ${\mathcal X}$ and density as $P_{\thet}$, so the expectation of ${\X}$ can be written as 
$$ \E{\X} = \int_{\mathcal{X}} {\X} f_{\X}({\X}\mid\thet)\ d{\X} = \int_{\mathcal{X}} {\X} \ dF_{\X}({\X}\mid\thet) = \int_{\mathcal{X}} {\X} \ dP_{\thet}.$$ 

It will often be the case, especially so in econometrics, that an element of $P_\thet \in \mathcal P$ is itself a set of distributions defined by some common properties.^[Sets with this type of structure are quite common in mathematics and can be formalized with equivelence relations and quotient groups.] 

::: {#exm-}

## IID Sample Parameterized by Mean

Suppose ${\X} = (X_1,\ldots, X_n)$ is a sample such that $X_i \iid F_X(x)$ and $\E{X_i} = \mu$ for all $i$. This data generating process constitutes a model parameterized by $\mu$. An element of $\mathcal P$ takes the form 
$$P_\mu = \left\{f_{\X}({\X})\ \Bigg|\ f_{\X}({\X}) = \prod_{i=1}^n f_{X_i}(x) \text{ and } f_{X_i} = f_{X_j}\ \forall i,j \text{ and }\E{X_i}=\mu\ \forall i\right\}.$$ For example, if we let $\mu = 5$, then $P_5\in\mathcal P$ is the set of all joint distributions such that the elements of $\X$ are independent and have an expected value of $5$. This is *a lot* of possible distributions. One possible distribution in this set is $$f_{\X}({\X}\mid \mu) = \prod_{i=1}^n\frac{1}{2\mu} = \frac{1}{(2\mu)^n},$$ which is the joint density of ${\X}$ when $X_i \iid \text{Uni}(0,2\mu)$. Because ${\X}$ is comprised of an iid sample, we could also define the elements of $\mathcal P$ using the marginal density $f_{X_i}(x\mid\theta)$, as this density is identical for all $X_i$. 
$$ P_\mu = \{f_{X_i}(x) \mid \E{X_i} = \mu\} .$$ What is important here is that each $P_\mu$ is an infinite collection of densities all with a common properties related to the parameter $\mu$. 
::: 



To differentiate models where $P_\thet \in \mathcal P$ is a single distribution opposed to a collection of distributions, we will introduce a new term (that is not standard across any sources).  

:::{#def-}
A model $\mathcal P = \{P_{\thet} \mid \thet \in \Thet\}$ is a <span style="color:red">**_regular model_**</span> if each $P_{\thet}$ is a probability distribution with density $f_{\X}({\X}\mid\thet)$.
:::

Not only can we consider just how many distributions make up one model value $P\in\mathcal P$, but we can also consider the dimension of the parameter space $\Thet$. 

:::{#def-}
Suppose a model $\mathcal P$ is parameterized by a mapping $\thet \mapsto P_\thet$ where $\dim(\Thet) = k$.

1. If $\dim(\Theta_j)$ is finite for all $j=1,\ldots, k$, then $\mathcal P = \{P_{\thet} \mid \thet \in \Thet\}$ is <span style="color:red">**_parametric_**</span>. 
2. If $\dim(\Theta_j)$ is infinite for all $j=1,\ldots, k$, then $\mathcal P = \{P_{\thet} \mid \thet \in \Thet\}$ is <span style="color:red">**_nonparametric_**</span>. 
3. If $\mathcal P$ is neither parametric, nor non-parametric, it is <span style="color:red">**_semiparametric_**</span>. This means that some components of $\Thet$ have finite dimension, while others have infinite dimension. 
:::

For the classic parameterization of the normal distribution, $\mu\in\R$ and $\sigma^2 \in \R^+$, where $\dim(\R) = \dim(\R^+) = 1$. For the exponential distribution, $\lambda \in \R^+$, where $\dim(\R) \in \R$. Where things get subjective is when we deal with models that are not regular (model values $P\in\mathcal P$ consist of entire sets of distributions), as we could take one of our parameters to be an infinite space of distributions.

::: {#exm-iidmean}

## IID Sample Parameterized by Mean

Suppose ${\X} = (X_1,\ldots, X_n)$ is a sample such that $X_i \iid F_X(x)$ and $\E{X_i} = \mu$ for all $i$. Originally, we defined an element $P_\mu \in \mathcal P$ to be a collection of distributions. We could instead reparameterize this model such that it is a regular model. Parameterize $\mathcal P$ with the mapping $(\mu, f_X)\to P_{(\mu, f_X)}$ where $\E{X_i}=\mu$ and $f_X$ is the common density function of all $X_i$. Now $P_{\mu, f_X}\in\mathcal P$ corresponds to the density that gives rise to the iid random sample ${\X}$, along with the mean $\mu$ of that density. The model is now semiparametric, as $\mu\in\R$ where $\dim(\R) = 1$, and $f_X$ is an element of an infinite dimensional space of functions. We could also parameterize this model only simply with the mapping $f_X \mapsto P_{f_X}$, as the parameter $\mu$ is implicitly given by $f_X$:
$$ \mu = \int_{\mathcal X} xf_X(x)\ dx.$$ In this case, the model is nonparametric.
:::

::: {.remark}

In the last example, we reparameterized @exm-iidmean by introducing the common density of the $X_i$, $f_X(x)$, as its own parameter. This made a model that was not regular (each element $P_\mu\in\mathcal P$ was an infinite collection of densities), regular and semiparametric (the space of all densities $f_X(x)$ with mean $\mu$ has infinite dimension). In general if we have a model $\mathcal P$ of the form 
\begin{align*}
\mathcal P &= \{P_{\thet} \mid \thet \in \Thet\}, \\
P_{\thet} & = \{f_{\X}({\X}) \mid f_{\X}({\X})\text{ satisfies some condition which may involve }\thet \},
\end{align*}
we can write reparameterize it as a regular semiparametric model where $(\thet, f_{\X}) \mapsto P_{(\thet, f_{\X})}$, taking the density $f_{\X}({\X})$ to be its own parameter. The difference between these two does not amount to much practice, but it can be a tad confusing when one author considers a model parametric while another considers it semiparametric. This is particularly relevant in econometrics, because many models do not specify the entire distribution (ex: the data is normally distributed), instead opting for weaker assumptions related to the distribution (ex: the distribution of the data is symmetric and centered at zero).
:::

## Statistics and Estimators

Given a realizations of a random vector ${\X}$, we can calculate statistics 

::: {#def-}
A <span style="color:red">**_statistic_**</span> $T$ is a function $T:{\mathcal X} \to \mathcal T$, where $\mathcal T$ is some space of values.
:::

In almost every case we are interested in $\mathcal T = \mathbb R^k$ for some $k$. Most of the descriptive statistics we think of (sample mean, sample variance, median, mode, etc.) are all statistics which take on values in $\mathbb R$. Because statistics are nothing more than functions of random variables, they themselves are random variables. This fact is paramount for a specific type of statistic.

::: {#def-}
Suppose ${\X} \sim P_{\theta_0}$, where $P_{\theta_0}\in \mathcal P$ for a known $\mathcal P$. An <span style="color:red">**_estimator_**</span> $\hat{\thet}$ is a statistic which maps the sample space ${\mathcal X}$ to the parameter space $\Theta$, whose goal is to estimate the <span style="color:red">**_estimand_**</span> $\thet_0$.^[An estimand could also be a function of the true underlying parameter $\thet_0$.] A realization of an estimator, $\hat{\theta}({\X})$ is an <span style="color:red">**_estimate_**</span>.
:::

The goal of an estimator is to "guess" the true value $P_{\thet_0}\in\mathcal P$ which generated the data, and do so by "guessing" the corresponding parameter $\thet_0$. To emphasize the fact that $\thet_0$ is the true value we have added a subscript 0, a practice that will be employed when. All this notation can be a bit confusing at first, so it helps to work through it with a very familiar setting.

::: {#exm-}
Suppose for a random vector ${\X} = (X_1,\ldots, X_n)$ that $X_i\sim N(\mu_0,\sigma_0^2)$. This is equivalent to ${\X}$ being distributed according to a multivariate normal distribution $N(\boldsymbol\mu,\Sig)$, where $\boldsymbol\mu_i = \mu$ for $i=1,\ldots,n$, and $\Sig$ is a diagonal matrix comprised entirely of $\sigma^2$. The sample space of our random vector is $\mathbb R^n$.^[Okay, it actually isn't $\mathbb R^n$. It's sigma-algebra generated by the Borel sets of $\mathbb R^n$, but measure theory is not our concern at the moment.] If $\varphi(x)$ denotes the standard normal distribution, then 
$$\mathcal P = \left\{ \prod_{i=1}^n \frac{1}{\sigma}\varphi\left(\frac{x-\mu}{\sigma}\right)\  \biggr\vert\  (\mu,\sigma)\in \mathbb R\times\mathbb R^+\right\}.$$ In order to estimate $\thet_0 = (\mu_0,\sigma_0)$, we define the following estimator:
\begin{align*}
\hat{\thet}({\X}) & = (\hat\theta_1({\X}), \hat\theta_2({\X}))\\
\hat\theta_1({\X}) & = \frac{1}{n}\sum_{i=1}^n X_i\\ 
\hat\theta_2({\X}) & = \left[\frac{1}{n-1}\sum_{i=1}^n (X_i - \hat\theta_1({\X}))^2\right]^{1/2}
\end{align*}
This is of course how we've been estimating $(\mu_0,\sigma_0)$ since we were in high school -- using the sample mean and sample standard deviation! These estimators are so important that they have their own special notation, $\bar X$ and $S({\X})$.
:::

::: {#exm-unident}
Let's modify the previous problem by changing our parameterization of $\mathcal P$. Instead of labeling each normal distribution with $(\mu,\sigma)$, let's instead use $(\alpha, \beta, \sigma)$ where $\alpha + \beta$ replaces $\mu$. That is, each $X_i$ is distributed according to $N(\alpha_0 + \beta_0, \sigma_0^2)$. Is it possible to estimate all three parameters? It seems that we can still estimate $\sigma_0$ with no issues, but the same cannot be said for $\alpha_0$ and $\beta_0$. What happens if we attempt to use the sample mean $\bar X$ and find that $\bar x = 3$? The best we can do is use this information to estimate $\alpha_0 + \beta_0$, but it's impossible to distinguish the individual values of each parameter, as there are infinite pairs of $(\alpha, \beta)$ which sum to 3. We could have $\alpha_0 = 0$ and $\beta_0 = 3$, or $\alpha_0 = 1$ and $\beta_0 = 2$, or $\alpha_0 = -412$ and $\beta_0 = 415$, etc. Even if we had an infinite amount of data, this problem would persist! 
:::

This last example highlights what will be one of the central topics in econometric theory, and it arose in how our model was parameterized. The catalyst of the problem we faced in estimating $\alpha_0$ and $\beta_0$, is that we did a poor job at "labeling" the model $\mathcal P$ with parameters. To be precise, the "labels" given to elements of $\mathcal P$ by the parameterization were not unique. If our goal is uncovering the true $P_{\thet_0}\in \mathcal P$ via estimating $\thet_0$, we need to make sure that there is a one-to-one relationship between $\thet$ and $P_{\thet}$.

::: {#def-ident}
A parameterization $\thet \mapsto P_{\thet}$ is <span style="color:red">**_identifiable_**</span> if it is an injective mapping. That is $$P_{\thet} \neq P_{\thet'} \implies \thet \neq \thet',$$ which is equivalent to 
$$ \thet = \thet'\implies P_{\thet} = P_{\thet'}.$$ We say a parameter $\theta_j$ (a component of the vector $\thet$) is <span style="color:red">**_identified_**</span> if the $j$-th component of the parameterization  $\thet \mapsto P_{\thet}$ is injective.  
:::

The parameterization in the previous problem is not identifiable, as multiple vectors of parameters $\thet$ map to a single element of $\mathcal P$. In situations like this, estimation is a nonstarter! We always need to make sure our model (and it's accompanying parameterization) is identified before we can even attempt to estimate any parameters.

It's often necessary to adopt assumptions in order to add additional structure to a model, facilitating identification. For example, if we revisit \@exm-unident, and assumed that $\alpha = 2\beta$, then we can identify $\alpha$ and $\beta$. We were able to estimate $\alpha + \beta$ using $\bar X$, and combining this with the additional assumption of $\alpha = 2\beta$ gives $\hat\beta = \bar X/3$ and $\hat\alpha = \bar X/3$. A hallmark of econometrics is determining which assumptions are required to identify parameters of interest, and if those assumptions are reasonable and consistent with economic theory and the observed behavior of economic agents.

::: {#exm-}

## IID Sample, Identifying the Mean

What parameters are and are not identified when our model generated data according to $X_i\iid F_X(x)$ where $\E{X_i} = \mu$? This depends on how we parameterize $\mathcal P$.

1. Define a regular model $\mathcal P$ such that each element $P_\mu$ is a distribution with expectation $\mu$. $$ \mathcal{P} = \left\{f_{X}(x) \mid \E{X} = \mu \right\}$$ With the parameterization $\mu\mapsto P_\mu$, the model is not identified, as any $\mu$ maps to an infinite number of distributions with an expectation of $\mu$. For example, $\mu = 5$ maps to $\text{Uni}(0,10)$, $\text{Uni}(4,6)$, $N(5,1)$, $\text{Exp}(1/5)$, and every other density such that $\int xf_X(x)\ dx = 5$.   
2. But do we really care about the precise distribution of $X_i$, or are we just interested in estimating the mean $\mu$? If we only want to estimate the mean $\mu$, then we can define our model such that $P_\mu \in \mathcal P$ is itself the entire collection of density with mean $\mu$.
\begin{align*}
\mathcal P &= \{P_\mu \mid \mu \in \R\} \\
P_\mu & = \left\{f_{\X}({\X})\ \bigg| \ \int xf_{\X}({\X})\ dx = \mu \right\}
\end{align*}
This model is not regular, but it is identified. 
3. We could define redefine our identified model as a a semiparametric model by taking $f_X$ to be its own parameter, and using the parameterization $(\mu, f_X)\mapsto P_{(\mu, f_X)}$. In this case the model is still identified, as $(\mu, f_X)$ uniquely determine an element of $\mathcal P$.    

So if the models from 2 and 3 are both identified, which one do we pick? As discussed earlier, it doesn't really matter in practice, *but* strictly speaking model 2 may be the better choice. The goal of estimating some parameter $\thet$ is to "guess" $P_{\thet}$. If $P_{\thet}$ does not specify a distribution which generates the data, then $P_{\thet}$ should include all the possible distributions that could have generated the data.
:::

## Unbiasedness


Before estimating $\thet$, we need to determine which estimator to use. How do we assess the quality of an estimator *ex-ante* (prior to calculating an estimate for observed data ${\X}$)? For the sake of simplicity, assume that $\Theta = \mathbb R$. Ideally, an estimator $\hat \theta$ will be "close" to the estimand $\theta$, which may look like $\hat\theta({\X}) -\theta\approx 0$ for a realization of a random vector ${\X}$,^[We could also use $\abs{\hat\theta({\X}) -\theta}\approx 0$, but this measure is unsigned. It may be useful to know whether or not an estimator is overestimating or underestimating $\theta$] but there is no way to know ${\X}$. We instead will calculate this discrepancy *in expectation*.

::: {#def-bias}
The <span style="color:red">**_bias_**</span> of an estimator $\hat\theta : {\mathcal X} \to \mathbb R$ is defined as 
$$\text{Bias}(\hat\theta, \theta) = \text{Bias}(\hat\theta) = \E{\hat\theta - \theta} = \E{\hat\theta} - \theta .$$ An estimator is <span style="color:red">**_unbiased_**</span> if $\text{Bias}(\hat\theta) = 0$, which is equivalent to $\E{\hat\theta} = \theta$. This is readily extended to an estimator of a vector of estimands, $\hat{\thet({\X})}$. 
:::

An unbiased estimator is, *on average*, correct. 

::: {#exm-}
Suppose ${\X} = (X_1, \ldots, X_n)$ is a random sample from a distribution with mean $\mu$ ($\E{X_i}=\mu$ for all $i$). The estimator $$\hat\theta(X) = \frac{1}{n}\sum_{i=1}^n X_i$$ is an unbiased estimator for $\mu$. 
\begin{align*}
\E{\hat\theta} & = \E{\frac{1}{n}\sum_{i=1}^n X_i} \\
              & = \frac{1}{n}\sum_{i=1}^n\E{X_i}  & (\text{Expectation is linear}) \\
              & = \frac{1}{n}\sum_{i=1}^n\mu & (\E{X_i}=\mu) \\
              & = \frac{1}{n}(n\mu)\\
              & = \mu
\end{align*}
:::

::: {#exm-var}
Suppose ${\X} = (X_1, \ldots, X_n)$ is a random sample from a distribution with variance $\sigma^2$ and mean $\mu$. If the sample mean is an unbiased estimator of the mean, then perhaps the sample analogue of the variance, $$ \hat\theta(X) = \frac{1}{n}\sum_{i=1}^n (X_i -\bar X)^2 ,$$ is unbiased as well? Recalling that $\var{\bar X} = \sigma^2/m$
\begin{align*}
\E{\hat\theta} & = \E{\frac{1}{n}\sum_{i=1}^n (X_i -\bar X)^2} \\
              & = \E{\frac{1}{n}\sum_{i=1}^n (X_i - \bar X + (\mu - \mu))^2}\\
              & = \E{\frac{1}{n}\sum_{i=1}^n ((X_i - \mu) - (\bar X - \mu))^2} \\
              & = \E{\frac{1}{n}\sum_{i=1}^n ((X_i - \mu)^2 - 2(X_i - \mu)(\bar X - \mu) + (\bar X - \mu)^2)} \\
              & = \E{\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - \frac{2}{n}(\bar X - \mu)\sum_{i=1}^n (X_i - \mu) + (\bar X - \mu)^2\frac{1}{n}\sum_{i=1}^n 1} \\
               & = \E{\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - \frac{2}{n}(\bar X - \mu)\left[\sum_{i=1}^n X_i - \sum_{i=1}^n \mu\right] + (\bar X - \mu)^2(1/n)n}\\
               & = \E{\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - \frac{2}{n}(\bar X - \mu)(n\bar X - n\mu) + (\bar X - \mu)^2} \\
                & = \E{\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - \frac{2}{n}(\bar X - \mu)n(\bar X - \mu) + (\bar X - \mu)^2} \\
                & = \E{\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - 2(\bar X - \mu)^2 + (\bar X - \mu)^2)}\\
                & = \E{\frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2 - (\bar X - \mu)^2 }\\
                & = \frac{1}{n}\E{\sum_{i=1}^n (X_i - \mu)^2} -  \E{(\bar X - \mu)^2 } \\ 
                & = \frac{1}{n}\sum_{i=1}^n\var{X_i} - \frac{\sigma^2}{n}\\
                & =  \frac{1}{n}(n\sigma^2) - \frac{\sigma^2}{n}\\
                & = \sigma^2 - \frac{\sigma^2}{n}\\
                & = \frac{n-1}{n}\sigma^2\\
                & \neq \sigma^2
\end{align*}
Our estimator is biased! We can "modify" $\hat\theta(X)$ in order to correct it's bias. Define a second estimator $\hat\theta^*$ as $$ \hat\theta^*({\X}) = \frac{n}{n-1}\hat\theta(X) = \frac{n}{n-1}\frac{1}{n}\sum_{i=1}^n (X_i -\bar X)^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i -\bar X)^2.$$ This correction gives an unbiased estimator for $\sigma^2$:
$$\E{\hat\theta^*} = \E{\frac{n}{n-1}\hat\theta} =  \frac{n}{n-1}\E{\hat\theta}= \frac{n}{n-1}\frac{n-1}{n}\sigma^2 = \sigma^2. $$
This correction is known as **_Bessel's correction_**, and its effect is easily demonstrated with a Monte Carlo simulation. Suppose $X_i \sim N(0,1)$ for $i=1,\ldots, 20$. Let's calculate $\hat\theta$ and $\hat\theta^*$ for our sample, repeat this for one million simulations, and look the sample mean of each estimator over the one million simulations.^[These sample means are themselves estimators of $\E{\hat\theta}$ and $\E{\hat\theta^*}$, but one million simulations is so many that our estimates will be very good. Why this works is a discussion for the next section on asymptotics.]
For $n = 20$, we have
\begin{align*}
\E{\hat\theta} & = \frac{n-1}{n}\sigma^2 = \frac{20-1}{20}= 0.95, \\ 
\E{\hat\theta^*} & = 1,
\end{align*}
so we should see that 
\begin{align*}
\frac{1}{1000000}\sum_{k}\hat\theta & \approx 0.95, \\ 
\frac{1}{1000000}\sum_{k}\hat\theta^* & \approx  1,
\end{align*}
where simulations are indexed by $k$.


```{r}
# for a given list of estimators, draw one realized estimate
draw_estimates <- function(estimators, n, dist, dist_params, t){
  X <- do.call(dist, append(n, dist_params))
  output <- map_dbl(estimators, \(f) f(X)) %>% 
  as_tibble() %>% 
  mutate(
    estimator = names(estimators),
    iter_num = t
  ) %>% 
  rename(
    estimate = value
  )
  return(output)
}

# for a given list of estimators, draw N realized estimates
draw_N_estimates <- function(N, estimators, n, dist, dist_params){
  output <- 1:N %>% 
    map_df(\(t) draw_estimates(estimators, n, dist, dist_params, t))
  return(output)
}

#Define estimators (including sample mean)
x_bar <- function(x){
  sum(x)/length(x)
}

theta_biased <- function(x){
  sum((x - x_bar(x))^2)/length(x)
}

theta_unbiased <- function(x){
  sum((x - x_bar(x))^2)/(length(x) - 1)
}

results <- draw_N_estimates(
  N = 1e6, 
  estimators = c(
    "biased" = theta_biased,
    "unbiased" = theta_unbiased
  ),
  n = 20, 
  dist = rnorm, 
  dist_params = list(
    mean = 0, 
    sd = 1
  )
)

results %>% 
  group_by(estimator) %>% 
  summarize(expected_value = mean(estimate))
```

Alternatively, we can calculate the bias of each estimator:

```{r}
results %>% 
  group_by(estimator) %>% 
  summarize(bias = 1 - mean(estimate))
```

It is also is informative to plot the density of our estimates, in effect illustrating the probability distributions of the estimators $\hat\theta(X)$ and $\hat\theta^*(X)$

```{r}
#| code-fold: true
#| label: fig-plot11
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: "Distribution of estimators for the population variance"
#| code-summary: "Show code which generates figure"

biased_mean <- mean(results$estimate[results$estimator == 'biased']) 
unbiased_mean <- mean(results$estimate[results$estimator == 'unbiased'])

results %>% 
  ggplot(aes(estimate, color = estimator)) +
  geom_density() +
  theme_minimal() +
  theme(legend.position = "bottom") +
  labs(x = "Estimates of Variance, True Value = 1", y = "Density") +
  scale_color_manual(values = c("red", "blue")) +
  geom_vline(xintercept = biased_mean, size = 0.5, color = "red", linetype = "dashed") + 
  geom_vline(xintercept = unbiased_mean, size = 0.5, color = "blue", linetype = "dashed") 
```

Because $\hat\theta^*$ is unbiased, it is the most frequent way of calculating the variance of a random variable using a sample, and is often notated by $S^2$. While informative, this doesn't really pin down *why* the uncorrected estimator is biased. The bias arises from the fact that $\hat\theta({\X})$ is a function of another estimator -- $\bar X$. In the event we knew $\mu$ and didn't need to estimate it with $\bar X$, then $\frac{1}{n}\sum_{i=1}^n (X_i -\mu)^2$ is actually unbiased. It's the "intermediate" estimation of $\mu$ via $\bar X$, as the discrepancy between $\bar X$ and $\mu$ factors in when taking the sum of squared deviations. Assuming we know $\mu$, we can estimate $\sigma$  as 
$$\hat\sigma({\X}) = \frac{1}{n}\sum_{i=1}^n [(X_i - \mu)]^2 = \frac{1}{n}\sum_{i=1}^n [(X_i - \bar X) + (\bar X - \mu)]^2,$$ which highlights that $X_i$'s deviation from the sample mean can be written as the sum of the discrepancy between $\mu$ and $\bar X$'s discrepancy from $\mu$. If we expand it we have 
\begin{align*}
\hat\sigma^2({\X}) &= \frac{1}{n}\sum_{i=1}^n [(X_i - \bar X)^2 + 2(X_i - \bar X)(\bar X - \mu) +(\bar X - \mu)^2] \\
              & = \frac{1}{n}\sum_{i=1}^n (X_i - \bar X)^2 + \frac{1}{n}\sum_{i=1}^n [2(X_i - \bar X)(\bar X - \mu) +(\bar X - \mu)^2]\\
              & = \hat\theta({\X}) + \frac{1}{n}\sum_{i=1}^n [2(X_i - \bar X)(\bar X - \mu) +(\bar X - \mu)^2]
\end{align*}
:::
If we try to use $\hat\theta({\X})$, we don't capture the second term, and are underestimating the variance.

## Relative Efficiency 

While unbiasedness is important, it isn't the end all be all when selecting an estimator. The following pathological example will highlight another concern.

::: {#exm-}
Suppose ${\X} = (X_1, \ldots, X_n)$ is a random sample from a distribution with mean $\mu$. Let's define two estimators for $\mu$:
\begin{align*}
\hat\theta_1(X) & = \bar X = \frac{1}{n}\sum_{i=1}^nX_i,\\
\hat\theta_2(X) & = \sum_{i=1}^n\frac{1}{2^i}X_i.
\end{align*}

We're familiar with the unbiased estimator $\hat\theta_2(X)$. The estimator $\hat\theta_2(X)$ is a weighted average, where weights are determined by the geometric series $1/2^i$. This estimator is also unbiased, $$ \E{\hat\theta_2} = \E{\sum_{i=1}^n\frac{1}{2^i}X_i} = \sum_{i=1}^n\frac{1}{2^i}\E{X_i}  = \sum_{i=1}^n\frac{1}{2^i}\mu = \mu \sum_{i=1}^n\frac{1}{2^i} = \mu(1)=\mu.$$ In fact, *any* weighted average where the (normalized) weights sum to one is unbiased. If both estimators are unbiased, how do we determine which is superior? To gain some insight, let's simulate some estimates, assuming $X_i \sim N(0,1)$ for $i = 1,\ldots, 20$.

```{r}
#define estimator
weighted_xbar <- function(x){
  #calculate weights
  w <- 1/(2^(1:length(x)))
  sum(w*x)
}

results <- draw_N_estimates(
  N = 1e6, 
  estimators = c(
    "x_bar" = x_bar,
    "weighted_xbar" = weighted_xbar
  ),
  n = 20, 
  dist = rnorm, 
  dist_params = list(
    mean = 0, 
    sd = 1
  )
)
```

If we use the simulated estimates to plot the (approximate) densities of the estimators, one fact sticks out -- $\var{\hat\theta_1} < \var{\hat\theta_2}$.

```{r}
#| code-fold: true
#| label: fig-plot12
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: "Distribution of estimators for the population mean"
#| code-summary: "Show code which generates figure"

results %>% 
  ggplot(aes(estimate, color = estimator)) +
  geom_density() +
  theme_minimal() +
  theme(legend.position = "bottom") +
  labs(x = "Estimates of Mean, True Value = 0", y = "Density") +
  scale_color_manual(values = c("red", "blue")) 
```
:::

The variance of an estimator gives us a sense for the dispersion of our estimates, but isn't desirable in and of itself. We can *always* find an estimator with a small variance, and by small variance I mean no variance! Suppose we set our estimator $\hat\theta(X) = 1$. Regardless of the realized sample ${\X}$, we have an estimate of $\hat\theta({\X}) = 1$, and $\var{\hat\theta} = 0$. This approach completely ignores the fact that $1$ may not even be remotely close to $\theta_0$, something addressed by $\text{Bias}(\hat\theta)$. Not only do we want a precise estimator (low variance), but we want an accurate estimator (low bias). The next example shows that there is a balancing act between these two properties.

::: {#exm-}
Return to the problem of estimating $\sigma^2$ using realizations of $X_i\sim N(0,1)$ for $i=1,\ldots,20.$ We've already considered two estimators: 
\begin{align*}
S^2({\X}) & = \frac{1}{n-1}\sum_{i=1}^n (X_i -\bar X)^2.\\
\hat\theta({\X}) & = \frac{1}{n}\sum_{i=1}^n (X_i -\bar X)^2.
\end{align*}

If we return to our estimates from @exm-var we can estimate the variance of each estimator using $S^2$. Just for clarification, we are using $S^2(S^2({\X})$ to estimate the variances of the estimators $S^2({\X})$ and $\hat\theta({\X})$. 

```{r}
results %>% 
  group_by(estimator) %>% 
  summarize(variance = var(estimate))
```
It seems that the biased estimator is more efficient than the unbiased estimator. It can be shown that 
\begin{align*}
\var{S^2} & = \frac{2\sigma^4}{n-1},\\
\var{\hat\theta} & = \frac{2(n-1)\sigma^4}{n^2}
\end{align*}
which gives
\begin{align*}
& 1  \le \frac{n-1}{n} \\
\implies & 1  < \frac{(n-1)^2}{n^2} \\
\implies & \frac{1}{n-1} < \frac{n-1}{n^2} \\
\implies & \frac{2\sigma^4}{n-1} < \frac{2(n-1)\sigma^4}{n^2} \\
\implies & \var{S^2} < \var{\hat\theta}.
\end{align*}
:::

If bias and variance are at odds, perhaps we can conceive of a third measure of an estimator's performance that supersedes both. We can quantify the **_cost_** of an estimator with a **_loss function_** $l:\Theta\times\Theta \to \mathbb R^+$ which captures how "wrong" an estimate $\hat\theta$ is for an estimand $\theta$. A loss function is a function of an estimator, making it a random variable. The expected loss of an estimator is given by a **_risk function_** $R:\Theta\times\Theta \to \mathbb R^+$,
$$R(\hat\theta, \theta) =  \E{l(\hat\theta,\theta)} = \int_{{\mathcal X}} l(\hat\theta(x), \theta)\ dF(x\mid\theta).$$ We've already seen one special case of a risk function in the form of $\text{Bias}(\hat\theta,\theta)$. If we define $l(\hat\theta,\theta) = \hat\theta - \theta$, we have $$ \E{l(\hat\theta,\theta)} = \E{\hat\theta - \theta} = \text{Bias}(\hat\theta,\theta).$$ 

One possible issue with the loss function $l(\hat\theta,\theta) = \hat\theta - \theta$ is that it is linear. This means that if $\theta_0 = 0$, the estimate $\hat\theta = 2$ is twice as costly as $\hat\theta =1$. In many situations, we think that a very bad estimate is not much worse than a bad estimate, but *much much* worse. We want a loss function that assigns more weight to poor estimates, capturing the idea that the marginal cost/loss of a underestimating/overestimating $\theta$ is increasing. This is achieved with a quadratic loss function $l(\hat\theta,\theta) = (\hat\theta-\theta)^2$. The risk function which corresponds to this choice of $l(\hat\theta,\theta)$ is so important that it gets its own name.     
```{r}
#| code-fold: true
#| label: fig-plot13
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: "Quadratic loss function"
#| code-summary: "Show code which generates figure"

tibble(x = (-1000:1000)/1000) %>% 
  mutate(y = x^2) %>% 
  ggplot(aes(x,y)) +
  geom_line() +
  theme_void() +
  geom_vline(xintercept = 0, size = 0.25) +
  geom_hline(yintercept = 0, size = 0.25) +
  annotate("text", x = 0.96, y = -0.05, label = "Estimated θ") +
  annotate("text", x = 0.07, y = -0.05, label = "True θ") + 
  annotate("text", x = 0.75, y = 0.2, label = "Loss(Estimaed θ, True θ)")
```

::: {#def-mse}
Suppose $\hat\theta:{\mathcal X} \to \Theta$ is an estimator for $\theta$. The <span style="color:red">**_mean squared error (MSE)_**</span> of $\hat\theta$ is $$\mse{\hat\theta} = \E{(\hat\theta - \theta)^2}.$$ In the event our estimator is a vector $\hat{\thet} = (\hat\theta_1,\ldots,\hat\theta_k)$, then 
$$ \mse{\hat{\thet}} = \E{\textstyle\sum_{i=1}^n(\hat\theta_i - \theta_i)^2}.$$
:::

How does this third measure help us? Don't we need to compare bias, variance, and MSE now? Fortunately, we do not, as MSE captures both the variance and bias of an estimator! By expanding $(\hat\theta - \theta)^2$ in the definition of MSE, we have 
\begin{align*}
\mse{\hat\theta} & = \E{\hat\theta^2 - 2\hat\theta\theta + \theta^2} \\
                 & = \E{\hat\theta^2} - 2\theta\E{\hat\theta} + \E{\theta^2} & (\text{Expectation is linear})\\
                 & = \E{\hat\theta^2} - 2\theta\E{\hat\theta} + \E{\theta^2} + \left(\E{\hat\theta}^2 - \E{\hat\theta}^2\right) \\ 
                 & = \left(\E{\hat\theta^2} - \E{\hat\theta}^2\right) - 2\theta\E{\hat\theta} + \E{\theta^2} + \E{\hat\theta}^2 \\ 
                 & = \var{\hat\theta} + \left(\E{\hat\theta}^2 - 2\theta\E{\hat\theta} + \theta^2 \right)\\
                 & = \var{\hat\theta} + \left(\E{\hat\theta} - \theta\right)^2 \\
                 & = \var{\hat\theta} + \text{Bias}(\hat\theta)^2.
\end{align*}
In a sense, mean square error is an aggregate of variance and bias, where we are more concerned with bias than variance (hence it being squared).  We would rather have a very precise (low variance) estimator that is inaccurate (high bias) then an accurate estimator (low bias) that is imprecise (high variance). What does this mean for our two estimators of $\sigma^2$?  

::: {#exm-}
Once again, define 
\begin{align*}
S^2({\X}) & = \frac{1}{n-1}\sum_{i=1}^n (X_i -\bar X)^2.\\
\hat\theta({\X}) & = \frac{1}{n}\sum_{i=1}^n (X_i -\bar X)^2.
\end{align*}
The MSE of these estimators are
\begin{align*}
\mse{S^2} &= \var{S^2} + \text{Bias}(S^2)^2 = \frac{2\sigma^4}{n-1} + 0 = \frac{2\sigma^4}{n-1},\\
\mse{\hat\theta} &= \var{\hat\theta} + \text{Bias}(\hat\theta)^2 = \frac{2\sigma^4(n-1)}{n-1} + \left(\frac{n-1}{n}\sigma^2 - \sigma^2\right)^2 = \frac{\sigma^4(2n-1)}{n^2}.
\end{align*} 
For all $n \ge 1$ we have, 
\begin{align*}
& 3n > 1 \\ 
\implies & -1 > -3n \\
\implies & 0 > 1-3n \\
\implies & 2n^2 > 2n^2 - 3n + 1 \\
\implies & 2n^2 > (2n-1)(n-1) \\
\implies & \frac{2n^2}{(n-1)n^2} > \frac{(2n-1)(n-1)}{(n-1)n^2}\\
\implies & \frac{2n^2}{n-1} > \frac{(2n-1)}{n^2}\\
\implies & \frac{2\sigma^4n^2}{n-1} > \frac{\sigma^4(2n-1)}{n^2}\\
\implies & \mse{S^2} > \mse{\hat\theta}.
\end{align*}

If our sole criterion for an estimator is mean-squared error, than we should opt to estimate $\sigma^2$ with the biased estimator.
:::

This example brings attention to an immediate result of Definition @def-mse.

::: {#prp-biasmse}

## MSE of an Unbiased Estimator

If an estimator $\hat\theta : {\mathcal X} \to \Theta$ is unbiased, then 
$$\mse{\hat\theta}=\var{\hat\theta}.$$
::: 

## Efficient Estimators, Fisher Information

With MSE in mind, we can define what it means to have an "optimal" estimator.

::: {#def-}
An estimator $\hat\theta$ is <span style="color:red">**_efficient_**</span> if $\mse{\hat\theta} < \mse{\hat\theta'}$ (for all values of $\theta$) for all other estimators $\hat\theta'$.^[In general, efficiency is subject to our choice of loss function, and is by no means restricted to a quadratic loss function.] If $\mse{\hat\theta_1} < \mse{\hat\theta_2}$, we say $\hat\theta_1$ **_is more efficient_** $\hat\theta_2$
::: 

Calculating the most efficient estimator can be quite difficult considering how many possible estimators there are, so it's common to look for the most efficient estimator among a smaller class of estimators that satisfy other desirable properties. The most common estimators to discard when finding an estimators are biased estimators. This approach takes unbiasedness as the lowest common denominator, and then picks the most efficient of the unbiased estimators. Note that by Proposition @prp-biasmse, the MSE of all unbiased estimators is simply their respective variances, so by restricting attention to this class of estimators, minimizing the MSE is the same thing as minimizing variance.

::: {#def-}
An estimator $\hat\theta$ is the <span style="color:red">**_(uniformly) minimum-variance unbiased estimator (MVUE)_**</span> if it is the most efficient estimator among all unbiased estimators.
::: 

Even with attention restricted to unbiased estimators, we will need an additional tool to determine if an estimator is efficient. This comes is a key result which bounds the variance of an estimator.

::: {#thm-CRbound}

## Information Inequality

Suppose $\hat\theta :{\mathcal X} \to \Theta$ is an estimator for $\theta$ where $\E{\hat\theta}=\psi(\theta)$. If the support of $f_{\X}({\X}\mid\theta)$ does not depend on $\theta$, then 
$$ \var{\hat\theta} \ge \frac{\abs{\psi'(\theta)}^2}{I(\theta)} $$ where $I(\theta)$ is defined as  $$I(\theta) = \E{\left(\frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid\theta)\right)^2},$$ and known as the <span style="color:red">**_Fisher information_**</span> of the sample. 
::: 

::: {.proof}
Suppose $\hat\theta$ is an estimator such that $\E{\hat\theta}=\psi(\theta)$, and $f_X(x\mid\theta)$ satisfies the aforementioned properties. First, we'll define a function of $f_X(x\mid \theta)$.
$$ V(x) = \frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid\theta)$$

Apply the chain rule to $V(x)$ gives:
$$V(x) = \frac{\partial}{\partial \theta} \log f_{\X}({\X}\mid\theta) = \frac{1}{f_{\X}({\X}\mid\theta)}\left[\frac{\partial }{\partial \theta} f_{\X}({\X}\mid\theta)\right].$$ Because $V$ depends on the random variable $X$, we can take it's expectation over the sample space ${\mathcal X}$.
\begin{align*}
\E{V} & = \int\frac{1}{f_{\X}({\X}\mid\theta)}\left[\frac{\partial }{\partial \theta} f_{\X}({\X}\mid\theta)\right]\ dF_{\X}({\X}\mid\theta)\\
 & = \int f_{\X}({\X}\mid\theta) \cdot \frac{1}{f_{\X}({\X}\mid\theta)}\left[\frac{\partial }{\partial \theta} f_{\X}({\X}\mid\theta)\right]\ d{\X}\\
 & = \int \frac{\partial }{\partial \theta} f_{\X}({\X}\mid\theta)\ d{\X}.
\end{align*}
This integral is taken over the support of $f_{\X}({\X}\mid\theta)$. We've assumed the support is not a function of $\theta$, so the bounds of integration do not depend on $\theta$ and we can use Liebniz's integral rule to interchange integration and differentiation:
$$ \E{V} = \int \frac{\partial }{\partial \theta} f_{\X}({\X}\mid\theta)\ d{\X} = \frac{\partial }{\partial \theta} \underbrace{\int  f_{\X}({\X}\mid\theta)\ d{\X}}_1 = 0.$$ Because $V$ has an expectation of zero, it's variance is the expectation of its square:
$$\var{V} = \E{V^2} - \underbrace{\E{V}^2}_0 = \cdot\E{\left(\frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid \theta)\right)^2} = I(\theta)$$ {#eq-vvar}
Using the fact that $\E{V} = 0$, the covariance between $V$ and $\hat\theta$ is 
\begin{align*}
\cov{V, \hat\theta} &= \E{V\hat\theta} -\underbrace{\E{V}}_0\E{\hat\theta}\\
& = \E{V\hat\theta}\\
& = \E{\frac{1}{f_{\X}({\X}\mid\theta)}\left[\frac{\partial }{\partial \theta} f_{\X}({\X}\mid\theta)\right]\hat\theta} \\ 
& = \int \frac{1}{f_{\X}({\X}\mid\theta)}\left[\frac{\partial }{\partial \theta} f_{\X}({\X}\mid\theta)\right]\hat\theta({\X}) \ dF_{\X}({\X}\mid \theta) \\
& = \frac{\partial }{\partial \theta} \int \frac{1}{f_{\X}({\X}\mid\theta)}f_{\X}({\X}\mid\theta)\hat\theta({\X}) \ dF_{\X}({\X}\mid \theta) \\
& = \frac{\partial }{\partial \theta} \int \hat\theta({\X}) \ dF_{\X}({\X}\mid \theta) \\
& = \frac{\partial }{\partial \theta} \E{\hat\theta} \\
& = \psi'(\theta).
\end{align*}
Again we were able to interchange differentiation and integration due to the assumption about $f_X(x\mid\theta)$'s support. In the context of probability, the famed Cauchy-Schwarz inequality takes the form 
$$ \abs{{\cov{V,\hat\theta}}^2} \le \var{V}\var{\hat\theta}.$$
Using this along with the fact that $\cov{V, \hat\theta} = \psi'(\theta)$ and @eq-vvar, gives:
\begin{align*}
& \abs{{\cov{V,\hat\theta}}^2} \le \var{V}\var{\hat\theta} \\
\implies & \abs{\psi'(\theta)}^2 \le I(\theta)\var{\hat\theta} \\ 
\implies & \frac{\abs{\psi'(\theta)}^2}{ I(\theta)} \le \var{\hat\theta} 
\end{align*}
This is the desired inequality
::: 

This bound on the variance of an estimator is often called the **_Cramér–Rao (CR) lower bound_**. Theorem @thm-CRbound presents this bound in more generality than is often required. We can simplify it in three cases:

1. $\hat\theta$ is unbiased, allowing us to eliminate reference to $\psi'(\theta)$.
2. The random vector ${\X}$ is comprised of iid random variables $X_i$, allowing us to write the Fisher information in terms to common marginal density $f_X(x\mid\theta)$.
3. If the second derivative of $\log f_{\X}({\X}\mid \theta)$ with respect to $\theta$ exists, we can write the fisher information with respect to this second derivative.


::: {#cor-}

## Information Inequality for Unbiased Estimators

Suppose $\hat\theta :{\mathcal X} \to \Theta$ is an unbiased estimator for $\theta$. If the support of $f_X(x\mid\theta)$ does not depend on $\theta$, then 
$$ \var{\hat\theta} \ge I(\theta)^{-1}.$$
::: 

::: {.proof}
If $\hat\theta$ is unbiased, then $\E{\hat\theta} = \theta$, and $\psi'(\theta) = 1$.
::: 

::: {#cor-}

## Information Inequality for IID Samples

Suppose $\hat\theta :{\mathcal X} \to \Theta$ is an estimator for $\theta$, and $X_i \overset{iid}{\sim} P_{\thet}a$ for all $i=1,\ldots,n$. If the support of $f_X(x\mid\theta)$ does not depend on $\theta$, then 
$$\var{\hat\theta} \ge \frac{\abs{\psi'(\theta)}^2}{n\E{\left(\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\right)^2}}$$ where $f_X(x\mid\theta)$ is the distribution shared by all $X_i$. 
::: 

::: {.proof}
The joint distribution $f_{\X}({\X}\mid\theta)$ can be written as the product of the $n$ identical marginal distributions.
$$f_{\X}({\X}\mid\theta) = \prod_{i=1}^nf_X(x_i\mid\theta)$$
If we use this equality we can write the Fisher information as,
\begin{align*}
I(\theta) &= \E{\left(\frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid\theta)\right)^2}\\
& = \E{\left(\frac{\partial}{\partial \theta}\log \prod_{i=1}^nf_X(x_i\mid\theta)\right)^2} \\
& = \E{\left(\frac{\partial}{\partial \theta}\sum_{i=1}^n\log f_X(x_i\mid\theta)\right)^2} & (\text{log properties})\\
& = \E{\left(\sum_{i=1}^n\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\right)^2} & (\text{linearity of derivative})\\
& = \E{\sum_{i=1}^n\left(\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\right)^2 + \sum_{i\neq j}\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\frac{\partial}{\partial \theta}\log f_X(x_j\mid\theta)} & (\text{expand square})\\
& = \sum_{i=1}^n\E{\left(\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\right)^2} + \sum_{i\neq j}\E{{\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\frac{\partial}{\partial \theta}\log f_X(x_j\mid\theta)}}& (\text{expectation is linear})\\
& = \sum_{i=1}^n\E{\left(\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\right)^2} + \sum_{i\neq j}\underbrace{\E{{\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)}}}_0\underbrace{\E{\frac{\partial}{\partial \theta}\log f_X(x_j\mid\theta)}}_0& (X_i\perp X_j) \\ 
& = n\E{\left(\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)\right)^2} & (\text{identical distributions}).
\end{align*}
The fact that $\E{{\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)}} = 0$ follows from the same argument we used to conclude $\E{V} = 0$ in the proof of Theorem @thm-CRbound.
::: 


::: {#cor-crb2nd}
Suppose $\hat\theta :{\mathcal X} \to \Theta$ is an estimator for $\theta$ where $\E{\hat\theta}=\psi(\theta)$. If the support of $f_{\X}({\X}\mid\theta)$ does not depend on $\theta$, and $\frac{\partial^2}{\partial \theta^2}\log f_{\X}({\X} \mid \theta)$ exists, then 
$$\var{\hat\theta} \ge \frac{\abs{\psi'(\theta)}^2}{-\E{\frac{\partial^2}{\partial \theta^2}\log f_{\X}({\X}\mid\theta)}}$$ 
::: 

::: {.proof}
Recall that in the proof of @thm-CRbound we established $\E{{\frac{\partial}{\partial \theta}\log f_X(x_i\mid\theta)}} = 0$. Using this, along with the Liebniz's rule and the assumption about the support of $f_{\X}({\X}\mid\theta)$ we have 

\begin{align*}
& \E{{\frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid\theta)}} = 0\\
\implies & \frac{\partial}{\partial \theta}\E{{\frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid\theta)}} = 0 \\
\implies & \frac{\partial}{\partial \theta}\int \frac{1}{f_{\X}({\X}\mid\theta)}\frac{\partial}{\partial \theta}f_{\X}({\X}\mid\theta)\ dF_{\X}({\X}\mid\theta) = 0\\
\implies & \frac{\partial}{\partial \theta}\int \frac{\partial}{\partial \theta}f_{\X}({\X}\mid\theta)\ d{\X} = 0\\
\implies & \int \frac{\partial^2}{\partial \theta^2}f_{\X}({\X}\mid\theta)\ d{\X} = 0\\
\implies & \int \frac{\frac{\partial^2}{\partial \theta^2}f_{\X}({\X}\mid\theta)}{f_{\X}({\X}\mid\theta)}f_{\X}({\X}\mid\theta)\ d{\X} = 0\\
\implies & \int \frac{\frac{\partial^2}{\partial \theta^2}f_{\X}({\X}\mid\theta)}{f_{\X}({\X}\mid\theta)}\ dF_{\X}({\X}\mid\theta) = 0 \\
\implies & \E{\frac{\frac{\partial^2}{\partial \theta^2}f_{\X}({\X}\mid\theta)}{f_{\X}({\X}\mid\theta)}} = 0
\end{align*}
This equality allows us to conclude $-\E{\frac{\partial^2}{\partial \theta^2}\log f_{\X}({\X}\mid\theta)} = \E{\left(\frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid\theta)\right)^2}$.

\begin{align*}
-\E{\frac{\partial^2}{\partial \theta^2}\log f_{\X}({\X}\mid\theta)} & = -\E{\frac{\partial}{\partial \theta}\left(\frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid\theta)\right)} \\
  & = -\E{\frac{\partial}{\partial \theta}\left(\frac{\frac{\partial}{\partial \theta}f_{\X}({\X}\mid\theta)}{f_{\X}({\X}\mid\theta)}\right)}& (\text{chain rule})\\
  & = -\E{\frac{f_{\X}({\X}\mid\theta)\frac{\partial^2}{\partial^2 \theta}f_{\X}({\X}\mid\theta) - \frac{\partial}{\partial \theta}f_{\X}({\X}\mid\theta)\frac{\partial}{\partial \theta}f_{\X}({\X}\mid\theta)}{f_{\X}({\X}\mid\theta)^2}}& (\text{quotient rule})\\
  & =-\E{\frac{\frac{\partial^2}{\partial^2 \theta}f_{\X}({\X}\mid\theta)}{f_{\X}({\X}\mid\theta)} - \left(\frac{\frac{\partial}{\partial \theta}f_{\X}({\X}\mid\theta)}{f_{\X}({\X}\mid\theta)}\right)^2 }\\
  & = -\underbrace{\E{\frac{\frac{\partial^2}{\partial^2 \theta}f_{\X}({\X}\mid\theta)}{f_{\X}({\X}\mid\theta)}}}_0 + \E{\left(\frac{\frac{\partial}{\partial \theta}f_{\X}({\X}\mid\theta)}{f_{\X}({\X}\mid\theta)}\right)^2 } & (\text{expectation is linear}) \\
  & =  \E{\left(\frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid\theta)\right)^2 } 
\end{align*}
If we apply the information inequality, the result follows.
:::

These three corollaries can make the Cramér–Rao lower bound a headache because it takes so many forms. The following table helps us make sense of the various cases, assuming that $\frac{\partial^2}{\partial \theta^2}\log f_{\X}({\X} \mid \theta)$ exists.

|                                   | $\hat\theta$ is unbiased | $\E{\hat\theta} = \psi(\theta)$ |
|---------------|--------------------------|---------------------------------|
| $X_i \overset{iid}{\sim}{P_{\thet}}$     | $\var{\hat\theta} \le \frac{1}{n\E{\left(\frac{\partial}{\partial \theta}\log f_X(x\mid\theta)\right)^2}} = -\frac{1}{n\E{\frac{\partial^2}{\partial \theta^2}\log f_X(x\mid\theta)}}$                     | $\var{\hat\theta} \le \frac{\abs{\psi'(\theta)}^2}{n\E{\left(\frac{\partial}{\partial \theta}\log f_X(x\mid\theta)\right)^2}} = -\frac{\abs{\psi'(\theta)}^2}{n\E{\frac{\partial^2}{\partial \theta^2}\log f_X(x\mid\theta)}}$                            |
| $X_i {\overset{iid}{\not\sim}}{P_{\thet}}$ |  $\var{\hat\theta} \le \frac{1}{\E{\left(\frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid\theta)\right)^2}} = -\frac{1}{\E{\frac{\partial^2}{\partial \theta^2}\log f_{\X}({\X}\mid\theta)}}$                          |  $\var{\hat\theta} \le \frac{\abs{\psi'(\theta)}^2}{\E{\left(\frac{\partial}{\partial \theta}\log f_{\X}({\X}\mid\theta)\right)^2}} = -\frac{\abs{\psi'(\theta)}^2}{\E{\frac{\partial^2}{\partial \theta^2}\log f_{\X}({\X}\mid\theta)}}$                                |

Because we're often concerned with finding a MVUE, we will rarely be concerned with the case where $\E[\hat\theta]$. Additionally, the assumption that the random sample is used to calculate estimator is exceedingly common. For these two reasons, the Cramér–Rao lower bound will almost always take the form 
$$\var{\hat\theta} \le \frac{1}{n\E{\left(\frac{\partial}{\partial \theta}\log f_X(x\mid\theta)\right)^2}} = -\frac{1}{n\E{\frac{\partial^2}{\partial \theta^2}\log f_X(x\mid\theta)}}.$$


::: {#exm-}
Suppose $X_i\overset{iid}{\sim}\text{Binom}(k,p)$, recalling $$f_X(x\mid p) = \binom{k}{p}p^x(1-p)^{k-x}$$ where $x$ is the number of realized successes of $k$ binomial trials. This random variable has an expected value of $kp$ and variance of $kp(1-p)$. Our goal is to estimate the probability of success $p$. If we observe one sequence of $k$ trials ($n=1$) A natural estimator for this is ratio of successes to trials, $\hat p = X/k$. This estimator is unbiased:
$$\E{\hat p} = \frac{\E{X}}{k} = \frac{kp}{k} = p.$$
The variance of the estimator is:
$$\var{\hat p} = \var{\frac{X}{k}} = \frac{\var{X}}{k^2} = \frac{kp(1-p)}{k^2} = \frac{p(1-p)}{k}.$$
Is $\hat p$ an MVUE? In order to determine this, we must calculate the Cramér–Rao lower bound.
\begin{align*}
& \log f_X(x\mid p) = \log \binom{k}{x} + x\log p + (k-x)\log (1-p)\\
\implies &  \frac{\partial}{\partial p}  \log f_X(x\mid p) = \frac{x-kp}{p(1-p)}\\
\implies & \left(\frac{\partial}{\partial p}  \log f_X(x\mid p)\right)^2 = \frac{(x-kp)^2}{p^2(1-p)^2}\\
\implies & \E{\left(\frac{\partial}{\partial p}  \log f_X(x\mid p)\right)^2 } = \frac{\E{(x-kp)^2}}{p^2(1-p)^2} = \frac{\var{X}}{p^2(1-p)^2} =  \frac{kp(1-p)}{p^2(1-p)^2} = \frac{k}{p(1-p)}\\ 
\implies & n\E{\left(\frac{\partial}{\partial p}  \log f_X(x\mid p)\right)^2 } = 1\cdot \frac{k}{p(1-p)} = \frac{k}{p(1-p)}\\
\implies & \var{\hat p} \le \frac{p(1-p)}{k}.
\end{align*}
We have that $\hat p =X/k$ is the MVUE. We could have calculated this bound using the second derivative of $\log f_X(x\mid\theta)$:

$$ \frac{\partial^2}{\partial p^2}  \log f_X(x\mid p)= \frac{\partial}{\partial p}\left[\frac{x-kp}{p(1-p)}\right]=-\frac{x}{p^2}-\frac{m-x}{(1-p)^2}=-\frac{(x-mp)^2}{p^2(1-p^2)} $$

It's important to remember that this means $\hat p$ is the most efficient estimator among unbiased estimators. It is entirely possible that there exists a biased estimator for $p$ that has a lower MSE than $\hat p$ for certain parameter values of $(k,p)$. Take for instance the estimator $\hat p'$, 
$$\hat p' = \frac{X+1}{k+2}.$$
For this estimator, we have 
\begin{align*}
\text{Bias}(\hat p ')& = \E{\hat p '} - p = \frac{\E{X} + 1}{k + 2} - p = \frac{kp + 1}{k + 2} - p = \frac{1-2p}{m+2}\\
\var{\hat p'} &= \var{\frac{X + 1}{k+2}} = \frac{\var{X+1}}{(k+2)^2}= \frac{kp(1-p)}{(k+2)^2}\\ 
\mse{\hat p '}& = \var{\hat p '} + \text{Bias}(\hat p')^2 = \frac{1 + (k-4)p - (k-4)p^2}{(k+2)^2}.
\end{align*}
We can graph $\mse{\hat p} = \var{\hat p}$ along with $\mse{\hat p'}$ for various values of $k$.

```{r}
#| code-fold: true
#| label: fig-plot14
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: "Distribution of estimates of variance for an unbiased estimator and a biased estimator"
#| code-summary: "Show code which generates figure"
#| warnings: false

p <- 0:100/100
k <- c(5,10,15, 20)
est <- c("Biased", "MVUE")

expand.grid(p, k, est) %>% 
  rename(
    p = Var1,
    k = Var2,
    Estimator = Var3
  ) %>% 
  mutate(
    mse = ifelse(Estimator == "Biased", (1+(k-4)*p - (k-4)*p^2)/((k+2)^2) , p*(1-p)/k)
  ) %>% 
  ggplot(aes(p, mse, color = Estimator)) +
  geom_line() +
  facet_wrap(~k) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  labs(x = "True Value of Estimad, p", y = "Mean Squared Error") +
  scale_color_manual(values = c("red", "blue")) 
```

The estimator $\hat p'$ does have a lower MSE than the UMVE at times.
::: 


::: {#exm-}
If we observe realizations of $X_i\overset{iid}{\sim}\text{Uni}(0,\theta)$, then the distribution $f_X(x\mid\theta) = 1/\theta$ (with a support of $[0,\theta]$) *does not* satisfy the information inequality's necessary condition -- the support of $f_X(x\mid\theta)$ is a function of the parameter $\theta$. Without this assumption, we can not apply Leibniz's rule in the proof for \@ref(thm:CRbound), and shouldn't expect the result to hold.

An unbiased estimator for $\theta$ is 
$$\hat\theta({\X})=\frac{n+1}{n}X_{(n)}$$ where $X_{(n)} = \max\{X_1,\ldots, X_n\}$ is the maximum order statistic. If the random variable $X_i$ has an upper bound of $\theta$, then using the largest value from the sample to estimate $\theta$ is rather intuitive. The order statistic $X_{(n)}$ has a density function of 
$$f_{X_{(n)}}(x\mid\theta) = nf_X(x\mid\theta)[F_X(x\mid\theta)]^{n-1} = n\left(\frac{1}{\theta}\right)\left(\frac{x}{\theta}\right)^{n-1} = n\frac{x^{n-1}}{\theta^n}.$$
The expectation of $X_{(n)}$ is 
$$\E{X_{(n)}} = \int_0^\theta x\ dF_{X_{(n)}}(x\mid\theta) = \frac{n}{\theta^n} \int_0^\theta x n\frac{x^{n-1}}{\theta^n}\ dx = \frac{n}{n+1} \theta,$$ so it's a biased estimator. We can correct for this bias by multiplying by $X_{(n)}$ by $(n+1)/n$, which gives $\hat\theta({\X})$. The variance of $\hat\theta$ is 
\begin{align*}
\var{\hat\theta}&=\var{\frac{n+1}{n}X_{(n)}}\\
&= \frac{(n+1)^2}{n^2} \left(\E{X_{(n)}^2} - \E{X_{(n)}}^2\right)\\
&= \frac{(n+1)^2}{n^2} \left(\int_0^\theta x^2\ dF_{X_{(n)}}(x\mid\theta) - \left(\frac{n}{n+1} \theta\right)^2\right)\\
& = \frac{\theta^2}{n(n+2)},
\end{align*}
while the Fisher information of the sample is 
\begin{align*}
& \log f_X(x\mid \theta) = -\log(\theta)\\
\implies & \frac{\partial}{\partial \theta} \log f_X(x\mid \theta) = -\frac{1}{\theta}\\
\implies &\left(\frac{\partial}{\partial \theta} \log f_X(x\mid \theta)\right)^2 = \frac{1}{\theta^2}\\
\implies & \E{\left(\frac{\partial}{\partial \theta} \log f_X(x\mid \theta)\right)^2} = \frac{1}{\theta^2}\\
\implies & n\E{\left(\frac{\partial}{\partial \theta} \log f_X(x\mid \theta)\right)^2} = \frac{n}{\theta^2}\\
\implies & I(\theta) = \frac{n}{\theta^2}.
\end{align*}

In this case, $\var{\hat\theta} < I(\theta)^{-1}$.
:::




We can extend \@ref(thm:CRbound) to the case where we estimate multiple parameters. Because our interest will almost always be in unbiased estimators calculated with IID samples, the multiparameter version of the information inequality will be presented in this context. 

::: {#thm-}

## Information Inequality for Multiple Parameters

Suppose $\hat{\thet} :{\mathcal X} \to \Theta$ is an unbiased estimator for $\thet$, and $X_i \overset{iid}{\sim} P_{\thet}a$ for all $i=1,\ldots,n$. If the support of $f_X(x\mid\thet)$ does not depend on $\thet$, then 
$$ \var{\hat{\thet}} \ge \mathbf I(\thet)^{-1},$$
where $\mathbf I(\thet)$ is known as the <span style="color:red">**_information matrix_**</span> and defined as 
$$\mathbf I(\thet)_{i,j} = -n \E{\frac{\partial}{\partial \theta_i\partial\theta_j}\log f_X(x\mid\theta)}$$
::: 

::: {#exm-}
If $X_i\sim N(\mu,\sigma^2)$, we can estimate $\mu$ and $\sigma^2$ jointly with an estimator $\hat{\thet}  = (\hat\mu, \hat\sigma^2)$.

This can be verified by using the information inequality for unbiased estimators and IID samples. In this case,
\begin{align*} f_X(x\mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right]
\implies \log f_X(x\mid \mu, \sigma^2) = \log(1) - \frac{1}{2}\log(2\pi\sigma^2) - \frac{(x-\mu)^2}{2\sigma^2}. 
\end{align*}

Differentiating gives:

\begin{align*}
-n \E{\frac{\partial^2}{\partial \mu^2}\log f_X(x\mid\theta)} &= -n\E{-\sigma^{-2}} = n\sigma^{-2}\\
-n \E{\frac{\partial}{\partial \mu\partial\sigma^2}\log f_X(x\mid\theta)} &= -n \E{\frac{\partial}{\partial\sigma^2\partial \mu}\log f_X(x\mid\theta)}=-n\sigma^{-4}\E{x-\mu}=0\\
-n \E{\frac{\partial^2}{\partial (\sigma^2)^2}\log f_X(x\mid\theta)} &= -n\E{\sigma^{-4}/2} = n\sigma^{-4}/2\\
\mathbf I(\mu, \sigma) & = \begin{bmatrix} n/\sigma^{-2} & 0 \\ 0  & n\sigma^{-4}/2 \end{bmatrix}.
\end{align*}
The information inequality takes the form 
$$\var{\hat\mu, \hat\sigma^2} \ge  \begin{bmatrix} \sigma^{2}/n & 0 \\ 0  & 2\sigma^{4}/n \end{bmatrix}.$$
Notice that $\var{\hat\mu} \ge \sigma^2/n$. This means that $\hat\mu = \bar X$ is the MVUE, as $\var{\bar X} =  \sigma^2/n$.
:::  

::: {.remark}
@thm-CRbound and its resulting corollaries have one main drawback. It is not constructive and provides no guidance as to constructing efficient estimators, and if such an estimator even exists. Addressing this is a matter of introducing a new property of estimators related to data reduction and information (**_sufficiency_**) and the Lehmann–Scheffé theorem (which is related to the perhaps more familiar Rao–Blackwell theorem).
:::


## The Distribution of an Estimator

Finally, we turn to the matter of an estimator's probability distribution. Like any other random variable, estimators have a distribution and density function. Knowing the distribution of an estimator allows us to put a point estimate in a broader context by using an estimator's distribution to calculate the probability of observing the estimate in question. 

::: {#exm-}
Suppose ${\X} = (X_1, \ldots, X_n)$ is a random sample from a normal distribution $N(\mu, \sigma^2)$. What is the distribution of $\bar X$?  The sum of *independent* normally distributed random variables is normally distributed as follows:
$$\sum_{i=1}^n X_i \sim N(u_1 + \mu_2 + \cdots + \mu_n, \sigma_1^2 + \sigma_2^2 + \cdots \sigma_n^2).$$ In our case, each $X_i$ are identically distributed as well, so $\mu_i = \mu$ and $\sigma_i^2 = \sigma^2$ for all $i$.
$$ \sum_{i=1}^n X_i\sim N(n\mu, n\sigma^2)$$
Finally if we scale the sum by $1/n$, giving $\bar X$, we have 
$$\bar X\sim N(\mu, \sigma^2/n)$$
by the properties of expectation and variance. If we simulate realizations of this estimator, the resulting histogram should betray that $\bar X$ is normally distributed. For these simulations, we will take $n = 100$, $\mu = 0$, and $\sigma^2 = 1$. We should see that 
\begin{align*}
\E{\bar X} &\approx 0\\ 
\var{\bar X} &\approx 0.01
\end{align*}

```{r}
results <- draw_N_estimates(
  N = 1e6, 
  estimators = c(
    "x_bar" = mean
  ),
  n = 100, 
  dist = rnorm, 
  dist_params = list(
    mean = 0, 
    sd = 1
  )
)
```

```{r}
#| code-fold: true
#| label: fig-plot15
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: "Distribution of estimates of variance for an unbiased estimator and a biased estimator"
#| code-summary: "Show code which generates figure"
#| 
results %>% 
  ggplot(aes(estimate)) +
  geom_histogram(aes(y = after_stat(density)), colour = 1, fill = "white", bins = 100) + 
  labs(x = "Estimates of μ") +
  theme_minimal() +
  stat_function(fun = dnorm, args = list(mean = 0, sd = sqrt(1/100)), color = "red")
```

Note that for this to hold, it must be the case that $X_i$ are iid normally distributed, which is rather restrictive. Ideally, we will be able to make some statements about estimators' distributions regardless of the underlying distribution which generates the observable data. Fortunately, the next section will equip us with the tools to do this. 
:::

## Further Reading

- @mccullagh2002statistical
- @bickel2015mathematical
- @lehmann2006theory
- @greene2003econometric, Appendix C