\DeclareMathOperator{\plim}{plim}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmax}
\newcommand{\var}[1]{\text{Var}\left(#1\right)}
\newcommand{\avar}[1]{\text{Avar}\left(#1\right)}
\newcommand{\E}[1]{\text{E}\left[#1\right]}
\newcommand{\cov}[1]{\text{Cov}\left(#1\right)}
\newcommand{\mse}[1]{\text{MSE}\left(#1\right)}
\newcommand{\se}[1]{\text{se}\left(#1\right)}
\newcommand{\limfunc}{lim} 
\newcommand{\X}{\mathbf{X}}
\newcommand{\Xm}{\mathbb{X}}
\newcommand{\EER}{\bar{\thet}_\text{EE}}
\newcommand{\NLS}{\hat{\bet}_\text{NLLS}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\rr}{\mathbf{r}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\Pe}{\mathbf{P}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\uu}{\mathbf{u}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\xm}{\mathbb{x}}
\newcommand{\Zm}{\mathbb{Z}}
\newcommand{\Wm}{\mathbb{W}}
\newcommand{\Hm}{\mathbb{H}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\Hess}{\mathbf{H}(\mathbf{\Z\mid\thet})}
\newcommand{\Score}{\mathbf{S}(\mathbf{\Z\mid\thet})}
\newcommand{\A}{\mathbf{A}}
\newcommand{\h}{\mathbf{h}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\F}{\mathbf{F}}
\newcommand{\G}{\mathbf{G}}
\newcommand{\I}{\mathbf{I}}
\renewcommand{\D}{\mathbf{D}}
\renewcommand{\C}{\mathbf{C}}
\newcommand{\zer}{\mathbf{0}}
\newcommand{\OLS}{\hat{\boldsymbol\beta}_\text{OLS} }
\newcommand{\OLSOV}{\hat{\boldsymbol\beta}_\text{OLS,OV} }
\newcommand{\OLSME}{\hat{\boldsymbol\beta}_\text{OLS,ME} }
\newcommand{\EE}{\hat{\boldsymbol\theta}_\text{EX} }
\newcommand{\ME}{\hat{\boldsymbol\theta}_\text{M} }
\newcommand{\MDE}{\hat{\boldsymbol\theta}_\text{MDE} }
\newcommand{\IV}{\hat{\boldsymbol\beta}_\text{IV} }
\newcommand{\TSLS}{\hat{\boldsymbol\beta}_\text{2SLS} }
\newcommand{\thet}{\boldsymbol{\theta}}
\newcommand{\et}{\boldsymbol{\eta}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Sig}{\boldsymbol{\Sigma}}
\newcommand{\ep}{\boldsymbol{\varepsilon}}
\newcommand{\Omeg}{\boldsymbol{\Omega}}
\newcommand{\Thet}{\boldsymbol{\Theta}}
\newcommand{\bet}{\boldsymbol{\beta}}
\newcommand{\rk}{rank}
\newcommand{\tsum}{\sum}
\newcommand{\tr}{tr}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\ms}{\overset{ms}{\to}}
\newcommand{\pto}{\overset{p}{\to}}
\newcommand{\iid}{\overset{iid}{\sim}}
\newcommand{\dto}{\overset{d}{\to}}
\newcommand{\asim}{\overset{a}{\sim}}



# Advanced Asymptotics


```{r}
#| echo: false
#| output: false
library(tidyverse)
library(ggpubr)
library(ggridges)
library(gganimate)
```


@sec-asy provided a crash course in asymptotic statistics that can get you pretty far in econometrics. Still there are certain situations where we need a more abstract and general understanding of stochastic convergence. The purpose of this chapter is to give a painfully brief introduction to the convergence of random variables defined on metric spaces, and discuss some of the related applications. 

## Reviewing More Probability Theory

Given a probability space $(\mathcal X, \mathcal F, P)$ and a set $E$ equipped with a sigma-algebra $\mathcal E$, a random variable is some function $X:\mathcal X\to E$ such that $X^{-1}(I) \in \mathcal F$ for all $I\in \mathcal E$. Until now, we've only dealt with the case $E = \R$ and $\mathcal E = \mathcal B(\R)$, so let's look at random variables on other spaces. In particular, we're interested in random variables defined on metric spaces. 

:::{#exm-}
## Random Vectors
We've been interpreting a random vector $\X = (X_1,\ldots,X_n)$ as a collection of $n$ random scalars $X_1:\mathcal X_1\to E_1,\ldots,X_n:\mathcal X_n\to E_n$. Alternatively, we could consider $\X$ to be a random variable mapping $\mathcal X_1\times\cdots\times \mathcal X_n$ to $E^n$.^[We're brushing over how products of sets play with sigma-algebras here, but the underlying math isn't trivial.] A special case of this is when we draw a sample of $n$ observations of $X_i \iid F_X$. If we execute ```rnorm(10)```, it will return $n$ realizations of $X:\R\to\R$ which can also be interpreted as one realization of a random vector $\X:\R^{10}\to\R^{10}$. Whether you think of the resulting data as 10 realizations of iid random variables or a single random vector will depend on the application. It's usually easier to model problems with random vectors when random scalars aren't iid. If we have $X_1 \sim N(0,1)$, $X_2 \sim N(1,2)$, and $\cov{X_1, X_2} = 1$, then all the information about $X_1$ and $X_2$ can easily be summarized by $\X = (X_1,X_2)$ where $$\X \sim N\left(\begin{bmatrix} 0\\ 1\end{bmatrix}, \begin{bmatrix} 1 & 1\\ 1 & 2\end{bmatrix}\right)$$
:::

:::{#exm-}
## Random Matrices
We can combine random vectors into a single random matrix just like we combined random scalars into a random vector. For the sake of ease, we'll only consider real valued random vectors here. Suppose we have $k$ random vectors $\X_1,\ldots,\X_k$, each of length $n$. We can form a $n\times k$ matrix using these vectors as column vectors. We'll denote this random matrix with $\Xm$.^[This notation is not standard, but eliminates a great deal of ambiguity when working with random matrices and vectors.] 

$$\Xm = \begin{bmatrix}\X_1 & \cdots &\X_k \end{bmatrix} =  \begin{bmatrix}X_{11} & \cdots &X_{k1} \\ \vdots & \ddots & \vdots\\ X_{1n} & \cdots &X_{kn} \end{bmatrix}$$ In this case, the random variable $\Xm$ takes on a value in the set of all $n\times k$ real matrices, $\R^{n\times k}$. We could also consider $\Xm$ to be comprised of $n$ random vectors of length $k$. 
$$ \Xm = \begin{bmatrix}\X_1\\ \vdots \\ \X_k \end{bmatrix}.$$ This interpretation is particularly important when considering regression models where we observe $n$ realizations of $k$ regressors $X_1,\ldots,X_k$.

:::

       A random object we've yet to talk about are stochastic processes. These will make for an excellent example in abstracting random variables beyond $\R$.

:::{#def-}
Given a probability space $(\mathcal X, \mathcal F, P)$, suppose we have a collection of random variables $\{X_t\}_{t\in T}$ indexed by some set $T$ which take on values in the measurable space $(S, \mathcal S)$. The collection $\{X_t\}_{t\in T}$ is a [***stochastic process***]{style="color:red"}. We refer to $S$ as the [***state space***]{style="color:red"} and $T$ as the [***index set***]{style="color:red"}. If $T = \mathbb N$ then $\{X_t\}_{t\in T}$ is a [***discrete-time stochastic process***]{style="color:red"}. If $T = \R^+$ [***continuous-time stochastic process***]{style="color:red"}
:::

       The index set is almost always denoted by $T$ because the main application of stochastic processes is modeling the evolution of random phenomena over time periods $t\in T$. Each value $t\in T$ is mapped to a random variable $X_t$, which takes on a realized value $X_t(\omega)\in S$ for some outcome $\omega\in\mathcal X$.^[We usually suppress the argument of a random variable, but in this case it's helpful to illustrate the additional time dimension.] Since $X_t$ is indexed by $T$, we can actually think of a stochastic process as a function $t\mapsto X_t$, and opt to write $X(t)$ or $X(t,\omega)$.^[In general, the process of translating a function of two arguments to a function that takes on values in a space of functions is called "currying". For example, define $f:\R^2\to \R$ as $f(x,y)=x+y$. We can write this as $(g(x))(y)$ where $g:\R\to \R^{\R}$. If we fix an input like $x=2$, we have $(g(2))(y)=2+y$, which is an element of $\R^{\R}$ since $g(2):\R\to \R$. This works for general sets since the set of functions from $X\times Y$ to $Z$, written as $Z^{X\times Y}$, is isomorphic to the set $(Z^Y)^X$. $$Z^{X\times Y}\cong (Z^Y)^X$$] This interpretation is how we'll be able to define a stochastic process as a random variable.

:::{#exm-}
## Stochastic Processes as RVs
Given a state space $S$ and an index set $T$, consider the set $S^T$ of all functions mapping $T$ to $S$. A stochastic process $\{X_t\}_{t\in T}$ defined on the probability space $(\mathcal X, \mathcal F, P)$ is a random variable $X:\mathcal X\to S^T$. One realization of $X(\omega)\in S^T$ is a function from $T$ to $S$. In the event that $T =\mathbb N$, $S^\mathbb N$ is the set of sequences in $S$, as a sequence in $S$ is just a function which assigns natural numbers to the elements of $S$. This means that $X:\mathcal X\to S^{\mathbb N}$ is a random sequence. We often refer to realizations of stochastic processes as sample paths to emphasize the additional dimension given by $T$.
:::

:::{#exm-}
## Simple Random Walk

Suppose we have a sequence of iid random variables $Y_1,Y_2,\ldots$ defined on the sample space $\{-1,1\}$ such that $\Pr(Y_t = 1) = p$ and $\Pr(Y_t = -1) = 1-p$ for all $t$. We can think of $Y_t$ as a modified Bernoulli random variable where a "failure" corresponds to the value $-1$ instead of $0$.^[The distribution corresponding to this random variable is known as the Rademacher distribution.] Using the sequence $Y_t$ we can define a new sequence of random variables $X_t$ as 
$$ X_t = \sum_{i=1}^t Y_i.$$ The sequence $X_n$ is a random walk. The name is due to a thought experiment corresponding to $Y_n$ and $X_n$. Suppose someone is standing at a reference point (the origin $0$) at time $t=0$. At $t=1$ they flip a coin. If the flip results in a head they will take one step forward, but if the flip results in a tail they will take a step backwards. This is repeated for $t=2,3,\ldots$. The outcome of the coin flip at time $t$ corresponds to the random variable $Y_t$. At any time $t$, the person's position relative to the reference points is given as $X_t$. Instead of considering $X_1,X_2,\ldots$ to be a sequence of separate random variables, we can think of $X=(X_1,X_2,\ldots)$ to be a single random variable defined on the space $\mathbb Z^{\mathbb N}$. Let's look at one realized value of $X$ taking $p = 1/2$,^[Rather, the first 20 terms of the random sequence.] in which case our random walk is symmetric.


```{r}
random_walk <- function(n, dist, dist_params, t){
  X <- do.call(dist, append(n, dist_params))
  output <- tibble(step = X) %>%
    mutate(
      time = row_number(),
      position = cumsum(step),
      iter_num = t
    ) %>%
    add_row(
      time = 0,
      position = 0,
      iter_num = t,
      .before = 0
    )

  return(output)
}

rredmacher <- function(n, p){
  return(sample(c(-1,1), n, replace = TRUE, prob = c(1-p, p)))
}
results <- random_walk(20, rredmacher, list(0.5), 1)
```

```{r}
#| code-fold: true
#| fig-align: center
#| label: fig-plot51
#| fig-asp: 1
#| warning: false
#| fig-width: 8
#| fig-cap: "One simulated realization of a random walk"
#| code-summary: "Show code which generates figure"
plot <- results %>% 
  select(-iter_num) %>% 
  pivot_longer(!time) %>% 
  mutate(name = factor(name, levels = c("step", "position"), labels = c("Step Y_t", "Position X_t"))) %>% 
  ggplot(aes(time, value)) + 
  geom_point() + 
  facet_wrap(~name, ncol = 1, scales = "free") +
  theme_minimal() +
  theme(legend.position = "none") +
  transition_states(time, transition_length = 0) +
  shadow_mark() +
  labs(x = "Time, t", y = "")

animate(plot, end_pause = 30)
```


Since this is just one realization, it's analogous to illustrating a random variable with a standard normal distribution with a histogram of a single value generated by `rnorm(n = 1)`. Let's look at 1,000 simulated realizations of our random walk $X=(X_1,X_2,\ldots)$.^[An interesting feature of this figure is that it emphasizes the combinatorial nature of a random walk. At time $t=20$ we've flipped the coin 20 times, with each flip having 2 outcomes. This means there are $2^{20}$ possible paths the random walk can take.] 



```{r}
sim <- function(N, n, dist, dist_params){
  output <- 1:N %>%  
    map(random_walk, n = n, dist = dist, dist_params = dist_params) %>% 
    bind_rows()
  
  return(output)
}

results <- sim(1e3, 20, rredmacher, list(0.5))
```

```{r}
#| code-fold: true
#| fig-align: center
#| label: fig-plot52
#| fig-asp: 0.7
#| warning: false
#| fig-width: 8
#| fig-cap: "1,000 simulated realizations of the random walk."
#| code-summary: "Show code which generates figure"

plot <- results %>% 
  ggplot(aes(x = time, y = position)) +
  geom_line(aes(group = factor(iter_num))) +
  theme_minimal() +
  transition_states(iter_num) +
  shadow_mark(alpha = 0.1, size = 0.1, color = "gray") +
  labs(title = 'Realizations Drawn: {closest_state}', x = "Time", y = "Position") 
  
animate(plot, end_pause = 30)
```


We can also investigate the individual terms of the random sequence $X = (X_1,X_2,\ldots)$ by treating them like their own random variables defined on $\mathbb Z$. For 100,000 simulated realizations of $X$, let's look at the distribution of the realized values of $X_1$, $X_{10}$, $X_{20}$, and $X_{30}$.


```{r}
results <- sim(1e5, 30, rredmacher, list(0.5)) %>% 
  filter(time %in% c(1, 10, 20, 30))
```

```{r}
#| code-fold: true
#| label: fig-plot53
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: "Distribution of simulated positions at various times."
#| code-summary: "Show code which generates figure"
#| warning: false


results %>% 
  group_by(time, position) %>% 
  summarize(freq = n()/1e5) %>%
  mutate(time = factor(time, levels = c(1,10,20,30), labels = c("t = 1", "t =10", "t = 20", "t = 30"))) %>% 
  ggplot(aes(position, freq)) +
  geom_point() +
  geom_segment(aes(x = position, xend = position, y = 0, yend = freq)) +
  facet_wrap(~time) +
  theme_minimal() +
  labs(x = "position", y = "frequency")
```


It is no coincidence that as $t$ increases, the distribution of $X_t$ appears to resemble a normal distribution. To show this we'll need to find the probability mass function of $X_t$ which gives $\Pr(X_t = x)$ for $x\in\{-t,-t+1,\ldots,t-1,t\}\subset \mathbb Z$. First note that the probability that $k$ steps forward and $t-k$ steps backwards are taken follows a binomial distribution, since $I(Y_i = 1)\sim \text{Ber}(p)$. 
$$ \Pr(\textstyle\sum_{i=1}^t I(Y_i = 1)) = \binom{t}{k}p^k(1-p)^{t-k}$$
We can write the number of steps forward $k$ in position $x$ by noting that $x = k - (t-k) = 2k-t,$ so $k = (t+x)/2$.
$$f_{X_t}(x \mid t, p) = \binom{t}{(t+x)/2}p^{(t+x)/2}(1-p)^{t-(t+x)/2}.$$
Since $X_t$ follows a variant of the binomial distribution, as $t\to\infty$ the normal approximation of the binomial distribution holds with increasing accuracy. The parameters of this normal distribution are given by the expected value and variance of $X_t$. These are
\begin{align*}
\E{X_t} &= \E{\sum_{i=1}^t Y_i} \\&= \sum_{i=1}^t \E{Y_i} & (\E{\cdot}\text{ linear})\\ &= t\left[p\cdot 1 + (1-p)(-1) \right] & (Y_i\text{ iid})\\ & = t(2p-1)\\
\var{X_t} &= \var{\sum_{i=1}^t Y_i}\\
        & = \sum_{i=1}^t \var{Y_i} & (Y_i\text{ iid})\\
        & = t\left[\E{Y_i^2} - \E{Y_i}^2\right] \\ 
        & = t[1 - (2p-1)^2] & (Y_i^2 = 1)\\
        & = 4tp(1-p)
\end{align*}
If $p = 1/2$ and $t = 100$, we should expect that the distribution of $X_t$ resembles $N(0,100)$. 


```{r}
results <- sim(1e5, 100, rredmacher, list(0.5)) %>% 
  filter(time == 100)
```

```{r}
#| code-fold: true
#| label: fig-plot54
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: "Position of random walk at time t=100 for 100,000 simulations with a normal distribution with µ=0 and σ=10 overlaid."
#| code-summary: "Show code which generates figure"
#| warning: false

t <- 100
p <- 1/2
  
results %>% 
  ggplot(aes(position)) +
  geom_histogram(aes(y = ..density..), colour = 1, fill = "white", binwidth = 2) +
  theme_minimal() +
  stat_function(fun = dnorm, args = list(mean = t*(2*p - 1), sd = sqrt(4*t*p*(1-p))), color = "red") +
  labs(y = "Density", x = "Random Walk Position at t=100")
```

This entire examples happens to be just one very stylized example of a random walk. There exists numerous interesting extensions with valuable applications.
:::

:::{#exm-}
## Wiener Process

The most famous random function is the the Wiener process. This continuous-time stochastic process plays a key role in physics as it's a mathematical formalization of Brownian motion.^[The term "Brownian motion" is often used in place of "Wiener process".] Brownian motion refers to the behavior of particles that are continuously moving randomly about. For instance, gas molecules move rapidly about colliding with other molecules, resulting in seemingly random movement. Formally, the Weiner process $W = \{W_t\}_{t\in \R^+}$ is a random variable $W:\mathcal X\to C( \R^+)$ for some probability space $(\mathcal X, \mathcal F, P)$ where $C( \R^+)$ is the set of all real continuous functions defined on the unit interval $ \R^+$.\The process $\{W_t\}_{t\in \R^+}$ satisfies:

1. $W_0 = 0$. The process' initial value at time $t=0$ is always $0$.
2. For all $t\in \R^+$, $W_{t+u} - W_t \perp W_s$ for $u \ge 0$ and $s < t$. The process has independent increments. In other words, the "movement" of the process between times $t$ and $t+u$ is independent of the processes location prior to $t$ (at time $s$). Another way to think of this property is that the process is "memoryless" in the sense that future movement of the process only depends on its current position and not how it got to that position.
3. For all $t\in \R^+$, $W_{t+u}-W_t\sim N(0,u)$ for $u \ge 0$. This is equivalent to $W_t - W_s \sim N(0,t-s)$ for $s\le t$, which can also be written as $W_t-W_s\sim \sqrt{t-s}\cdot N(0,1)$

A fourth property that we've implicitly assumed by defining $W$ to be a $C( \R^+)$-valued random variable is that $W$ is continuous. *A priori*, it isn't obvious that there exists a process which satisfies all of these properties, but we will take its existence as given.^[See Section 37 of @billingsley2008probability for a proof.] 

One of the limitations of turning to simulations to illlustrate $W$ is that we aren't able to graph a truly continuous (non-linear) function on a computer.^[If we wanted to graph $f(x) = \sqrt x$ on $[0,1]$, we would need to evaluate $f(x)$ at many points in $[0,1]$ and then plot the results. *Technically* we're approximating the graph of $f(x)$ via linear interpolation between the points at which we evaluated $f$ using `geom_line()`. It just happens that it's easy to pick such a sufficiently large number of points to evaluate $f$ at that we get the desired graph of $f$.] You may be thinking" "well no duh, this is obvious! Why are you bringing up something so basic and trivial?" It's worth emphasizing because this entire example is meant to illustrate a continuous stochastic process, which unfortunately we need to do by simulating a discrete analog. Suppose we have $N$ evenly spaced points in $[0,1]$, such that time increments are $dt = 1/N$. We have 
\begin{align*}
W_0 & = 0,\\
W_j & = W_{j-1} + dW_{j}, & (j = 1,\ldots, N)\\
dW_j & \iid N(0, dt).
\end{align*}
We'll set $N = 1\times10^7$, which is such a large value that we'll be able to just treat $W$ as if it was truly continuous. Let's simulate a single realization of $W$.


```{r}
# draw one realization of wiener process
#   N = number of discrete points to calculate process at
#   T_max = maximum time value to calculate process at
#   s = parameter to iterate over if we want to draw multiple realizations
wiener <- function(N, T_max, s){
  dt <- T_max/N
  output <- tibble(
    dW = rnorm(N, sd = sqrt(dt)),
    time = seq(dt, T_max, dt),
    iter_num = s
  ) %>% 
    mutate(value = cumsum(dW)) %>% 
    add_row(value = 0, time = 0, iter_num = s, .before = 0)
  return(output)
}

results <- wiener(1e5, 1, 1)
```

```{r}
#| code-fold: true
#| label: fig-plot55
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: "One realization of the Wiener process on the unit interval [0,1]."
#| code-summary: "Show code which generates figure"
#| warning: false
results %>% 
  ggplot(aes(time, value)) +
  geom_line(size = 0.1) +
  theme_minimal() +
  labs(x = "Time t", y = "Value of W at t") 
```


Since $W$ is continuous it's defined for all values in the interval $[0,1]$, so we can "zoom" in on increasingly small subsets of $[0,1]$, and $W$ will still be defined for all $t$.^[Of course as we look at smaller and smaller intervals, the number of discrete points used to visualize the process will shrink, so the path may not appear quite as "jagged".]  


```{r}
#| code-fold: true
#| label: fig-plot56
#| fig-align: center
#| fig-asp: 1
#| fig-width: 8
#| fig-cap: "``Zooming in'' on the Wiener process."
#| code-summary: "Show code which generates figure"
#| warning: false

zoom_in <- function(W, t){
  output <- W %>% 
    filter(time <= t) %>% 
    mutate(
      max_time = t,
      zoom_factor = 1/t
    )
  return(output)
}

plot <- (1/(2^(0:6))) %>% 
  map(zoom_in, W = results) %>% 
  bind_rows() %>% 
  ggplot(aes(time, value)) + 
  geom_line(size = 0.5) +  
  transition_states(zoom_factor) +
  view_follow() +
  theme_minimal() +
  labs(x = "Time t", y = "Value of W at t")

animate(plot, nframes = 300)
```


Note that as we "zoom in" the sample path has the same jagged appearance of $W$ on all of $[0,1]$. This is because the Wiener process is closely linked to the idea of a fractal.^[Reference to john cook article] Another interesting feature is that $W$ is nowhere differentiable despite being continuous on account of it being comprised of an infinite number of "kinks".

Now that we've seen one sample path, let's draw 100 realizations of $W$ to get a sense of the relative frequency of certain paths.


```{r}
draw_wiener <- function(M, N, T_max){
  output <- 1:M %>% 
    map(wiener, N = N, T_max = T_max) %>% 
    bind_rows() 
  return(output)
}

results <- draw_wiener(1e2, 1e4, 1)
```

```{r}
#| code-fold: true
#| label: fig-plot57
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: "1,000 simulated realizations of the Wiener process"
#| code-summary: "Show code which generates figure"
#| warning: false
plot <- results %>% 
  ggplot(aes(time, value)) +
  geom_line(aes(group = iter_num)) +
  theme_minimal() +
  transition_states(iter_num) +
  shadow_mark(alpha = 0.2, size = 0.1) +
  labs(title = 'Realizations Drawn: {closest_state}', x = "Time", y = "Value") 

animate(plot, end_pause = 30)
```


Another way to visualize these realizations is by looking at the distribution of the simulated values of $W$ at various values of $t$. Since $W_t - W_s \sim N(0, t-s)$ and $W_0 = 0$, we should expect that $$W_t = W_t - 0  = W_t - W_0 \sim N(0,t-0) = N(0,t).$$


```{r}
# Draw one realization of wiener process at times t (approximately)
wiener_t <- function(N, T_max, t, s){ 
  
  W <- wiener(N, T_max, s)

  # Pick the values where W_t is calculated that are closest to the times of interest
  t_filter <- W %>% 
    select(time) %>% 
    expand_grid(given_t = t) %>% 
    mutate(dist = abs(time - given_t)) %>% 
    group_by(given_t) %>% 
    filter(dist == min(dist)) %>% 
    ungroup() %>% 
    select(time) %>% 
    unlist()
  
  output <- W %>% 
    filter(time %in% t_filter) %>% 
    select(-dW)
  
  return(output)
}

# Draw M realizations of wiener process at vector of times t_vals
draw_wiener_t <- function(M, t, T_max, N){
  output <- 1:M %>% 
    map(wiener_t, N = N, T_max = T_max, t = t) %>% 
    bind_rows()
  return(output)
}

results <- draw_wiener_t(1e5, (1:10)/10, 1, 1e3)
```

```{r}
#| code-fold: true
#| label: fig-plot58
#| fig-align: center
#| fig-asp: 1
#| fig-width: 8
#| fig-cap: "The top panel shows the density of the Wiener process at 10 fixed times calculated using 100,000 simulations. The bottom panel shows the sample analogs of expectation and variance for the process calculated at the fixed times, along with the theoretical values for comparison."
#| code-summary: "Show code which generates figure"
#| warning: false

p1 <- results %>% 
  ggplot(aes(x = value, y = time, group = time)) +
  geom_density_ridges2(scale = 5, alpha = .8, rel_min_height = 0.007) +
  xlim(-3,3) +
  scale_y_continuous(breaks = (1:10)/10) +
  theme_minimal() + 
  labs(x = "W_t", y = "Time t")


p2 <- results %>% 
  group_by(time) %>% 
  summarize(
    'Sample Mean' = mean(value),
    'Sample Variance' = var(value)
  ) %>% 
  pivot_longer(!time) %>% 
  ggplot(aes(time, value, color = name)) +
  geom_abline(size = 0.3) +
  geom_hline(yintercept = 0, size = 0.3) +
  geom_point() +
  scale_y_continuous(breaks = (1:10)/10) + 
  scale_x_continuous(breaks = (1:10)/10) + 
  theme_minimal() +
  annotate("text", x=0.7, y = 0.6, label= "Var(W_t) = t") +
  annotate("text", x=0.7, y = 0.05, label= "E[W_t] = 0") +
  theme(legend.position = "bottom") +
  labs(x = "Time t", y = "", color = "") +
  scale_color_manual(values = c("red","blue")) 


ggarrange(p1, p2, ncol = 1)
```


For further confirmation that $W_t - W_s \sim N(0, t-s)$, let's calculate the sample variance associated with our realizations of $W_t - W_s$ for all $s,t\in[0.1, 0.2,\ldots,0.9,1]$ satisfying $s \le t$.


```{r}
# For a grid of time values, calculate var_st at each point {(s,t) ∈ t_grid² | s ≤ t} over M simulations
sim <- function(M, t_grid, T_max, N){
  
  # define function to calculate variance
  var_st <- function(W, s, t){
    if(t == s){
      return(0)
    } else{
      output <- W %>% 
        filter(time %in% c(s,t)) %>% 
        group_by(iter_num) %>% 
        mutate(diff = value - lag(value, order_by = time)) %>% 
        ungroup() %>% 
        summarize(var(diff, na.rm = T)) %>% 
        as.numeric()
      
      return(output)
    }
  }
  
  # draw M realizations of W at grid of times
  W <- draw_wiener_t(M, t_grid, T_max, N)
  
  # Apply var_st to all combinations of times
  output <- expand_grid(
    s = unique(W$time),
    t = unique(W$time)
  ) %>% 
    filter(s <= t) %>% 
    rowwise() %>% 
    mutate(var = var_st(W, s, t))
  
  return(output)
}

results <- sim(1e5, (0:10)/10, 1, 1e3)
```

```{r}
#| code-fold: true
#| label: fig-plot59
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: "The sample variance associated with W_t - W_s calculated using 100,000 simulations."
#| code-summary: "Show code which generates figure"
#| warning: false

results %>% 
  ggplot(aes(s,t, fill = var)) +
  geom_tile(color = "white", lwd = 0.2, linetype = 1) +
  geom_text(aes(label = round(var,3)), color = "white", size = 4) +
  theme_minimal() +
  scale_y_continuous(breaks = (0:10)/10, expand = c(0,0)) + 
  scale_x_continuous(breaks = (0:10)/10, expand = c(0,0)) +
  theme_classic() + 
  labs(fill = "Var(W_t - W_s)") +
  theme(legend.position = "none", axis.line = element_blank())
```

We have $\widehat{\text{Var}}(W_t-W_s) \approx t-s$ as anticipated! Finally, we can provide visual evidence that $W_{t+u} - W_t \perp W_s$ for $u \ge 0$ and $s < t$. We'll let $t = 0.3$, $u=0.1$, and $s = 0.2$.


```{r}
results <- draw_wiener_t(1e5, (2:4)/10, 1, 1e3)
```

```{r}
#| code-fold: true
#| label: fig-plot595
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: "100,000 realized values of (W_4 - W_3) plotted against the corresponding 100,000 realized values of W_2."
#| code-summary: "Show code which generates figure"
#| warning: false

tibble(
  x = results$value[results$time == 0.4] - results$value[results$time == 0.3],
  y = results$value[results$time == 0.2]
) %>% 
  ggplot(aes(x, y)) +
  theme_minimal() +
  geom_point(size = 0.1) +
  labs(x = "W_4 - W_3", y = "W_2")
```


From this figure, the two random variables certainly appear to be independent. 

:::

       The Wiener process is just one instance of a stochastic process that "looks like" a normal distribution at specific points $t\in T$. These types of processes are so special that they are simply known as Gaussian.

:::{#def-}
Let $\{X_t\}_{t\in T}$ be a stochastic process over the state space $\R$. The process is a [***Gaussian process***]{style="color:red"} if for any finite set of indices $t_1,\ldots,t_k\in T$, the random vector $\X = (X_{t_1},\ldots, X_{t_k})$ is distributed according to a multivariate normal distribution. 
:::

       The defining properties of the Wiener process immediately establish it as a Gaussian process. To find the explicit normal distribution referenced in the definition of a Gaussian process we need to calculate the covariance of $W_t$ and $W_s$.
\begin{align*}
\cov{W_t,W_s} &= \begin{cases} \cov{W_t-0, W_s} & t > s\\ \cov{W_t, W_s + 0} & s\le t \end{cases} \\&= \begin{cases} \cov{W_t-W_s+W_s, W_s} & t > s\\ \cov{W_t, W_s + W_t- W_t} & s\le t\end{cases}\\
&= \begin{cases} \cov{W_t-W_s, W_s}+\cov{W_s,W_s} & t > s\\ \cov{W_t, W_s-W_t} + \cov{W_t,W_t}& s\le t \end{cases}\\
& = \begin{cases} 0 + \var{W_s} & t > s\\ 0 + \var{W_t}& s\le t \end{cases} & (\text{independent increments})\\
& = \begin{cases} s & t > s\\ t& s\le t \end{cases}\\
& = \min\{s,t\}
\end{align*}
Therefore, for any $\mathbf W = (W_{t_1},\ldots, W_{t_k})$ where $t_1,\ldots,t_k \in [0,T]$ we have 
\begin{align*}
\mathbf W &\sim N(\zer,\Sig),\\
\Sig_{j\ell}&=\begin{cases}t_j & j = \ell \\ \min\{t_j,t_{\ell}\} & \text{otherwise} \end{cases}.
\end{align*}

Let's look at another example of a Gaussian process by tweaking the Wiener process. 

:::{#exm-}

## Brownian Bridge

A process closely related to the Wiener Process is a Brownian bridge. For all intents and purposes, a Brownian bridge is a Wiener-like process which "bridges" two points $a,b\in \R$ on on the interval $[0,T]$. The simplest form of the Brownian bridge has $a=b=0$, and is defined via conditioning as 
$$ B(t)  = (W(t) \mid W(T) = 0)$$ for $t\in [0,T]$. This is often called the "standard" Brownian bridge, and often has $T=1$. We can construct this process by defining $B(t)$ as 
$$B(t) = W(t) - \frac{t}{T}W(T).$$ To bridge a general choice of $a,b\in\R$, we'll start by connecting the two points with a straight line parameterized by $t\in[0,T]$, $(1-t/T)a+(t/T)b$. To modify this line, we'll add the standard Brownian bridge $B(t)$.

\begin{align*}
B(t\mid a,b) &= \left(1-\frac{t}{T}\right)a + \frac{t}{T}b + B(t)\\
& = \left(1-\frac{t}{T}\right)a + \frac{t}{T}b + W(t) - \frac{t}{T}W(T)
\end{align*}


```{r}
brownian_bridge <- function(N, T_max = 1, a = 0, b = 0, s){
  # Start by drawing standard Wiener process on [0,T]
  output <- wiener(N, T_max, 1) %>%
    mutate(
      # define B(t) using W(t)
      value = (1 - time/T_max)*a + (time/T_max)*b + value - (time/T_max)*last(value, order_by = time),
      iter_num = s
    )
  return(output)
}


draw_brownian_bridge <- function(n, N, T_max = 1, a = 0, b = 0){
  output <- 1:n %>%
    map(brownian_bridge, N = N, T_max = T_max, a = a, b = b) %>%
    bind_rows()
  return(output)
}

results <- draw_brownian_bridge(7e3, 1e4, T_max = 2, a = 1, b = 3)
```

```{r}
#| code-fold: true
#| label: fig-plot5952
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: "100 realizations of the standard Brownian bridge"
#| code-summary: "Show code which generates figure"
#| warning: false 
results %>% 
  filter(iter_num <= 100) %>% 
  ggplot(aes(time, value)) + 
  geom_line(aes(group = iter_num), linewidth = 0.2, alpha = 0.4) +
  theme_minimal()
```

Let's consider the random variable given by the Brownian bridge takes on at a specific point in time $t\in[t_1,t_2]$ which we will write as $B_{t\mid a,b, t_1, t_2}$. We can derive the distribution of this random variable using properties of the normal distribution and $W_t\sim N(0,t)$.

\begin{align*}
B_{t\mid a,b}& = \left(1-\frac{t}{T}\right)a + \frac{t}{T}b + W(t) - \frac{t}{T}W(T) \\
& \sim \left(1-\frac{t}{T}\right)a + \frac{t}{T}b + N(0,t) - \frac{t}{T}N(0,T)\\
& = \left(1-\frac{t}{T}\right)a + \frac{t}{T}b + N(0,t) - N(0,t^2/T)\\
& = \left(1-\frac{t}{T}\right)a + \frac{t}{T}b + N(0,t(T - t)/T)\\
& = N\left( \left(1-\frac{t}{T}\right)a + \frac{t}{T}b, \left(\frac{t(T-t)}{T}\right)\right)
\end{align*}

To calculate the covariance of the Brownian bridge at two points, we first need to calculate the covariance 

\begin{align*}
\cov{B_{t\mid a,b}, B_{s\mid a,b}} &= \cov{\left(1-\frac{t}{T}\right)a + \frac{t}{T}b + W_t - \frac{t}{T}W_T, \left(1-\frac{s}{T}\right)a + \frac{s}{T}b + W_s - \frac{s}{T}W_T}\\
& = \cov{W_t - \frac{t}{T}W_T, W_s - \frac{s}{T}W_T} \\ 
& = \cov{W_t, W_s} - \frac{s}{T}\cov{W_t, W_T} - \frac{t}{T}\cov{W_T, W_s} + \frac{st}{T^2}\cov{W_T, W_T}\\
& = \min\{t,s\} - \frac{s}{T}\min\{t, T\} - \frac{t}{T}\min\{s, T\} + \frac{st}{T^2}\var{W_T}\\
& = \min\{t,s\} - \frac{st}{T} - \frac{st}{T} - \frac{stT}{T^2}\\
& = \min\{t,s\} - \frac{st}{T}
\end{align*}

If we combine these two calculations, we can conclude that for any $t_1,\ldots,t_k\in [0,T]$ we have 
\begin{align*}
\mathbf W &\sim N(\boldsymbol{\mu},\Sig),\\
\boldsymbol{\mu}_j &= \left(1-\frac{t}{T}\right)a + \frac{t}{T}b,\\
\Sig_{j\ell}&=\begin{cases}\frac{t_j(T-t_j)}{T} & j = \ell \\ \min\{t_j,t_{\ell}\} - \frac{t_jt_\ell}{T} & \text{otherwise} \end{cases},
\end{align*}
making $B(t\mid a,b)$ a Gaussian process. The Brownian bridge is an important tool and can be generalized to arbitrary intervals $[t_1,t_2]\subseteq [0,T]$. For details see pages 82-86 of @glasserman2004monte. 

:::

       One last example of a stochastic process is especially important in econometrics and is much easier to think of as a random function than more traditional examples of stochastic processes which are typically given as an indexed collection of random variables. 

:::{#exm-rf}
## A Random Function

Imagine we observe an iid random sample $\X = (X_1,\ldots, X_n)$ where $X_i\iid N(\mu,\sigma^2)$, and we want to measure the distance a number $t\in\R$ is from the sample $\X$. One way to do this is with the Euclidean metric. If $\mathbf e$ is the unit vector of length $n$ then the distance between $\X$ and $t$ is 
$$ \norm{\X - t\cdot\mathbf e}_2 = \left(\textstyle\sum_{i=1}^n(X_i - t)^2\right)^{1/2}.$$ If we define $Q_t = \norm{\X - t\cdot\mathbf e}_2$, then $Q = \{Q_t\}_{t\in\R}$ is a stochastic process which maps $\R$ to functions in the space $\R^\R$ (the set of all single variable real functions). Let's look at 100 simulated values of the process $Q$ where $\mu = 0$ and $\sigma^2 = 1$.


```{r}
# define random function Q_t
obj <- function(sample, t){
  sqrt(sum((sample-t)^2))
}

# draw n values and calculate Q_t on a given domain
iter <- function(f, domain, n, dist, dist_params, s){
  X <- do.call(dist, append(n, dist_params))
  output <- tibble(
    t = domain, 
    iter_num = s,
    Q = map_dbl(domain, f, sample = X),
    sample_size = n
  )
  
  return(output)
}

# draw N realizations of Q_t on the domain
sim <- function(N, f, domain, n, dist, dist_params){
  output <- 1:N %>% 
    map(iter, f = f, domain = domain, n = n, dist = dist, dist_params = dist_params) %>% 
    bind_rows()
  
  return(output)
}


results <- sim(100, obj, seq(-1, 1, length = 1000), 100, rnorm, list(0,1))
```

```{r}
#| code-fold: true
#| label: fig-plot510
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: "100 realizations of an aggregated distance of a real number from a random sample calculated using the Euclidean metric."
#| code-summary: "Show code which generates figure"
#| warning: false 

results %>% 
  ggplot(aes(t, Q)) +
  geom_line(aes(group = iter_num), size = 0.5, alpha = 0.4) +
  theme_minimal() +
  labs(y = 'Objective Function', x = 't')
```


In each case the realized value of $Q$ is a parabola which happens to be continuously differentiable, so $Q:\R\to C^2(\R)$. We could also define $Q$ using the the norm $\norm{\cdot}_1$ (giving taxi-cab metric) or $\norm{\cdot}_\infty$ (giving the supremum metric).
\begin{align*}
\norm{\X - t\cdot\mathbf e}_1 &= \left(\textstyle\sum_{i=1}^n\abs{X_i - t}\right)\\
\norm{\X - t\cdot\mathbf e}_\infty & = \max_{i}\abs{X_i-t}
\end{align*}

In both these cases, the realized value of $Q$ will not be differentiable on all of $\R$ since $\abs{x_i - t}$ is not differentiable at the point $t=x_i$.


```{r}
obj <- function(sample,t){
  sum(abs(sample-t))
}

results <- sim(100, obj, seq(-1, 1, length = 1000), 100, rnorm, list(0,1))
```

```{r}
#| code-fold: true
#| label: fig-plot511
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: "100 realizations of an aggregated distance of a real number from a random sample calculated using the taxi-cab metric. While the realization of the stochastic process appears smooth, it has slight kinks at which it is not differentiable."
#| code-summary: "Show code which generates figure"
#| warning: false 
#| 
results %>% 
  ggplot(aes(t, Q)) +
  geom_line(aes(group = iter_num), size = 0.5, alpha = 0.4) +
  theme_minimal() +
  labs(y = 'Objective Function', x = 't')
```

```{r}
obj <- function(sample,t){
  max(abs(sample-t))
}

results <- sim(100, obj, seq(-1, 1, length = 1000), 100, rnorm, list(0,1))
```

```{r}
#| code-fold: true
#| label: fig-plot512
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: "100 realizations of an aggregated distance of a real number from a random sample calculated using the supremum metric."
#| code-summary: "Show code which generates figure"
#| warning: false 
#| 
results %>% 
  ggplot(aes(t, Q)) +
  geom_line(aes(group = iter_num), size = 0.5, alpha = 0.4) +
  theme_minimal() +
  labs(y = 'Objective Function', x = 't')
```


:::

## Convergence

Like any sequence of random variables, a sequence of stochastic processes can converge in a number of ways. A sequence of stochastic processes is just a probabilistic version of a sequence of functions. As such, we can retrofit concepts such as pointwise convergence, uniform convergence, and equicontinuity to apply to stochastic processes. The notation around stochastic processes is cumbersome to begin with, but introducing an index to sequence them makes things even messier. We'll stick to writing a stochastic process $\{X\}_{t\in T}$ as $X(t)$, and will denote a sequence of stochastic processes $\{X_t(t), X_2(t), \ldots\}$ as $X_n(t)$.

:::{#exm-rf2}
Return to @exm-rf where we defined
$$Q(t) = \left(\textstyle\sum_{i=1}^n(X_i - t)^2\right)^{1/2} $$ for $X_i\iid N(\mu,\sigma^2)$ and a sample size of $n$. If we let $n$ vary, we have a sequence $Q_n(t)$ indexed by $n$. We'll modify the process $Q_n(t)$ by dividing by $n$ and eliminating the square root, such that $Q_n(t)$ is the average distance between $t$ and $X_i$.
\begin{align*}
Q_1(t) &= \left((X_1 - t)^2\right)^{1/2} = X_1-t\\
Q_2(t) & = \frac{1}{2}\left((X_1 - t)^2 + (X_2 - t)^2\right)\\
&\vdots\\
Q_n(t) & = \frac{1}{n} \textstyle\sum_{i=1}^n(X_i - t)^2\\
 \vdots 
\end{align*}
The key question is how does the behavior of $Q_n(t)$ change as $n\to\infty$? Let's repeat the same simulation as we did in @exm-rf, but do so for increasingly large sample sizes $n$.


```{r}
obj <- function(sample, t, n){
  X <- sample[1:n]
  output <- sum((X-t)^2) / length(X)
  return(output)
}

# perform simulation over varying sample sizes given by vector n_vals
iter <- function(f, domain, sample, n){
  output <- tibble(
    t = domain,
    sample_size = n,
    Q = map_dbl(domain, f, sample = sample, n = n)
  )
  return(output)
}

sim <- function(n_vals, f, domain, n, dist, dist_params){
  X <- do.call(dist, append(n, dist_params))
  output <- n_vals %>% 
    map(iter, f = f, domain = domain, sample = X) %>% 
    bind_rows()
  return(output)
}

results <- sim(c(1:9, (1:9)*10, (1:9)*100), obj, seq(-3, 3, length = 1000), 1e3, rnorm, list(0,1))
```

```{r}
#| code-fold: true
#| label: fig-plot513
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: "100 realizations of the process at various sample sizes."
#| warning: false 
#| code-summary: "Show code which generates figure"
results %>%
  ggplot(aes(t, Q)) +
  geom_line(aes(group = sample_size)) +
  theme_minimal() +
  transition_states(sample_size) + 
  labs(y = 'Q', x = 't') +
  geom_function(fun = function(x){1 + x^2}, colour = "red") +
  labs(title = 'Sample Size: {closest_state}')
```


As the sample size increases, it appears that the stochastic process approaches the deterministic function $f(x) = x^2 + 1$.

:::

       Instead of focusing just on stochastic processes, we'll generalize the definition of convergence in probability to any random variable defined on a metric space.^[Sometimes the definition is given for separable metric spaces (also known as Polish spaces). This additional assumption gives some convenient topological properties.] 

:::{#def-}
Suppose $X_n$ is a sequence of random variables defined on a metric space $E$ (equipped with a metric $d:E\times E\to[0,\infty]$). The sequence $X_n$ [***converges in probability***]{style="color:red"} to a random variable $X$ (defined on the same metric space) if 
$$ \lim_{n\to\infty}\Pr\left(d(X,X_n) > \varepsilon \right) = 0.$$ 
:::


:::{#exm-}
If $X_n$ is a sequence of random variables defined on $\R$ equipped with the Euclidean metric $d(x,y) = \abs{x-y}$, then the generalized definition of convergence in probability reduces 
$$ \lim_{n\to\infty}\Pr\left(\abs{X-X_n} > \varepsilon \right) = 0.$$
:::

       This "new" definition of convergence in probability allows us to define convergence in probability of stochastic processes. If $X_n(t)$ is a sequence of stochastic processes which map the sample space $\mathcal X$ to the space of functions $S^T$, then we can define $X_n(t)\pto X$ as long as $S^T$ is a metric space. The most familiar such space of functions is the set of real-valued continuous functions on a closed interval $[a,b]$, $C([a,b])$. The standard metric on this space is the supremum/uniform metric defined as 
$$ d(f,g) = \norm{f-g}_\infty = \sup_{x\in[a,b]}\abs{f(x) - g(x)}.$$ If a sequence of (deterministic) $\{f_n\}$ in the space $C([a,b])$ converges with respect to the supremum metric, then the sequence of real functions converges uniformly (hence the alternate name of "uniform" metric). Uniform convergence is one of the most important concepts in real analysis because it plays nicely with operations like integration and differentiation (opposed to pointwise convergence). The probabilistic extension of uniform convergence of stochastic processes arises when considering convergence in probability with respect to the supremum metric. 

:::{#def-}
Suppose $X_n(t)$ is a sequence of stochastic processes defined on the set $C(E)$, where $E\subseteq \R$, equipped with the metric $d(f,g) = \norm{f-g}_\infty$. We say that $X_n$  [***converges in probability uniformly***]{style="color:red"} to a stochastic process $X$ (defined on the same metric space) if
$$ \lim_{n\to\infty}\Pr\left(\sup_{t\in E}\abs{X_n(t) - X(t)} > \varepsilon \right) = 0.$$ 
:::


       The definition of uniform convergence in probability may look intimidating, but it's nothing more then a special case of convergence in probability that arises from looking at a metric space comprised of functions. 

       One definition that is relevant to the convergence of stochastic processes is related to continuity. Recall that a function $f:X\to Y$, defined on two metric spaces with respective metrics $d_X$ and $d_Y,$ is continuous at a point $x_0\in X$ if for all $\varepsilon > 0$ there exists a $\delta_{x_0} > 0$ such that $$d_Y(f(x_0), f(x)) < \varepsilon $$ for all $x\in X$ satisfying $d_X(x_0,x) < \delta_{x_0}$. In the event that a function is continuous at every point in some subset $E\subseteq X$, then $f$ is continuous on $E$. It's important to realize that the value $\delta_{x_0}$ in the definition of continuity may depend on the point $x_0$, hence the subscript. Even if $f$ is continuous on $E$, $d_X(x_0',x) < \delta_{x_0}$ *does not* imply that $d_Y(f(x_0'), f(x)) < \varepsilon$ for some $x_0'\in E$. If we require that there is one "silver-bullet" $\delta$ that works for all points on $E$, then we say $f$ is uniformly continuous on $E$. In other words, for all $\varepsilon > 0$, there exists a single $\delta$ such that $$d_Y(f(x_0), f(x)) < \varepsilon$$ whenever $d_X(x_0,x) < \delta$ *for all* $x_0\in E$. This is a much stronger form of continuity called uniform continuity, and it can also be written as $$ \sup_{d(x,x_0) < \delta} d_Y(f(x_0), f(x)) < \varepsilon.$$ If $d_Y(f(x_0), f(x)) < \varepsilon$ for the "largest" such value of $d_Y(f(x_0), f(x))$ where $d(x,x_0) < \delta$, then it must hold for all $d_Y(f(x_0), f(x))$ satisfying $d(x,x_0) < \delta$.^[I'm using "largest" loosely here because it's possible that this value isn't attained which is why we use the supremum.]

       If we think about uniform continuity in the context of sequences of functions $\{f_n\}$ where $f_n:X\to Y$, we can introduce a form of continuity even stronger than uniform continuity. Suppose we have a sequence of uniformly continuous functions $\{f_n\}$. For each $f_n$, we have some $\delta_n$ such that $d_X(x_0, x) < \delta_n$ implies that $d_Y(f_n(x_0), f(x))<\varepsilon$ for all $x_0 \in E$. In general, for two functions $f_i$ and $f_j$ (which are in the sequence $\{f_n\}$), $\delta_i\neq\delta_j$. All the functions in the sequence may be uniformly continuous, but their uniform continuity is not related to other elements in anyway. What would happen if we required this though? In other words, what if we were to require that there is a single $\delta$ that is independent of $n$ that works across all $f_n$? This would be like uniform continuity on steroids, and in fact has a name. A group of functions $\mathcal F$ (in our case our interest is when $\mathcal F$ is a sequence of functions) is (uniformly) equicontinuous if for all $\varepsilon >0$ there exists a $\delta >0$ such that $d_X(x_0, x) < \delta$ implies $d_Y(f(x_0),f(x)) < \varepsilon$ *for all* $f\in \mathcal F$ and *all* $x_0\in E$. Another way of saying this is that 
\begin{align*}
\sup_{d(x,x_0) < \delta} d_Y(f(x_0), f(x)) &< \varepsilon & (\forall f\in \mathcal F)
\end{align*}
To be even more succinct, we can write this in terms of the choice of $f$ which gives the "largest" value of $\sup_{d(x,x_0) < \delta} d_Y(f(x_0), f(x))$, as this would imply it holds for all $f\in \mathcal F$.
\begin{align*}
\sup_{f\in\mathcal F}\sup_{d(x,x_0) < \delta} d_Y(f(x_0), f(x)) &< \varepsilon
\end{align*}

       We want to develop a probabilistic version of equicontinuity in the context of stochastic processes. To makes things easier, we'll only concern ourselves with processes defined on real continuous functions defined on $E\subseteq \mathbb R$.^[The definition is readily extended to a general stochastic process.] If our process $X_n(t)$ is indexed by $n$, then the probabilistic extension of uniform continuity would look like something like
$$\Pr\left(\sup_{\abs{s-t} < \delta} \abs{X_n(t)-X_n(s)} > \eta \right) < \varepsilon$$ for all $\eta,\varepsilon > 0$. To ensure this holds for all $X_n$, we take the supremum over $n$

$$\sup_{n}\Pr\left(\sup_{\abs{s-t} < \delta} \abs{X_n(t)-X_n(s)} > \eta \right) < \varepsilon.$$ Since we're considering the asymptotics of $\{X_n\}$ we'd like this inequality to hold as $n\to\infty$ which gives the condition 

$$\limsup_{n\to\infty}\Pr\left(\sup_{\abs{s-t} < \delta} \abs{X_n(t)-X_n(s)} > \eta \right) < \varepsilon.$$



:::{#def-}
Suppose $X_n(t)$ is a sequence of stochastic processes defined on the set $C(E)$, where $E\subseteq \R$. We say that this collection of stochastic processes is [***stochastically equicontinuous***]{style="color:red"} if 

$$\limsup_{n\to\infty}\Pr\left(\sup_{\abs{s-t} < \delta} \abs{X_n(t)-X_n(s)} > \eta \right) < \varepsilon.$$

:::

       Informally, stochastic equicontinuity ensures that if $t$ and $s$ are close, then the probability that $X_n(s)$ and $X_n(t)$ are close is very high as $n\to\infty$. As one might expect from the grotesque definition, proving a sequence of random functions exhibits stochastic equicontinuity can be quite tricky. @andrews1994empirical provides several conditions which imply stochastic continuity that are more readily verified. 

## Convergence in Distribution {#sec-dist}

A sequence of real-valued random variables $X_n$ converges in distribution to $X$ if $\lim_{n\to\infty}F_{n}(x) = F_X(x)$. This form of convergence is what the various forms of the CLT were concerned with, and it's also the type of convergence that underpins statistical inference by allowing us to derive the asymptotic distribution of test statistics. It stands to reason then that if convergence in distribution is so important, it should be generalized to stochastic processes. While it can be, it will look a bit different, and this is why you sometimes hear people refer to convergence in distribution as "weak convergence". This is due to the term "(cumulative) distribution function" being specific to real-valued random variables. It's worth digging a bit deeper on the measure theory behind this. 

       If $(\mathcal X, \mathcal F, P)$ is a probability space and $X:\mathcal X\to E$ is a random variable defined on the measurable space $(E, \mathcal E)$, then we can define a special measure on $(E, \mathcal E)$ using the probability measure $P:\mathcal F\to[0,1]$. We do this by composing $P$ and $X^{-1}$ to get what is called the "law" of $X$, written as 
$$\mathcal L(X) = P\circ X^{-1}.$$ Since $X$ is a random variable, $X^{-1}(A)\in \mathcal F$ for all $A\in \mathcal E$, so we know that $P$ is defined for all $X^{-1}(A)$, i.e $\mathcal L(X)$ is defined on all of $\mathcal E$. The law of $X$ evaluated at $A\in\mathcal E$, $P\circ X^{-1}(A)$, tells us "what is the probability that $X$ takes on a value in the set $A$?" We can define the law of $X$ *for any* random variable. In the event that $E = \R$ and $\mathcal E = \mathcal B(\R)$, making $X$ a real-valued random variable, then we can use the law of $X$ to define the familiar distribution function as follows:
$$ P\circ X^{-1}((-\infty,x]) = P(X\in (-\infty, x]) = P(X\le x) = F_X(x).$$
Generalizing convergence in distribution to random variables defined on arbitrary metric spaces really means defining convergence in laws (which is synonymous with "weak convergence"). The mathematical details of this are interesting, but fairly rigorous. @billingsley2008probability, which is one of the *de facto* references for probability theory, only scratches the surface of convergence in laws, instead treating it in it's own book (@billingsleycon). For this reason, it's better to just cover the basics. 

       The first step in defining weak convergence for random functions is with a useful results that gives several equivalent formulations for convergence in distribution for real variables. We'll provide only one of these equivalences.

:::{#lem-}

## Portmanteau Lemma

Suppose $X_n$ is a sequence of real-valued random variables. We have $X_n\dto X$  *if and only if* $\E{f(X_n)}\to\E{f(X)}$ for all bounded continuous functions $f$. 

:::


       The proof is not particularly important.^[It's actually almost immediate from the definition of integration with respect to a measure.] What is, is that we can retrofit this definition of weak convergence for random variables defined on any metric space. 

:::{#def-}

Suppose $X_n$ is a sequence of random variables defined on a metric space $E$. We say that $X_n$ [***converges weakly***]{style="color:red"} to a random variable $X$ if $\E{f(X_n)}\to\E{f(X)}$ for all bounded and continuous functions $f:E\to\R$. To be consistent with existing notation, this will be written as $X_n\dto X$. 

:::

       So now what? We have a definition of weak convergence we can apply to random functions in $C(E)$ for $E\subseteq \R$, but how do we even begin to apply this definition to show that $X_n \dto X$?! It turns out, if we use stochastic equicontinuity, we can break the problem into much smaller parts that we know how to solve. Recall that one of the easiest ways to understand the Wiener process $W$ was looking at the real-valued random variables $\{W(t_1), W(t_2), \ldots, W(t_k)\}$ at fixed times $t_1,t_2,\ldots,t_k$. What happens if this collection of real-valued random variables $(W(t_1), W(t_2),\ldots, W(t_k))$, which forms a random vector, converges in distribution? Could we then conclude that the entire process $W$ converges in distribution? *Maybe*! In typical real analysis fashion, we need to impose a property related to convergence and continuity to iron out some wacky edge cases that could mess things up. It just so happens that the property we need is stochastic equicontinuity. This gives rise to a useful characterization of weak convergence of random functions, the proof of which can be found in Chapter 7 of @billingsleycon.

:::{#thm-spconv}

Suppose $X_n(t)$ is a sequence of stochastic processes defined on the set $C(E)$, where $E\subseteq \R$, equipped with the metric $d(f,g) = \norm{f-g}_\infty$. We have $X_n\dto X$ *if and only if* the two following conditions holds:

1. For every finite set of points $t_1,\ldots, t_k\in E$ the random vector $(X_n(t_1),\ldots,X_n(t_k))$ converges in distribution to the random vector $(X(t_1),\ldots X(t_k))$.
$$ (X_n(t_1),\ldots,X_n(t_k))\dto (X(t_1),\ldots X(t_k))$$
2. There exists a finite partition of $E =\cup_{j=1}^J E_j$ such that $X_n$ is stochastically equicontinuous on $E_j$ for all $j=1,\ldots,J$.

:::


## Application: Empirical Process Theory

A great deal of the asymptotic theory dealing with stochastic processes was developed in the context of one *very* important stochastic process. This particular process happens to be one of the most important estimators in statistics, and is used to estimate the cumulative distribution function of a random variable.

:::{#def-}

Let $\mathcal F$ be the set of all distribution functions of a real-valued random variable defined on $\mathcal X$, and $(X_1,\ldots,X_n)$ be a set of iid real-valued random variables defined on $\mathcal X$.^[This is not at all a technical approach to defining this space. For a rigorous approach, look up "Skorokhod space".] If The [***empirical (cumulative) distribution function (ecdf)***]{style="color:red"} $\hat F:\mathcal X\to\mathcal F$ is an estimator defined as 
$$\hat F_X(x) = \frac{1}{n}\sum_{i=1}^nI[X_i \le x].$$ This estimator can be generalized to an iid sample of real-valued random vectors as follows:

$$ \hat F_{\X}(\x) = \frac{1}{n}\sum_{i=1}^nI[X_{i,k} \le x_k\ \forall k]$$

:::

       To estimate the probability of $\Pr(X\le x) = \hat F_X(x)$ at a given point $x\in\mathcal X$, the estimator $\hat F$ just counts the number of realized values of $X_i$ which are less than $X$. This proportion is often called the *empirical probability*, and is the sample-counterpart to the true probability.

:::{#exm-}
Let's draw a sample of $n=100$ from $X\iid N(0,1)$, calculate $\hat F_X(0)$, and then compare $F_X$ and $\hat F_X$ with a figure.


```{r}
# Define "function factory" to create estimator F_X(x) given X = sample
emp_cdf <- function(sample){
  n <- length(sample)
  output <- function(x){
    return(sum(sample < x) / n)
  }
  return(Vectorize(output))
}

F_hat <- emp_cdf(rnorm(100))
F_hat(0)
```

```{r}
#| code-fold: true
#| label: fig-plot5135
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: " iii"
#| warning: false 
#| code-summary: "Show code which generates figure"
tibble(
  x = seq(-4, 4, length = 1000)
) %>% 
  mutate(
    `Empirical Distribution` = F_hat(x),
    `Actual Distribution` = pnorm(x)
  ) %>% 
  pivot_longer(!x) %>% 
  ggplot(aes(x, value, color = name)) +
  geom_line() +
  theme_minimal() +
  labs(color = "", x = "x", y = "Pr(X < x)") +
  theme(legend.position = "bottom") +
  scale_color_manual(values = c("red", "black"))
```



:::

       The empirical distribution function is one of the most important estimators in statistics (right up there with sample mean), which is why there is an entire field of research dedicated to it (along with generalizations) called empirical process theory. Before introducing the highlights of empirical process theory, let's consider $\hat F$ evaluated at some fixed value $x_0\in \mathcal X$. This gives an estimator $\hat F_X(x_0):\mathcal X\to[0,1]$ which is fairly easy to analyze. 

:::{#thm-}

## Pointwise Properties of the ECDF

Suppose $\X = (X_1,\ldots, X_n)$ is an iid sequence of real-valued random variables defined on $\mathcal X$. *For a fixed* value $x_0\in \mathcal X$, the estimator $\hat F_X(x_0):\mathcal X\to[0,1]$ exhibits the following properties:

1. $\hat F_X(x_0)$ is an unbiased estimator for $F_X(x_0)$ and has a variance of $\frac{1}{n} F_X(x_0)(1-F_X(x_0)$;
2. $\hat F_X(x_0)$ is a consistent estimator for $F_X(x_0)$;
3. $\sqrt{n}\left[\hat F_X(x_0) - F_X(x_0)\right]\dto N(0, F_X(x_0)(1-F_X(x_0)))$.

:::

:::{.proof}

We have $I[X_i \le x] \iid \text{Bernoulli}(F_X(x_0))$, so the sum of these $n$ of these indicator functions is a binomial random variable which we will denote as $Y$.
$$Y=\sum_{i=1}^nI[X_i \le x] \sim \text{Ber}(n, F_X(x_0))$$
This makes the estimator $\hat F_X(x_0)= Y/n$ a scaled version of the binomial distribution, a fact which we can use to prove the claimed properties.

1. The estimator's unbiasedness follows from the expectation of the binomial random variable $Y$. $$\E{\hat F_X(x_0)} = \E{Y/n} = \frac{1}{n}\E{Y} = \frac{1}{n}\left[nF_X(x_0)\right]=F_X(x_0)$$ The variance follows from a similar calculation. $$\var{\hat F_X(x_0)} = \var{Y/n} = \frac{1}{n^2}\var{Y} = \frac{1}{n^2}\left[nF_X(x_0)(1-F_X(x_0))\right]=\frac{1}{n} F_X(x_0)(1-F_X(x_0).$$

2. The estimator $\hat F_X(x_0)$ is the sample average of idd random bernoulli variables $I[X_i \le x]$, so by the LLN we have 
$$\hat F_X(x_0) = \frac{1}{n}\sum_{i=1}^n I[X_i \le x] \pto \E{I[X_i \le x]} = F_X(x_0) .$$

3. One way to show this is by applying Slutsky's theorem to the normal approximation of the binomial distribution. 

\begin{align*}
& Y  \sim \text{Ber}(n, F_X(x_0))\\
\implies & Y\dto N(nF_X(x_0), F_X(x_0)(1-F_X(x_0))\\
\implies & \frac{1}{n}Y \dto N\left(F_X(x_0), \frac{1}{n}F_X(x_0)(1-F_X(x_0)\right)\\
\implies & \hat F_X(x_0) \dto  N\left(F_X(x_0), \frac{1}{n}F_X(x_0)(1-F_X(x_0)\right)\\
\implies & \sqrt{n}\left[\hat F_X(x_0) - F_X(x_0)\right]\dto N(0, F_X(x_0)(1-F_X(x_0)))
\end{align*}
Another way of showing this is directly applying the CLT to $\hat F_X(x_0)$, which is a sample average of Bernoulli random variables. 
:::

       One place we rely on $\hat F_X(x_0)$ being a "good" estimator is when performing Monte Carlo simulations.


:::{#exm-}

## Simulations and Hypothesis Tests

In @sec-testing we relied on Monte Carlo simulations to approximate the probability of committing a type I or type II error. This actually relies on the empirical CDF evaluated at a point! Suppose $H_0:\mu  \le \mu_0$ and $H_1: \mu > \mu_0$, where $X_i \iid N(0,1)$ for a sample size of $n=1000$, $\mu = 0$, and $\mu_0 = 0$. We can test this using the decision rule $\delta(\X) = I[T(\X) \ge 1.645]$ where the decision space is $\mathcal A = \{0,1\}$. We have 
$$ \Pr(\text{correct decision}) = \Pr(\delta(\X) = 0) = \Pr(T(\X) < 1.645) = F_T(1.645),$$ so we can estimate the probability we make the correct decision using $\hat F_T(1.645)$. If we simulate $N$ decisions (uppercase $N$ here, as $n$ is the sample size used to calculate our test statistic) then 
$$ \hat F_T(1.645) = \frac{1}{N}\sum_{k=1}^NI[T(\X) < 1.645] $$ for simulations $k=1,\ldots,N$. We will set $N = 1000$.


```{r}
T_stat <- function(X, mu_0){
  n <- length(X)
  output <- (mean(X)-mu_0)/(sd(X)/sqrt(n))
  return(output)
}

iter <- function(mu_0, n, dist, dist_params, t){
  X <- do.call(dist, append(n, dist_params))
  output <- tibble(
    iter_num = t, 
    T_stat = T_stat(X, mu_0)
  )
  return(output)
}



sim <- function(x, N, mu_0, n, dist, dist_params, s){
  output <- 1:N %>% 
    map(iter, mu_0 = mu_0, n = n, dist = dist, dist_params = dist_params) %>% 
    bind_rows() %>% 
    summarize(ecdf_x = mean(T_stat < x)) %>% 
    mutate(iter_num = s)
  
  return(output)
}

results <- sim(qt(0.95, 100-1), 1e3, 0, 100, rnorm, list(0,1), 1)
results$ecdf_x
```


Our estimate is close to the true value of $0.95$! Now let's repeat this $M=10,000$ times to get $10,000$ simulated values of $\hat F_T(1.645)$.^[The nesting of simulations may be a little confusing here. We're drawing $n$ realizations of $X_i$ to calculate one value of $T(\X)$. We then repeat this $N$ times to get $N$ values of $T(\X)$ to calculate one value of $\hat F_T(1.645)$, so we've drawn $n*N$ values of $X_i$. Now we want to repeat *all of this* $M$ times to get $M$ values of $\hat F_T(1.645)$, so we've drawn $n\times N\times M$ values of $X_i$!] 


```{r}
outer_sim <- function(M, x, N, mu_0, n, dist, dist_params){
 output <- 1:M %>%
   map(sim, x = x, N = N, mu_0 = mu_0, n = n, dist = dist, dist_params = dist_params) %>%
   bind_rows()
 return(output)
}

results <- outer_sim(1e4, qt(0.95, 100-1), 1e3, 0, 100, rnorm, list(0,1))
```


In theory, we have 
\begin{align*}
\E{\hat F_T(1.645)} &= F_T(1.645) = 0.95,\\
\var{\hat F_T(1.645)} & = \frac{1}{N}\cdot F_T(1.645)\cdot\left[1-F_T(1.645)\right] = \frac{0.95(1-0.95)}{1000} = 4.75\times 10^{-5},
\end{align*}
so we should see the sample mean and sample variance of our estimates come close to these values.


```{r}
results %>% 
  summarize(
    mean = mean(ecdf_x),
    variance = var(ecdf_x)
  )
```


Finally, we should see these estimates (approximately) follow a normal distribution.


```{r}
#| code-fold: true
#| label: fig-plot514
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: "Histogram of simulated values along with the asymptotic distribution given in Theorem 5.2."
#| warning: false 
#| code-summary: "Show code which generates figure"
results %>% 
  ggplot(aes(ecdf_x)) + 
  geom_histogram(aes(y = ..density..), binwidth = 2e-3, color = "black", fill = "white") +
  geom_function(fun = dnorm, args = list(mean = 0.95, sd = sqrt( (0.95)*(1-0.95)/1000 )), color = "red") +
  labs(x = "Empricial CDF evaluated at 1.645", y = "Probability Density") +
  theme_minimal()
```


:::


:::{#exm-}

### Simulations with Consistent Estimators 

In @exm-lln1 we actually used the empirical CDF evaluated at a point to show that $\bar X\pto \mu$, it was just hidden behind the vague wording "observed probability". Using the full definition of convergence in probability, the LLN states 
$$ \lim_{n\to\infty} \Pr\left(\abs{\bar X -\mu} > \varepsilon\right) = 0.$$ Let's fix $n = 100$ and $\varepsilon = 0.1$, and draw $n$ observations of $X\sim F_X$ to calculate $\bar X$. If we repeat this $N$ times we have $N$ realized values of $\bar X$, giving $N$ realized values of $\abs{\bar X -\mu} > \varepsilon$. We can now estimate the probability that $\Pr\left(\abs{\bar X -\mu} > \varepsilon\right)$.

\begin{align*}
\hat{\Pr}\left(\abs{\bar X -\mu} > \varepsilon\right) &= 1 - \hat{\Pr}\left(\abs{\bar X -\mu} \le \varepsilon\right) = 1 - \hat{F}_{\abs{\bar X-\mu}}(\varepsilon) = 1 - \frac{1}{N}\sum_{i = 1}^N I\left[\abs{\bar X -\mu} \le \varepsilon\right].
\end{align*}

Let's do this for $N = 1,000$ and $X\sim N(0,1)$. 


```{r}
draw_X_bar <- function(n, dist, dist_params, t){
  X <- do.call(dist, append(n, dist_params))
  output <- tibble(
    estimate = mean(X),
    iter_num = t,
    sample_size = n
  )
  return(output)
}

F_hat <- function(N, epsilon, mu, n, dist, dist_params){
  output <- 1:N %>%
    map(draw_X_bar, n = n, dist = dist, dist_params = dist_params) %>% 
    bind_rows() %>% 
    summarize(
      estimate = sum(abs(estimate - mu) <= epsilon) / N,
      sample_size = N,
      e = epsilon,
    )
  return(output)
}

F_hat(1e3, 0.1, 0, 100, rnorm, list(0,1))
```

How does this compare to the true value $\hat{F}_{\abs{\bar X-\mu}}(\varepsilon)$? We have 
\begin{align*}
\hat{F}_{\abs{\bar X-\mu}}(\varepsilon) & = \Pr(\abs{\bar X -\mu} \le \varepsilon)\\
& = \Pr(\bar X \in [\mu - \varepsilon, \mu + \varepsilon])
& = \Pr(\bar X \le \mu + \varepsilon) - \Pr(\bar X \le \mu - \varepsilon). 
\end{align*}
Since $\bar X \sim N(\mu, \sigma/\sqrt{n})$, this can be readily calculated.


```{r}
pnorm(0.1, 0, 1/10) - pnorm(-0.1, 0, 1/10)
```


Our estimate for these particular values of $n$ and $\varepsilon$ is spot-on. Going back to @exm-lln1, we repeat this process for increasingly  values of $n$ to show $\Pr\left(\abs{\bar X -\mu} > \varepsilon\right)$. This must hold for any $\varepsilon > 0$, so it's also helpful to confirm this trend holds for a handful of $\varepsilon$ values.
:::


       With the key properties of estimator $\hat F_X(x_0):\mathcal X\to[0,1]$, let's now shift gears and consider the empirical distribution function $\hat F:\mathcal X\to \mathcal F$ as an estimator of a random function in the space $\mathcal F$.


### Glivenko–Cantelli Theorem 

The first property we'll establish is consistency. Since the space of distribution functions $\mathcal F$ is comprised entirely by bounded functions, the *de facto* metric $d(F,G)=\norm{F-G}_\infty$ is well defined. By Definition 5.2, $\hat F_X\pto F_X$ if 
$$ \lim_{n\to\infty}\Pr\left(\norm{\hat F_X-F_X}_\infty > \varepsilon \right) = 0$$ for all $\varepsilon > 0$. In other words, we want to show that 
$$ \norm{\hat F_X-F_X}_\infty \pto 0 .$$ 


:::{#thm-}

## Glivenko–Cantelli

The empirical distribution function $\hat F_X:\mathcal X\to[0,1]$ satisfies 
$$ \norm{\hat F_X-F_X}_\infty \overset{as}{\to} 0 $$

:::

:::{.proof}
In the event that $X$ is a discrete random variable, then
:::

:::{#cor-}

The empirical distribution function $\hat F_X:\mathcal X\to[0,1]$ is a consistent estimator for $F_X$. 

:::


:::{#exm-}

To illustrate that $\hat F_X$ is a consistent estimator for $F_X$, the first thing we need to do is get a sense of what $\norm{\hat F_X-F_X}_\infty$ "looks like". For the entirety of this example, we'll just assume that $X_i\sim N(0,1)$. To calculate $\norm{\hat F_X-F_X}_\infty$, we need to find the supremum of $\abs{\hat F_X(x)-F_X(x)}$ over the set $\mathcal X = \R$. This is not only impossible because the supremum may not be attained, but also because we cannot compute an uncountably infinite number of values. Instead, we'll take a discretized version of the problem as an approximation. If we calculate $\norm{\hat F_X-F_X}_\infty$ at $J$ values $\{x_1,\ldots,x_J\}\subseteq \R$, then we can use the approximation 
$$\norm{\hat F_X-F_X}_\infty\approx \max_{x_1,\ldots,x_j} \abs{\hat F_X(x)-F_X(x)}$$ for a large value of $J$. To see this approximation in action, we'll draw $n=1,000$ values of $X_i$ to calculate $\hat F_X$ on the discrete domain of the $J=100,000$ evenly spaced points between $-4$ and $4$.^[Since $\Pr(X_i \notin[-4,4]) \approx 0.00006 $, our discrete approximation doesn't lose much accuracy from clipping the tails of the sample space.]   


```{r}
F_hat <- function(t, X){
  sum(X < t)/length(X)
} 

norm <- function(N, X){
  
  # given dist, define quantile function
  qdist <- function(x){
    rnorm %>% 
      substitute() %>% 
      paste() %>% 
      str_replace("r", "q") %>% 
      do.call(list(x))
  }
  
  # define upper and lower bound of domain using qdist 
  if(qdist(0) == -Inf){
    lower = qdist(1e-7)
  } else {
    lower = qdist(0)
  }
  if(qdist(1) == Inf){
    upper = qdist(1 - 1e-7)
  } else {
    upper = qdist(1)
  }
  
  # discretize domain into N points
  domain <- seq(lower, upper, length = N)

  df <- tibble(
    domain,
    F_hat = map_dbl(domain, F_hat, X = X),
    F_0 = pnorm(domain)
  ) %>%
    mutate(
      difference = F_hat - F_0,
      distance = abs(F_hat - F_0)
    )

  norm <- df %>%
    summarize(max(distance)) %>%
    as.numeric()

  attained_at <- df %>%
    filter(distance == max(distance)) %>%
    select(domain) %>%
    as.numeric()

  output <- list(
    "norm" = norm,
    "norm_attained_at" = attained_at,
    "data" = df
  )

  return(output)
}

results <- norm(1e6, rnorm(1e3))
results[1:2]
```


Now let's visualize what this looks like by comparing $\hat F_X$ and $F_X$, along with showing the values of $\abs{\hat F_X(x)-F_X(x)}$ that we took the maximum of. 


```{r}
#| code-fold: true
#| label: fig-plot515
#| fig-align: center
#| fig-asp: 1.2
#| fig-width: 8
#| fig-cap: "test"
#| code-summary: "Show code which generates figure"
plot1 <- results$data %>% 
  rename(
    `Empirical Distribution, F_hat` = F_hat,
    `Actual Distribution, F` = F_0
  ) %>% 
  select(-distance, -difference) %>% 
  pivot_longer(!domain) %>% 
  ggplot(aes(domain, value, color = name)) +
  geom_line() +
  theme_minimal() +
  labs(x = "t", y = "Pr(X < t)", color = "") +
  scale_color_manual(values = c("red", "black")) +
  theme(legend.position = "bottom")

plot2 <- results$data %>% 
  ggplot(aes(domain, difference)) +
  geom_line() +
  labs(x = "t", y = "F - F_hat") +
  theme_minimal()

plot3 <- results$data %>% 
  ggplot(aes(domain, distance)) +
  geom_line() +
  geom_segment(
    aes(x = results$norm_attained_at, y = 0, xend = results$norm_attained_at, yend = results$norm),
    color = "red",
    linetype = "dashed"
  ) +
  geom_segment(
    aes(x = results$norm_attained_at, y  =results$norm, xend = -4, yend = results$norm),
    color = "red",
    linetype = "dashed"
  ) + 
  labs(x = "t", y = "|F - F_hat|") +
  theme_minimal() 

ggarrange(plot1, plot2, plot3, ncol = 1)
```


We'll now repeat this process *many* times, letting sample size grow for each drawn sample. 


```{r}
#| warning: false
iter <- function(N, X, k){
  sample <- X[1:k]
  output <- tibble(
    norm_val = norm(N, sample)$norm,
    sample_size = k
  )
  return(output)
}

sim <- function(n_vals, N, dist, dist_params, s){
  X <- do.call(dist, append(max(n_vals), dist_params))
  output <- n_vals %>% 
    map(iter, N = N, X = X) %>% 
    bind_rows() %>% 
    mutate(iter_num = s)
  return(output)
}

outer_sim <- function(M, n_vals, N, dist, dist_params){
  output <- 1:M %>% 
    map(sim, n_vals = n_vals, N = N, dist = dist, dist_params = dist_params) %>% 
    bind_rows()
  return(output)
}

results <- outer_sim(1e4, seq(0, 1e3, length = 20), 1e2, rnorm, list(0,1))
```


If we plot the results of our simulation, we see that as $n\to\infty$, it appears that the proportion of lines "far away" from zero shrinks. In other words, the probability that $\norm{\hat F_X-F_X}_\infty >  \varepsilon$ shrinks as $n\to\infty$.


```{r}
#| code-fold: true
#| label: fig-plot516
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: "Approximate values of the norm of the difference between the empirical distribution and true distribution calculated for 10,000 simulations."
#| code-summary: "Show code which generates figure."
#| warning: false

results %>% 
  ggplot(aes(sample_size, norm_val)) +
  geom_line(aes(group = iter_num), linewidth = 0.1, alpha = 0.1) +
  labs(x = "Sample Size, n", y = "sup|F - F_hat|") +
  scale_y_log10() +
  theme_minimal() 
```


But we're not done yet! We can go a step further here by estimating the probability that $\norm{\hat F_X-F_X}_\infty >  \varepsilon$ by estimating the empirical distribution function of $\norm{\hat F_X-F_X}_\infty$ at the point $\varepsilon$! 

\begin{align*}
\widehat\Pr\left(\norm{\hat F_X-F_X}_\infty >  \varepsilon \right) &= 1 - \widehat\Pr\left(\norm{\hat F_X-F_X}_\infty \le  \varepsilon \right) = 1 - \hat F_{\norm{\hat F_X-F_X}_\infty}(\varepsilon)
\end{align*}

This amounts to finding the proportion of lines which fall outside the interval $[0,\varepsilon)$ for a particular sample size. Since this probability should approach zero for all $\varepsilon >0$, we'll estimate these probabilities at a handful of values of $\varepsilon$.


```{r}
#| code-fold: true
#| label: fig-plot517
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: "The empirical probability that the norm in question is \"far\" from zero shrinks as sample size grows."
#| code-summary: "Show code which generates figure."
#| message: false
#| warning: false

results %>% 
  expand_grid(e = (5:9)/100) %>% 
  group_by(sample_size, e) %>% 
  summarize(prob = mean(norm_val > e)) %>% 
  ggplot(aes(sample_size, prob, color = as.factor(e))) + 
  geom_line() +
  labs(color = "ε", y = "Pr(Norm > ε)", x = "Sample Size, n") +
  theme_minimal() +
  theme(legend.position = "bottom")
```


:::

### Donsker's Theorem 

@sec-dist explored what it meant for a stochastic process to converge in distribution. If the process $\hat F_X$ is so important, it stands to reason that people have established whether or not $\hat F_x$ converges weakly. It in fact does, and this is the content of Donsker's Theorem which is also known as the "functional" central limit theorem or the invariance principle. 

       To arrive at the result, let's start with the simple setting of $X\iid \text{Uni}(0,1)$ and apply @thm-spconv to $\sqrt{n}(\hat F_X - F_X)$. For ease we will denote this process as $s_n$ and simplify its expression. 
\begin{align*}
S_n(t) &= \sqrt{n}(\hat F_X(t) - F_X(t))\\
&= \sqrt{n}\left(\frac{1}{n}\sum_{i=1}^n 1[X_i\le t] - t\right) & (F_X(t) = t)\\
& =  \sqrt{n}\left(\frac{1}{n}\sum_{i=1}^n 1[X_i\le t] - \frac{1}{n}\sum_{i=1}^n t\right) & \left(\frac{1}{n}\sum_{i=1}^n t = t\right)\\
& = \frac{1}{\sqrt n}\sum_{i=1}^n\left(1[X_i\le t] - t\right)\\
& = \frac{1}{\sqrt n}\sum_{i=1}^nU_i(t) & (U_i(t) = 1[X_i\le t] - t).
\end{align*} 
Let $t_1,\ldots,t_k$ be an arbitrary set of points in $[0,1]$. We can verify that the first condition of @thm-spconv is met by applying the multivariate CLT to $(U_i(t_1),\ldots,U_i(t_k))$.^[We'll simply generalize the proof of Theorem 5.2] For any $t_\ell$ we have 
\begin{align*}
\E{U_i(t_\ell)} = \E{1[X_i\le t_\ell] - t_\ell} = \E{1[X_i\le t_\ell]} - t_\ell = \Pr(X_i\le t_\ell) - t_i = t_\ell - t_\ell = 0.
\end{align*}
For any two $t_\ell,t_j$ in our arbitrary set of points in $[0,1]$ we have 
\begin{align*}
\var{\begin{bmatrix}U_i(t_\ell)\\U_i(t_j) \end{bmatrix}} & = \E{\left(\begin{bmatrix}U_i(t_\ell)\\U_i(t_j) \end{bmatrix} -\E{\begin{bmatrix}U_i(t_\ell)\\U_i(t_j) \end{bmatrix}}\right)\left(\begin{bmatrix}U_i(t_\ell)\\U_i(t_j) \end{bmatrix} -\E{\begin{bmatrix}U_i(t_\ell)\\U_i(t_j) \end{bmatrix}}\right)'}\\
& = \E{\begin{bmatrix}U_i(t_\ell)\\U_i(t_j) \end{bmatrix}\begin{bmatrix}U_i(t_\ell)\\U_i(t_j) \end{bmatrix}'} & (\E{U_i(t)} = 0\ \forall t\in [0,1])\\
& = \begin{bmatrix} \E{U_i(t_\ell)^2} &\E{U_i(t_\ell)U_i(t_j)} \\ \E{U_i(t_\ell)U_i(t_j)}& \E{U_i(t_j)^2} \end{bmatrix}\\
\end{align*}




```{r}
emp_cdf <- function(sample, n){
  output <- function(x){
    return(sum(sample[1:n] < x) / n)
  }
  return(Vectorize(output))
}

iter <- function(sample, n, domain){
  output <- tibble(
    x = domain, 
    F_hat = emp_cdf(sample, n)(domain),
    n = n
  )
  return(output)
}

sim <- function(n_vals, domain, sample_size, dist, dist_params){
  X <- do.call(dist, append(sample_size, dist_params))
  output <- n_vals %>% 
    map(iter, sample = X, domain = domain) %>% 
    bind_rows() %>% 
    mutate(
      F_true = punif(x),
      scaled_centered = sqrt(n)*(F_hat - F_true)
    )
  return(output)
}

results <- sim(c(1:9, (1:9)*10, (1:9)*100, (1:9)*1e3, (1:10)*1e4), seq(0, 1, length = 1e3), 1e5, runif, list(0, 1))
```

```{r}
#| code-fold: true
#| label: fig-plot518
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: "10 "
#| code-summary: "Show code which generates figure."
#| message: false
#| warning: false


plot <- results  %>% 
  rename(
    `Actual Distribution` = F_true,
    `Empirical Distribution` = F_hat
  ) %>% 
  pivot_longer(!c(x,n)) %>% 
  mutate(group = ifelse(name == "scaled_centered", "Scaled Difference, √n(F_hat - F_0)", "Distribution")) %>% 
  ggplot(aes(x, value, color = name)) + 
  geom_line() + 
  facet_wrap(~group, ncol = 1, scales = "free") +
  theme_minimal() +
  transition_states(n) +
  labs(title = 'Sample Size: {closest_state}') +
  theme(legend.position = "none") + 
  scale_color_manual(values = c("red", "black", "black")) +
  labs(x = "", y = "", color = "")
```



### Hypothesis Testing 


```{r}
emp_cdf <- function(sample){
  n <- length(sample)
  output <- function(x){
    return(sum(sample < x) / n)
  }
  return(Vectorize(output))
}

iter <- function(domain, n, dist, dist_params, t){
  X <- do.call(dist, append(n, dist_params))
  output <- tibble(
    x = domain,
    value = abs(sqrt(n)*(emp_cdf(X)(domain) - punif(domain))),
    iter_num = t
  ) %>% 
    mutate(norm = max(value))
  return(output)
}

sim <- function(N, domain, n, dist, dist_params){
  output <- 1:N %>% 
    map(iter, domain = domain, n = n, dist = dist, dist_params = dist_params) %>% 
    bind_rows()
  return(output)
}

results <- sim(1e3, seq(0, 1, length = 1e3), 1e3, runif, list(0, 1))
```

```{r}
#| code-fold: true
#| label: fig-plot519
#| fig-align: center
#| fig-asp: 0.7
#| fig-width: 8
#| fig-cap: "10 "
#| code-summary: "Show code which generates figure."
#| message: false
#| warning: false

norms <- results %>% 
  select(
    iter_num, 
    value = norm
  ) %>% 
  mutate(
    x = 0,
    group = "norm"
  ) %>% 
  unique()

plot <- results %>% 
  ggplot(aes(x, value)) +
  geom_line(aes(group = iter_num), color = "red") +
  theme_minimal() +
  geom_point(data = norms, color = "red") +
  transition_states(iter_num, transition_length = 0) +
  shadow_mark(alpha = 0.2, size = 0.2, color = "black") +
  labs(title = 'Realizations Drawn: {closest_state}', x = "x", y = "|√n(F_hat - F_0)|") 

animate(plot, end_pause = 30)
```

```{r}
results %>% 
  select(
    iter_num,
    norm
  ) %>% 
  unique() %>% 
  ggplot(aes(norm)) +
  geom_histogram(aes(y = after_stat(density)), color = "black", fill = "white", bins = 15)  +
  theme_minimal()
```

```{r}
pkolg <- function(x){
  k <- 1e5
  if(x <= 0){
    return(0)
  } else {
    return(1-2*sum((-1)^((1:k)-1)*exp(-2*(1:k)^2*x^2)))
  }
}
```

```{r}
x <- rnorm(100)
n <- length(x)
x2 <- pnorm(sort(x)) - (0 : (n-1)) / n
D <- max(c(x2, 1/n - x2))
ks.test(x, "pnorm")


D
1 - pkolg(sqrt(n)*D)

kolg_test <- function(x, dist){
  
}
```



https://github.com/SurajGupta/r-source/blob/master/src/library/stats/src/ks.c

https://github.com/SurajGupta/r-source/blob/master/src/library/stats/R/ks.test.R

\begin{align*}
F_D(x) &= 1 - 2\sum_{k=1}^\infty(-1)^{k-1}\exp(-2k^2x^2)\\
f_D(x) & = 8\sum_{k=1}^\infty(-1)^{k-1}xk^2\exp(-2k^2x^2)
\end{align*}

## Further Reading



@newey1991uniform, @hansen2022probability chapter 18, @billingsleycon, @billingsley2008probability, @pollard2012convergence, @durbin1973distribution
