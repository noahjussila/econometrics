<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Advanced Econometrics with Examples - 4&nbsp; Exponential Families</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./stochastic.html" rel="next">
<link href="./testing.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./estimators.html">Statistical Theory</a></li><li class="breadcrumb-item"><a href="./exp_fam.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exponential Families</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Advanced Econometrics with Examples</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preliminaries</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Statistical Theory</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimators.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Finite Sample Properties of Estimators</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./asymptotics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Asymptotic Properties of Estimators</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exp_fam.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exponential Families</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stochastic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./adv_asymptotics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Advanced Asymptotics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Linear Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">The Classical Linear Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./endog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Endogeniety I: IV and 2SLS</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gls.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Generalized Least Squares</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./simul.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Endogeniety II: Simultaneity and Multiple Regressions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Estimation Frameworks</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./extremum.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Extremum Estimators</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gmm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">The Generalized Method of Moments</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mle.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Basic Microeconometrics and Time Series</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./binary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Binary Choice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./panel.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Panel Data I: The Basics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./time_series.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Basic Time Series</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nonpar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Nonparametrics I: Distribution and Density Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nonpar_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Nonparametrics II: Functional Forms</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./semipar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Semiparametrics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sieve.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Sieve Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./partial_id.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Partial Identification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./endog_nonlinear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Endogeniety III: Nonlinear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./weak_inst.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Endogeniety IV: Weak Instruments</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Microeconometrics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./discrete_choice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Discrete Choice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tobit.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Tobit</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./survival.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Survival Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./count.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Count Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mixed.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Mixed and Multilevel Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./GLM.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./panel2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Panel Methods II</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
 <span class="menu-text">Time Series</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./time_math.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Math Review</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
 <span class="menu-text">Causal Inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./causal1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Potential Outcomes Model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./matching.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Matching Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rdd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Regression Discontinuity Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./did.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Difference in Differences</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./LATE.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Non-Compliance and LATE</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hetero.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Heterogeneous Treatment Effects</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true">
 <span class="menu-text">More Statistical Theory</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayes1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Bayesian Estimation I: Theory</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayes2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Bayesian Estimation II: Computation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayes3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Bayesian Estimation III: Applications</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./modelselection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./robust.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Robust Statistics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="true">
 <span class="menu-text">Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./learning_theory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Statistical Learning Theory</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ML1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Supervised Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ML2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Dimmension Reduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ML3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Unsupervised Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ML4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Text Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ML5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">ML meets Causal Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#motivation-sufficiency" id="toc-motivation-sufficiency" class="nav-link active" data-scroll-target="#motivation-sufficiency"><span class="header-section-number">4.1</span> Motivation – Sufficiency</a></li>
  <li><a href="#exponential-families" id="toc-exponential-families" class="nav-link" data-scroll-target="#exponential-families"><span class="header-section-number">4.2</span> Exponential Families</a></li>
  <li><a href="#properties" id="toc-properties" class="nav-link" data-scroll-target="#properties"><span class="header-section-number">4.3</span> Properties</a></li>
  <li><a href="#entropy-and-the-maximum-entropy-principle" id="toc-entropy-and-the-maximum-entropy-principle" class="nav-link" data-scroll-target="#entropy-and-the-maximum-entropy-principle"><span class="header-section-number">4.4</span> Entropy and the Maximum Entropy Principle</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">4.5</span> Further reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./estimators.html">Statistical Theory</a></li><li class="breadcrumb-item"><a href="./exp_fam.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exponential Families</span></a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Exponential Families</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This section is not <em>essential</em> to understanding econometrics, but it will provide some neat context later on. Many problems in statistics become much simpler if are data is generated from a special type of distribution. We already saw this in <a href="#sec-testing" class="quarto-xref"><span class="quarto-unresolved-ref">sec-testing</span></a> when talking about UMP tests and the MLR property. It turns out there is a general class of probability distributions which not only satisfy the MLR property, but also satisfy many other convenient properties which will make our lives easier later on. This section will introduce this class of distributions which are known as the exponential family of distributions. Right from the get go, a rather large disclaimer is in order. The only time we will be able to reap the benefits of distributions in the exponential family is when we assume that our model <span class="math inline">\(\mathcal P\)</span> is a parametric model which is also regular (each element <span class="math inline">\(P_\boldsymbol{\theta}\in \mathcal P\)</span> corresponds to a unique distribution <span class="math inline">\(f_{\mathbf{X}}(\mathbf{x}\mid \boldsymbol{\theta})\)</span>).<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Many common models in econometrics will not fall into this category as it requires specifying a distribution of some unobservable quantity. This is why exponential families are usually emphasized more in the setting of statistics, particularly when covering standard/classical topics.</p>
<section id="motivation-sufficiency" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="motivation-sufficiency"><span class="header-section-number">4.1</span> Motivation – Sufficiency</h2>
<p>In a sense, a statistic <span class="math inline">\(T:\mathcal X \to\mathcal T\)</span> (where <span class="math inline">\(\mathcal T\)</span> is almost always <span class="math inline">\(\mathbb R^k\)</span>) is a way of boiling down information about a sample <span class="math inline">\(\mathbf{x}\)</span> to a single value <span class="math inline">\(T(\mathbf{x})\)</span>, where <span class="math inline">\(\mathbf{X}\sim P_\boldsymbol{\theta}\)</span> for some regular parametric model <span class="math inline">\(\mathcal P\)</span>. We then use the information “captured” in the statistic <span class="math inline">\(T(\mathbf{x})\)</span> to estimate <span class="math inline">\(\boldsymbol{\theta}\)</span>. Once we have the value <span class="math inline">\(T(\mathbf{x})\)</span>, does the particular value of <span class="math inline">\(\mathbf{x}\)</span> matter? Is it possible that we can just ignore <span class="math inline">\(\mathbf{x}\)</span> if <span class="math inline">\(T(\mathbf{x})\)</span> encompasses all the relevant information provided by our sample? Here are two examples that show that this may, or may not, be the case.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.1</strong></span> Suppose <span class="math inline">\(X_i \overset{iid}{\sim}\text{Ber}(p)\)</span>. If we want to estimate <span class="math inline">\(p\)</span>, the natural estimator is <span class="math inline">\(\hat p(\mathbf{X}) = \sum_{i=1}^n X_i / n\)</span> when we observe a sample <span class="math inline">\(\mathbf{X}= (X_1,\ldots, X_n)\)</span>. Let’s just focus on the part of this function that depends on <span class="math inline">\(\mathbf{X}\)</span>, that being the statistic <span class="math inline">\(T(\mathbf{X}) = \sum_{i=1}^n X_i\)</span> which records the number of successes over <span class="math inline">\(n\)</span> Bernoulli trials. We can think of our estimation process as follows: we observe <span class="math inline">\(\mathbf{x}\)</span>, we calculate <span class="math inline">\(T(\mathbf{x})\)</span>, we calculate our estimate <span class="math inline">\(\hat p\)</span> using only the value <span class="math inline">\(T(\mathbf{x})\)</span>. Do we lose anything by only using <span class="math inline">\(T(x)\)</span>? If this were the case, then what would that look like mathematically? For some insight, let’s look at the distribution of our sample <span class="math inline">\(f_{\mathbf{X}}(\mathbf{x}\mid p)\)</span>. If <span class="math inline">\(f_X(x_i\mid p)\)</span> is the distribution associated with <span class="math inline">\(\text{Ber}(p)\)</span>, then <span class="math display">\[f_{\mathbf{X}}(\mathbf{x}\mid p) = \prod_{i=1}^nf_X(x_i\mid p) = \prod_{i=1}^np^{x_1}(1-p)^{1-x_i} = p^{\sum_{i=1}^n x_i}(1-p)^{n-\sum_{i=1}^n x_i} = p^{T(\mathbf{x})}(1-p)^{n-T(\mathbf{x})}.\]</span> Substituting <span class="math inline">\(T(\mathbf{x})\)</span> into this distribution illuminates a striking feature – <span class="math inline">\(f_{\mathbf{X}}(\mathbf{x}\mid p)\)</span> depends on <span class="math inline">\(\mathbf{x}\)</span> only through the statistic <span class="math inline">\(T(\mathbf{x})\)</span>. In other words, if we are estimating <span class="math inline">\(\hat p\)</span> (or rather estimating <span class="math inline">\(f_{\mathbf{X}}(\mathbf{x}\mid p)\)</span> vicariously through <span class="math inline">\(\hat p\)</span>), then we only need to know the value of our statistic <span class="math inline">\(T(\mathbf{x})\)</span>. The statistic contains all the relevant information needed for estimation. Another way to think about this is with an experiment. Suppose an unfair coin lands on heads with probability <span class="math inline">\(p\)</span>, and <span class="math inline">\(\mathbf{X}= (X_1,\ldots X_n)\)</span> records the number of heads observed (<span class="math inline">\(X_i = 1\)</span> means the <span class="math inline">\(i\)</span>th flip is heads). If you are going to estimate <span class="math inline">\(p\)</span> with <span class="math inline">\(\hat p(\mathbf{X}) = \sum_{i=1}^n X_i / n\)</span>, does it matter the order in which the coins landed on heads? If <span class="math inline">\(n = 2\)</span>, is there a difference between observing <span class="math inline">\(\mathbf{x}= (1,0)\)</span> and <span class="math inline">\(\mathbf{x}' = (0,1)\)</span>? No – the only information you really care about is the fact that the coin landed on heads once, i.e <span class="math inline">\(T(\mathbf{x}) = T(\mathbf{x}') = 1\)</span>. In an effort to beat a dead horse, let <span class="math inline">\(n = 10\)</span>, and <span class="math inline">\(T(\mathbf{x}) = 7\)</span> (the coin lands on heads 7 times). There are <span class="math inline">\(3,628,800\)</span> possible permutations where we get <span class="math inline">\(7\)</span> heads, all of which will give the same estimate of <span class="math inline">\(\hat p(\mathbf{x}) = 0.7\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">7</span>), <span class="fu">rep</span>(<span class="dv">0</span>, n <span class="sc">-</span> <span class="dv">7</span>))</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>samples <span class="ot">&lt;-</span> <span class="fu">permn</span>(x)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">#write estimator s.t we specify which sample we want to use from the permutations</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="cf">function</span>(i){</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> samples[[i]]</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>(x)<span class="sc">/</span>n</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>estimates <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(samples), p_hat)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">#What % of our estimates are 0.7?</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(estimates <span class="sc">==</span> <span class="fl">0.7</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1</code></pre>
</div>
</div>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.2</strong></span> Suppose <span class="math inline">\(X_i \overset{iid}{\sim}N(\mu,\sigma^2)\)</span> for a known <span class="math inline">\(\sigma^2\)</span>. We could estimate using the median <span class="math inline">\(\hat \mu(\mathbf{X})\)</span>, <span class="math display">\[\hat \mu(\mathbf{X}) = \begin{cases}X_{(n+1)/2} &amp; n\text{ odd}\\ \frac{X_{n/2} + X_{n/2 + 1}}{2} &amp; n\text{ even} \end{cases}.\]</span> In this case, is <span class="math inline">\(\hat \mu(\mathbf{X})\)</span> calculated via a statistic which encapsulates all the relevant information about <span class="math inline">\(\mathbf{x}\)</span>? Heuristically, it seems like this is not the case. The only real statistic that <span class="math inline">\(\hat\mu(\mathbf{X})\)</span> is a function of is itself, so let <span class="math inline">\(T(\mathbf{X}) = \hat\mu(\mathbf{X})\)</span>. Suppose we observe <span class="math inline">\(\mathbf{x}= (-100, 0, 0.1)\)</span> and <span class="math inline">\(\mathbf{x}' = (-0.1,0, 100)\)</span>. For both of these observations we have <span class="math inline">\(T(\mathbf{X}) = \hat \mu(\mathbf{x}) = 0\)</span>, but discarding <span class="math inline">\(\mathbf{x}'\)</span> and <span class="math inline">\(\mathbf{x}\)</span> doesn’t seem like the best idea here, because they have drastically different sample means, numbers that seem especially relevant here. Admittedly, this argument lacks the rigor of the previous one, but it hopefully illustrates that not all estimators can be calculated with some magic statistic that perfectly captures an observation <span class="math inline">\(\mathbf{x}\)</span>.</p>
</div>
<p>To formalize this property of optimal data reduction, we will consider the distribution <span class="math inline">\(f_{\mathbf{X}}(\mathbf{x}\mid \boldsymbol{\theta})\)</span> like we did in the first example. The definition is due to <span class="citation" data-cites="fisher1922mathematical">Fisher (<a href="#ref-fisher1922mathematical" role="doc-biblioref">1922</a>)</span>.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.1</strong></span> Let <span class="math inline">\(\mathbf{X}\sim P_\boldsymbol{\theta}\)</span> for some <span class="math inline">\(P_\boldsymbol{\theta}\in \mathcal P\)</span>, where <span class="math inline">\(\mathcal P\)</span> is a regular parametric model. A statistic <span class="math inline">\(T(\mathbf{X})\)</span> is <span style="color:red"><strong><em>sufficient</em></strong></span> for <span class="math inline">\(P_\boldsymbol{\theta}\in \mathcal P\)</span> (or for <span class="math inline">\(\boldsymbol{\theta}\)</span>), if <span class="math inline">\(f_{\mathbf{X}}(\mathbf{x}\mid T(\mathbf{x}), \boldsymbol{\theta})\)</span> is not a function of <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
</div>
<p>Sufficiency can seem a little abstract at first, but there are a few different ways to think about it that may make it a bit clearer. One way was already highlighted by the Bernoulli trials example. Sufficiency means that if we have <span class="math inline">\(T(\mathbf{x}) = T(\mathbf{x}')\)</span> for any two observed samples <span class="math inline">\(\mathbf{x},\mathbf{x}' \in \mathcal X\)</span>, then <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{x}'\)</span> provide us the same amount of information about <span class="math inline">\(\boldsymbol{\theta}\)</span>. Another way of thinking about sufficiency is via a thought experiment involving two statisticians. Suppose Statistician A and Statistician B want to estimate <span class="math inline">\(\boldsymbol{\theta}\)</span>. Statistician A has access to some random sample <span class="math inline">\(\mathbf{x}\)</span>, while Statistician B only knows <span class="math inline">\(T(\mathbf{x})\)</span>. If <span class="math inline">\(T\)</span> is a sufficient statistic, then Statistician B is at no disadvantage because he can generate his own random sample! He may not know <span class="math inline">\(\boldsymbol{\theta}\)</span>, but he knows <span class="math inline">\(T(\mathbf{x})\)</span>, and <span class="math inline">\(f_{\mathbf{X}}(\mathbf{x}\mid T(\mathbf{x}), \boldsymbol{\theta})\)</span> does not depend on <span class="math inline">\(\boldsymbol{\theta}\)</span>, so he can just simulate a random sample from <span class="math inline">\(f_{\mathbf{X}}(\mathbf{x}\mid T(\mathbf{x}))\)</span>.</p>
<p>Using the definition of sufficiency to verify a statistic has the property can be a bit cumbersome, so we usually do so using a famous theorem.</p>
<div id="thm-fac" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.1 (Fisher–Neyman factorization theorem)</strong></span> Let <span class="math inline">\(\mathbf{X}\sim P_\boldsymbol{\theta}\)</span> for some <span class="math inline">\(P_\boldsymbol{\theta}\in \mathcal P\)</span>, where <span class="math inline">\(\mathcal P\)</span> is a regular parametric model. The statistic <span class="math inline">\(T(\mathbf{X})\)</span> is sufficient <em>if and only if</em> there exist non-negative functions <span class="math inline">\(g:\mathcal T\times \Theta\to\mathbb R\)</span> and <span class="math inline">\(h:\mathcal X\to \mathbb R\)</span> such that <span class="math display">\[f_\mathbf{X}(\mathbf{x}\mid\boldsymbol{\theta}) = g(T(\mathbf{x}) \mid \boldsymbol{\theta})h(\mathbf{x}).\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span style="color:white">space</span></p>
<p><span class="math inline">\((\Longrightarrow)\)</span> Suppose <span class="math inline">\(T\)</span> is a sufficient statistic. The statistic <span class="math inline">\(T\)</span> is a function of <span class="math inline">\(\mathbf{x}\)</span>, so the joint density of <span class="math inline">\((\mathbf{X}, T(\mathbf{X})\)</span> is simply the density of <span class="math inline">\(\mathbf{X}\)</span>. <span class="math display">\[f_{\mathbf{X}}(\mathbf{x}\mid \boldsymbol{\theta}) = f_{\mathbf{X}, T(\mathbf{X})}(\mathbf{x}, T(\mathbf{x})\mid \boldsymbol{\theta})\]</span> BY properties of conditional variables, <span class="math display">\[ f_{\mathbf{X}}(\mathbf{x}\mid \boldsymbol{\theta}) = f_{\mathbf{X}, T(\mathbf{X})}(\mathbf{x}, T(\mathbf{x})\mid \boldsymbol{\theta}) = \underbrace{f_{X\mid T(\mathbf{X})}(\mathbf{x}\mid T(\mathbf{x}), \boldsymbol{\theta})}_{h(\mathbf{x})}\underbrace{f_{T(\mathbf{X})}(T(\mathbf{x})\mid\boldsymbol{\theta})}_{g(T(\mathbf{x}) \mid \boldsymbol{\theta})}.\]</span> We know that <span class="math inline">\(f_{X\mid T(\mathbf{X})}(\mathbf{x}\mid T(\mathbf{x}), \boldsymbol{\theta})\)</span> is a suitable candidate for <span class="math inline">\(h(\mathbf{x})\)</span>, because <span class="math inline">\(T\)</span> is sufficient, so <span class="math inline">\(f_{X\mid T(\mathbf{X})}(\mathbf{x}\mid T(\mathbf{x}), \boldsymbol{\theta})\)</span> is not a function of <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
<p><span class="math inline">\((\Longleftarrow)\)</span> Suppose <span class="math inline">\(f_\mathbf{X}(\mathbf{x}\mid\boldsymbol{\theta}) = g(T(\mathbf{x}) \mid \boldsymbol{\theta})h(\mathbf{x})\)</span>, and fix <span class="math inline">\(T(\mathbf{x}) = t\)</span>. By the definition of conditional expectation, <span class="math display">\[\begin{align*}
f_{\mathbf{X}\mid T(\mathbf{X})}(\mathbf{x}\mid t, \boldsymbol{\theta}) &amp;= \frac{f_{X, T(\mathbf{X})}(\mathbf{x},t\mid\boldsymbol{\theta})}{f_{T(\mathbf{x})}(t\mid \boldsymbol{\theta})}\\
&amp; = \frac{f_{X}(\mathbf{x}\mid\boldsymbol{\theta})}{f_{T(\mathbf{x})}(t\mid \boldsymbol{\theta})} &amp;(T(\mathbf{x})\text{ function of }\mathbf{x})\\
&amp; = \frac{g(t \mid \boldsymbol{\theta})h(\mathbf{x})}{f_{T(\mathbf{x})}(t\mid \boldsymbol{\theta})} &amp;(f_\mathbf{X}(\mathbf{x}\mid\boldsymbol{\theta}) = g(t \mid \boldsymbol{\theta})h(\mathbf{x}))\\
&amp; = \frac{g(t \mid \boldsymbol{\theta})h(\mathbf{x})}{f_{T(\mathbf{x})}(t\mid \boldsymbol{\theta})}
\end{align*}\]</span> We can write the denominator in terms of <span class="math inline">\(f_{T(\mathbf{x})}(t\mid \boldsymbol{\theta})\)</span> by integrating <span class="math inline">\(f_\mathbf{X}(\mathbf{x}\mid\boldsymbol{\theta})\)</span> over all <span class="math inline">\(\mathbf{x}\in \mathcal X\)</span> such that <span class="math inline">\(T(\mathbf{x}) = t\)</span> for some fixed <span class="math inline">\(t\)</span>: <span class="math display">\[f_{T(\mathbf{x})}(t\mid \boldsymbol{\theta}) = \int_{\{\mathbf{x}\mid T(\mathbf{x}) = t\}} f_\mathbf{X}(\mathbf{x}\mid\boldsymbol{\theta})\ d\mathbf{x}= \int_{\{\mathbf{x}\mid T(\mathbf{x}) = t\}} g(t\mid \boldsymbol{\theta})h(\mathbf{x})\ d\mathbf{x}=  g(t\mid \boldsymbol{\theta})\int_{\{\mathbf{x}\mid T(\mathbf{x}) = t\}}h(\mathbf{x})\ d\mathbf{x}\]</span> Therefore, <span class="math display">\[ f_{\mathbf{X}\mid T(\mathbf{X})}(\mathbf{x}\mid t, \boldsymbol{\theta}) = \frac{g(t \mid \boldsymbol{\theta})h(\mathbf{x})}{g(t\mid \boldsymbol{\theta})\int_{\{\mathbf{x}\mid T(\mathbf{x}) = t\}}h(\mathbf{x})\ d\mathbf{x}} = \frac{h(\mathbf{x})}{\int_{\{\mathbf{x}\mid T(\mathbf{x}) = t\}}h(\mathbf{x})\ d\mathbf{x}},\]</span> which is not a function of <span class="math inline">\(\boldsymbol{\theta}\)</span>. This makes <span class="math inline">\(T(\mathbf{x})\)</span> a sufficient statistic.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.3</strong></span> Suppose <span class="math inline">\(X_i \overset{iid}{\sim}N(\mu, \sigma^2)\)</span> for a known <span class="math inline">\(\sigma^2\)</span>. After some calculation, we can conclude <span class="math display">\[\begin{align*}
f_\mathbf{X}(\mathbf{x}\mid \mu)&amp; = \prod_{i=1}^n f_X(x_i \mid \mu )\\
&amp; =  \prod_{i=1}^n \frac{1}{\sigma\sqrt{2\pi}}\exp \left[-\frac{(x_i - \mu)^2}{2\sigma^2}\right]\\
&amp; = (2\pi\sigma^2)^{-n/2}\exp\left[-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i - \bar x)^2\right] \exp\left[-\frac{n}{2\sigma^2}(\mu - \bar x)^2\right]\\
&amp; =  \underbrace{(2\pi\sigma^2)^{-n/2}\exp\left[-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i - \bar x)^2\right]}_{h(\mathbf{x})} \underbrace{\exp\left[-\frac{n}{2\sigma^2}(\mu -  T(\mathbf{x}))^2\right]}_{g(T(\mathbf{x})\mid \mu)} &amp; (T(\mathbf{x}) = \bar x),
\end{align*}\]</span> so <span class="math inline">\(T(\mathbf{X}) = \bar X\)</span> is a sufficient statistic for <span class="math inline">\(\mu\)</span>.</p>
</div>
</section>
<section id="exponential-families" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="exponential-families"><span class="header-section-number">4.2</span> Exponential Families</h2>
<p>Whether <span class="math inline">\(T(\mathbf{X})\)</span> is sufficient for <span class="math inline">\(P_\boldsymbol{\theta}\)</span> depends entirely on the distribution <span class="math inline">\(f_{\mathbf{X}}(\mathbf{x}\mid \boldsymbol{\theta})\)</span>. Is it possible to describe the entire class of distributions which admit a sufficient statistic? Do they all take a similar form? As proved independently by <span class="citation" data-cites="pitman1936sufficient">Pitman (<a href="#ref-pitman1936sufficient" role="doc-biblioref">1936</a>)</span>, <span class="citation" data-cites="koopman1936distributions">Koopman (<a href="#ref-koopman1936distributions" role="doc-biblioref">1936</a>)</span>, and <span class="citation" data-cites="darmois1935lois">Darmois (<a href="#ref-darmois1935lois" role="doc-biblioref">1935</a>)</span>, the answer is yes (sort-of)! We will broaden our view a bit by considering a vector of statistics <span class="math inline">\(\mathbf T(\mathbf{X})\)</span>.</p>
<div id="thm-kpd" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.2 (Pitman–Koopman–Darmois theorem)</strong></span> Suppose <span class="math inline">\(X_i \overset{iid}{\sim}P_\boldsymbol{\theta}\)</span> where the support of <span class="math inline">\(f_{\mathbf{X}}(x_i\mid\boldsymbol{\theta})\)</span> does not depend on <span class="math inline">\(\boldsymbol{\theta}\)</span>. There exists a sufficient statistic <span class="math inline">\(\mathbf T:\mathcal X\to \mathbb R^k\)</span> such that <span class="math inline">\(k\)</span> is fixed for all sample sizes <span class="math inline">\(n\)</span> <em>if and only if</em> <span class="math inline">\(f_{\mathbf{X}}\)</span> can be written as <span class="math display">\[f_{\mathbf{X}}(\mathbf{x}\mid\boldsymbol{\theta})= h(\mathbf{x})\exp[\boldsymbol \eta(\boldsymbol{\theta})\cdot\mathbf T(\mathbf{x}) - A(\boldsymbol{\eta})]\]</span> for functions <span class="math inline">\(h:\mathcal X \to \mathbb R\)</span>, <span class="math inline">\(\boldsymbol \eta:\boldsymbol \Theta \to \mathbb R^k\)</span>, <span class="math inline">\(A:\mathcal X \to \mathbb R\)</span>.</p>
</div>
<p>Proving the sufficiency of this condition is a direct application of Theorem <a href="#thm-fac" class="quarto-xref">Theorem&nbsp;<span class="quarto-unresolved-ref">thm-fac</span></a>, as we can just let <span class="math inline">\(g(\mathbf T(\mathbf{x}) \mid \boldsymbol{\theta}) = \exp[\boldsymbol \eta(\boldsymbol{\theta})\cdot\mathbf T(\mathbf{x}) - A(\boldsymbol{\eta})]\)</span>. Proving this is a necessary condition is a bit more complicated and relies on the assumptions that the dimension of <span class="math inline">\(\mathbf T\)</span> is fixed, and that the support of <span class="math inline">\(f_{\mathbf{X}}(x_i\mid\boldsymbol{\theta})\)</span> is independent of <span class="math inline">\(\boldsymbol{\theta}\)</span>. The second condition should seem familiar, as it plays a crucial role in proving the Cramér–Rao lower bound holds (see <a href="#sec-est" class="quarto-xref"><span class="quarto-unresolved-ref">sec-est</span></a>). The distributions given by the Pitman–Koopman–Darmois theorem merit their own definition.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.2</strong></span> A regular parametric model <span class="math inline">\(\mathcal P\)</span> is an <span style="color:red"><strong><em>exponential family</em></strong></span> if <span class="math display">\[f_{\mathbf{X}}(\mathbf{x}\mid\boldsymbol{\theta})= h(\mathbf{x})\exp[\boldsymbol \eta(\boldsymbol{\theta})\cdot\mathbf T(\mathbf{x}) - A(\boldsymbol{\eta})].\]</span> We refer to <span class="math inline">\(\mathbf T(\mathbf{x})\)</span> as the <span style="color:red"><strong><em>sufficient statistic</em></strong></span>, <span class="math inline">\(\boldsymbol\eta(\boldsymbol{\theta})\)</span> as the <span style="color:red"><strong><em>natural parameter</em></strong></span>, and <span class="math inline">\(A(\boldsymbol{\eta})\)</span> as the <span style="color:red"><strong><em>cumulant function</em></strong></span>. In the event <span class="math inline">\(\boldsymbol\eta(\boldsymbol{\theta}) = \boldsymbol{\theta}\)</span>, the exponential family is in <span style="color:red"><strong><em>canonical form</em></strong></span>. If <span class="math inline">\(\boldsymbol\eta(\boldsymbol{\theta}) = \boldsymbol{\theta}\)</span> and <span class="math inline">\(\mathbf T(\mathbf{x}) = \mathbf{x}\)</span>, we say our model is a <span style="color:red"><strong><em>natural exponential family</em></strong></span>.</p>
</div>
<p>Sometimes, people will opt to write the cumulant function in terms of the parameter <span class="math inline">\(\boldsymbol{\theta}\)</span>, which is completely fine.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.4</strong></span> If <span class="math inline">\(X \overset{iid}{\sim}\text{Ber}(p)\)</span> where <span class="math inline">\(n=1\)</span>, then <span class="math inline">\(T(\mathbf{X}) = \sum_{i=1}^n X_i = X\)</span> is a sufficient statistic for <span class="math inline">\(p\)</span>, so <span class="math inline">\(f_{X}(x\mid p)\)</span> is an exponential family by the Pitman–Koopman–Darmois theorem. <span class="math display">\[\begin{align*}
f_{X}(x\mid p) &amp; = p^x(1-p)^{1-x}\\
&amp; = \exp[\log(p^x(1-p)^{1-x})]\\
&amp; = \exp[x\log(p) + (1-x)\log(1-p))]\\
&amp; = \exp[x(\log(p) - \log(1-p)) + \log(1-p)]\\
&amp; = 1\cdot \exp\left[x\log\left(\frac{p}{1-p}\right) + \log(1-p)\right]\\
h(x) &amp; = 1\\
T(x) &amp; = x\\
\eta(p) &amp; = \log\left(\frac{p}{1-p}\right)\\
A(\eta)&amp; = -\log(1-p)\\
&amp; =  \log\left(1 + \frac{p}{1-p}\right)\\
&amp; = \log\left[1 + \exp\left[\log\left(\frac{p}{1-p}\right)\right]\right]\\
&amp; = \log[1 + \exp(\eta)]
\end{align*}\]</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.5</strong></span> For <span class="math inline">\(X\sim N(\mu,\sigma^2)\)</span> where both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> are unknown,</p>
<p><span class="math display">\[\begin{align*}
f_X(x\mid \mu,\sigma^2) &amp;= \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{1}{2\sigma^2}(x-\mu)^2\right]\\
&amp; = \frac{\sigma^{-1}}{\sqrt{2\pi}}\exp\left[-\frac{1}{2\sigma^2}(x^2-2x\mu +\mu^2)\right]\\
&amp; = \frac{\exp[-\log(\sigma)]}{\sqrt{2\pi}}\exp\left[\frac{\mu}{\sigma^2} - \frac{1}{2\sigma^2}x^2 - \frac{1}{2\sigma^2}\mu^2\right]\\
&amp; = \frac{1}{\sqrt{2\pi}}\exp\left[\frac{\mu}{\sigma^2}x - \frac{1}{2\sigma^2}x^2 - \frac{1}{2\sigma^2}\mu^2-\log(\sigma)\right]\\
h(x) &amp; = \frac{1}{\sqrt{2\pi}}\\
\boldsymbol \eta(\mu, \sigma^2) &amp; = [\mu/\sigma^2, -1/2\sigma^2]'\\
\mathbf T(x) &amp; = [x, x^2]' \\
A(\boldsymbol{\eta}) &amp;= \frac{\mu^2}{2\sigma^2} + \log \sigma\\
&amp; = -\frac{\eta_1^2}{4\eta_2} - \frac{\log(-2\eta_2)}{2}
\end{align*}\]</span></p>
</div>
<p>Almost all the distributions we rely on happen to be exponential families. A select collection of these, along with their associated sample and parameter space, are:</p>
<table class="table">
<colgroup>
<col style="width: 25%">
<col style="width: 18%">
<col style="width: 27%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Distribution/Model</th>
<th><span class="math inline">\(\mathcal X\)</span></th>
<th><span class="math inline">\(\boldsymbol{\theta}\)</span></th>
<th><span class="math inline">\(\boldsymbol \Theta\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bernoulli Distribution</td>
<td><span class="math inline">\(\{0,1\}\)</span></td>
<td><span class="math inline">\(p\)</span></td>
<td><span class="math inline">\([0,1]\)</span></td>
</tr>
<tr class="even">
<td>Binomial Distribution (<span class="math inline">\(n\)</span> known)</td>
<td><span class="math inline">\(\{0,1,\ldots,n\}\)</span></td>
<td><span class="math inline">\(p\)</span></td>
<td><span class="math inline">\([0,1]\)</span></td>
</tr>
<tr class="odd">
<td>Negative Binomial Distribution (failures <span class="math inline">\(r\)</span> known)</td>
<td><span class="math inline">\(\mathbb N\)</span></td>
<td><span class="math inline">\(p\)</span></td>
<td><span class="math inline">\([0,1]\)</span></td>
</tr>
<tr class="even">
<td>Geometric Distribution</td>
<td><span class="math inline">\(\mathbb N\backslash\{0\}\)</span></td>
<td><span class="math inline">\(p\)</span></td>
<td><span class="math inline">\([0,1]\)</span></td>
</tr>
<tr class="odd">
<td>Exponential Distribution</td>
<td><span class="math inline">\([0,\infty)\)</span></td>
<td><span class="math inline">\(\lambda\)</span></td>
<td><span class="math inline">\(\mathbb R^+\)</span></td>
</tr>
<tr class="even">
<td>Poisson Distribution</td>
<td><span class="math inline">\(\mathbb N\)</span></td>
<td><span class="math inline">\(\lambda\)</span></td>
<td><span class="math inline">\(\mathbb R^+\)</span></td>
</tr>
<tr class="odd">
<td>Normal Distribution</td>
<td><span class="math inline">\(\mathbb R\)</span></td>
<td><span class="math inline">\((\mu,\sigma^2)\)</span></td>
<td><span class="math inline">\(\mathbb R\times \mathbb R^+\)</span></td>
</tr>
<tr class="even">
<td>Chi-squared Distribution</td>
<td><span class="math inline">\([0,\infty)\)</span></td>
<td><span class="math inline">\(k\)</span></td>
<td><span class="math inline">\(\mathbb N\)</span></td>
</tr>
<tr class="odd">
<td>Gamma Distribution</td>
<td><span class="math inline">\(\mathbb R^+\)</span></td>
<td><span class="math inline">\((\alpha,\beta)\)</span></td>
<td><span class="math inline">\(\mathbb R^+\times \mathbb R^+\)</span></td>
</tr>
<tr class="even">
<td>Beta Distribution</td>
<td><span class="math inline">\([0,1]\)</span></td>
<td><span class="math inline">\((\alpha,\beta)\)</span></td>
<td><span class="math inline">\(\mathbb R^+\times \mathbb R^+\)</span></td>
</tr>
<tr class="odd">
<td>Multivariate Normal Distribution</td>
<td><span class="math inline">\(\mathbb R^k\)</span></td>
<td><span class="math inline">\((\boldsymbol \mu, \boldsymbol \Sigma)\)</span></td>
<td><span class="math inline">\(\mathbb R^k\times (\mathbb R^+)^k \times \mathbb R^{k\times(k-1)}\)</span></td>
</tr>
<tr class="even">
<td>Multinomial Distribution (<span class="math inline">\(n\)</span> known)</td>
<td><span class="math inline">\(\{0,1,\ldots,n\}^k\)</span></td>
<td><span class="math inline">\(\mathbf p\)</span>, where <span class="math inline">\(\sum_{j=1}^k p_j = 1\)</span></td>
<td><span class="math inline">\(k-\)</span>simplex over <span class="math inline">\([0,1]\)</span></td>
</tr>
</tbody>
</table>
<p>Technically, listing some of these are redundant. The chi-squared distribution and exponential distribution both special cases of the gamma distribution. The Bernoulli distribution is a binomial distribution where <span class="math inline">\(n=1\)</span>. The The natural parameters, sufficient statistic, and cumulant function of each of these distributions are:</p>
<table class="table">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th>Distribution/Model</th>
<th><span class="math inline">\(\boldsymbol \eta(\boldsymbol{\theta})\)</span></th>
<th><span class="math inline">\(h(\mathbf{x})\)</span></th>
<th><span class="math inline">\(\mathbf T(\mathbf{x})\)</span></th>
<th><span class="math inline">\(A(\boldsymbol \eta)\)</span></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bernoulli Distribution</td>
<td><span class="math inline">\(\log[1/(1-p)]\)</span></td>
<td><span class="math inline">\(1\)</span></td>
<td><span class="math inline">\(x\)</span></td>
<td><span class="math inline">\(\log[1 + \exp(\eta)]\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>Binomial Distribution (<span class="math inline">\(n\)</span> known)</td>
<td><span class="math inline">\(\log[1/(1-p)]\)</span></td>
<td><span class="math inline">\(\binom{n}{x}\)</span></td>
<td><span class="math inline">\(x\)</span></td>
<td><span class="math inline">\(n\log[1 + \exp(\eta)]\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Negative Binomial Distribution (failures <span class="math inline">\(r\)</span> known)</td>
<td><span class="math inline">\(\log p\)</span></td>
<td><span class="math inline">\(\binom{x+r-1}{x}\)</span></td>
<td><span class="math inline">\(x\)</span></td>
<td><span class="math inline">\(-r\log[1-\exp(\eta)]\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>Geometric Distribution</td>
<td><span class="math inline">\(\log(1-p)\)</span></td>
<td><span class="math inline">\(1\)</span></td>
<td><span class="math inline">\(x\)</span></td>
<td><span class="math inline">\(\eta - \log[1-\exp(\eta)]\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Exponential Distribution</td>
<td><span class="math inline">\(-\lambda\)</span></td>
<td><span class="math inline">\(1\)</span></td>
<td><span class="math inline">\(x\)</span></td>
<td><span class="math inline">\(-\log(-\eta)\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>Poisson Distribution</td>
<td><span class="math inline">\(\log\lambda\)</span></td>
<td><span class="math inline">\(1/x!\)</span></td>
<td><span class="math inline">\(x\)</span></td>
<td><span class="math inline">\(\exp(\eta)\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Normal Distribution</td>
<td><span class="math inline">\([\mu/\sigma^2, -1/2\sigma^2]'\)</span></td>
<td><span class="math inline">\(1/\sqrt{2\pi}\)</span></td>
<td><span class="math inline">\([x,x^2]'\)</span></td>
<td><span class="math inline">\(-\eta_1^2/4\eta_2 - \log(-2\eta_2)/2\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>Chi-squared Distribution</td>
<td><span class="math inline">\(k/2-1\)</span></td>
<td><span class="math inline">\(\exp(-x/2)\)</span></td>
<td><span class="math inline">\(\log x\)</span></td>
<td><span class="math inline">\(\log[\Gamma(\eta+1)] + (\eta+1)\log2\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Gamma Distribution</td>
<td><span class="math inline">\([\alpha - 1, -\beta]'\)</span></td>
<td><span class="math inline">\(1\)</span></td>
<td><span class="math inline">\([\log x, x]'\)</span></td>
<td><span class="math inline">\(\log[\Gamma(\eta_1+1)] - (\eta_1+1)\log(-\eta_2)\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>Beta Distribution</td>
<td><span class="math inline">\([\alpha - 1,\beta - 1]\)</span></td>
<td><span class="math inline">\(1/[x(1-x)]\)</span></td>
<td><span class="math inline">\([\log x, \log(1-x)]'\)</span></td>
<td><span class="math inline">\(\log[\Gamma(\eta_1 + 1)] + \log[\Gamma(\eta_2 + 1)] - \log[\Gamma(\eta_1+\eta_2+1)]\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Multivariate Normal Distribution</td>
<td><span class="math inline">\([\boldsymbol \Sigma^{-1}\boldsymbol \mu, -\boldsymbol\Sigma^{-1}/2]'\)</span></td>
<td><span class="math inline">\((2\pi)^{-k/2}\)</span></td>
<td><span class="math inline">\([\mathbf{x},\mathbf{x}\mathbf{x}']\)</span></td>
<td><span class="math inline">\(-\frac{1}{4}\boldsymbol{\eta}_1'\boldsymbol{\eta}_2^{-1}\boldsymbol{\eta}_1 - \frac{1}{2}\log|-2\boldsymbol{\eta}_2|\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>Multinomial Distribution (<span class="math inline">\(n\)</span> known)</td>
<td><span class="math inline">\([\log(p_1/p_k),\ldots, \log(p_{k-1}/p_k),0]'\)</span></td>
<td><span class="math inline">\(\mathbf{x}\)</span></td>
<td><span class="math inline">\(\frac{n!}{\prod_{i=1}^kx_i!}\)</span></td>
<td><span class="math inline">\(n\log[1+\sum_{i=1}^{k-1}\exp(\eta_i)]\)</span></td>
<td></td>
</tr>
</tbody>
</table>
<p>It’s worth noting there are some really important distributions that are not exponential families, namely the student’s <span class="math inline">\(t-\)</span>distribution, the uniform distribution, and the <span class="math inline">\(F-\)</span>distribution. Interestingly, the <span class="math inline">\(F-\)</span>distribution is asymptotically equivalent to a <span class="math inline">\(\chi^2\)</span> distribution, and the <span class="math inline">\(t-\)</span>distribution is asymptotically equivalent to the standard normal distribution, so as <span class="math inline">\(n\to\infty\)</span>, these “become” exponential families.</p>
</section>
<section id="properties" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="properties"><span class="header-section-number">4.3</span> Properties</h2>
<p>Exponential families have a myriad of properties that make them easy to work with. First, let’s look into the cumulant function <span class="math inline">\(A(\boldsymbol{\eta})\)</span>. The role of <span class="math inline">\(A(\boldsymbol{\eta})\)</span> is to normalize the density <span class="math inline">\(f_{\mathbf{X}}(\mathbf{x}\mid\boldsymbol{\theta})\)</span> when we express it as an exponential family. Without getting into the proof behind the Pitman–Koopman–Darmois theorem, it would seem that the function <span class="math inline">\(h(\mathbf{x})\exp[\boldsymbol \eta(\boldsymbol{\theta})\cdot\mathbf T(\mathbf{x})]\)</span> would suffice for <span class="math inline">\(\mathbf T\)</span> to be a sufficient statistic for <span class="math inline">\(\boldsymbol{\theta}\)</span>, because we could just let <span class="math inline">\(g(\mathbf T(\mathbf{x})\mid \boldsymbol{\theta}) = \exp[\boldsymbol \eta(\boldsymbol{\theta})\cdot\mathbf T(\mathbf{x})]\)</span> and apply the Fisher–Neyman factorization theorem. The issue with this is that <span class="math inline">\(h(\mathbf{x})\exp[\boldsymbol \eta(\boldsymbol{\theta})\cdot\mathbf T(\mathbf{x})]\)</span> may not be a valid density function which integrates to <span class="math inline">\(1\)</span> over <span class="math inline">\(\mathcal X\)</span>. To ensure it is a valid density, we need to find some normalizing scalar <span class="math inline">\(A\)</span> which satisfies:</p>
<p><span class="math display">\[ \int_{\mathcal X}\frac{1}{A}\left[h(\mathbf{x})\exp[\boldsymbol \eta(\boldsymbol{\theta})\cdot\mathbf T(\mathbf{x})]\right]\ d\mathbf{x}= 1 \]</span> We could also take the scalar to be <span class="math inline">\(\exp \kappa\)</span>, giving <span class="math display">\[ \int_{\mathcal X}h(\mathbf{x})\exp[\boldsymbol \eta(\boldsymbol{\theta})\cdot\mathbf T(\mathbf{x})-A]\ d\mathbf{x}= 1.\]</span></p>
<p>If we solve for <span class="math inline">\(A\)</span>, we have</p>
<p><span class="math display">\[\begin{align*}
\implies &amp; \int_{\mathcal X}h(\mathbf{x})\exp[\boldsymbol \eta(\boldsymbol{\theta})\cdot\mathbf T(\mathbf{x})-A]\ d\mathbf{x}= 1\\
\implies &amp; \exp(-A)\int_{\mathcal X}h(\mathbf{x})\exp[\boldsymbol \eta(\boldsymbol{\theta})\cdot\mathbf T(\mathbf{x})]\ d\mathbf{x}= 1\\
\implies &amp; A =  \log\left(\int_{\mathcal X}h(\mathbf{x})\exp[\boldsymbol \eta(\boldsymbol{\theta})\cdot\mathbf T(\mathbf{x})]\ d\mathbf{x}\right)
\end{align*}\]</span> This constant is a function of <span class="math inline">\(\boldsymbol{\eta}\)</span> only, as the dependence on <span class="math inline">\(\mathbf{x}\)</span> is eliminated when integrating, so our constant really should be <span class="math inline">\(A(\boldsymbol{\eta})\)</span>. This is the cumulant function. Besides it role in normalizing the reparameterized density, the cumulant function is inherently related to the moments of <span class="math inline">\(\mathbf T(\mathbf{x})\)</span>.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.3</strong></span> Suppose <span class="math inline">\(\mathbf{X}\sim F_{\mathbf{X}}\)</span>. The <span style="color:red"><strong><em>moment-generating function (MGF)</em></strong></span>, denoted as <span class="math inline">\(M_{\mathbf{X}}(\mathbf t)\)</span>, is defined as <span class="math display">\[ M_{\mathbf{X}}(\mathbf t) = \text{E}\left[\exp(\mathbf t'\mathbf{X})\right].\]</span> The <span style="color:red"><strong><em>cumulant-generating function (CMF)</em></strong></span>, denoted as <span class="math inline">\(K_{\mathbf{X}}(\mathbf t)\)</span>, is defined as <span class="math display">\[K_{\mathbf{X}}(\mathbf t) = \log(\text{E}\left[\exp(\mathbf t'\mathbf{X})\right]) = \log M_{\mathbf{X}}(\mathbf t).\]</span></p>
</div>
<p>The cumulant-generating function is an alternative to the more common moment-generating function. Both aim to provide a more convenient way to work with random variables than working directly with the density <span class="math inline">\(f_{\mathbf{X}}\)</span> or distribution <span class="math inline">\(F_{\mathbf{X}}\)</span>, both of which often require integration.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> The defining property of moment-generating functions and cumulant-generating functions is that we can calculate quantities like expected value and variance via differentiation. This is a win, because differentiation much more straightforward than integration (in theory and in practice). The following lemma solidifies this fact.</p>
<div id="lem-" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 4.1</strong></span> Let <span class="math inline">\(M_{\mathbf{X}}(\mathbf t)\)</span> and <span class="math inline">\(K_{\mathbf{X}}(\mathbf t)\)</span> be the MGF and CMF, respectively, of a random vector <span class="math inline">\(\mathbf{X}\)</span>. For any integer <span class="math inline">\(r = r_1 + \cdots + r_n\)</span>, we have <span class="math display">\[\begin{align*}
\frac{\partial^r M_{\mathbf{X}}}{\partial t_1^{r_1}\cdots \partial t_n^{r_n}}(\mathbf{0}) &amp; = \text{E}\left[X_1^{r_1}\cdots X_n^{r_n}\right],\\
\frac{\partial K_{\mathbf{X}}}{\partial \mathbf t}(\mathbf{0}) &amp; = \text{E}\left[\mathbf{X}\right],\\
\frac{\partial^2 K_{\mathbf{X}}}{\partial \mathbf t \partial \mathbf t'}(\mathbf{0}) &amp; = \text{Var}\left(\mathbf{X}\right)
\end{align*}\]</span> The various derivatives of <span class="math inline">\(K_{\mathbf{X}}(\mathbf t)\)</span> are known as <span style="color:red"><strong><em>cumulants of <span class="math inline">\(X\)</span></em></strong></span>, and happen to coincide with expectation and variance (both specific moments of <span class="math inline">\(X\)</span>) for the derivatives shown above.</p>
</div>
<p>Proving this is a neat application of Taylor series, and you may have seen it in an undergrad probability course. When applying this to exponential families, we can relate the cumulant function <span class="math inline">\(A(\boldsymbol{\eta})\)</span> to the expectation and variance of <span class="math inline">\(\mathbf{X}\)</span>, hence its name.</p>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 4.1</strong></span> Suppose <span class="math inline">\(\mathbf{X}\sim P_\boldsymbol{\theta}\)</span>, where <span class="math inline">\(P_\boldsymbol{\theta}\in \mathcal P\)</span> for an exponential family <span class="math inline">\(\mathcal P\)</span>. The MGF and KGF of <span class="math inline">\(\mathbf T(\mathbf{X})\)</span> are given as: <span class="math display">\[\begin{align*}
M_{\mathbf T(\mathbf{X})}(\mathbf t) &amp; = \exp[A(\boldsymbol{\eta}+ \mathbf t) - A(\boldsymbol{\eta})],\\
K_{\mathbf T(\mathbf{X})}(\mathbf t) &amp; = A(\boldsymbol{\eta}+ \mathbf t) - A(\boldsymbol{\eta}).
\end{align*}\]</span> Consequently, we have <span class="math display">\[\begin{align*}
\text{E}\left[\mathbf T(\mathbf{X})\right] &amp; = \frac{\partial A(\boldsymbol{\eta})}{\partial \boldsymbol{\eta}} = \nabla_\boldsymbol{\eta}A(\boldsymbol{\eta}),\\
\text{Var}\left(\mathbf T(\mathbf{X})\right) &amp; = \frac{\partial ^2A(\boldsymbol{\eta})}{\partial \boldsymbol{\eta}\partial \boldsymbol{\eta}'}.
\end{align*}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[\begin{align*}
M_{\mathbf T(\mathbf{X})}(\mathbf t) &amp; = \text{E}\left[\exp(\mathbf t'\mathbf T(\mathbf{X}))\right] = \int_{\mathcal X} \exp(\mathbf t'\mathbf T(\mathbf{X}))h(\mathbf{x})\exp[\boldsymbol \eta(\boldsymbol{\theta})\cdot\mathbf T(\mathbf{x}) - A(\boldsymbol{\eta})]\ d\mathbf{x}\\
&amp; =  \exp[-A(\boldsymbol{\eta})]\int_{\mathcal X}h(\mathbf{x})\exp[(\boldsymbol \eta(\boldsymbol{\theta}) + \mathbf t)'\mathbf T(\mathbf{x})]\ d\mathbf{x}\\
&amp; = \exp[-A(\boldsymbol{\eta})]\exp\left[\log\left(\int_{\mathcal X}h(\mathbf{x})\exp[(\boldsymbol \eta(\boldsymbol{\theta}) + \mathbf t)'\mathbf T(\mathbf{x})]\ d\mathbf{x}\right)\right]\\
&amp; = \exp[-A(\boldsymbol{\eta})]\exp[A(\boldsymbol{\eta}+\mathbf t)]\\
&amp; = \exp[A(\boldsymbol{\eta}+\mathbf t) - A(\boldsymbol{\eta})]\\
K_{\mathbf T(\mathbf{X})}(\mathbf t) &amp; = \log M_{\mathbf{X}}(\mathbf t)\\
&amp; = A(\boldsymbol{\eta}+\mathbf t) - A(\boldsymbol{\eta})
\end{align*}\]</span> We can not differentiate <span class="math inline">\(K_{\mathbf T(\mathbf{X})}(\mathbf t)\)</span> to calculate the expectation and variance. <span class="math display">\[\begin{align*}
\text{E}\left[\mathbf T(\mathbf{X})\right] &amp; = \frac{\partial K_{\mathbf{X}}}{\partial \mathbf t}(\mathbf{0}) = \left[\frac{\partial}{\partial \mathbf t}[A(\boldsymbol{\eta}+\mathbf t) - A(\boldsymbol{\eta})]\right]_{\mathbf t = \mathbf{0}} = \left[\frac{\partial}{\partial \mathbf t}[\boldsymbol{\eta}+ \mathbf t]\frac{\partial A}{\partial \boldsymbol{\eta}}\right]_{\mathbf t = \mathbf{0}} = \frac{\partial A(\boldsymbol{\eta})}{\partial \boldsymbol{\eta}}\\
\text{Var}\left(\mathbf T(\mathbf{X})\right) &amp; = \frac{\partial^2 K_{\mathbf{X}}}{\partial \mathbf t \partial \mathbf t'}(\mathbf{0})  =\left[\frac{\partial}{\partial \mathbf t'}\left[\frac{\partial K_{\mathbf{X}}}{\partial \mathbf t}\right]\right]_{\mathbf t = \mathbf{0}} =  \left[\frac{\partial}{\partial \mathbf t '}\left[\frac{\partial}{\partial \mathbf t}[A(\boldsymbol{\eta}+\mathbf t) - A(\boldsymbol{\eta})]\right]\right]_{\mathbf t = \mathbf{0}} = \frac{\partial ^2A(\boldsymbol{\eta})}{\partial \boldsymbol{\eta}\partial \boldsymbol{\eta}'}.
\end{align*}\]</span></p>
</div>
<div id="cor-" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 4.1</strong></span> Suppose <span class="math inline">\(\mathbf{X}\sim P_\boldsymbol{\theta}\)</span>, where <span class="math inline">\(P_\boldsymbol{\theta}\in \mathcal P\)</span> for a natural exponential family <span class="math inline">\(\mathcal P\)</span>. Then <span class="math display">\[\begin{align*}
\text{E}\left[\mathbf{X}\right] &amp; = \frac{\partial A(\boldsymbol{\eta})}{\partial \boldsymbol{\eta}} = \nabla_\boldsymbol{\eta}A(\boldsymbol{\eta}),\\
\text{Var}\left(\mathbf{X}\right) &amp; = \frac{\partial ^2A(\boldsymbol{\eta})}{\partial \boldsymbol{\eta}\partial \boldsymbol{\eta}'}.
\end{align*}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If <span class="math inline">\(\mathcal P\)</span> is a natural exponential family, then <span class="math inline">\(T(\mathbf{X}) = \mathbf{X}\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.6</strong></span> If <span class="math inline">\(X \sim N(\mu,\sigma^2)\)</span>, then <span class="math inline">\(A(\boldsymbol{\eta}) = -\eta_1^2/4\eta_2 - \log(-2\eta_2)/2\)</span> for <span class="math inline">\(\boldsymbol{\eta}= [\mu/\sigma^2, -1/2\sigma^2]'\)</span>. The sufficient statistic is <span class="math inline">\([x,x^2]'\)</span>. We have: <span class="math display">\[\begin{align*}
\text{E}\left[\mathbf T(\mathbf{X})\right] &amp; = \frac{\partial}{\partial \boldsymbol{\eta}}[-\eta_1^2/4\eta_2 - \log(-2\eta_2)/2]\\
&amp; = \begin{bmatrix} - \frac{\eta_1}{2\eta_2} &amp; \frac{\eta_1^2}{4\eta_2^2} - \frac{1}{2\eta_2} \end{bmatrix} \\
&amp; = \begin{bmatrix} - \frac{\mu/\sigma^2}{2(-1/2\sigma^2)} &amp; \frac{(\mu/\sigma^2)^2}{4(-1/2\sigma^2)^2} - \frac{1}{2(-1/2\sigma^2)} \end{bmatrix}\\
&amp; = \begin{bmatrix} \mu &amp; \mu^2 - \sigma^2 \end{bmatrix}
\end{align*}\]</span></p>
</div>
<p>Exponential families also exhibit convexity in two respects.</p>
<div id="prp-" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 4.2</strong></span> Suppose <span class="math inline">\(\mathbf{X}\sim P_\boldsymbol{\theta}\)</span>, where <span class="math inline">\(P_\boldsymbol{\theta}\in \mathcal P\)</span> for an exponential family <span class="math inline">\(\mathcal P\)</span>. The <span style="color:red"><strong><em>natural parameter space</em></strong></span>, defined as <span class="math display">\[ \mathcal N = \left\{\boldsymbol{\eta}\ \bigg|\ \int_{\mathcal X}\exp[\boldsymbol{\eta}\cdot \mathbf T(\mathbf{x})]\ d\mathbf{x}&lt;\infty \right\},\]</span> is a convex set. In addition, the cumulant function <span class="math inline">\(A(\boldsymbol{\eta})\)</span> is convex on the set <span class="math inline">\(\mathcal N\)</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>To show the convexity of <span class="math inline">\(\mathcal N\)</span>, we must show that <span class="math inline">\(\alpha\boldsymbol{\eta}_1 + (1-\alpha)\boldsymbol{\eta}_2 \in \mathcal N\)</span> for any <span class="math inline">\(\alpha \in [0,1]\)</span>. This means we must verify that the following integral is finite: <span class="math display">\[ \int_{\mathcal X}h(\mathbf{x})\exp[(\alpha\boldsymbol{\eta}_1 + (1-\alpha)\boldsymbol{\eta}_2)\cdot \mathbf T(\mathbf{x})]\ d\mathbf{x}&lt;\infty .\]</span> This happens to be an application of <a href="https://mathworld.wolfram.com/HoeldersInequalities.html">Hölder’s Inequality</a>. <span class="math display">\[\begin{align*}
\int_{\mathcal X}h(\mathbf{x})\exp[(\alpha\boldsymbol{\eta}_1 + (1-\alpha)\boldsymbol{\eta}_2)\cdot \mathbf T(\mathbf{x})]\ d\mathbf{x}&amp; =\left(\int_{\mathcal X}h(\mathbf{x})\exp[(\alpha\boldsymbol{\eta}_1 + (1-\alpha)\boldsymbol{\eta}_2)\cdot \mathbf T(\mathbf{x})]\ d\mathbf{x}\right)^1 \\ &amp; = \left(\int_{\mathcal X}h(\mathbf{x})\exp[(\alpha\boldsymbol{\eta}_1 + (1-\alpha)\boldsymbol{\eta}_2)\cdot \mathbf T(\mathbf{x})]\ d\mathbf{x}\right)^{\alpha + (1-\alpha)}  \\&amp; = \left(\int_{\mathcal X}h(\mathbf{x})\exp[(\alpha\boldsymbol{\eta}_1)\cdot \mathbf T(\mathbf{x})]\exp[((1-\alpha)\boldsymbol{\eta}_2)\cdot \mathbf T(\mathbf{x})]\ d\mathbf{x}\right)^ {\alpha + (1-\alpha)}\\
&amp; \le \underbrace{\left(\int_{\mathcal X}h(\mathbf{x})\exp[(\alpha\boldsymbol{\eta}_1)\cdot \mathbf T(\mathbf{x})]\ d\mathbf{x}\right)^{\alpha}}_{\boldsymbol{\eta}_2 \in \mathcal N \implies &lt; \infty}\underbrace{\left(\int_{\mathcal X}h(\mathbf{x})\exp[((1-\alpha)\boldsymbol{\eta}_2)\cdot \mathbf T(\mathbf{x})]\ d\mathbf{x}\right)^{1-\alpha}}_{\boldsymbol{\eta}_2 \in \mathcal N \implies &lt; \infty}
\end{align*}\]</span> The integral is finite, so <span class="math inline">\(\mathcal N\)</span> is convex. If we take the logarithm of both sides of this inequality, we find that <span class="math inline">\(A(\boldsymbol{\eta})\)</span> is a convex function, recalling that <span class="math inline">\(A(\boldsymbol{\eta})\)</span> can be written as the log of the integral of <span class="math inline">\(\exp[\boldsymbol{\eta}\cdot \mathbf T(\mathbf{x})]\)</span> over <span class="math inline">\(\mathcal X\)</span>. <span class="math display">\[\begin{align*}
&amp;\log\left(\int_{\mathcal X}h(\mathbf{x})\exp[(\alpha\boldsymbol{\eta}_1 + (1-\alpha)\boldsymbol{\eta}_2)\cdot \mathbf T(\mathbf{x})]\ d\mathbf{x}\right)   \le \alpha \log\left(\int_{\mathcal X}h(\mathbf{x})\exp[(\alpha\boldsymbol{\eta}_1)\cdot \mathbf T(\mathbf{x})]\ d\mathbf{x}\right) + (1- \alpha) \log\left(\int_{\mathcal X}h(\mathbf{x})\exp[((1-\alpha)\boldsymbol{\eta}_2)\cdot \mathbf T(\mathbf{x})]\ d\mathbf{x}\right)\\
\implies &amp; A[(\alpha\boldsymbol{\eta}_1 + (1-\alpha)\boldsymbol{\eta}_2)] \le \alpha A(\boldsymbol{\eta}_1) + (1-\alpha)A(\boldsymbol{\eta}_2)
\end{align*}\]</span> This makes <span class="math inline">\(A\)</span> convex.</p>
</div>
<p>Finally, we can show that in one dimension, exponential families exhibit the MLR property when <span class="math inline">\(\eta\)</span> is an increasing function. Consequently, we can always apply the Karlin-Rubin theorem from <a href="#sec-testing" class="quarto-xref"><span class="quarto-unresolved-ref">sec-testing</span></a> in this case.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.3 (Exponential Families and MLR)</strong></span> When <span class="math inline">\(\dim(\boldsymbol{\theta}) = 1\)</span> and <span class="math inline">\(\eta(\theta)\)</span> is non-decreasing, exponential families exhibit the MLR property in that sufficient statistic <span class="math inline">\(T(x)\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>When <span class="math inline">\(f_X(\mathbf{x}\mid \theta) = h(x)\exp\left[\eta(\theta)T(\mathbf{x}) - A(\theta)\right]\)</span>, then the likelihood ratio is <span class="math display">\[ \frac{f_X(\mathbf{x}\mid \theta_1)}{f_X(\mathbf{x}\mid \theta_0)} = \exp\left[(\eta(\theta_1)-\eta(\theta_0)T(x)) - (A(\theta_1) - A(\theta_0))\right].\]</span> The derivative of this ratio with respect to the statistic <span class="math inline">\(T(x)\)</span> is <span class="math display">\[ [\eta(\theta_1)-\eta(\theta_0)]\cdot \frac{f_X(\mathbf{x}\mid \theta_1)}{f_X(\mathbf{x}\mid \theta_0)},\]</span> where <span class="math inline">\([\eta(\theta_1)-\eta(\theta_0)] &gt; 0\)</span> because <span class="math inline">\(\eta\)</span> is non-decreasing, and <span class="math inline">\(f_X(\mathbf{x}\mid \theta_1)/f_X(\mathbf{x}\mid \theta_0) &gt; 0\)</span> because it is the ratio of two probability densities. The derivative is therefore positive, and the likelihood ratio is monotonically increasing in <span class="math inline">\(T(x)\)</span>.</p>
</div>
<p>This theorem is particularly useful in the context of hypothesis testing. If our test statistic is a sufficient statistic, then by <a href="#thm-kpd" class="quarto-xref">Theorem&nbsp;<span class="quarto-unresolved-ref">thm-kpd</span></a> it is distributed according to an exponential family, exhibits the MLR property, and we can use <a href="#thm-KR" class="quarto-xref">Theorem&nbsp;<span class="quarto-unresolved-ref">thm-KR</span></a> to construct a UMP test.</p>
</section>
<section id="entropy-and-the-maximum-entropy-principle" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="entropy-and-the-maximum-entropy-principle"><span class="header-section-number">4.4</span> Entropy and the Maximum Entropy Principle</h2>
<p>Sufficiency is not the only means of arriving at the exponential family. A second derivation deals with some basic concepts from information theory. Loosely speaking, information theory studies how information is stored and communicated. The discipline exists at the intersection of probability, computer science, electrical engineering, physics, and statistical mechanics. The foundations of information theory were outline in <span class="citation" data-cites="shannon1948mathematical">Shannon (<a href="#ref-shannon1948mathematical" role="doc-biblioref">1948</a>)</span>, an article which happens to be the fourth most cited paper ever (according to Google Scholar).</p>
<p>A crucial aspect of the transmission of information is uncertainty. If we have a probability space <span class="math inline">\((\mathcal X,\mathcal F, P)\)</span> and some random variable <span class="math inline">\(X\)</span>, how do we measure how “surprising” an event <span class="math inline">\(x\in \mathcal X\)</span> is? The greater <span class="math inline">\(\Pr(X = x)\)</span>, the less surprising the outcome <span class="math inline">\(x\)</span> is. We want to define some measure <span class="math inline">\(\text{Surprise}(x)\)</span> such that:</p>
<ol type="1">
<li><span class="math inline">\(\text{Surprise}(x) \to 1\)</span> as <span class="math inline">\(\Pr(X = x)\to 0\)</span> and <span class="math inline">\(\text{Surprise}(x) \to 0\)</span> as <span class="math inline">\(\Pr(X = x)\to \infty\)</span>.</li>
<li><span class="math inline">\(\text{Surprise}(x)\)</span> is monotonic in <span class="math inline">\(\Pr(X= x)\)</span>.</li>
</ol>
<p>These properties are satisfied by the function <span class="math inline">\(\log(1/\Pr(X= x))\)</span>.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.4</strong></span> The <span style="color:red"><strong><em>information content</em></strong></span> of an outcome <span class="math inline">\(x\in \mathcal X\)</span> is <span class="math display">\[ I_X(x) = \log_b\left(\frac{1}{\Pr(X=x)}\right) = -\log_b[\Pr(X= x)]\]</span> for a base <span class="math inline">\(b\)</span>. If <span class="math inline">\(b = 2\)</span> then the unit <span class="math inline">\(I_X(x)\)</span> is given in <span style="color:red"><strong><em>bits</em></strong></span>. If <span class="math inline">\(b\)</span> is the natural exponent, the unit is <span style="color:red"><strong><em>nat</em></strong></span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.7</strong></span> Suppose <span class="math inline">\(X \sim \text{Bernoulli}(p)\)</span>, where <span class="math inline">\(\mathcal X = \{0,1\}\)</span> and <span class="math inline">\(\Pr(X = 1) = p\)</span>. The information content for <span class="math inline">\(x = 1\)</span> (a “success”) is <span class="math display">\[I_X(1) = \log_2(1/p).\]</span></p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">p =</span> (<span class="dv">0</span><span class="sc">:</span><span class="dv">1000</span>)<span class="sc">/</span><span class="dv">1000</span>,</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">I =</span> <span class="sc">-</span><span class="fu">log2</span>(p)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(p, I)) <span class="sc">+</span> </span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Pr(x = 1)"</span>,</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Information Content of x = 1"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot41" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot41-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="exp_fam_files/figure-html/fig-plot41-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot41-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.1: Information for x = 1 for various values of the parameter p
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>If we average the information content over the sample space <span class="math inline">\(\mathcal X\)</span>, we get the entropy of a random variable.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.5</strong></span> The <span style="color:red"><strong><em>entropy</em></strong></span> of a random variable <span class="math inline">\(X\)</span> is <span class="math display">\[H(X) = \text{E}\left[I_X(x)\right] = -\int_\mathcal X  \log_b[f(x)]\ dF_X = -\int_\mathcal X f(x) \log_b[f(x)]\ dx.\]</span></p>
</div>
<p>Entropy captures the average amount of information inherent in a random variable’s outcomes.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.8</strong></span> Again, suppose <span class="math inline">\(X \sim \text{Bernoulli}(p)\)</span>. The entropy of <span class="math inline">\(X\)</span> is <span class="math display">\[\begin{align*}
H(X) = - \sum_{x\in \{0,1\}} \Pr(x) \log_2[\Pr(x)] = - (1-p)\log_2(1-p) - p \log_2(p).
\end{align*}\]</span></p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">p =</span> (<span class="dv">0</span><span class="sc">:</span><span class="dv">1000</span>)<span class="sc">/</span><span class="dv">1000</span>,</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">I =</span> <span class="sc">-</span>(<span class="dv">1</span><span class="sc">-</span>p)<span class="sc">*</span><span class="fu">log2</span>(<span class="dv">1</span><span class="sc">-</span>p) <span class="sc">-</span> p<span class="sc">*</span><span class="fu">log2</span>(p)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(p, I)) <span class="sc">+</span> </span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"p = Pr(x = 1)"</span>,</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Entropy of X"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Removed 2 rows containing missing values or values outside the scale range
(`geom_line()`).</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-plot42" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot42-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="exp_fam_files/figure-html/fig-plot42-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot42-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.2: The entropy of a bernoulli random variable as a function of the parameter p
</figcaption>
</figure>
</div>
</div>
</div>
<p>The greater the entropy, the greater the uncertainty associated with a random variables outcomes. In the event <span class="math inline">\(p\in\{0,1\}\)</span>, then we’re certain that <span class="math inline">\(x = 1\)</span> or <span class="math inline">\(x=0\)</span>, and there is no uncertainty. If <span class="math inline">\(p=0.5\)</span>, it’s equally likely that <span class="math inline">\(x=1\)</span> as it is that <span class="math inline">\(x=0\)</span>, so things are less certain.</p>
</div>
<p>We can also measure the relative entropy between two distributions. Henceforth we’ll stick to the natural logarithm.</p>
<div id="def-" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.6</strong></span> Suppose <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables with distributions <span class="math inline">\(f_X\)</span> and <span class="math inline">\(F_Y\)</span>, respectively. The <span style="color:red"><strong><em>Kullback–Leibler (KL) divergence/relative entropy</em></strong></span> of <span class="math inline">\(f_X\)</span> and <span class="math inline">\(F_Y\)</span>, denoted <span class="math inline">\(D_{KL}(f_X \mid\mid F_Y)\)</span> is defined as <span class="math display">\[ D_{KL}(f_X \mid\mid F_Y) = \int_{\mathcal X} f_X(t)\cdot \log \frac{f_X(t)}{f_Y(t)}\ dt.\]</span></p>
</div>
<p>KL divergence measures how “close” <span class="math inline">\(f_X\)</span> is to <span class="math inline">\(F_Y\)</span>. Despite measuring “distance”, it is not a valid metric because it is not symmetric <span class="math inline">\(( D_{KL}(f_X \mid\mid F_Y)  \neq  D_{KL}(F_Y \mid\mid f_X) )\)</span> and does not satisfy the triangle inequality.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.9</strong></span> Suppose the role of an unfair six-sided die corresponds to a random variable <span class="math inline">\(X\)</span> whose density is <span class="math inline">\(f_X(t) = x/21\)</span> for <span class="math inline">\(t=1,\ldots,6\)</span>. If we want to model the role of the die, wrongfully assuming it is fair, we would pick <span class="math inline">\(f_Y(x) = 1/6\)</span> for <span class="math inline">\(t=1,\ldots,6\)</span>.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">expand_grid</span>(</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">gr =</span> <span class="fu">c</span>(<span class="st">"Y"</span>, <span class="st">"X"</span>), </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">t =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">f =</span> <span class="fu">ifelse</span>(gr <span class="sc">==</span> <span class="st">"Y"</span>, <span class="dv">1</span><span class="sc">/</span><span class="dv">6</span>,  t<span class="sc">/</span><span class="dv">21</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(t, f)) <span class="sc">+</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> t, <span class="at">xend =</span> t, <span class="at">y=</span> <span class="dv">0</span>, <span class="at">yend =</span> f)) <span class="sc">+</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>gr, <span class="at">ncol =</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Value of Die Roll"</span>, <span class="at">y =</span> <span class="st">"Probability Density"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot43" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot43-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="exp_fam_files/figure-html/fig-plot43-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot43-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.3: Respective densities of X and Y
</figcaption>
</figure>
</div>
</div>
</div>
<p>In this case <span class="math inline">\(D_{KL}(f_X \mid\mid F_Y)\)</span> measures the expected excess surprise from modeling the die roll with the random variable <span class="math inline">\(Y\)</span> instead of <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[\begin{align*}
D_{KL}(f_X \mid\mid F_Y) &amp; = \sum_{t \in \mathcal X} f_X(t)\cdot \log \frac{f_X(t)}{f_Y(t)}\\
&amp; = \sum_{t=1}^6 \frac{t}{21}\cdot \log\left(\frac{t/21}{1/6}\right)\\
&amp; = \frac{1}{21}\sum_{t=1}^6 t\cdot \log\left(2t/7\right)\\
&amp; \approx 0.1293825
\end{align*}\]</span></p>
</div>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.4 (Gibb’s Inequality)</strong></span> <span class="math inline">\(D_{KL}(f_X \mid\mid F_Y) \ge 0\)</span> <em>if and only if</em> <span class="math inline">\(f_X \neq F_Y\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The function <span class="math inline">\(-\log\)</span> is convex, so by Jensen’s inequality <span class="math display">\[\begin{align*}
D_{KL}(f_X \mid\mid F_Y) &amp; = \int_{\mathcal X} f_X(t)\cdot \log \frac{f_X(t)}{f_Y(t)}\ dt\\
&amp; = \int_{\mathcal X} f_X(t)\cdot -\log \frac{f_Y(t)}{f_X(t)}\ dt\\
&amp; \ge -\log\left[\int_{\mathcal X} f_X(t)\frac{f_Y(t)}{f_X(t)}\ dt\right]\\
&amp; = -\log\left[\int_{\mathcal X} f_Y(t)\ dt\right]\\
&amp; = -\log 1\\
&amp; = 0
\end{align*}\]</span> <span style="color:white"> space </span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.10 (Entropy of Uniform Distribution)</strong></span> Suppose <span class="math inline">\(X \sim \text{Uni}(a,b)\)</span>. The entropy of <span class="math inline">\(f_X\)</span> is <span class="math display">\[ H(X) = -\int_a^b\frac{1}{b-a}\log\left(\frac{1}{b-a}\right)\ dt = \log(b-a).\]</span> It turns out, that the uniform distribution has the maximum entropy of all distributions contained on the interval <span class="math inline">\([a,b]\)</span>. Intuitively, if the probability <span class="math inline">\(X = x\)</span> is uniform over <span class="math inline">\(x\in\mathcal X\)</span>, then there is no certainty about our outcomes. If you were asked to guess a realized value of <span class="math inline">\(X\)</span> beforehand, you would have zero confidence in your guess, because all outcomes are equally likely. Formally, we want to solve the problem <span class="math display">\[\max_{f} H(x)\text{ such that }\int_{a}^bf(t)\ dt= 1.\]</span> The Lagrangian associated with this problem is <span class="math display">\[\begin{align*}
\mathcal L(f) &amp;= -H(t) - \lambda \left(\int_{a}^b f(t)\ dt - 1\right)\\
&amp; = \int_a^b f(t) \log f(t)\ dt - \lambda \left(\int_{a}^b f(t)\ dt - 1 \right)\\
&amp; = \int_a^b f(t) \log f(t) - \lambda f(t) \ dt - \lambda
\end{align*}\]</span> Okay, but how do we optimize a function with respect to another function? We’re not picking some value to minimize <span class="math inline">\(\mathcal L\)</span>, we’re picking some function <span class="math inline">\(f_X\)</span> (which happens to be a valid density on the support <span class="math inline">\([a,b]\)</span>). Optimization problems like these are solved using the calculus of variations (see <span class="citation" data-cites="clarke2013functional">Clarke (<a href="#ref-clarke2013functional" role="doc-biblioref">2013</a>)</span>).<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> If <span class="math inline">\(\mathcal F\)</span> is the set of all real functions <span class="math inline">\(f:\mathbb R\to \mathbb R\)</span>, then <span class="math inline">\(\mathcal L:\mathcal F\to \mathbb R\)</span>. Mappings which take functions to numbers are known as <strong><em>functionals</em></strong>. The <strong><em>functional derivative</em></strong> of <span class="math inline">\(\mathcal L\)</span> with respect to <span class="math inline">\(f\)</span> is given as <span class="math display">\[\frac{\delta \mathcal L}{\delta f} = \lim_{\varepsilon \to 0} \frac{\mathcal L(f + \varepsilon g) - \mathcal L(f)}{\varepsilon} \]</span> for some arbitrary function <span class="math inline">\(g\in \mathcal F\)</span>.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> We can calculate functional derivatives directly appealing to the definition, but that’s a pain in the butt. Instead we’ll use the <a href="https://mathworld.wolfram.com/Euler-LagrangeDifferentialEquation.html">Euler-Lagrange equation</a> which gives the derivative in the case where <span class="math inline">\(\mathcal L\)</span> can be expressed as an integral: <span class="math display">\[\mathcal L(f) = \int J(t,f(t), f'(t))\ dt \implies \frac{\delta \mathcal L}{\delta f} = \frac{\partial J}{\partial f} - \frac{d}{dt}\frac{\partial J}{\partial f'}.\]</span> If we apply this to the Lagrangian we have the following first order conditions: <span class="math display">\[\begin{align*}
    \log f(t) &amp; = -1 - \lambda \\
    \int_{a}^b f(t)\ dt &amp; = 1
\end{align*}\]</span> We can solve for <span class="math inline">\(f(t) = \exp(-1-\lambda)\)</span>, which is constant, so our distribution <span class="math inline">\(f\)</span> is constant over <span class="math inline">\([a,b]\)</span>, making it the uniform distribution. Explicitly, we have <span class="math display">\[\begin{align*}
&amp;\int_{a}^b f(t)\ dt  = 1 \\
\implies &amp; \exp(-1-\lambda) \int_{a}^b \ dt  = 1\\
\implies &amp; \exp(-1-\lambda) = \frac{1}{b-a}\\
\implies &amp; f(t)  = \frac{1}{b-a}.
\end{align*}\]</span> Therefore <span class="math inline">\(f_X(t) = 1/(b-a)\)</span> maximizes entropy.</p>
</div>
<p>So what is appealing about maximizing entropy? In a sense, a distribution with maximal entropy comes with minimal assumptions. This concept is known as the “principle of maximum entropy” as is due to <span class="citation" data-cites="jaynes1957information">Jaynes (<a href="#ref-jaynes1957information" role="doc-biblioref">1957</a>)</span>. If we want to model a natural phenomenon with a probability distribution <span class="math inline">\(F_X\)</span>, the class of which define a regular model <span class="math inline">\(\mathcal P\)</span>, and we only know that <span class="math inline">\(\mathcal X = [a,b]\)</span>, then we should assume <span class="math inline">\(X\sim \text{Uni}(a,b)\)</span> according to the principle of maximum entropy. What if we have additional information? For instance, we may have data that allows us to estimate <span class="math inline">\(\text{E}\left[X\right]\)</span> or <span class="math inline">\(\text{Var}\left(X\right)\)</span>, something that can be done without specifying a regular model <span class="math inline">\(\mathcal P\)</span>.</p>
<p>Formally, consider defining a model <span class="math inline">\(\mathcal P\)</span> where <span class="math inline">\(\mathbf{X}\sim P_{\boldsymbol{\theta}}\)</span> such that <span class="math inline">\(\text{E}\left[\mathbf T(\mathbf{X})\right] = \boldsymbol{\theta}\)</span> for some function <span class="math inline">\(\mathbf g(\mathbf{X}) = [T_1(\mathbf{X}), \ldots, T_k(\mathbf{X})]\)</span>. The function <span class="math inline">\(\mathbf T\)</span> corresponds to all the distributional assumptions we are willing to make about <span class="math inline">\(\mathbf{X}\)</span>, and these assumptions come in the form of moment conditions. Where do these assumptions come from? If we observe <span class="math inline">\(n\)</span> realizations of <span class="math inline">\((\mathbf{X}_1,\ldots, \mathbf{X}_n)\)</span> then we can consistently estimate <span class="math inline">\(\text{E}\left[\mathbf T(\mathbf{X})\right]\)</span>, so for a sufficiently large <span class="math inline">\(n\)</span> we will be able to approximate <span class="math inline">\(\boldsymbol{\theta}\)</span>. We could define the model as <span class="math display">\[\begin{align*}
\mathcal P &amp;= \{ P_{\boldsymbol{\theta}} \},\\
P_{\boldsymbol{\theta}} &amp;= \{F_\mathbf{X}\mid \text{E}\left[\mathbf T(\mathbf{X})\right] = \boldsymbol{\theta}\},
\end{align*}\]</span> where each model value <span class="math inline">\(P_{\boldsymbol{\theta}}\)</span> is an infinite collection of distributions satisfying our moment conditions. This model is parametric but is not regular, as <span class="math inline">\(P_{\boldsymbol{\theta}}\)</span> is not a singleton for all <span class="math inline">\(P_{\boldsymbol{\theta}} \in \mathcal P\)</span>. If we insisted on a regular model, we need to go beyond moment conditions and actually assume the functional form of <span class="math inline">\(F_X\)</span>. The principle of maximum entropy gives us a criterion to appeal to here. We will define <span class="math inline">\(\mathcal P\)</span> such that each <span class="math inline">\(P_{\boldsymbol{\theta}} \in \mathcal P\)</span> is a single distribution given by <span class="math display">\[\begin{align*}
&amp; \max_{f} H(\mathbf t)\text{ such that }\int_{\mathcal X}f(\mathbf t)\ d\mathbf t= 1\text{ and }\text{E}\left[\mathbf T(\mathbf{X})\right] = \boldsymbol{\theta}\\
\implies &amp; \max_{f} H(\mathbf t)\text{ such that }\int_{\mathcal X}f(\mathbf t)\ d\mathbf t= 1\text{ and }\int_{\mathcal X}\mathbf T(\mathbf t)f(\mathbf t) \ d\mathbf t= \boldsymbol{\theta}
\end{align*}\]</span> The Lagrangian associated with this problem is <span class="math display">\[\begin{align*}
\mathcal L(f) &amp;= - H(f) - \lambda \left(\int_{\mathcal X}f(\mathbf t)\ d\mathbf t-1\right) - \boldsymbol \eta \left(\int_{\mathcal X}\mathbf T(\mathbf t)f(\mathbf t)\ d\mathbf t-\boldsymbol{\theta}\right)\\
&amp; = \int_{\mathcal X}f(\mathbf t) \log f(\mathbf t)\ d\mathbf t - \lambda \left(\int_{\mathcal X}f(\mathbf t)\ d\mathbf t-1\right) - \sum_{j=1}^k\eta_j\left(\int_{\mathcal X} T_j(\mathbf t)f(\mathbf t)\ d\mathbf t-c_j\right)\\
&amp; = \int_{\mathcal X}\left[f(\mathbf t) \log f(\mathbf t)- \lambda(\mathbf t)- \sum_{j=1}^k\eta_jT_j(\mathbf t) \ d\mathbf t \right]- \lambda- \boldsymbol \eta \cdot \boldsymbol{\theta}
\end{align*}\]</span> where the multiplier <span class="math inline">\(\lambda\)</span> corresponds to the first constraint (<span class="math inline">\(f\)</span> is a valid density), and the multipliers <span class="math inline">\(\boldsymbol{\eta}\)</span> correspond to the second constraint (the moment conditions are satisfied). The corresponding first order conditions are: <span class="math display">\[\begin{align*}
&amp;\frac{\delta \mathcal L}{\delta f}  = \log f(\mathbf{x}) + 1 - \lambda - \boldsymbol \eta \cdot \mathbf T(\mathbf{x}) = 0\\
&amp;\int_{\mathcal X}f(\mathbf t)\ d\mathbf t  = 1\\
&amp;\int_{\mathcal X}\mathbf T(\mathbf t)f(\mathbf t) \ d\mathbf t = \boldsymbol{\theta}
\end{align*}\]</span> Solving the first equation for <span class="math inline">\(f(\mathbf{x})\)</span> gives <span class="math display">\[ f(\mathbf{x}) = \exp(\lambda - 1)\exp(\boldsymbol{\eta}\cdot \mathbf T(\mathbf{x})).\]</span> Substituting this into the second condition gives: <span class="math display">\[\begin{align*}
&amp;\int_{\mathcal X}\exp(\lambda- 1)\exp(\boldsymbol{\eta}\cdot \mathbf T(\mathbf{x}))\ d\mathbf{x}= 1\\
\implies &amp; \exp(\lambda - 1)\int_{\mathcal X}\exp(\boldsymbol{\eta}\cdot \mathbf T(\mathbf{x}))\ d\mathbf{x}= 1\\
\implies &amp; \int_{\mathcal X}\exp(\boldsymbol{\eta}\cdot \mathbf T(\mathbf{x}))\ d\mathbf{x}= \exp(1-\lambda)
\end{align*}\]</span> We integrate over <span class="math inline">\(\mathbf{x}\)</span>, but <span class="math inline">\(\exp(1-\lambda)\)</span> is still a function of <span class="math inline">\(\boldsymbol{\eta}\)</span>. Define <span class="math inline">\(A(\boldsymbol{\eta})\)</span> as <span class="math display">\[ A(\boldsymbol{\eta}) =  \log\left[\int_{\mathcal X}\exp(\boldsymbol{\eta}\cdot \mathbf T(\mathbf{x}))\ d\mathbf{x}\right]\]</span> such that <span class="math inline">\(\exp(\lambda - 1) = \exp (-A(\boldsymbol{\eta}))\)</span>. <span class="math display">\[\begin{align*}
f(\mathbf{x}) &amp;= \exp(\lambda - 1)\exp(\boldsymbol{\eta}\cdot \mathbf T(\mathbf{x})) \\ &amp;= \exp(-A(\boldsymbol{\eta}))\exp(\boldsymbol{\eta}\cdot \mathbf T(\mathbf{x}))\\ &amp; = \exp\left[\boldsymbol{\eta}\cdot \mathbf T(\mathbf{x}) - A(\boldsymbol{\eta})\right].
\end{align*}\]</span> It turns out that <span class="math inline">\(f(\mathbf{x})\)</span> is an exponential family where <span class="math inline">\(h(\mathbf{x}) = 1\)</span>. The reason <span class="math inline">\(h(\mathbf{x})\)</span> is normalized in this instance has to do with a change of probability measure, but in general exponential families are those with maximum entropy.</p>
<div id="thm-exmax" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.5 (Exponential Families Maximize Entropy)</strong></span> For all probability densities <span class="math inline">\(g(\mathbf{x})\)</span> satisfying <span class="math inline">\(\text{E}\left[\mathbf T(\mathbf{x})\right] = \boldsymbol{\theta}\)</span>, <span class="math display">\[ H(f) \ge H(g)\]</span> where <span class="math inline">\(f(\mathbf{x})= \exp\left[\boldsymbol{\eta}\cdot \mathbf T(\mathbf{x}) - A(\boldsymbol{\eta})\right]\)</span> is define as above.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.11 (Numerical Optimization)</strong></span> In a perfect world we could confirm the fact that exponential families maximize entropy by telling our computer “solve this constrained optimization problem” and confirming the result is an exponential family. Unfortunately, this is only feasible for discrete random variable. For continuous random variables, the solution to the maximum-entropy problem is a continuous function, so it’s not clear how to solve the problem numerically. Fortunately, we can approximate the optimization problem arbitrarily well via “discretization”, just like how we can approximate integrals with finite Riemann sums. Suppose our sample space <span class="math inline">\(\mathcal X\)</span> is an interval of <span class="math inline">\(\mathbb{R}\)</span>. Instead of calculating the entropy by integrating over all of <span class="math inline">\(\mathcal X\)</span>, we can approximate it by calculating the sum of the entropy at a set of discrete points in <span class="math inline">\(\mathbf{X}\)</span>. If these points are <span class="math inline">\(\{x_i\}_{i=1}^n\)</span>, and the <span class="math inline">\(p_i=f(x_i)\)</span> for a density function <span class="math inline">\(f\)</span>, then we have <span class="math display">\[ H(t) = -\int_{\mathcal X}f(t)\log f(t)\ dt \approx - \sum_{i=1}^n p_i\log p_i \cdot\underbrace{(x_{i-1}-x_i)}_{\Delta x_i}.\]</span> Similarly, our approximate constraints are <span class="math display">\[\begin{align*}
\int_{\mathcal X}f(t)\ dt &amp;\approx \sum_{i=1}^n p_i\cdot\Delta x_i = 1,\\
\int_{\mathcal X}\mathbf T(\mathbf t)f(\mathbf t) \ d\mathbf t &amp;\approx \sum_{i=1}^n\mathbf T(x_i)p_i\cdot\Delta x_i = \boldsymbol{\theta}.
\end{align*}\]</span> Our discretized problem is <span class="math display">\[ \max_{\mathbf p} - \sum_{i=1}^n p_i\log p_i \cdot\Delta x_i \text{ such that }\sum_{i=1}^n p_i\cdot\Delta x_i = 1 \text{ and } \sum_{i=1}^n\mathbf T(x_i)p_i\cdot\Delta x_i = \boldsymbol{\theta},\]</span> where the vector <span class="math inline">\(\mathbf{x}\)</span> is the finite set of points which we approximate the sample space <span class="math inline">\(\mathcal X\)</span> with, and <span class="math inline">\(\mathbf p = f(\mathbf{x})\)</span> is the probability assigned to each of these points. For a concrete example, consider the problem of maximizing the entropy of a distribution over the interval <span class="math inline">\([0,1]\)</span> with no other constraints. We’ve already shown that the resulting distribution is the uniform distribution using the calculus of variations, but let’s arrive at the same conclusion by solving the discretized version of the problem. We’ll divide the interval <span class="math inline">\([0,1]\)</span> using 100 equally spaced points <span class="math inline">\(\{0.01,0.02,\ldots,0.99,1\}\)</span> (<span class="math inline">\(\Delta x_i = 1/100\)</span> for all <span class="math inline">\(i\)</span>). We will find the vector <span class="math inline">\(\mathbf p\in\mathbb{R}^{100}\)</span> which solves <span class="math display">\[ \max_{\mathbf p} - \sum_{i=1}^n \frac{p_i\log p_i}{100} \text{ such that }\sum_{i=1}^n \frac{p_i}{100} = 1.\]</span> We could solve this problem using R’s <code>optim()</code>, but instead we’ll use the <code>CVXR</code> package due to <span class="citation" data-cites="fu2017cvxr">Fu, Narasimhan, and Boyd (<a href="#ref-fu2017cvxr" role="doc-biblioref">2017</a>)</span> based on the work of <span class="citation" data-cites="grant2006disciplined">Grant, Boyd, and Ye (<a href="#ref-grant2006disciplined" role="doc-biblioref">2006</a>)</span>. This package is made specifically for convex optimization problems (a category which our problem falls into), and is very user-friendly.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">#set the dimension of the problem</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>delta_x <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>n</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co">#define variable, objective, constraints, and problem</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">Variable</span>(n)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co">#make sure to use CVXR's entr() function</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>objective <span class="ot">&lt;-</span> <span class="fu">Maximize</span>(<span class="fu">sum</span>(<span class="fu">entr</span>(p)<span class="sc">*</span>delta_x))</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>constraints <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="fu">sum</span>(p<span class="sc">*</span>delta_x) <span class="sc">==</span> <span class="dv">1</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>problem <span class="ot">&lt;-</span> <span class="fu">Problem</span>(objective, constraints)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co">#solve problem</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">solve</span>(problem)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If we plot our solution, we see that it corresponds perfectly to the uniform distribution.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a> <span class="at">x =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length =</span> n),</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a> <span class="at">p =</span> result<span class="sc">$</span><span class="fu">getValue</span>(p)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, p)) <span class="sc">+</span> </span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_function</span>(<span class="at">fun =</span> dunif, <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">"Uniform Distribution"</span>)) <span class="sc">+</span> </span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.5</span>, <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">"Numerical Solution"</span>)) <span class="sc">+</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>,<span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="st">""</span>, <span class="at">values =</span> <span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"red"</span>)) <span class="sc">+</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot44" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot44-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="exp_fam_files/figure-html/fig-plot44-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot44-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.4: Analytic and numerical solution to maximum entropy problem.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>We’ll demonstrate <a href="#thm-exmax" class="quarto-xref">Theorem&nbsp;<span class="quarto-unresolved-ref">thm-exmax</span></a> with two more examples where we’ll derive exponential families by maximizing entropy. In each case we’ll confirm our work by solving the corresponding discretized optimization problem numerically.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.12 (Exponential Distribution)</strong></span> Suppose we want to model a random variable <span class="math inline">\(X\)</span> with a sample space <span class="math inline">\(\mathcal X =[0,\infty)\)</span> according to the principle of maximum entropy such that <span class="math inline">\(\text{E}\left[X\right]= \theta\)</span>. The Lagrangian is <span class="math display">\[ \mathcal L(f) = \int_0^\infty f(t)\log f(t)\ dt - \lambda\left(\int_0^\infty f(t) \ dt - 1\right) - \eta\left(\int_0^\infty t\cdot f(t) \ dt - \theta\right),\]</span></p>
<p>which gives first order conditions:</p>
<p><span class="math display">\[\begin{align*}
&amp;\frac{\delta \mathcal L}{\delta f} = \log f(x) + 1 - \lambda - \eta x = 0\\
&amp;\int_0^\infty f(t) \ dt = 1\\
&amp;\int_0^\infty t\cdot f(t) = \theta
\end{align*}\]</span> Solving the first equation for <span class="math inline">\(f(x)\)</span> gives <span class="math inline">\(f(x) = \exp(1-\lambda)\exp(\eta x)\)</span>. If we plug this into the second equation (the first constraint) we have: <span class="math display">\[\begin{align*}
&amp;  \int_0^\infty \exp(\lambda - 1)\exp(\eta x) = 1\\
\implies &amp; \exp(\lambda - 1)\int_0^\infty \exp(\eta x) = 1\\
\implies &amp; \exp(\lambda - 1)\left[\frac{1}{\eta}\exp(\eta x)\right]_0^\infty = 1\\
\implies &amp; \exp(\lambda - 1)(-1/\eta) = 1 &amp; (-1/\eta &lt; 0)
\end{align*}\]</span> If <span class="math inline">\(-1/\eta \ge 0\)</span>, then the improper integral will not converge. Let’s repeat this step with the second integral: <span class="math display">\[\begin{align*}
&amp; \int_0^\infty x\exp(\lambda - 1)\exp(\eta x) = \theta\\
\implies &amp; \exp(\lambda - 1)\int_0^\infty x\exp(\eta x) = \theta\\
\implies &amp; \exp(\lambda - 1)\left[\frac{x\exp(\eta x)}{\eta} - \frac{\exp(\eta x)}{\eta}\right]_0^\infty = 1 &amp; (\text{integration by parts})\\
\implies &amp; \exp(\lambda - 1)(1/\eta^2) = \theta
\end{align*}\]</span> If we divide the two constraints by each other, we have <span class="math inline">\(-\eta = 1/\theta\)</span>: <span class="math display">\[\begin{align*}
&amp; \frac{\exp(\lambda - 1)(-1/\eta)}{\exp(\lambda - 1)(1/\eta^2)} = \frac{1}{\theta}\\
\implies &amp; -\eta = 1/\theta\\
\implies &amp; -1/\eta = \theta
\end{align*}\]</span> Therefore, <span class="math display">\[\begin{align*}
&amp;\exp(\lambda - 1)(-1/\eta) = 1\\
\implies &amp;\exp(\lambda - 1)\theta = 1\\
\implies &amp; \exp(\lambda - 1) = 1/\theta
\end{align*}\]</span> so <span class="math display">\[f(x) = \exp(1-\lambda)\exp(\eta x) = \frac{1}{\theta}\exp(-x/\theta).\]</span> This is the exponential distribution which is parameterized by <span class="math inline">\(\theta\)</span>, where <span class="math inline">\(\theta\)</span> comes from the constraint <span class="math inline">\(\text{E}\left[X\right] = \theta\)</span>.</p>
<p>To discretize the problem, we’ll approximate the sample space <span class="math inline">\(\mathcal X =(0,\infty)\)</span> with the <span class="math inline">\(n=250\)</span> points <span class="math inline">\(\{0.04, 0.08, \ldots, 10\}\)</span> (<span class="math inline">\(\Delta x_i = 1/100\)</span> for <span class="math inline">\(i=1,\ldots,250\)</span>). The exponential distribution has negligible density on the interval <span class="math inline">\((10,\infty)\)</span>, so our approximation of <span class="math inline">\([0,\infty)\)</span> is still valid despite the points being a subset of <span class="math inline">\([0,10]\)</span>. The approximated problem is</p>
<p><span class="math display">\[\begin{align*}
&amp; \max_{\mathbf p} - \sum_{i=1}^n p_i\log p_i \cdot \frac{1}{250},\\
&amp;\text{such that} \sum_{i=1}^n p_i\cdot  \frac{1}{250} = 1, \\
&amp;\text{and} \sum_{i=1}^n p_i x_i  \frac{1}{250} = \theta .\\
\end{align*}\]</span></p>
<p>We’ll take <span class="math inline">\(\theta = 1\)</span> for this problem.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">#set the dimension of the problem</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">250</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.04</span>, <span class="dv">10</span>, <span class="at">length =</span> n)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>delta_x <span class="ot">&lt;-</span> x[<span class="dv">2</span>] <span class="sc">-</span> x[<span class="dv">1</span>]</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">#define variable, objective, constraints, and problem</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">Variable</span>(n)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>objective <span class="ot">&lt;-</span> <span class="fu">Maximize</span>(<span class="fu">sum</span>(<span class="fu">entr</span>(p)<span class="sc">*</span>delta_x))</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>constraints <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="fu">sum</span>(p<span class="sc">*</span>delta_x) <span class="sc">==</span> <span class="dv">1</span>, </span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>                    <span class="fu">sum</span>(p<span class="sc">*</span>x<span class="sc">*</span>delta_x) <span class="sc">==</span> theta)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>problem <span class="ot">&lt;-</span> <span class="fu">Problem</span>(objective, constraints)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co">#solve problem</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">solve</span>(problem)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If we plot our solution, we find that it is in almost perfect alignment with the exponential distribution.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a> <span class="at">x =</span> x,</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a> <span class="at">p =</span> result<span class="sc">$</span><span class="fu">getValue</span>(p)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, p)) <span class="sc">+</span> </span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_function</span>(<span class="at">fun =</span> dexp, <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">"Exponential Distribution, θ = 1"</span>)) <span class="sc">+</span> </span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.1</span>, <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">"Numerical Solution"</span>)) <span class="sc">+</span> </span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="st">""</span>, <span class="at">values =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"black"</span>)) <span class="sc">+</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot45" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot45-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="exp_fam_files/figure-html/fig-plot45-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot45-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.5: Analytic and numerical solution to maximum entropy problem.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.13 (Normal Distribution)</strong></span> Suppose we want to model a random variable <span class="math inline">\(X\)</span> with a sample space <span class="math inline">\(\mathcal X =\mathbb R\)</span> according to the principle of maximum entropy such that <span class="math inline">\(\text{E}\left[X\right]= \mu\)</span>, and <span class="math inline">\(\text{Var}\left(X\right) = \sigma^2\)</span>. We can combine these into a single constraint <span class="math inline">\(\text{E}\left[(x-\mu)^2\right]=\sigma^2\)</span>. The Lagrangian is <span class="math display">\[ \mathcal L(f) = \int_{-\infty}^\infty f(t)\log f(t)\ dt - \lambda\left(\int_{-\infty}^\infty f(t) \ dt - 1\right) - \eta\left(\int_{-\infty}^\infty (t-\mu)^2\cdot f(t) \ dt - \sigma^2\right),\]</span> which gives the first order conditions: <span class="math display">\[\begin{align*}
&amp;\frac{\delta \mathcal L}{\delta f} = \log f(x) + 1 - \lambda - \eta (x-\mu)^2  = 0\\
&amp;\int_{-\infty}^\infty f(t) \ dt  = 1\\
&amp;\int_{-\infty}^\infty  (t-\mu)^2\cdot f(t) \ dt = \sigma^2
\end{align*}\]</span> Solving the first equation gives <span class="math display">\[ f(x) = \exp(\lambda - 1)\exp(\eta(x-\mu)^2),\]</span> which we can substitute into the first constraint. <span class="math display">\[\begin{align*}
&amp; \int_{-\infty}^\infty \exp(\lambda - 1)\exp(\eta(t-\mu)^2) \ dt = 1\\
\implies &amp; \exp(\lambda - 1)\int_{-\infty}^\infty \exp(\eta(t-\mu)^2) \ dt = 1\\
\implies &amp; \exp(\lambda - 1)(-\pi/\eta)^{1/2} = 1 &amp; \left(\int_{-\infty}^\infty \exp(a(t+b)^2)\ dt = \sqrt{\pi/a}\right)\\
\implies &amp; \exp(\lambda - 1) = (-\eta/\pi)^{1/2}
\end{align*}\]</span> The key step was recognizing the <a href="https://mathworld.wolfram.com/GaussianIntegral.html">integral of the Gaussian function</a>. Now we can turn to the second constraint. <span class="math display">\[\begin{align*}
&amp; \int_{-\infty}^\infty  (t-\mu)^2\cdot f(t) \ dt = \sigma^2\\
\implies &amp; \int_{-\infty}^\infty  (t-\mu)^2\cdot \exp(\lambda - 1)\exp(\eta(x-\mu)^2) \ dt = \sigma^2\\
\implies &amp; (-\eta/\pi)^{1/2}\int_{-\infty}^\infty  (t-\mu)^2\cdot \exp(\eta(x-\mu)^2) \ dt = \sigma^2 &amp; (\exp(\lambda - 1) = (-\eta/\pi)^{1/2})\\
\implies &amp; (-\eta/\pi)^{1/2}\cdot \frac{1}{2}(-\pi/\eta^3)^{1/2} = \sigma^2\\
\implies &amp; \eta = -\frac{1}{2\sigma^2}
\end{align*}\]</span> The integral of <span class="math inline">\((t-\mu)^2\cdot \exp(\eta(x-\mu)^2)\)</span> follows from a generalization of the integral of the Gaussian function. Therefore, <span class="math display">\[\begin{align*}
f(x) &amp;= \exp(\lambda - 1)\exp(\eta(x-\mu)^2)\\
&amp; = (-\eta/\pi)^{1/2}\exp\left(-\frac{1}{2\sigma^2}(x-\mu)^2\right)\\
&amp; = (-(-1/2\sigma^2)/\pi)^{1/2}\exp\left(-\frac{1}{2\sigma^2}(x-\mu)^2\right)\\
&amp; = \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left[-\frac{(x-\mu^2)}{2\sigma^2}\right]
\end{align*}\]</span></p>
<p>The normal distribution has negligible density outside the interval <span class="math inline">\([-4,4]\subset\mathcal X=\mathbb{R}\)</span>, so we can approximate it with <span class="math inline">\(n=250\)</span> equally spaced points on <span class="math inline">\([-4,4]\)</span> (<span class="math inline">\(\Delta\)</span>). The discretized problem is</p>
<p><span class="math display">\[\begin{align*}
&amp; \max_{\mathbf p} - \sum_{i=1}^n p_i\log p_i \cdot \frac{8}{250},\\
&amp;\text{such that} \sum_{i=1}^n p_i\cdot  \frac{8}{250} = \theta_1, \\
&amp;\text{and} \sum_{i=1}^n p_i x_i^2 \cdot \frac{8}{250} = \theta_2 .\\
\end{align*}\]</span></p>
<p>We’ll let <span class="math inline">\(\boldsymbol{\theta}= (0,1)\)</span>, which should give the standard normal distribution.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co">#set the dimension of the problem</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">250</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span> <span class="sc">+</span> <span class="dv">8</span><span class="sc">/</span>n, <span class="dv">4</span>, <span class="at">length =</span> n)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>delta_x <span class="ot">&lt;-</span> x[<span class="dv">2</span>] <span class="sc">-</span> x[<span class="dv">1</span>]</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">#define variable, objective, constraints, and problem</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">Variable</span>(n)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>objective <span class="ot">&lt;-</span> <span class="fu">Maximize</span>(<span class="fu">sum</span>(<span class="fu">entr</span>(p)<span class="sc">*</span>delta_x))</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>constraints <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="fu">sum</span>(p<span class="sc">*</span>delta_x) <span class="sc">==</span> <span class="dv">1</span>, </span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>                    <span class="fu">sum</span>(p<span class="sc">*</span>x<span class="sc">*</span>delta_x) <span class="sc">==</span> theta[<span class="dv">1</span>],</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>                    <span class="fu">sum</span>(p<span class="sc">*</span>x<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>delta_x) <span class="sc">==</span> theta[<span class="dv">2</span>])</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>problem <span class="ot">&lt;-</span> <span class="fu">Problem</span>(objective, constraints)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co">#solve problem</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">solve</span>(problem)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once again, we have a solution that looks nearly identitical to the distribution we derived analytically.</p>
<div class="cell" data-layout-align="center">
<details class="code-fold">
<summary>Show code which generates figure</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a> <span class="at">x =</span> x,</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a> <span class="at">p =</span> result<span class="sc">$</span><span class="fu">getValue</span>(p)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, p)) <span class="sc">+</span> </span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_function</span>(<span class="at">fun =</span> dnorm, <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">"Standard Normal Distribution"</span>)) <span class="sc">+</span> </span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.1</span>, <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">"Numerical Solution"</span>)) <span class="sc">+</span> </span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="st">""</span>, <span class="at">values =</span> <span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"red"</span>)) <span class="sc">+</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-plot46" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plot46-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="exp_fam_files/figure-html/fig-plot46-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plot46-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.6: Analytic and numerical solution to maximum entropy problem.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="further-reading" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">4.5</span> Further reading</h2>
<p><strong><em>Exponential Families</em></strong>: Chapter 18 of <span class="citation" data-cites="dasgupta2011probability">DasGupta (<a href="#ref-dasgupta2011probability" role="doc-biblioref">2011</a>)</span>, Section 1.6 of <span class="citation" data-cites="bickel2015mathematical">Bickel and Doksum (<a href="#ref-bickel2015mathematical" role="doc-biblioref">2015</a>)</span>, Section 1.5 of <span class="citation" data-cites="lehmann2006theory">Lehmann and Casella (<a href="#ref-lehmann2006theory" role="doc-biblioref">1998</a>)</span>, these <a href="https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf">notes</a></p>
<p><strong><em>Information Theory</em></strong>: Chapter 6 of <span class="citation" data-cites="pml1Book">Murphy (<a href="#ref-pml1Book" role="doc-biblioref">2022</a>)</span>, Section 1.6 <span class="citation" data-cites="bishop2006pattern">Bishop and Nasrabadi (<a href="#ref-bishop2006pattern" role="doc-biblioref">2006</a>)</span></p>
<p><strong><em>Maximum Entropy Principle</em></strong>: <a href="http://www.di.fc.ul.pt1/n/~jpn/r/maxent/maxent.html">Here</a>, <a href="https://mtlsites.mit.edu/Courses/6.050/2003/notes/chapter10.pdf">here</a>, <a href="https://sgfin.github.io/2017/03/16/Deriving-probability-distributions-using-the-Principle-of-Maximum-Entropy/">here</a>, and <a href="https://bjlkeng.github.io/posts/maximum-entropy-distributions/">here</a>. This <a href="https://homes.cs.washington.edu/~jrl/teaching/cse599swi16/">course page</a></p>
<p><strong><em>All of the Above</em></strong>: <span class="citation" data-cites="jaynes2003probability">Jaynes (<a href="#ref-jaynes2003probability" role="doc-biblioref">2003</a>)</span></p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-bickel2015mathematical" class="csl-entry" role="listitem">
Bickel, Peter J, and Kjell A Doksum. 2015. <em>Mathematical Statistics: Basic Ideas and Selected Topics, Volume i</em>. 2nd ed. CRC Press.
</div>
<div id="ref-bishop2006pattern" class="csl-entry" role="listitem">
Bishop, Christopher M, and Nasser M Nasrabadi. 2006. <em>Pattern Recognition and Machine Learning</em>. Vol. 4. 4. Springer.
</div>
<div id="ref-clarke2013functional" class="csl-entry" role="listitem">
Clarke, Francis. 2013. <em>Functional Analysis, Calculus of Variations and Optimal Control</em>. Vol. 264. Springer.
</div>
<div id="ref-darmois1935lois" class="csl-entry" role="listitem">
Darmois, Georges. 1935. <span>“Sur Les Lois de Probabilit<span>é</span>a Estimation Exhaustive.”</span> <em>CR Acad. Sci. Paris</em> 260 (1265): 85.
</div>
<div id="ref-dasgupta2011probability" class="csl-entry" role="listitem">
DasGupta, Anirban. 2011. <em>Probability for Statistics and Machine Learning: Fundamentals and Advanced Topics</em>. Springer.
</div>
<div id="ref-fisher1922mathematical" class="csl-entry" role="listitem">
Fisher, Ronald A. 1922. <span>“On the Mathematical Foundations of Theoretical Statistics.”</span> <em>Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character</em> 222 (594-604): 309–68.
</div>
<div id="ref-fu2017cvxr" class="csl-entry" role="listitem">
Fu, Anqi, Balasubramanian Narasimhan, and Stephen Boyd. 2017. <span>“CVXR: An r Package for Disciplined Convex Optimization.”</span> <em>arXiv Preprint arXiv:1711.07582</em>.
</div>
<div id="ref-grant2006disciplined" class="csl-entry" role="listitem">
Grant, Michael, Stephen Boyd, and Yinyu Ye. 2006. <span>“Disciplined Convex Programming.”</span> In <em>Global Optimization</em>, 155–210. Springer.
</div>
<div id="ref-jaynes1957information" class="csl-entry" role="listitem">
Jaynes, Edwin T. 1957. <span>“Information Theory and Statistical Mechanics.”</span> <em>Physical Review</em> 106 (4): 620.
</div>
<div id="ref-jaynes2003probability" class="csl-entry" role="listitem">
———. 2003. <em>Probability Theory: The Logic of Science</em>. Cambridge university press.
</div>
<div id="ref-koopman1936distributions" class="csl-entry" role="listitem">
Koopman, Bernard Osgood. 1936. <span>“On Distributions Admitting a Sufficient Statistic.”</span> <em>Transactions of the American Mathematical Society</em> 39 (3): 399–409.
</div>
<div id="ref-lehmann2006theory" class="csl-entry" role="listitem">
Lehmann, Erich L, and George Casella. 1998. <em>Theory of Point Estimation</em>. 2nd ed. Springer.
</div>
<div id="ref-pml1Book" class="csl-entry" role="listitem">
Murphy, Kevin P. 2022. <em>Probabilistic Machine Learning: An Introduction</em>. MIT Press. <a href="https://probml.ai">probml.ai</a>.
</div>
<div id="ref-pitman1936sufficient" class="csl-entry" role="listitem">
Pitman, Edwin James George. 1936. <span>“Sufficient Statistics and Intrinsic Accuracy.”</span> In <em>Mathematical Proceedings of the Cambridge Philosophical Society</em>, 32:567–79. 4. Cambridge University Press.
</div>
<div id="ref-shannon1948mathematical" class="csl-entry" role="listitem">
Shannon, Claude Elwood. 1948. <span>“A Mathematical Theory of Communication.”</span> <em>The Bell System Technical Journal</em> 27 (3): 379–423.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Disclaimer: the term “regular” is often defined to mean something else in exponential families. It won’t be discussed here though.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>An even better alternative is the characteristic function of a random variable. This function is the Fourier transform of <span class="math inline">\(f_{\mathbf{X}}\)</span> and has some nice theoretical properties that make it exceptionally important in probability theory.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The calculus of variations comes in handy when working with optimal control problems and dynamic optimization as well.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Whether this derivative exists deals with Frechet differentiability and the existence of a continuous linear operator between Banach spaces. This linear operator is the functional derivative, and is itself a functional. As such, we can actually write the derivative as an integral, because all linear functionals can be expressed as integrals by The Riesz-representation theorem (roughly speaking): <span class="math display">\[\lim_{\varepsilon \to 0} \frac{\mathcal L(f + \varepsilon g) - \mathcal L(f)}{\varepsilon} = \int \frac{\delta \mathcal L}{\delta f}g(x)\ dx \]</span><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./testing.html" class="pagination-link" aria-label="Hypothesis Testing">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./stochastic.html" class="pagination-link" aria-label="Stochastic Processes">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Stochastic Processes</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb13" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\plim}{plim}</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\argmin}{argmin}</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>\DeclareMathOperator{\argmax}{argmax}</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>\newcommand{\var}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Var}\left(#1\right)}</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>\newcommand{\avar}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Avar}\left(#1\right)}</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>\newcommand{\E}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{E}\left<span class="co">[</span><span class="ot">#1\right</span><span class="co">]</span>}</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>\newcommand{\cov}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{Cov}\left(#1\right)}</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>\newcommand{\mse}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{MSE}\left(#1\right)}</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>\newcommand{\se}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\text{se}\left(#1\right)}</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>\newcommand{\limfunc}{lim} </span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>\newcommand{\X}{\mathbf{X}}</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>\newcommand{\Xm}{\mathbb{X}}</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>\newcommand{\EER}{\bar{\thet}_\text{EE}}</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>\newcommand{\NLS}{\hat{\bet}_\text{NLLS}}</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>\newcommand{\z}{\mathbf{z}}</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>\newcommand{\rr}{\mathbf{r}}</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>\newcommand{\C}{\mathbf{C}}</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>\newcommand{\Pe}{\mathbf{P}}</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>\newcommand{\y}{\mathbf{y}}</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>\newcommand{\Y}{\mathbf{Y}}</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>\newcommand{\uu}{\mathbf{u}}</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>\newcommand{\e}{\mathbf{e}}</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>\newcommand{\D}{\mathbf{D}}</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>\newcommand{\x}{\mathbf{x}}</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>\newcommand{\xm}{\mathbb{x}}</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>\newcommand{\Zm}{\mathbb{Z}}</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>\newcommand{\Wm}{\mathbb{W}}</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>\newcommand{\Hm}{\mathbb{H}}</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>\newcommand{\W}{\mathbf{W}}</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>\newcommand{\Z}{\mathbf{Z}}</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>\newcommand{\Hess}{\mathbf{H}(\mathbf{\Z\mid\thet})}</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>\newcommand{\Score}{\mathbf{S}(\mathbf{\Z\mid\thet})}</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>\newcommand{\A}{\mathbf{A}}</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>\newcommand{\h}{\mathbf{h}}</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>\newcommand{\Q}{\mathbf{Q}}</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>\newcommand{\F}{\mathbf{F}}</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>\newcommand{\G}{\mathbf{G}}</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>\newcommand{\I}{\mathbf{I}}</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>\renewcommand{\D}{\mathbf{D}}</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>\renewcommand{\C}{\mathbf{C}}</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>\newcommand{\zer}{\mathbf{0}}</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLS}{\hat{\boldsymbol\beta}_\text{OLS} }</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLSOV}{\hat{\boldsymbol\beta}_\text{OLS,OV} }</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>\newcommand{\OLSME}{\hat{\boldsymbol\beta}_\text{OLS,ME} }</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>\newcommand{\EE}{\hat{\boldsymbol\theta}_\text{EX} }</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>\newcommand{\ME}{\hat{\boldsymbol\theta}_\text{M} }</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>\newcommand{\MDE}{\hat{\boldsymbol\theta}_\text{MDE} }</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>\newcommand{\IV}{\hat{\boldsymbol\beta}_\text{IV} }</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>\newcommand{\TSLS}{\hat{\boldsymbol\beta}_\text{2SLS} }</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>\newcommand{\thet}{\boldsymbol{\theta}}</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>\newcommand{\et}{\boldsymbol{\eta}}</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>\newcommand{\R}{\mathbb{R}}</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>\newcommand{\Sig}{\boldsymbol{\Sigma}}</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>\newcommand{\ep}{\boldsymbol{\varepsilon}}</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>\newcommand{\Omeg}{\boldsymbol{\Omega}}</span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a>\newcommand{\Thet}{\boldsymbol{\Theta}}</span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a>\newcommand{\bet}{\boldsymbol{\beta}}</span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>\newcommand{\rk}{rank}</span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>\newcommand{\tsum}{\sum}</span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a>\newcommand{\tr}{tr}</span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>\newcommand{\norm}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\left\lVert#1\right\rVert}</span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a>\newcommand{\abs}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\left\lvert#1\right\rvert}</span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>\newcommand{\ms}{\overset{ms}{\to}}</span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a>\newcommand{\pto}{\overset{p}{\to}}</span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a>\newcommand{\iid}{\overset{iid}{\sim}}</span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a>\newcommand{\dto}{\overset{d}{\to}}</span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a>\newcommand{\asim}{\overset{a}{\sim}}</span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a><span class="fu"># Exponential Families</span></span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: false</span></span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(combinat)</span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(CVXR)</span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a>This section is not *essential* to understanding econometrics, but it will provide some neat context later on. Many problems in statistics become much simpler if are data is generated from a special type of distribution. We already saw this in @sec-testing when talking about UMP tests and the MLR property. It turns out there is a general class of probability distributions which not only satisfy the MLR property, but also satisfy many other convenient properties which will make our lives easier later on. This section will introduce this class of distributions which are known as the exponential family of distributions. Right from the get go, a rather large disclaimer is in order. The only time we will be able to reap the benefits of distributions in the exponential family is when we assume that our model $\mathcal P$ is a parametric model which is also regular (each element $P_\thet \in \mathcal P$ corresponds to a unique distribution $f_{\X}(\x\mid \thet)$).^<span class="co">[</span><span class="ot">Disclaimer: the term "regular" is often defined to mean something else in exponential families. It won't be discussed here though.</span><span class="co">]</span> Many common models in econometrics will not fall into this category as it requires specifying a distribution of some unobservable quantity. This is why exponential families are usually emphasized more in the setting of statistics, particularly when covering standard/classical topics.  </span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-84"><a href="#cb13-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-85"><a href="#cb13-85" aria-hidden="true" tabindex="-1"></a><span class="fu">## Motivation -- Sufficiency </span></span>
<span id="cb13-86"><a href="#cb13-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-87"><a href="#cb13-87" aria-hidden="true" tabindex="-1"></a>In a sense, a statistic $T:\mathcal X \to\mathcal T$ (where $\mathcal T$ is almost always $\mathbb R^k$) is a way of boiling down information about a sample $\x$ to a single value $T(\x)$, where $\X \sim P_\thet$ for some regular parametric model $\mathcal P$. We then use the information "captured" in the statistic $T(\x)$ to estimate $\thet$. Once we have the value $T(\x)$, does the particular value of $\x$ matter? Is it possible that we can just ignore $\x$ if $T(\x)$ encompasses all the relevant information provided by our sample? Here are two examples that show that this may, or may not, be the case. </span>
<span id="cb13-88"><a href="#cb13-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-89"><a href="#cb13-89" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb13-90"><a href="#cb13-90" aria-hidden="true" tabindex="-1"></a>Suppose $X_i \iid \text{Ber}(p)$. If we want to estimate $p$, the natural estimator is $\hat p(\X) = \sum_{i=1}^n X_i / n$ when we observe a sample $\X = (X_1,\ldots, X_n)$. Let's just focus on the part of this function that depends on $\X$, that being the statistic $T(\X) = \sum_{i=1}^n X_i$ which records the number of successes over $n$ Bernoulli trials. We can think of our estimation process as follows: we observe $\x$, we calculate $T(\x)$, we calculate our estimate $\hat p$ using only the value $T(\x)$. Do we lose anything by only using $T(x)$? If this were the case, then what would that look like mathematically? For some insight, let's look at the distribution of our sample $f_{\X}(\x\mid p)$. If $f_X(x_i\mid p)$ is the distribution associated with $\text{Ber}(p)$, then</span>
<span id="cb13-91"><a href="#cb13-91" aria-hidden="true" tabindex="-1"></a>$$f_{\X}(\x\mid p) = \prod_{i=1}^nf_X(x_i\mid p) = \prod_{i=1}^np^{x_1}(1-p)^{1-x_i} = p^{\sum_{i=1}^n x_i}(1-p)^{n-\sum_{i=1}^n x_i} = p^{T(\x)}(1-p)^{n-T(\x)}.$$ Substituting $T(\x)$ into this distribution illuminates a striking feature -- $f_{\X}(\x\mid p)$ depends on $\x$ only through the statistic $T(\x)$. In other words, if we are estimating $\hat p$ (or rather estimating $f_{\X}(\x\mid p)$ vicariously through $\hat p$), then we only need to know the value of our statistic $T(\x)$. The statistic contains all the relevant information needed for estimation. Another way to think about this is with an experiment. Suppose an unfair coin lands on heads with probability $p$, and $\X = (X_1,\ldots X_n)$ records the number of heads observed ($X_i = 1$ means the $i$th flip is heads). If you are going to estimate $p$ with $\hat p(\X) = \sum_{i=1}^n X_i / n$, does it matter the order in which the coins landed on heads? If $n = 2$, is there a difference between observing $\x = (1,0)$ and $\x' = (0,1)$? No -- the only information you really care about is the fact that the coin landed on heads once, i.e $T(\x) = T(\x') = 1$. In an effort to beat a dead horse, let $n = 10$, and $T(\x) = 7$ (the coin lands on heads 7 times). There are $3,628,800$ possible permutations where we get $7$ heads, all of which will give the same estimate of $\hat p(\x) = 0.7$.  </span>
<span id="cb13-92"><a href="#cb13-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-95"><a href="#cb13-95" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb13-96"><a href="#cb13-96" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb13-97"><a href="#cb13-97" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">7</span>), <span class="fu">rep</span>(<span class="dv">0</span>, n <span class="sc">-</span> <span class="dv">7</span>))</span>
<span id="cb13-98"><a href="#cb13-98" aria-hidden="true" tabindex="-1"></a>samples <span class="ot">&lt;-</span> <span class="fu">permn</span>(x)</span>
<span id="cb13-99"><a href="#cb13-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-100"><a href="#cb13-100" aria-hidden="true" tabindex="-1"></a><span class="co">#write estimator s.t we specify which sample we want to use from the permutations</span></span>
<span id="cb13-101"><a href="#cb13-101" aria-hidden="true" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="cf">function</span>(i){</span>
<span id="cb13-102"><a href="#cb13-102" aria-hidden="true" tabindex="-1"></a>  x <span class="ot">&lt;-</span> samples[[i]]</span>
<span id="cb13-103"><a href="#cb13-103" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>(x)<span class="sc">/</span>n</span>
<span id="cb13-104"><a href="#cb13-104" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-105"><a href="#cb13-105" aria-hidden="true" tabindex="-1"></a>estimates <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(samples), p_hat)</span>
<span id="cb13-106"><a href="#cb13-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-107"><a href="#cb13-107" aria-hidden="true" tabindex="-1"></a><span class="co">#What % of our estimates are 0.7?</span></span>
<span id="cb13-108"><a href="#cb13-108" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(estimates <span class="sc">==</span> <span class="fl">0.7</span>)</span>
<span id="cb13-109"><a href="#cb13-109" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-110"><a href="#cb13-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-111"><a href="#cb13-111" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-112"><a href="#cb13-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-113"><a href="#cb13-113" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb13-114"><a href="#cb13-114" aria-hidden="true" tabindex="-1"></a>Suppose $X_i \iid N(\mu,\sigma^2)$ for a known $\sigma^2$. We could estimate using the median $\hat \mu(\X)$, $$\hat \mu(\X) = \begin{cases}X_{(n+1)/2} &amp; n\text{ odd}<span class="sc">\\</span> \frac{X_{n/2} + X_{n/2 + 1}}{2} &amp; n\text{ even} \end{cases}.$$ In this case, is $\hat \mu(\X)$ calculated via a statistic which encapsulates all the relevant information about $\x$? Heuristically, it seems like this is not the case. The only real statistic that $\hat\mu(\X)$ is a function of is itself, so let $T(\X) = \hat\mu(\X)$. Suppose we observe $\x = (-100, 0, 0.1)$ and $\x' = (-0.1,0, 100)$. For both of these observations we have $T(\X) = \hat \mu(\x) = 0$, but discarding $\x'$ and $\x$ doesn't seem like the best idea here, because they have drastically different sample means, numbers that seem especially relevant here. Admittedly, this argument lacks the rigor of the previous one, but it hopefully illustrates that not all estimators can be calculated with some magic statistic that perfectly captures an observation $\x$.</span>
<span id="cb13-115"><a href="#cb13-115" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-116"><a href="#cb13-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-117"><a href="#cb13-117" aria-hidden="true" tabindex="-1"></a>To formalize this property of optimal data reduction, we will consider the distribution $f_{\X}(\x\mid \thet)$ like we did in the first example. The definition is due to @fisher1922mathematical.</span>
<span id="cb13-118"><a href="#cb13-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-119"><a href="#cb13-119" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb13-120"><a href="#cb13-120" aria-hidden="true" tabindex="-1"></a>Let  $\X \sim P_\thet$ for some $P_\thet \in \mathcal P$, where $\mathcal P$ is a regular parametric model. A statistic $T(\X)$ is  &lt;span style="color:red"&gt;**_sufficient_**&lt;/span&gt; for $P_\thet\in \mathcal P$ (or for $\thet$), if $f_{\X}(\x \mid T(\x), \thet)$ is not a function of $\thet$. </span>
<span id="cb13-121"><a href="#cb13-121" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-122"><a href="#cb13-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-123"><a href="#cb13-123" aria-hidden="true" tabindex="-1"></a>Sufficiency can seem a little abstract at first, but there are a few different ways to think about it that may make it a bit clearer. One way was already highlighted by the Bernoulli trials example. Sufficiency means that if we have $T(\x) = T(\x')$ for any two observed samples $\x,\x' \in \mathcal X$, then $\x$ and $\x'$ provide us the same amount of information about $\thet$. Another way of thinking about sufficiency is via a thought experiment involving two statisticians. Suppose Statistician A and Statistician B want to estimate $\thet$. Statistician A has access to some random sample $\x$, while Statistician B only knows $T(\x)$. If $T$ is a sufficient statistic, then Statistician B is at no disadvantage because he can generate his own random sample! He may not know $\thet$, but he knows $T(\x)$, and $f_{\X}(\x \mid T(\x), \thet)$ does not depend on $\thet$, so he can just simulate a random sample from $f_{\X}(\x \mid T(\x))$.  </span>
<span id="cb13-124"><a href="#cb13-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-125"><a href="#cb13-125" aria-hidden="true" tabindex="-1"></a>Using the definition of sufficiency to verify a statistic has the property can be a bit cumbersome, so we usually do so using a famous theorem. </span>
<span id="cb13-126"><a href="#cb13-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-127"><a href="#cb13-127" aria-hidden="true" tabindex="-1"></a>:::{#thm-fac}</span>
<span id="cb13-128"><a href="#cb13-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-129"><a href="#cb13-129" aria-hidden="true" tabindex="-1"></a><span class="fu">## Fisher–Neyman factorization theorem</span></span>
<span id="cb13-130"><a href="#cb13-130" aria-hidden="true" tabindex="-1"></a>Let  $\X \sim P_\thet$ for some $P_\thet \in \mathcal P$, where $\mathcal P$ is a regular parametric model. The statistic $T(\X)$ is sufficient *if and only if* there exist non-negative functions $g:\mathcal T\times \Theta\to\mathbb R$ and $h:\mathcal X\to \mathbb R$ such that $$f_\X(\x\mid\thet) = g(T(\x) \mid \thet)h(\x).$$</span>
<span id="cb13-131"><a href="#cb13-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-132"><a href="#cb13-132" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-133"><a href="#cb13-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-134"><a href="#cb13-134" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb13-135"><a href="#cb13-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-136"><a href="#cb13-136" aria-hidden="true" tabindex="-1"></a>&lt;span style="color:white"&gt;space&lt;/span&gt;</span>
<span id="cb13-137"><a href="#cb13-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-138"><a href="#cb13-138" aria-hidden="true" tabindex="-1"></a>$(\Longrightarrow)$ Suppose $T$ is a sufficient statistic. The statistic $T$ is a function of $\x$, so the joint density of $(\X, T(\X)$ is simply the density of $\X$.  $$f_{\X}(\x \mid \thet) = f_{\X, T(\X)}(\x, T(\x)\mid \thet)$$ BY properties of conditional variables, </span>
<span id="cb13-139"><a href="#cb13-139" aria-hidden="true" tabindex="-1"></a>$$ f_{\X}(\x \mid \thet) = f_{\X, T(\X)}(\x, T(\x)\mid \thet) = \underbrace{f_{X\mid T(\X)}(\x\mid T(\x), \thet)}_{h(\x)}\underbrace{f_{T(\X)}(T(\x)\mid\thet)}_{g(T(\x) \mid \thet)}.$$ We know that $f_{X\mid T(\X)}(\x\mid T(\x), \thet)$ is a suitable candidate for $h(\x)$, because $T$ is sufficient, so $f_{X\mid T(\X)}(\x\mid T(\x), \thet)$ is not a function of $\thet$.</span>
<span id="cb13-140"><a href="#cb13-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-141"><a href="#cb13-141" aria-hidden="true" tabindex="-1"></a>$(\Longleftarrow)$ Suppose $f_\X(\x\mid\thet) = g(T(\x) \mid \thet)h(\x)$, and fix $T(\x) = t$. By the definition of conditional expectation, </span>
<span id="cb13-142"><a href="#cb13-142" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-143"><a href="#cb13-143" aria-hidden="true" tabindex="-1"></a>f_{\X \mid T(\X)}(\x \mid t, \thet) &amp;= \frac{f_{X, T(\X)}(\x,t\mid\thet)}{f_{T(\x)}(t\mid \thet)}<span class="sc">\\</span></span>
<span id="cb13-144"><a href="#cb13-144" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{f_{X}(\x\mid\thet)}{f_{T(\x)}(t\mid \thet)} &amp;(T(\x)\text{ function of }\x)<span class="sc">\\</span></span>
<span id="cb13-145"><a href="#cb13-145" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{g(t \mid \thet)h(\x)}{f_{T(\x)}(t\mid \thet)} &amp;(f_\X(\x\mid\thet) = g(t \mid \thet)h(\x))<span class="sc">\\</span></span>
<span id="cb13-146"><a href="#cb13-146" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{g(t \mid \thet)h(\x)}{f_{T(\x)}(t\mid \thet)}</span>
<span id="cb13-147"><a href="#cb13-147" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-148"><a href="#cb13-148" aria-hidden="true" tabindex="-1"></a>We can write the denominator in terms of $f_{T(\x)}(t\mid \thet)$ by integrating $f_\X(\x\mid\thet)$ over all $\x \in \mathcal X$ such that $T(\x) = t$ for some fixed $t$:</span>
<span id="cb13-149"><a href="#cb13-149" aria-hidden="true" tabindex="-1"></a>$$f_{T(\x)}(t\mid \thet) = \int_{<span class="sc">\{</span>\x \mid T(\x) = t<span class="sc">\}</span>} f_\X(\x\mid\thet)\ d\x = \int_{<span class="sc">\{</span>\x \mid T(\x) = t<span class="sc">\}</span>} g(t\mid \thet)h(\x)\ d\x =  g(t\mid \thet)\int_{<span class="sc">\{</span>\x \mid T(\x) = t<span class="sc">\}</span>}h(\x)\ d\x$$ Therefore, </span>
<span id="cb13-150"><a href="#cb13-150" aria-hidden="true" tabindex="-1"></a>$$ f_{\X \mid T(\X)}(\x \mid t, \thet) = \frac{g(t \mid \thet)h(\x)}{g(t\mid \thet)\int_{<span class="sc">\{</span>\x \mid T(\x) = t<span class="sc">\}</span>}h(\x)\ d\x} = \frac{h(\x)}{\int_{<span class="sc">\{</span>\x \mid T(\x) = t<span class="sc">\}</span>}h(\x)\ d\x},$$ which is not a function of $\thet$. This makes $T(\x)$ a sufficient statistic.</span>
<span id="cb13-151"><a href="#cb13-151" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-152"><a href="#cb13-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-153"><a href="#cb13-153" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb13-154"><a href="#cb13-154" aria-hidden="true" tabindex="-1"></a>Suppose $X_i \iid N(\mu, \sigma^2)$ for a known $\sigma^2$. After some calculation, we can conclude </span>
<span id="cb13-155"><a href="#cb13-155" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-156"><a href="#cb13-156" aria-hidden="true" tabindex="-1"></a>f_\X(\x\mid \mu)&amp; = \prod_{i=1}^n f_X(x_i \mid \mu )<span class="sc">\\</span></span>
<span id="cb13-157"><a href="#cb13-157" aria-hidden="true" tabindex="-1"></a>&amp; =  \prod_{i=1}^n \frac{1}{\sigma\sqrt{2\pi}}\exp \left<span class="co">[</span><span class="ot">-\frac{(x_i - \mu)^2}{2\sigma^2}\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb13-158"><a href="#cb13-158" aria-hidden="true" tabindex="-1"></a>&amp; = (2\pi\sigma^2)^{-n/2}\exp\left<span class="co">[</span><span class="ot">-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i - \bar x)^2\right</span><span class="co">]</span> \exp\left<span class="co">[</span><span class="ot">-\frac{n}{2\sigma^2}(\mu - \bar x)^2\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb13-159"><a href="#cb13-159" aria-hidden="true" tabindex="-1"></a>&amp; =  \underbrace{(2\pi\sigma^2)^{-n/2}\exp\left<span class="co">[</span><span class="ot">-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i - \bar x)^2\right</span><span class="co">]</span>}_{h(\x)} \underbrace{\exp\left[-\frac{n}{2\sigma^2}(\mu -  T(\x))^2\right]}_{g(T(\x)\mid \mu)} &amp; (T(\x) = \bar x),</span>
<span id="cb13-160"><a href="#cb13-160" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-161"><a href="#cb13-161" aria-hidden="true" tabindex="-1"></a>so $T(\X) = \bar X$ is a sufficient statistic for $\mu$. </span>
<span id="cb13-162"><a href="#cb13-162" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-163"><a href="#cb13-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-164"><a href="#cb13-164" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exponential Families </span></span>
<span id="cb13-165"><a href="#cb13-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-166"><a href="#cb13-166" aria-hidden="true" tabindex="-1"></a>Whether $T(\X)$ is sufficient for $P_\thet$ depends entirely on the distribution $f_{\X}(\x \mid \thet)$. Is it possible to describe the entire class of distributions which admit a sufficient statistic? Do they all take a similar form? As proved independently by @pitman1936sufficient, @koopman1936distributions, and @darmois1935lois, the answer is yes (sort-of)! We will broaden our view a bit by considering a vector of statistics $\mathbf T(\X)$.  </span>
<span id="cb13-167"><a href="#cb13-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-168"><a href="#cb13-168" aria-hidden="true" tabindex="-1"></a>:::{#thm-kpd}</span>
<span id="cb13-169"><a href="#cb13-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-170"><a href="#cb13-170" aria-hidden="true" tabindex="-1"></a><span class="fu">## Pitman–Koopman–Darmois theorem</span></span>
<span id="cb13-171"><a href="#cb13-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-172"><a href="#cb13-172" aria-hidden="true" tabindex="-1"></a>Suppose  $X_i \iid P_\thet$ where the support of $f_{\X}(x_i\mid\thet)$ does not depend on $\thet$. There exists a sufficient statistic $\mathbf T:\mathcal X\to \mathbb R^k$ such that $k$ is fixed for all sample sizes $n$ *if and only if* $f_{\X}$ can be written as </span>
<span id="cb13-173"><a href="#cb13-173" aria-hidden="true" tabindex="-1"></a>$$f_{\X}(\x \mid\thet)= h(\x)\exp<span class="co">[</span><span class="ot">\boldsymbol \eta(\thet)\cdot\mathbf T(\x) - A(\et)</span><span class="co">]</span>$$ for functions $h:\mathcal X \to \mathbb R$,  $\boldsymbol \eta:\boldsymbol \Theta \to \mathbb R^k$,  $A:\mathcal X \to \mathbb R$.</span>
<span id="cb13-174"><a href="#cb13-174" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-175"><a href="#cb13-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-176"><a href="#cb13-176" aria-hidden="true" tabindex="-1"></a>Proving the sufficiency of this condition is a direct application of Theorem @thm-fac, as we can just let $g(\mathbf T(\x) \mid \thet) = \exp<span class="co">[</span><span class="ot">\boldsymbol \eta(\thet)\cdot\mathbf T(\x) - A(\et)</span><span class="co">]</span>$. Proving this is a necessary condition is a bit more complicated and relies on the assumptions that the dimension of $\mathbf T$ is fixed, and that the support of $f_{\X}(x_i\mid\thet)$ is independent of $\thet$. The second condition should seem familiar, as it plays a crucial role in proving the Cramér–Rao lower bound holds (see @sec-est). The distributions given by the Pitman–Koopman–Darmois theorem merit their own definition.</span>
<span id="cb13-177"><a href="#cb13-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-178"><a href="#cb13-178" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb13-179"><a href="#cb13-179" aria-hidden="true" tabindex="-1"></a>A regular parametric model $\mathcal P$ is an  &lt;span style="color:red"&gt;**_exponential family_**&lt;/span&gt; if $$f_{\X}(\x \mid\thet)= h(\x)\exp[\boldsymbol \eta(\thet)\cdot\mathbf T(\x) - A(\et)].$$ We refer to $\mathbf T(\x)$ as the &lt;span style="color:red"&gt;**_sufficient statistic_**&lt;/span&gt;, $\boldsymbol\eta(\thet)$ as the  &lt;span style="color:red"&gt;**_natural parameter_**&lt;/span&gt;, and $A(\et)$ as the &lt;span style="color:red"&gt;**_cumulant function_**&lt;/span&gt;. In the event $\boldsymbol\eta(\thet) = \thet$, the exponential family is in &lt;span style="color:red"&gt;**_canonical form_**&lt;/span&gt;. If $\boldsymbol\eta(\thet) = \thet$ and $\mathbf T(\x) = \x$, we say our model is a  &lt;span style="color:red"&gt;**_natural exponential family_**&lt;/span&gt;.</span>
<span id="cb13-180"><a href="#cb13-180" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-181"><a href="#cb13-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-182"><a href="#cb13-182" aria-hidden="true" tabindex="-1"></a>Sometimes, people will opt to write the cumulant function in terms of the parameter $\thet$, which is completely fine.</span>
<span id="cb13-183"><a href="#cb13-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-184"><a href="#cb13-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-185"><a href="#cb13-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-186"><a href="#cb13-186" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb13-187"><a href="#cb13-187" aria-hidden="true" tabindex="-1"></a>If $X \iid \text{Ber}(p)$ where $n=1$, then $T(\X) = \sum_{i=1}^n X_i = X$ is a sufficient statistic for $p$, so $f_{X}(x\mid p)$ is an exponential family by the Pitman–Koopman–Darmois theorem. </span>
<span id="cb13-188"><a href="#cb13-188" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-189"><a href="#cb13-189" aria-hidden="true" tabindex="-1"></a>f_{X}(x\mid p) &amp; = p^x(1-p)^{1-x}<span class="sc">\\</span></span>
<span id="cb13-190"><a href="#cb13-190" aria-hidden="true" tabindex="-1"></a>&amp; = \exp<span class="co">[</span><span class="ot">\log(p^x(1-p)^{1-x})</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb13-191"><a href="#cb13-191" aria-hidden="true" tabindex="-1"></a>&amp; = \exp<span class="co">[</span><span class="ot">x\log(p) + (1-x)\log(1-p))</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb13-192"><a href="#cb13-192" aria-hidden="true" tabindex="-1"></a>&amp; = \exp<span class="co">[</span><span class="ot">x(\log(p) - \log(1-p)) + \log(1-p)</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb13-193"><a href="#cb13-193" aria-hidden="true" tabindex="-1"></a>&amp; = 1\cdot \exp\left<span class="co">[</span><span class="ot">x\log\left(\frac{p}{1-p}\right) + \log(1-p)\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb13-194"><a href="#cb13-194" aria-hidden="true" tabindex="-1"></a>h(x) &amp; = 1<span class="sc">\\</span></span>
<span id="cb13-195"><a href="#cb13-195" aria-hidden="true" tabindex="-1"></a>T(x) &amp; = x<span class="sc">\\</span></span>
<span id="cb13-196"><a href="#cb13-196" aria-hidden="true" tabindex="-1"></a>\eta(p) &amp; = \log\left(\frac{p}{1-p}\right)<span class="sc">\\</span></span>
<span id="cb13-197"><a href="#cb13-197" aria-hidden="true" tabindex="-1"></a>A(\eta)&amp; = -\log(1-p)<span class="sc">\\</span></span>
<span id="cb13-198"><a href="#cb13-198" aria-hidden="true" tabindex="-1"></a>&amp; =  \log\left(1 + \frac{p}{1-p}\right)<span class="sc">\\</span></span>
<span id="cb13-199"><a href="#cb13-199" aria-hidden="true" tabindex="-1"></a>&amp; = \log\left<span class="co">[</span><span class="ot">1 + \exp\left[\log\left(\frac{p}{1-p}\right)\right]\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb13-200"><a href="#cb13-200" aria-hidden="true" tabindex="-1"></a>&amp; = \log<span class="co">[</span><span class="ot">1 + \exp(\eta)</span><span class="co">]</span></span>
<span id="cb13-201"><a href="#cb13-201" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-202"><a href="#cb13-202" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-203"><a href="#cb13-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-204"><a href="#cb13-204" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb13-205"><a href="#cb13-205" aria-hidden="true" tabindex="-1"></a>For $X\sim N(\mu,\sigma^2)$ where both $\mu$ and $\sigma^2$ are unknown, </span>
<span id="cb13-206"><a href="#cb13-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-207"><a href="#cb13-207" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-208"><a href="#cb13-208" aria-hidden="true" tabindex="-1"></a>f_X(x\mid \mu,\sigma^2) &amp;= \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left<span class="co">[</span><span class="ot">-\frac{1}{2\sigma^2}(x-\mu)^2\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb13-209"><a href="#cb13-209" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{\sigma^{-1}}{\sqrt{2\pi}}\exp\left<span class="co">[</span><span class="ot">-\frac{1}{2\sigma^2}(x^2-2x\mu +\mu^2)\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb13-210"><a href="#cb13-210" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{\exp<span class="co">[</span><span class="ot">-\log(\sigma)</span><span class="co">]</span>}{\sqrt{2\pi}}\exp\left<span class="co">[</span><span class="ot">\frac{\mu}{\sigma^2} - \frac{1}{2\sigma^2}x^2 - \frac{1}{2\sigma^2}\mu^2\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb13-211"><a href="#cb13-211" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{1}{\sqrt{2\pi}}\exp\left<span class="co">[</span><span class="ot">\frac{\mu}{\sigma^2}x - \frac{1}{2\sigma^2}x^2 - \frac{1}{2\sigma^2}\mu^2-\log(\sigma)\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb13-212"><a href="#cb13-212" aria-hidden="true" tabindex="-1"></a>h(x) &amp; = \frac{1}{\sqrt{2\pi}}<span class="sc">\\</span></span>
<span id="cb13-213"><a href="#cb13-213" aria-hidden="true" tabindex="-1"></a>\boldsymbol \eta(\mu, \sigma^2) &amp; = <span class="co">[</span><span class="ot">\mu/\sigma^2, -1/2\sigma^2</span><span class="co">]</span>'<span class="sc">\\</span></span>
<span id="cb13-214"><a href="#cb13-214" aria-hidden="true" tabindex="-1"></a>\mathbf T(x) &amp; = <span class="co">[</span><span class="ot">x, x^2</span><span class="co">]</span>' <span class="sc">\\</span></span>
<span id="cb13-215"><a href="#cb13-215" aria-hidden="true" tabindex="-1"></a>A(\et) &amp;= \frac{\mu^2}{2\sigma^2} + \log \sigma<span class="sc">\\</span></span>
<span id="cb13-216"><a href="#cb13-216" aria-hidden="true" tabindex="-1"></a>&amp; = -\frac{\eta_1^2}{4\eta_2} - \frac{\log(-2\eta_2)}{2}</span>
<span id="cb13-217"><a href="#cb13-217" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-218"><a href="#cb13-218" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-219"><a href="#cb13-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-220"><a href="#cb13-220" aria-hidden="true" tabindex="-1"></a>Almost all the distributions we rely on happen to be exponential families. A select collection of these, along with their associated sample and parameter space, are:</span>
<span id="cb13-221"><a href="#cb13-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-222"><a href="#cb13-222" aria-hidden="true" tabindex="-1"></a>| Distribution/Model                    | $\mathcal X$                  | $\thet$                                   | $\boldsymbol \Theta$                          |</span>
<span id="cb13-223"><a href="#cb13-223" aria-hidden="true" tabindex="-1"></a>|-------------------------------------- |----------------------------   |-----------------------------------------  |-------------------------------------------    |</span>
<span id="cb13-224"><a href="#cb13-224" aria-hidden="true" tabindex="-1"></a>| Bernoulli Distribution                | $<span class="sc">\{</span>0,1<span class="sc">\}</span>$                     | $p$                                       | $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$                                       |</span>
<span id="cb13-225"><a href="#cb13-225" aria-hidden="true" tabindex="-1"></a>| Binomial Distribution ($n$ known)     | $<span class="sc">\{</span>0,1,\ldots,n<span class="sc">\}</span>$            | $p$                                       | $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$                                       |</span>
<span id="cb13-226"><a href="#cb13-226" aria-hidden="true" tabindex="-1"></a>| Negative Binomial Distribution (failures $r$ known)       | $\mathbb N$                   | $p$                                       | $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$                                       |</span>
<span id="cb13-227"><a href="#cb13-227" aria-hidden="true" tabindex="-1"></a>| Geometric Distribution                | $\mathbb N\backslash<span class="sc">\{</span>0<span class="sc">\}</span>$    | $p$                                       | $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$                                       |</span>
<span id="cb13-228"><a href="#cb13-228" aria-hidden="true" tabindex="-1"></a>| Exponential Distribution              | $[0,\infty)$                  | $\lambda$                                 | $\mathbb R^+$                                 |</span>
<span id="cb13-229"><a href="#cb13-229" aria-hidden="true" tabindex="-1"></a>| Poisson Distribution                  | $\mathbb N$                   | $\lambda$                                 | $\mathbb R^+$                                 |</span>
<span id="cb13-230"><a href="#cb13-230" aria-hidden="true" tabindex="-1"></a>| Normal Distribution                   | $\mathbb R$                   | $(\mu,\sigma^2)$                          | $\mathbb R\times \mathbb R^+$                 |</span>
<span id="cb13-231"><a href="#cb13-231" aria-hidden="true" tabindex="-1"></a>| Chi-squared Distribution              | $[0,\infty)$                  | $k$                                       | $\mathbb N$                                   |</span>
<span id="cb13-232"><a href="#cb13-232" aria-hidden="true" tabindex="-1"></a>| Gamma Distribution                    | $\mathbb R^+$                 | $(\alpha,\beta)$                          | $\mathbb R^+\times \mathbb R^+$               |</span>
<span id="cb13-233"><a href="#cb13-233" aria-hidden="true" tabindex="-1"></a>| Beta Distribution                     | $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$                       | $(\alpha,\beta)$                          | $\mathbb R^+\times \mathbb R^+$               |</span>
<span id="cb13-234"><a href="#cb13-234" aria-hidden="true" tabindex="-1"></a>| Multivariate Normal Distribution      | $\mathbb R^k$                 | $(\boldsymbol \mu, \boldsymbol \Sigma)$   | $\mathbb R^k\times (\mathbb R^+)^k \times \mathbb R^{k\times(k-1)}$   |</span>
<span id="cb13-235"><a href="#cb13-235" aria-hidden="true" tabindex="-1"></a>| Multinomial Distribution ($n$ known)  | $<span class="sc">\{</span>0,1,\ldots,n<span class="sc">\}</span>^k$          | $\mathbf p$, where $\sum_{j=1}^k p_j = 1$     | $k-$simplex over $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$                      |</span>
<span id="cb13-236"><a href="#cb13-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-237"><a href="#cb13-237" aria-hidden="true" tabindex="-1"></a>Technically, listing some of these are redundant. The chi-squared distribution and exponential distribution both special cases of the gamma distribution. The Bernoulli distribution is a binomial distribution where $n=1$. The The natural parameters, sufficient statistic, and cumulant function of each of these distributions are: </span>
<span id="cb13-238"><a href="#cb13-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-239"><a href="#cb13-239" aria-hidden="true" tabindex="-1"></a>| Distribution/Model | $\boldsymbol \eta(\thet)$ | $h(\x)$ | $\mathbf T(\x)$ | $A(\boldsymbol \eta)$ |</span>
<span id="cb13-240"><a href="#cb13-240" aria-hidden="true" tabindex="-1"></a>|---|---|---|---|---|---|</span>
<span id="cb13-241"><a href="#cb13-241" aria-hidden="true" tabindex="-1"></a>| Bernoulli Distribution | $\log<span class="co">[</span><span class="ot">1/(1-p)</span><span class="co">]</span>$ | $1$ | $x$ |  $\log<span class="co">[</span><span class="ot">1 + \exp(\eta)</span><span class="co">]</span>$ |</span>
<span id="cb13-242"><a href="#cb13-242" aria-hidden="true" tabindex="-1"></a>| Binomial Distribution ($n$ known) | $\log<span class="co">[</span><span class="ot">1/(1-p)</span><span class="co">]</span>$ | $\binom{n}{x}$ |  $x$ |  $n\log<span class="co">[</span><span class="ot">1 + \exp(\eta)</span><span class="co">]</span>$ |</span>
<span id="cb13-243"><a href="#cb13-243" aria-hidden="true" tabindex="-1"></a>| Negative Binomial Distribution (failures $r$ known) | $\log p$ |  $\binom{x+r-1}{x}$| $x$ |  $-r\log<span class="co">[</span><span class="ot">1-\exp(\eta)</span><span class="co">]</span>$ |</span>
<span id="cb13-244"><a href="#cb13-244" aria-hidden="true" tabindex="-1"></a>| Geometric Distribution | $\log(1-p)$ | $1$  | $x$ | $\eta - \log<span class="co">[</span><span class="ot">1-\exp(\eta)</span><span class="co">]</span>$ |</span>
<span id="cb13-245"><a href="#cb13-245" aria-hidden="true" tabindex="-1"></a>| Exponential Distribution | $-\lambda$ | $1$ | $x$ |   $-\log(-\eta)$|</span>
<span id="cb13-246"><a href="#cb13-246" aria-hidden="true" tabindex="-1"></a>| Poisson Distribution | $\log\lambda$ |  $1/x!$| $x$ |  $\exp(\eta)$ |</span>
<span id="cb13-247"><a href="#cb13-247" aria-hidden="true" tabindex="-1"></a>| Normal Distribution | $<span class="co">[</span><span class="ot">\mu/\sigma^2, -1/2\sigma^2</span><span class="co">]</span>'$ | $1/\sqrt{2\pi}$ | $<span class="co">[</span><span class="ot">x,x^2</span><span class="co">]</span>'$ |   $-\eta_1^2/4\eta_2 - \log(-2\eta_2)/2$ |</span>
<span id="cb13-248"><a href="#cb13-248" aria-hidden="true" tabindex="-1"></a>| Chi-squared Distribution | $k/2-1$ |  $\exp(-x/2)$ |  $\log x$| $\log<span class="co">[</span><span class="ot">\Gamma(\eta+1)</span><span class="co">]</span> + (\eta+1)\log2$ |</span>
<span id="cb13-249"><a href="#cb13-249" aria-hidden="true" tabindex="-1"></a>| Gamma Distribution | $<span class="co">[</span><span class="ot">\alpha - 1, -\beta</span><span class="co">]</span>'$ | $1$ | $<span class="co">[</span><span class="ot">\log x, x</span><span class="co">]</span>'$ |  $\log<span class="co">[</span><span class="ot">\Gamma(\eta_1+1)</span><span class="co">]</span> - (\eta_1+1)\log(-\eta_2)$  |</span>
<span id="cb13-250"><a href="#cb13-250" aria-hidden="true" tabindex="-1"></a>| Beta Distribution | $<span class="co">[</span><span class="ot">\alpha - 1,\beta - 1</span><span class="co">]</span>$ |  $1/<span class="co">[</span><span class="ot">x(1-x)</span><span class="co">]</span>$| $<span class="co">[</span><span class="ot">\log x, \log(1-x)</span><span class="co">]</span>'$ |  $\log<span class="co">[</span><span class="ot">\Gamma(\eta_1 + 1)</span><span class="co">]</span> + \log<span class="co">[</span><span class="ot">\Gamma(\eta_2 + 1)</span><span class="co">]</span> - \log<span class="co">[</span><span class="ot">\Gamma(\eta_1+\eta_2+1)</span><span class="co">]</span>$  |</span>
<span id="cb13-251"><a href="#cb13-251" aria-hidden="true" tabindex="-1"></a>| Multivariate Normal Distribution |  $<span class="co">[</span><span class="ot">\boldsymbol \Sigma^{-1}\boldsymbol \mu, -\boldsymbol\Sigma^{-1}/2</span><span class="co">]</span>'$ | $(2\pi)^{-k/2}$ | $<span class="co">[</span><span class="ot">\x,\x\x'</span><span class="co">]</span>$ | $-\frac{1}{4}\et_1'\et_2^{-1}\et_1 - \frac{1}{2}\log|-2\et_2|$ |  </span>
<span id="cb13-252"><a href="#cb13-252" aria-hidden="true" tabindex="-1"></a>| Multinomial Distribution ($n$ known) | $<span class="co">[</span><span class="ot">\log(p_1/p_k),\ldots, \log(p_{k-1}/p_k),0</span><span class="co">]</span>'$ | $\x$ | $\frac{n!}{\prod_{i=1}^kx_i!}$ | $n\log<span class="co">[</span><span class="ot">1+\sum_{i=1}^{k-1}\exp(\eta_i)</span><span class="co">]</span>$    |  </span>
<span id="cb13-253"><a href="#cb13-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-254"><a href="#cb13-254" aria-hidden="true" tabindex="-1"></a>It's worth noting there are some really important distributions that are not exponential families, namely the student's $t-$distribution, the uniform distribution, and the $F-$distribution. Interestingly, the $F-$distribution is asymptotically equivalent to a $\chi^2$ distribution, and  the $t-$distribution is asymptotically equivalent to the standard normal distribution, so as $n\to\infty$, these "become" exponential families.  </span>
<span id="cb13-255"><a href="#cb13-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-256"><a href="#cb13-256" aria-hidden="true" tabindex="-1"></a><span class="fu">## Properties </span></span>
<span id="cb13-257"><a href="#cb13-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-258"><a href="#cb13-258" aria-hidden="true" tabindex="-1"></a>Exponential families have a myriad of properties that make them easy to work with. First, let's look into the cumulant function $A(\et)$. The role of $A(\et)$ is to normalize the density $f_{\X}(\x\mid\thet)$ when we express it as an exponential family. Without getting into the proof behind the Pitman–Koopman–Darmois theorem, it would seem that the function $h(\x)\exp<span class="co">[</span><span class="ot">\boldsymbol \eta(\thet)\cdot\mathbf T(\x)</span><span class="co">]</span>$ would suffice for $\mathbf T$ to be a sufficient statistic for $\thet$, because we could just let $g(\mathbf T(\x)\mid \thet) = \exp<span class="co">[</span><span class="ot">\boldsymbol \eta(\thet)\cdot\mathbf T(\x)</span><span class="co">]</span>$ and apply the Fisher–Neyman factorization theorem. The issue with this is that $h(\x)\exp<span class="co">[</span><span class="ot">\boldsymbol \eta(\thet)\cdot\mathbf T(\x)</span><span class="co">]</span>$ may not be a valid density function which integrates to $1$ over $\mathcal X$. To ensure it is a valid density, we need to find some normalizing scalar $A$ which satisfies: </span>
<span id="cb13-259"><a href="#cb13-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-260"><a href="#cb13-260" aria-hidden="true" tabindex="-1"></a>$$ \int_{\mathcal X}\frac{1}{A}\left<span class="co">[</span><span class="ot">h(\x)\exp[\boldsymbol \eta(\thet)\cdot\mathbf T(\x)]\right</span><span class="co">]</span>\ d\x = 1 $$</span>
<span id="cb13-261"><a href="#cb13-261" aria-hidden="true" tabindex="-1"></a>We could also take the scalar to be $\exp \kappa$, giving </span>
<span id="cb13-262"><a href="#cb13-262" aria-hidden="true" tabindex="-1"></a>$$ \int_{\mathcal X}h(\x)\exp<span class="co">[</span><span class="ot">\boldsymbol \eta(\thet)\cdot\mathbf T(\x)-A</span><span class="co">]</span>\ d\x = 1.$$</span>
<span id="cb13-263"><a href="#cb13-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-264"><a href="#cb13-264" aria-hidden="true" tabindex="-1"></a>If we solve for $A$, we have </span>
<span id="cb13-265"><a href="#cb13-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-266"><a href="#cb13-266" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-267"><a href="#cb13-267" aria-hidden="true" tabindex="-1"></a>\implies &amp; \int_{\mathcal X}h(\x)\exp<span class="co">[</span><span class="ot">\boldsymbol \eta(\thet)\cdot\mathbf T(\x)-A</span><span class="co">]</span>\ d\x = 1<span class="sc">\\</span></span>
<span id="cb13-268"><a href="#cb13-268" aria-hidden="true" tabindex="-1"></a>\implies &amp; \exp(-A)\int_{\mathcal X}h(\x)\exp<span class="co">[</span><span class="ot">\boldsymbol \eta(\thet)\cdot\mathbf T(\x)</span><span class="co">]</span>\ d\x = 1<span class="sc">\\</span></span>
<span id="cb13-269"><a href="#cb13-269" aria-hidden="true" tabindex="-1"></a>\implies &amp; A =  \log\left(\int_{\mathcal X}h(\x)\exp<span class="co">[</span><span class="ot">\boldsymbol \eta(\thet)\cdot\mathbf T(\x)</span><span class="co">]</span>\ d\x\right)</span>
<span id="cb13-270"><a href="#cb13-270" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-271"><a href="#cb13-271" aria-hidden="true" tabindex="-1"></a>This constant is a function of $\et$ only, as the dependence on $\x$ is eliminated when integrating, so our constant really should be $A(\et)$. This is the cumulant function. Besides it role in normalizing the reparameterized density, the cumulant function is inherently related to the moments of $\mathbf T(\x)$. </span>
<span id="cb13-272"><a href="#cb13-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-273"><a href="#cb13-273" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb13-274"><a href="#cb13-274" aria-hidden="true" tabindex="-1"></a>Suppose $\X \sim F_{\X}$. The &lt;span style="color:red"&gt;**_moment-generating function (MGF)_**&lt;/span&gt;, denoted as $M_{\X}(\mathbf t)$, is defined as </span>
<span id="cb13-275"><a href="#cb13-275" aria-hidden="true" tabindex="-1"></a>$$ M_{\X}(\mathbf t) = \E{\exp(\mathbf t'\X)}.$$ The &lt;span style="color:red"&gt;**_cumulant-generating function (CMF)_**&lt;/span&gt;, denoted as $K_{\X}(\mathbf t)$, is defined as $$K_{\X}(\mathbf t) = \log(\E{\exp(\mathbf t'\X)}) = \log M_{\X}(\mathbf t).$$</span>
<span id="cb13-276"><a href="#cb13-276" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-277"><a href="#cb13-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-278"><a href="#cb13-278" aria-hidden="true" tabindex="-1"></a>The cumulant-generating function is an alternative to the more common moment-generating function. Both aim to provide a more convenient way to work with random variables than working directly with the density $f_{\X}$ or distribution $F_{\X}$, both of which often require integration.^<span class="co">[</span><span class="ot">An even better alternative is the characteristic function of a random variable. This function is the Fourier transform of $f_{\X}$ and has some nice theoretical properties that make it exceptionally important in probability theory.</span><span class="co">]</span> The defining property of moment-generating functions and cumulant-generating functions is that we can calculate quantities like expected value and variance via differentiation. This is a win, because differentiation much more straightforward than integration (in theory and in practice). The following lemma solidifies this fact.</span>
<span id="cb13-279"><a href="#cb13-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-280"><a href="#cb13-280" aria-hidden="true" tabindex="-1"></a>:::{#lem-}</span>
<span id="cb13-281"><a href="#cb13-281" aria-hidden="true" tabindex="-1"></a>Let $M_{\X}(\mathbf t)$ and $K_{\X}(\mathbf t)$ be the MGF and CMF, respectively, of a random vector $\X$. For any integer $r = r_1 + \cdots + r_n$, we have </span>
<span id="cb13-282"><a href="#cb13-282" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-283"><a href="#cb13-283" aria-hidden="true" tabindex="-1"></a>\frac{\partial^r M_{\X}}{\partial t_1^{r_1}\cdots \partial t_n^{r_n}}(\zer) &amp; = \E{X_1^{r_1}\cdots X_n^{r_n}},<span class="sc">\\</span></span>
<span id="cb13-284"><a href="#cb13-284" aria-hidden="true" tabindex="-1"></a>\frac{\partial K_{\X}}{\partial \mathbf t}(\zer) &amp; = \E{\X},<span class="sc">\\</span></span>
<span id="cb13-285"><a href="#cb13-285" aria-hidden="true" tabindex="-1"></a>\frac{\partial^2 K_{\X}}{\partial \mathbf t \partial \mathbf t'}(\zer) &amp; = \var{\X}</span>
<span id="cb13-286"><a href="#cb13-286" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-287"><a href="#cb13-287" aria-hidden="true" tabindex="-1"></a>The various derivatives of $K_{\X}(\mathbf t)$ are known as &lt;span style="color:red"&gt;**_cumulants of $X$_**&lt;/span&gt;, and happen to coincide with expectation and variance (both specific moments of $X$) for the derivatives shown above. </span>
<span id="cb13-288"><a href="#cb13-288" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-289"><a href="#cb13-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-290"><a href="#cb13-290" aria-hidden="true" tabindex="-1"></a>Proving this is a neat application of Taylor series, and you may have seen it in an undergrad probability course. When applying this to exponential families, we can relate the cumulant function $A(\et)$ to the expectation and variance of $\X$, hence its name. </span>
<span id="cb13-291"><a href="#cb13-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-292"><a href="#cb13-292" aria-hidden="true" tabindex="-1"></a>:::{#prp-}</span>
<span id="cb13-293"><a href="#cb13-293" aria-hidden="true" tabindex="-1"></a>Suppose $\X \sim P_\thet$, where $P_\thet \in \mathcal P$ for an exponential family $\mathcal P$. The MGF and KGF of $\mathbf T(\X)$ are given as: </span>
<span id="cb13-294"><a href="#cb13-294" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-295"><a href="#cb13-295" aria-hidden="true" tabindex="-1"></a>M_{\mathbf T(\X)}(\mathbf t) &amp; = \exp<span class="co">[</span><span class="ot">A(\et  + \mathbf t) - A(\et)</span><span class="co">]</span>,<span class="sc">\\</span></span>
<span id="cb13-296"><a href="#cb13-296" aria-hidden="true" tabindex="-1"></a>K_{\mathbf T(\X)}(\mathbf t) &amp; = A(\et  + \mathbf t) - A(\et).</span>
<span id="cb13-297"><a href="#cb13-297" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-298"><a href="#cb13-298" aria-hidden="true" tabindex="-1"></a>Consequently, we have </span>
<span id="cb13-299"><a href="#cb13-299" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-300"><a href="#cb13-300" aria-hidden="true" tabindex="-1"></a>\E{\mathbf T(\X)} &amp; = \frac{\partial A(\et)}{\partial \et} = \nabla_\et A(\et),<span class="sc">\\</span></span>
<span id="cb13-301"><a href="#cb13-301" aria-hidden="true" tabindex="-1"></a>\var{\mathbf T(\X)} &amp; = \frac{\partial ^2A(\et)}{\partial \et\partial \et'}.</span>
<span id="cb13-302"><a href="#cb13-302" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-303"><a href="#cb13-303" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-304"><a href="#cb13-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-305"><a href="#cb13-305" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb13-306"><a href="#cb13-306" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-307"><a href="#cb13-307" aria-hidden="true" tabindex="-1"></a>M_{\mathbf T(\X)}(\mathbf t) &amp; = \E{\exp(\mathbf t'\mathbf T(\X))} = \int_{\mathcal X} \exp(\mathbf t'\mathbf T(\X))h(\x)\exp<span class="co">[</span><span class="ot">\boldsymbol \eta(\thet)\cdot\mathbf T(\x) - A(\et)</span><span class="co">]</span>\ d\x<span class="sc">\\</span></span>
<span id="cb13-308"><a href="#cb13-308" aria-hidden="true" tabindex="-1"></a>&amp; =  \exp<span class="co">[</span><span class="ot">-A(\et)</span><span class="co">]</span>\int_{\mathcal X}h(\x)\exp<span class="co">[</span><span class="ot">(\boldsymbol \eta(\thet) + \mathbf t)'\mathbf T(\x)</span><span class="co">]</span>\ d\x<span class="sc">\\</span></span>
<span id="cb13-309"><a href="#cb13-309" aria-hidden="true" tabindex="-1"></a>&amp; = \exp<span class="co">[</span><span class="ot">-A(\et)</span><span class="co">]</span>\exp\left<span class="co">[</span><span class="ot">\log\left(\int_{\mathcal X}h(\x)\exp[(\boldsymbol \eta(\thet) + \mathbf t)'\mathbf T(\x)]\ d\x\right)\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb13-310"><a href="#cb13-310" aria-hidden="true" tabindex="-1"></a>&amp; = \exp<span class="co">[</span><span class="ot">-A(\et)</span><span class="co">]</span>\exp<span class="co">[</span><span class="ot">A(\et +\mathbf t)</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb13-311"><a href="#cb13-311" aria-hidden="true" tabindex="-1"></a>&amp; = \exp<span class="co">[</span><span class="ot">A(\et +\mathbf t) - A(\et)</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb13-312"><a href="#cb13-312" aria-hidden="true" tabindex="-1"></a>K_{\mathbf T(\X)}(\mathbf t) &amp; = \log M_{\X}(\mathbf t)<span class="sc">\\</span></span>
<span id="cb13-313"><a href="#cb13-313" aria-hidden="true" tabindex="-1"></a>&amp; = A(\et +\mathbf t) - A(\et)</span>
<span id="cb13-314"><a href="#cb13-314" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-315"><a href="#cb13-315" aria-hidden="true" tabindex="-1"></a>We can not differentiate $K_{\mathbf T(\X)}(\mathbf t)$ to calculate the expectation and variance. </span>
<span id="cb13-316"><a href="#cb13-316" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-317"><a href="#cb13-317" aria-hidden="true" tabindex="-1"></a>\E{\mathbf T(\X)} &amp; = \frac{\partial K_{\X}}{\partial \mathbf t}(\zer) = \left<span class="co">[</span><span class="ot">\frac{\partial}{\partial \mathbf t}[A(\et +\mathbf t) - A(\et)]\right</span><span class="co">]</span>_{\mathbf t = \zer} = \left[\frac{\partial}{\partial \mathbf t}[\et + \mathbf t]\frac{\partial A}{\partial \et}\right]_{\mathbf t = \zer} = \frac{\partial A(\et)}{\partial \et}<span class="sc">\\</span></span>
<span id="cb13-318"><a href="#cb13-318" aria-hidden="true" tabindex="-1"></a>\var{\mathbf T(\X)} &amp; = \frac{\partial^2 K_{\X}}{\partial \mathbf t \partial \mathbf t'}(\zer)  =\left<span class="co">[</span><span class="ot">\frac{\partial}{\partial \mathbf t'}\left[\frac{\partial K_{\X}}{\partial \mathbf t}\right]\right</span><span class="co">]</span>_{\mathbf t = \zer} =  \left[\frac{\partial}{\partial \mathbf t '}\left[\frac{\partial}{\partial \mathbf t}[A(\et +\mathbf t) - A(\et)]\right]\right]_{\mathbf t = \zer} = \frac{\partial ^2A(\et)}{\partial \et\partial \et'}.</span>
<span id="cb13-319"><a href="#cb13-319" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-320"><a href="#cb13-320" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-321"><a href="#cb13-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-322"><a href="#cb13-322" aria-hidden="true" tabindex="-1"></a>:::{#cor-}</span>
<span id="cb13-323"><a href="#cb13-323" aria-hidden="true" tabindex="-1"></a>Suppose $\X \sim P_\thet$, where $P_\thet \in \mathcal P$ for a natural exponential family $\mathcal P$. Then</span>
<span id="cb13-324"><a href="#cb13-324" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-325"><a href="#cb13-325" aria-hidden="true" tabindex="-1"></a>\E{\X} &amp; = \frac{\partial A(\et)}{\partial \et} = \nabla_\et A(\et),<span class="sc">\\</span></span>
<span id="cb13-326"><a href="#cb13-326" aria-hidden="true" tabindex="-1"></a>\var{\X} &amp; = \frac{\partial ^2A(\et)}{\partial \et\partial \et'}.</span>
<span id="cb13-327"><a href="#cb13-327" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-328"><a href="#cb13-328" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-329"><a href="#cb13-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-330"><a href="#cb13-330" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb13-331"><a href="#cb13-331" aria-hidden="true" tabindex="-1"></a>If $\mathcal P$ is a natural exponential family, then $T(\X) = \X$.</span>
<span id="cb13-332"><a href="#cb13-332" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-333"><a href="#cb13-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-334"><a href="#cb13-334" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb13-335"><a href="#cb13-335" aria-hidden="true" tabindex="-1"></a>If $X \sim N(\mu,\sigma^2)$, then $A(\et) = -\eta_1^2/4\eta_2 - \log(-2\eta_2)/2$ for $\et = <span class="co">[</span><span class="ot">\mu/\sigma^2, -1/2\sigma^2</span><span class="co">]</span>'$. The sufficient statistic is $<span class="co">[</span><span class="ot">x,x^2</span><span class="co">]</span>'$. We have: </span>
<span id="cb13-336"><a href="#cb13-336" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-337"><a href="#cb13-337" aria-hidden="true" tabindex="-1"></a>\E{\mathbf T(\X)} &amp; = \frac{\partial}{\partial \et}<span class="co">[</span><span class="ot">-\eta_1^2/4\eta_2 - \log(-2\eta_2)/2</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb13-338"><a href="#cb13-338" aria-hidden="true" tabindex="-1"></a>&amp; = \begin{bmatrix} - \frac{\eta_1}{2\eta_2} &amp; \frac{\eta_1^2}{4\eta_2^2} - \frac{1}{2\eta_2} \end{bmatrix} <span class="sc">\\</span></span>
<span id="cb13-339"><a href="#cb13-339" aria-hidden="true" tabindex="-1"></a>&amp; = \begin{bmatrix} - \frac{\mu/\sigma^2}{2(-1/2\sigma^2)} &amp; \frac{(\mu/\sigma^2)^2}{4(-1/2\sigma^2)^2} - \frac{1}{2(-1/2\sigma^2)} \end{bmatrix}<span class="sc">\\</span></span>
<span id="cb13-340"><a href="#cb13-340" aria-hidden="true" tabindex="-1"></a>&amp; = \begin{bmatrix} \mu &amp; \mu^2 - \sigma^2 \end{bmatrix}</span>
<span id="cb13-341"><a href="#cb13-341" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-342"><a href="#cb13-342" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-343"><a href="#cb13-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-344"><a href="#cb13-344" aria-hidden="true" tabindex="-1"></a>Exponential families also exhibit convexity in two respects. </span>
<span id="cb13-345"><a href="#cb13-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-346"><a href="#cb13-346" aria-hidden="true" tabindex="-1"></a>:::{#prp-}</span>
<span id="cb13-347"><a href="#cb13-347" aria-hidden="true" tabindex="-1"></a>Suppose $\X \sim P_\thet$, where $P_\thet \in \mathcal P$ for an exponential family $\mathcal P$. The &lt;span style="color:red"&gt;**_natural parameter space_**&lt;/span&gt;, defined as $$ \mathcal N = \left<span class="sc">\{</span>\et\ \bigg|\ \int_{\mathcal X}\exp<span class="co">[</span><span class="ot">\et\cdot \mathbf T(\x)</span><span class="co">]</span>\ d\x &lt;\infty \right<span class="sc">\}</span>,$$ is a convex set. In addition, the cumulant function $A(\et)$ is convex on the set $\mathcal N$</span>
<span id="cb13-348"><a href="#cb13-348" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-349"><a href="#cb13-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-350"><a href="#cb13-350" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb13-351"><a href="#cb13-351" aria-hidden="true" tabindex="-1"></a>To show the convexity of $\mathcal N$, we must show that $\alpha\et_1 + (1-\alpha)\et_2 \in \mathcal N$ for any $\alpha \in <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$. This means we must verify that the following integral is finite: </span>
<span id="cb13-352"><a href="#cb13-352" aria-hidden="true" tabindex="-1"></a>$$ \int_{\mathcal X}h(\x)\exp<span class="co">[</span><span class="ot">(\alpha\et_1 + (1-\alpha)\et_2)\cdot \mathbf T(\x)</span><span class="co">]</span>\ d\x &lt;\infty .$$ </span>
<span id="cb13-353"><a href="#cb13-353" aria-hidden="true" tabindex="-1"></a>This happens to be an application of <span class="co">[</span><span class="ot">Hölder's Inequality</span><span class="co">](https://mathworld.wolfram.com/HoeldersInequalities.html)</span>. </span>
<span id="cb13-354"><a href="#cb13-354" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-355"><a href="#cb13-355" aria-hidden="true" tabindex="-1"></a>\int_{\mathcal X}h(\x)\exp<span class="co">[</span><span class="ot">(\alpha\et_1 + (1-\alpha)\et_2)\cdot \mathbf T(\x)</span><span class="co">]</span>\ d\x &amp; =\left(\int_{\mathcal X}h(\x)\exp<span class="co">[</span><span class="ot">(\alpha\et_1 + (1-\alpha)\et_2)\cdot \mathbf T(\x)</span><span class="co">]</span>\ d\x\right)^1 <span class="sc">\\</span> &amp; = \left(\int_{\mathcal X}h(\x)\exp<span class="co">[</span><span class="ot">(\alpha\et_1 + (1-\alpha)\et_2)\cdot \mathbf T(\x)</span><span class="co">]</span>\ d\x\right)^{\alpha + (1-\alpha)}  <span class="sc">\\</span>&amp; = \left(\int_{\mathcal X}h(\x)\exp<span class="co">[</span><span class="ot">(\alpha\et_1)\cdot \mathbf T(\x)</span><span class="co">]</span>\exp<span class="co">[</span><span class="ot">((1-\alpha)\et_2)\cdot \mathbf T(\x)</span><span class="co">]</span>\ d\x\right)^ {\alpha + (1-\alpha)}<span class="sc">\\</span></span>
<span id="cb13-356"><a href="#cb13-356" aria-hidden="true" tabindex="-1"></a>&amp; \le \underbrace{\left(\int_{\mathcal X}h(\x)\exp<span class="co">[</span><span class="ot">(\alpha\et_1)\cdot \mathbf T(\x)</span><span class="co">]</span>\ d\x\right)^{\alpha}}_{\et_2 \in \mathcal N \implies &lt; \infty}\underbrace{\left(\int_{\mathcal X}h(\x)\exp[((1-\alpha)\et_2)\cdot \mathbf T(\x)]\ d\x\right)^{1-\alpha}}_{\et_2 \in \mathcal N \implies &lt; \infty}</span>
<span id="cb13-357"><a href="#cb13-357" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-358"><a href="#cb13-358" aria-hidden="true" tabindex="-1"></a>The integral is finite, so $\mathcal N$ is convex. If we take the logarithm of both sides of this inequality, we find that $A(\et)$ is a convex function, recalling that $A(\et)$ can be written as the log of the integral of $\exp<span class="co">[</span><span class="ot">\et\cdot \mathbf T(\x)</span><span class="co">]</span>$ over $\mathcal X$.</span>
<span id="cb13-359"><a href="#cb13-359" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-360"><a href="#cb13-360" aria-hidden="true" tabindex="-1"></a>&amp;\log\left(\int_{\mathcal X}h(\x)\exp<span class="co">[</span><span class="ot">(\alpha\et_1 + (1-\alpha)\et_2)\cdot \mathbf T(\x)</span><span class="co">]</span>\ d\x\right)   \le \alpha \log\left(\int_{\mathcal X}h(\x)\exp<span class="co">[</span><span class="ot">(\alpha\et_1)\cdot \mathbf T(\x)</span><span class="co">]</span>\ d\x\right) + (1- \alpha) \log\left(\int_{\mathcal X}h(\x)\exp<span class="co">[</span><span class="ot">((1-\alpha)\et_2)\cdot \mathbf T(\x)</span><span class="co">]</span>\ d\x\right)<span class="sc">\\</span></span>
<span id="cb13-361"><a href="#cb13-361" aria-hidden="true" tabindex="-1"></a>\implies &amp; A<span class="co">[</span><span class="ot">(\alpha\et_1 + (1-\alpha)\et_2)</span><span class="co">]</span> \le \alpha A(\et_1) + (1-\alpha)A(\et_2) </span>
<span id="cb13-362"><a href="#cb13-362" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-363"><a href="#cb13-363" aria-hidden="true" tabindex="-1"></a>This makes $A$ convex.</span>
<span id="cb13-364"><a href="#cb13-364" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-365"><a href="#cb13-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-366"><a href="#cb13-366" aria-hidden="true" tabindex="-1"></a>Finally, we can show that in one dimension, exponential families exhibit the MLR property when $\eta$ is an increasing function. Consequently, we can always apply the Karlin-Rubin theorem from @sec-testing in this case. </span>
<span id="cb13-367"><a href="#cb13-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-368"><a href="#cb13-368" aria-hidden="true" tabindex="-1"></a>:::{#thm-}</span>
<span id="cb13-369"><a href="#cb13-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-370"><a href="#cb13-370" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exponential Families and MLR</span></span>
<span id="cb13-371"><a href="#cb13-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-372"><a href="#cb13-372" aria-hidden="true" tabindex="-1"></a>When $\dim(\thet) = 1$ and $\eta(\theta)$ is non-decreasing, exponential families exhibit the MLR property in that sufficient statistic $T(x)$.</span>
<span id="cb13-373"><a href="#cb13-373" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-374"><a href="#cb13-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-375"><a href="#cb13-375" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb13-376"><a href="#cb13-376" aria-hidden="true" tabindex="-1"></a>When $f_X(\x \mid \theta) = h(x)\exp\left<span class="co">[</span><span class="ot">\eta(\theta)T(\x) - A(\theta)\right</span><span class="co">]</span>$, then the likelihood ratio is </span>
<span id="cb13-377"><a href="#cb13-377" aria-hidden="true" tabindex="-1"></a>$$ \frac{f_X(\x \mid \theta_1)}{f_X(\x \mid \theta_0)} = \exp\left<span class="co">[</span><span class="ot">(\eta(\theta_1)-\eta(\theta_0)T(x)) - (A(\theta_1) - A(\theta_0))\right</span><span class="co">]</span>.$$</span>
<span id="cb13-378"><a href="#cb13-378" aria-hidden="true" tabindex="-1"></a>The derivative of this ratio with respect to the statistic $T(x)$ is </span>
<span id="cb13-379"><a href="#cb13-379" aria-hidden="true" tabindex="-1"></a>$$ <span class="co">[</span><span class="ot">\eta(\theta_1)-\eta(\theta_0)</span><span class="co">]</span>\cdot \frac{f_X(\x \mid \theta_1)}{f_X(\x \mid \theta_0)},$$ where $<span class="co">[</span><span class="ot">\eta(\theta_1)-\eta(\theta_0)</span><span class="co">]</span> &gt; 0$ because $\eta$ is non-decreasing, and $f_X(\x \mid \theta_1)/f_X(\x \mid \theta_0) &gt; 0$ because it is the ratio of two probability densities. The derivative is therefore positive, and the likelihood ratio is monotonically increasing in $T(x)$.</span>
<span id="cb13-380"><a href="#cb13-380" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-381"><a href="#cb13-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-382"><a href="#cb13-382" aria-hidden="true" tabindex="-1"></a>This theorem is particularly useful in the context of hypothesis testing. If our test statistic is a sufficient statistic, then by @thm-kpd it is distributed according to an exponential family, exhibits the MLR property, and we can use @thm-KR to construct a UMP test. </span>
<span id="cb13-383"><a href="#cb13-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-384"><a href="#cb13-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-385"><a href="#cb13-385" aria-hidden="true" tabindex="-1"></a><span class="fu">## Entropy and the Maximum Entropy Principle</span></span>
<span id="cb13-386"><a href="#cb13-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-387"><a href="#cb13-387" aria-hidden="true" tabindex="-1"></a>Sufficiency is not the only means of arriving at the exponential family. A second derivation deals with some basic concepts from information theory. Loosely speaking, information theory studies how information is stored and communicated. The discipline exists at the intersection of probability, computer science, electrical engineering, physics, and statistical mechanics. The foundations of information theory were outline in @shannon1948mathematical, an article which happens to be the fourth most cited paper ever (according to Google Scholar).</span>
<span id="cb13-388"><a href="#cb13-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-389"><a href="#cb13-389" aria-hidden="true" tabindex="-1"></a>A crucial aspect of the transmission of information is uncertainty. If we have a probability space $(\mathcal X,\mathcal F, P)$ and some random variable $X$, how do we measure how "surprising" an event $x\in \mathcal X$ is? The greater $\Pr(X = x)$, the less surprising the outcome $x$ is. We want to define some measure $\text{Surprise}(x)$ such that:</span>
<span id="cb13-390"><a href="#cb13-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-391"><a href="#cb13-391" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\text{Surprise}(x) \to 1$ as $\Pr(X = x)\to 0$ and $\text{Surprise}(x) \to 0$ as $\Pr(X = x)\to \infty$.</span>
<span id="cb13-392"><a href="#cb13-392" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\text{Surprise}(x)$ is monotonic in $\Pr(X= x)$.</span>
<span id="cb13-393"><a href="#cb13-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-394"><a href="#cb13-394" aria-hidden="true" tabindex="-1"></a>These properties are satisfied by the function $\log(1/\Pr(X= x))$.</span>
<span id="cb13-395"><a href="#cb13-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-396"><a href="#cb13-396" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb13-397"><a href="#cb13-397" aria-hidden="true" tabindex="-1"></a>The &lt;span style="color:red"&gt;**_information content_**&lt;/span&gt; of an outcome $x\in \mathcal X$ is </span>
<span id="cb13-398"><a href="#cb13-398" aria-hidden="true" tabindex="-1"></a>$$ I_X(x) = \log_b\left(\frac{1}{\Pr(X=x)}\right) = -\log_b<span class="co">[</span><span class="ot">\Pr(X= x)</span><span class="co">]</span>$$ for a base $b$. If $b = 2$ then the unit $I_X(x)$ is given in &lt;span style="color:red"&gt;**_bits_**&lt;/span&gt;. If $b$ is the natural exponent, the unit is  &lt;span style="color:red"&gt;**_nat_**&lt;/span&gt;.</span>
<span id="cb13-399"><a href="#cb13-399" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-400"><a href="#cb13-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-401"><a href="#cb13-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-402"><a href="#cb13-402" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb13-403"><a href="#cb13-403" aria-hidden="true" tabindex="-1"></a>Suppose $X \sim \text{Bernoulli}(p)$, where $\mathcal X = <span class="sc">\{</span>0,1<span class="sc">\}</span>$ and $\Pr(X = 1) = p$. The information content for $x = 1$ (a "success") is </span>
<span id="cb13-404"><a href="#cb13-404" aria-hidden="true" tabindex="-1"></a>$$I_X(1) = \log_2(1/p).$$</span>
<span id="cb13-407"><a href="#cb13-407" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb13-408"><a href="#cb13-408" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb13-409"><a href="#cb13-409" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot41</span></span>
<span id="cb13-410"><a href="#cb13-410" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb13-411"><a href="#cb13-411" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb13-412"><a href="#cb13-412" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb13-413"><a href="#cb13-413" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Information for x = 1 for various values of the parameter p"</span></span>
<span id="cb13-414"><a href="#cb13-414" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb13-415"><a href="#cb13-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-416"><a href="#cb13-416" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb13-417"><a href="#cb13-417" aria-hidden="true" tabindex="-1"></a>  <span class="at">p =</span> (<span class="dv">0</span><span class="sc">:</span><span class="dv">1000</span>)<span class="sc">/</span><span class="dv">1000</span>,</span>
<span id="cb13-418"><a href="#cb13-418" aria-hidden="true" tabindex="-1"></a>  <span class="at">I =</span> <span class="sc">-</span><span class="fu">log2</span>(p)</span>
<span id="cb13-419"><a href="#cb13-419" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb13-420"><a href="#cb13-420" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(p, I)) <span class="sc">+</span> </span>
<span id="cb13-421"><a href="#cb13-421" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb13-422"><a href="#cb13-422" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb13-423"><a href="#cb13-423" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Pr(x = 1)"</span>,</span>
<span id="cb13-424"><a href="#cb13-424" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Information Content of x = 1"</span>)</span>
<span id="cb13-425"><a href="#cb13-425" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-426"><a href="#cb13-426" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-427"><a href="#cb13-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-428"><a href="#cb13-428" aria-hidden="true" tabindex="-1"></a>If we average the information content over the sample space $\mathcal X$, we get the entropy of a random variable. </span>
<span id="cb13-429"><a href="#cb13-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-430"><a href="#cb13-430" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb13-431"><a href="#cb13-431" aria-hidden="true" tabindex="-1"></a>The &lt;span style="color:red"&gt;**_entropy_**&lt;/span&gt; of a random variable $X$ is </span>
<span id="cb13-432"><a href="#cb13-432" aria-hidden="true" tabindex="-1"></a>$$H(X) = \E{I_X(x)} = -\int_\mathcal X  \log_b<span class="co">[</span><span class="ot">f(x)</span><span class="co">]</span>\ dF_X = -\int_\mathcal X f(x) \log_b<span class="co">[</span><span class="ot">f(x)</span><span class="co">]</span>\ dx.$$</span>
<span id="cb13-433"><a href="#cb13-433" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-434"><a href="#cb13-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-435"><a href="#cb13-435" aria-hidden="true" tabindex="-1"></a>Entropy captures the average amount of information inherent in a random variable's outcomes. </span>
<span id="cb13-436"><a href="#cb13-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-437"><a href="#cb13-437" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb13-438"><a href="#cb13-438" aria-hidden="true" tabindex="-1"></a>Again, suppose $X \sim \text{Bernoulli}(p)$. The entropy of $X$ is </span>
<span id="cb13-439"><a href="#cb13-439" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-440"><a href="#cb13-440" aria-hidden="true" tabindex="-1"></a>H(X) = - \sum_{x\in <span class="sc">\{</span>0,1<span class="sc">\}</span>} \Pr(x) \log_2<span class="co">[</span><span class="ot">\Pr(x)</span><span class="co">]</span> = - (1-p)\log_2(1-p) - p \log_2(p).</span>
<span id="cb13-441"><a href="#cb13-441" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-442"><a href="#cb13-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-445"><a href="#cb13-445" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb13-446"><a href="#cb13-446" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb13-447"><a href="#cb13-447" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot42</span></span>
<span id="cb13-448"><a href="#cb13-448" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb13-449"><a href="#cb13-449" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb13-450"><a href="#cb13-450" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb13-451"><a href="#cb13-451" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The entropy of a bernoulli random variable as a function of the parameter p"</span></span>
<span id="cb13-452"><a href="#cb13-452" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb13-453"><a href="#cb13-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-454"><a href="#cb13-454" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb13-455"><a href="#cb13-455" aria-hidden="true" tabindex="-1"></a>  <span class="at">p =</span> (<span class="dv">0</span><span class="sc">:</span><span class="dv">1000</span>)<span class="sc">/</span><span class="dv">1000</span>,</span>
<span id="cb13-456"><a href="#cb13-456" aria-hidden="true" tabindex="-1"></a>  <span class="at">I =</span> <span class="sc">-</span>(<span class="dv">1</span><span class="sc">-</span>p)<span class="sc">*</span><span class="fu">log2</span>(<span class="dv">1</span><span class="sc">-</span>p) <span class="sc">-</span> p<span class="sc">*</span><span class="fu">log2</span>(p)</span>
<span id="cb13-457"><a href="#cb13-457" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb13-458"><a href="#cb13-458" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(p, I)) <span class="sc">+</span> </span>
<span id="cb13-459"><a href="#cb13-459" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb13-460"><a href="#cb13-460" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb13-461"><a href="#cb13-461" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"p = Pr(x = 1)"</span>,</span>
<span id="cb13-462"><a href="#cb13-462" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Entropy of X"</span>)</span>
<span id="cb13-463"><a href="#cb13-463" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-464"><a href="#cb13-464" aria-hidden="true" tabindex="-1"></a>The greater the entropy, the greater the uncertainty associated with a random variables outcomes. In the event $p\in<span class="sc">\{</span>0,1<span class="sc">\}</span>$, then we're certain that $x = 1$ or $x=0$, and there is no uncertainty. If $p=0.5$, it's equally likely that $x=1$ as it is that $x=0$, so things are less certain. </span>
<span id="cb13-465"><a href="#cb13-465" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-466"><a href="#cb13-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-467"><a href="#cb13-467" aria-hidden="true" tabindex="-1"></a>We can also measure the relative entropy between two distributions. Henceforth we'll stick to the natural logarithm. </span>
<span id="cb13-468"><a href="#cb13-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-469"><a href="#cb13-469" aria-hidden="true" tabindex="-1"></a>:::{#def-}</span>
<span id="cb13-470"><a href="#cb13-470" aria-hidden="true" tabindex="-1"></a>Suppose $X$ and $Y$ are random variables with distributions $f_X$ and $F_Y$, respectively. The &lt;span style="color:red"&gt;**_Kullback–Leibler (KL) divergence/relative entropy_**&lt;/span&gt; of $f_X$ and $F_Y$, denoted $D_{KL}(f_X \mid\mid F_Y)$ is defined as </span>
<span id="cb13-471"><a href="#cb13-471" aria-hidden="true" tabindex="-1"></a>$$ D_{KL}(f_X \mid\mid F_Y) = \int_{\mathcal X} f_X(t)\cdot \log \frac{f_X(t)}{f_Y(t)}\ dt.$$</span>
<span id="cb13-472"><a href="#cb13-472" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-473"><a href="#cb13-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-474"><a href="#cb13-474" aria-hidden="true" tabindex="-1"></a>KL divergence measures how "close" $f_X$ is to $F_Y$. Despite measuring "distance", it is not a valid metric because it is not symmetric $( D_{KL}(f_X \mid\mid F_Y)  \neq  D_{KL}(F_Y \mid\mid f_X) )$ and does not satisfy the triangle inequality. </span>
<span id="cb13-475"><a href="#cb13-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-476"><a href="#cb13-476" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb13-477"><a href="#cb13-477" aria-hidden="true" tabindex="-1"></a>Suppose the role of an unfair six-sided die corresponds to a random variable $X$ whose density is </span>
<span id="cb13-478"><a href="#cb13-478" aria-hidden="true" tabindex="-1"></a>$f_X(t) = x/21$ for $t=1,\ldots,6$. If we want to model the role of the die, wrongfully assuming it is fair, we would pick $f_Y(x) = 1/6$ for $t=1,\ldots,6$.</span>
<span id="cb13-479"><a href="#cb13-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-482"><a href="#cb13-482" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb13-483"><a href="#cb13-483" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb13-484"><a href="#cb13-484" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot43</span></span>
<span id="cb13-485"><a href="#cb13-485" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb13-486"><a href="#cb13-486" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb13-487"><a href="#cb13-487" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb13-488"><a href="#cb13-488" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Respective densities of X and Y"</span></span>
<span id="cb13-489"><a href="#cb13-489" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb13-490"><a href="#cb13-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-491"><a href="#cb13-491" aria-hidden="true" tabindex="-1"></a><span class="fu">expand_grid</span>(</span>
<span id="cb13-492"><a href="#cb13-492" aria-hidden="true" tabindex="-1"></a>  <span class="at">gr =</span> <span class="fu">c</span>(<span class="st">"Y"</span>, <span class="st">"X"</span>), </span>
<span id="cb13-493"><a href="#cb13-493" aria-hidden="true" tabindex="-1"></a>  <span class="at">t =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span></span>
<span id="cb13-494"><a href="#cb13-494" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span> </span>
<span id="cb13-495"><a href="#cb13-495" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">f =</span> <span class="fu">ifelse</span>(gr <span class="sc">==</span> <span class="st">"Y"</span>, <span class="dv">1</span><span class="sc">/</span><span class="dv">6</span>,  t<span class="sc">/</span><span class="dv">21</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb13-496"><a href="#cb13-496" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(t, f)) <span class="sc">+</span></span>
<span id="cb13-497"><a href="#cb13-497" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> t, <span class="at">xend =</span> t, <span class="at">y=</span> <span class="dv">0</span>, <span class="at">yend =</span> f)) <span class="sc">+</span></span>
<span id="cb13-498"><a href="#cb13-498" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb13-499"><a href="#cb13-499" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb13-500"><a href="#cb13-500" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>gr, <span class="at">ncol =</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb13-501"><a href="#cb13-501" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Value of Die Roll"</span>, <span class="at">y =</span> <span class="st">"Probability Density"</span>)</span>
<span id="cb13-502"><a href="#cb13-502" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-503"><a href="#cb13-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-504"><a href="#cb13-504" aria-hidden="true" tabindex="-1"></a>In this case $D_{KL}(f_X \mid\mid F_Y)$ measures the expected excess surprise from modeling the die roll with the random variable $Y$ instead of $X$.</span>
<span id="cb13-505"><a href="#cb13-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-506"><a href="#cb13-506" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-507"><a href="#cb13-507" aria-hidden="true" tabindex="-1"></a>D_{KL}(f_X \mid\mid F_Y) &amp; = \sum_{t \in \mathcal X} f_X(t)\cdot \log \frac{f_X(t)}{f_Y(t)}<span class="sc">\\</span></span>
<span id="cb13-508"><a href="#cb13-508" aria-hidden="true" tabindex="-1"></a>&amp; = \sum_{t=1}^6 \frac{t}{21}\cdot \log\left(\frac{t/21}{1/6}\right)<span class="sc">\\</span></span>
<span id="cb13-509"><a href="#cb13-509" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{1}{21}\sum_{t=1}^6 t\cdot \log\left(2t/7\right)<span class="sc">\\</span></span>
<span id="cb13-510"><a href="#cb13-510" aria-hidden="true" tabindex="-1"></a>&amp; \approx 0.1293825</span>
<span id="cb13-511"><a href="#cb13-511" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-512"><a href="#cb13-512" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-513"><a href="#cb13-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-514"><a href="#cb13-514" aria-hidden="true" tabindex="-1"></a>:::{#thm-}</span>
<span id="cb13-515"><a href="#cb13-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-516"><a href="#cb13-516" aria-hidden="true" tabindex="-1"></a><span class="fu">## Gibb's Inequality</span></span>
<span id="cb13-517"><a href="#cb13-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-518"><a href="#cb13-518" aria-hidden="true" tabindex="-1"></a>$D_{KL}(f_X \mid\mid F_Y) \ge 0$ *if and only if* $f_X \neq F_Y$.</span>
<span id="cb13-519"><a href="#cb13-519" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-520"><a href="#cb13-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-521"><a href="#cb13-521" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb13-522"><a href="#cb13-522" aria-hidden="true" tabindex="-1"></a>The function $-\log$ is convex, so by Jensen's inequality </span>
<span id="cb13-523"><a href="#cb13-523" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-524"><a href="#cb13-524" aria-hidden="true" tabindex="-1"></a>D_{KL}(f_X \mid\mid F_Y) &amp; = \int_{\mathcal X} f_X(t)\cdot \log \frac{f_X(t)}{f_Y(t)}\ dt<span class="sc">\\</span></span>
<span id="cb13-525"><a href="#cb13-525" aria-hidden="true" tabindex="-1"></a>&amp; = \int_{\mathcal X} f_X(t)\cdot -\log \frac{f_Y(t)}{f_X(t)}\ dt<span class="sc">\\</span></span>
<span id="cb13-526"><a href="#cb13-526" aria-hidden="true" tabindex="-1"></a>&amp; \ge -\log\left<span class="co">[</span><span class="ot">\int_{\mathcal X} f_X(t)\frac{f_Y(t)}{f_X(t)}\ dt\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb13-527"><a href="#cb13-527" aria-hidden="true" tabindex="-1"></a>&amp; = -\log\left<span class="co">[</span><span class="ot">\int_{\mathcal X} f_Y(t)\ dt\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb13-528"><a href="#cb13-528" aria-hidden="true" tabindex="-1"></a>&amp; = -\log 1<span class="sc">\\</span></span>
<span id="cb13-529"><a href="#cb13-529" aria-hidden="true" tabindex="-1"></a>&amp; = 0</span>
<span id="cb13-530"><a href="#cb13-530" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-531"><a href="#cb13-531" aria-hidden="true" tabindex="-1"></a>&lt;span style="color:white"&gt; space &lt;/span&gt;</span>
<span id="cb13-532"><a href="#cb13-532" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-533"><a href="#cb13-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-534"><a href="#cb13-534" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb13-535"><a href="#cb13-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-536"><a href="#cb13-536" aria-hidden="true" tabindex="-1"></a><span class="fu">## Entropy of Uniform Distribution</span></span>
<span id="cb13-537"><a href="#cb13-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-538"><a href="#cb13-538" aria-hidden="true" tabindex="-1"></a>Suppose $X \sim \text{Uni}(a,b)$. The entropy of $f_X$ is </span>
<span id="cb13-539"><a href="#cb13-539" aria-hidden="true" tabindex="-1"></a>$$ H(X) = -\int_a^b\frac{1}{b-a}\log\left(\frac{1}{b-a}\right)\ dt = \log(b-a).$$ It turns out, that the uniform distribution has the maximum entropy of all distributions contained on the interval $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$. Intuitively, if the probability $X = x$ is uniform over $x\in\mathcal X$, then there is no certainty about our outcomes. If you were asked to guess a realized value of $X$ beforehand, you would have zero confidence in your guess, because all outcomes are equally likely. Formally, we want to solve the problem </span>
<span id="cb13-540"><a href="#cb13-540" aria-hidden="true" tabindex="-1"></a>$$\max_{f} H(x)\text{ such that }\int_{a}^bf(t)\ dt= 1.$$</span>
<span id="cb13-541"><a href="#cb13-541" aria-hidden="true" tabindex="-1"></a>The Lagrangian associated with this problem is </span>
<span id="cb13-542"><a href="#cb13-542" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-543"><a href="#cb13-543" aria-hidden="true" tabindex="-1"></a>\mathcal L(f) &amp;= -H(t) - \lambda \left(\int_{a}^b f(t)\ dt - 1\right)<span class="sc">\\</span></span>
<span id="cb13-544"><a href="#cb13-544" aria-hidden="true" tabindex="-1"></a>&amp; = \int_a^b f(t) \log f(t)\ dt - \lambda \left(\int_{a}^b f(t)\ dt - 1 \right)<span class="sc">\\</span></span>
<span id="cb13-545"><a href="#cb13-545" aria-hidden="true" tabindex="-1"></a>&amp; = \int_a^b f(t) \log f(t) - \lambda f(t) \ dt - \lambda</span>
<span id="cb13-546"><a href="#cb13-546" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-547"><a href="#cb13-547" aria-hidden="true" tabindex="-1"></a>Okay, but how do we optimize a function with respect to another function? We're not picking some value to minimize $\mathcal L$, we're picking some function $f_X$ (which happens to be a valid density on the support $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$). Optimization problems like these are solved using the calculus of variations (see @clarke2013functional).^<span class="co">[</span><span class="ot">The calculus of variations comes in handy when working with optimal control problems and dynamic optimization as well.</span><span class="co">]</span> If $\mathcal F$ is the set of all real functions $f:\mathbb R\to \mathbb R$, then $\mathcal L:\mathcal F\to \mathbb R$. Mappings which take functions to numbers are known as **_functionals_**. The **_functional derivative_** of $\mathcal L$ with respect to $f$ is given as </span>
<span id="cb13-548"><a href="#cb13-548" aria-hidden="true" tabindex="-1"></a>$$\frac{\delta \mathcal L}{\delta f} = \lim_{\varepsilon \to 0} \frac{\mathcal L(f + \varepsilon g) - \mathcal L(f)}{\varepsilon} $$</span>
<span id="cb13-549"><a href="#cb13-549" aria-hidden="true" tabindex="-1"></a>for some arbitrary function $g\in \mathcal F$.^[Whether this derivative exists deals with Frechet differentiability and the existence of a continuous linear operator between Banach spaces. This linear operator is the functional derivative, and is itself a functional. As such, we can actually write the derivative as an integral, because all linear functionals can be expressed as integrals by The Riesz-representation theorem (roughly speaking): </span>
<span id="cb13-550"><a href="#cb13-550" aria-hidden="true" tabindex="-1"></a>$$\lim_{\varepsilon \to 0} \frac{\mathcal L(f + \varepsilon g) - \mathcal L(f)}{\varepsilon} = \int \frac{\delta \mathcal L}{\delta f}g(x)\ dx $$] We can calculate functional derivatives directly appealing to the definition, but that's a pain in the butt. Instead we'll use the <span class="co">[</span><span class="ot">Euler-Lagrange equation</span><span class="co">](https://mathworld.wolfram.com/Euler-LagrangeDifferentialEquation.html)</span> which gives the derivative in the case where $\mathcal L$ can be expressed as an integral:</span>
<span id="cb13-551"><a href="#cb13-551" aria-hidden="true" tabindex="-1"></a>$$\mathcal L(f) = \int J(t,f(t), f'(t))\ dt \implies \frac{\delta \mathcal L}{\delta f} = \frac{\partial J}{\partial f} - \frac{d}{dt}\frac{\partial J}{\partial f'}.$$</span>
<span id="cb13-552"><a href="#cb13-552" aria-hidden="true" tabindex="-1"></a>If we apply this to the Lagrangian we have the following first order conditions:</span>
<span id="cb13-553"><a href="#cb13-553" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-554"><a href="#cb13-554" aria-hidden="true" tabindex="-1"></a>    \log f(t) &amp; = -1 - \lambda <span class="sc">\\</span></span>
<span id="cb13-555"><a href="#cb13-555" aria-hidden="true" tabindex="-1"></a>    \int_{a}^b f(t)\ dt &amp; = 1 </span>
<span id="cb13-556"><a href="#cb13-556" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-557"><a href="#cb13-557" aria-hidden="true" tabindex="-1"></a>We can solve for $f(t) = \exp(-1-\lambda)$, which is constant, so our distribution $f$ is constant over $<span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$, making it the uniform distribution. Explicitly, we have </span>
<span id="cb13-558"><a href="#cb13-558" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-559"><a href="#cb13-559" aria-hidden="true" tabindex="-1"></a>&amp;\int_{a}^b f(t)\ dt  = 1 <span class="sc">\\</span></span>
<span id="cb13-560"><a href="#cb13-560" aria-hidden="true" tabindex="-1"></a>\implies &amp; \exp(-1-\lambda) \int_{a}^b \ dt  = 1<span class="sc">\\</span></span>
<span id="cb13-561"><a href="#cb13-561" aria-hidden="true" tabindex="-1"></a>\implies &amp; \exp(-1-\lambda) = \frac{1}{b-a}<span class="sc">\\</span></span>
<span id="cb13-562"><a href="#cb13-562" aria-hidden="true" tabindex="-1"></a>\implies &amp; f(t)  = \frac{1}{b-a}.</span>
<span id="cb13-563"><a href="#cb13-563" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-564"><a href="#cb13-564" aria-hidden="true" tabindex="-1"></a>Therefore $f_X(t) = 1/(b-a)$ maximizes entropy.</span>
<span id="cb13-565"><a href="#cb13-565" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-566"><a href="#cb13-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-567"><a href="#cb13-567" aria-hidden="true" tabindex="-1"></a>So what is appealing about maximizing entropy? In a sense, a distribution with maximal entropy comes with minimal assumptions. This concept is known as the "principle of maximum entropy" as is due to @jaynes1957information. If we want to model a natural phenomenon with a probability distribution $F_X$, the class of which define a regular model $\mathcal P$, and we only know that $\mathcal X = <span class="co">[</span><span class="ot">a,b</span><span class="co">]</span>$, then we should assume $X\sim \text{Uni}(a,b)$ according to the principle of maximum entropy. What if we have additional information? For instance, we may have data that allows us to estimate $\E{X}$ or $\var{X}$, something that can be done without specifying a regular model $\mathcal P$. </span>
<span id="cb13-568"><a href="#cb13-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-569"><a href="#cb13-569" aria-hidden="true" tabindex="-1"></a>Formally, consider defining a model $\mathcal P$ where $\X \sim P_{\thet}$ such that $\E{\mathbf T(\X)} = \thet$ for some function $\mathbf g(\X) = <span class="co">[</span><span class="ot">T_1(\X), \ldots, T_k(\X)</span><span class="co">]</span>$. The function $\mathbf T$ corresponds to all the distributional assumptions we are willing to make about $\X$, and these assumptions come in the form of moment conditions. Where do these assumptions come from? If we observe $n$ realizations of $(\X_1,\ldots, \X_n)$ then we can consistently estimate $\E{\mathbf T(\X)}$, so for a sufficiently large $n$ we will be able to approximate $\thet$.  We could define the model as </span>
<span id="cb13-570"><a href="#cb13-570" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-571"><a href="#cb13-571" aria-hidden="true" tabindex="-1"></a>\mathcal P &amp;= <span class="sc">\{</span> P_{\thet} <span class="sc">\}</span>,<span class="sc">\\</span></span>
<span id="cb13-572"><a href="#cb13-572" aria-hidden="true" tabindex="-1"></a>P_{\thet} &amp;= <span class="sc">\{</span>F_\X \mid \E{\mathbf T(\X)} = \thet<span class="sc">\}</span>,</span>
<span id="cb13-573"><a href="#cb13-573" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-574"><a href="#cb13-574" aria-hidden="true" tabindex="-1"></a>where each model value $P_{\thet}$ is an infinite collection of distributions satisfying our moment conditions. This model is parametric but is not regular, as $P_{\thet}$ is not a singleton for all $P_{\thet} \in \mathcal P$. If we insisted on a regular model, we need to go beyond moment conditions and actually assume the functional form of $F_X$. The principle of maximum entropy gives us a criterion to appeal to here. We will define $\mathcal P$ such that each $P_{\thet} \in \mathcal P$ is a single distribution given by </span>
<span id="cb13-575"><a href="#cb13-575" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-576"><a href="#cb13-576" aria-hidden="true" tabindex="-1"></a>&amp; \max_{f} H(\mathbf t)\text{ such that }\int_{\mathcal X}f(\mathbf t)\ d\mathbf t= 1\text{ and }\E{\mathbf T(\X)} = \thet<span class="sc">\\</span></span>
<span id="cb13-577"><a href="#cb13-577" aria-hidden="true" tabindex="-1"></a>\implies &amp; \max_{f} H(\mathbf t)\text{ such that }\int_{\mathcal X}f(\mathbf t)\ d\mathbf t= 1\text{ and }\int_{\mathcal X}\mathbf T(\mathbf t)f(\mathbf t) \ d\mathbf t= \thet</span>
<span id="cb13-578"><a href="#cb13-578" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-579"><a href="#cb13-579" aria-hidden="true" tabindex="-1"></a>The Lagrangian associated with this problem is </span>
<span id="cb13-580"><a href="#cb13-580" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-581"><a href="#cb13-581" aria-hidden="true" tabindex="-1"></a>\mathcal L(f) &amp;= - H(f) - \lambda \left(\int_{\mathcal X}f(\mathbf t)\ d\mathbf t-1\right) - \boldsymbol \eta \left(\int_{\mathcal X}\mathbf T(\mathbf t)f(\mathbf t)\ d\mathbf t-\thet\right)<span class="sc">\\</span></span>
<span id="cb13-582"><a href="#cb13-582" aria-hidden="true" tabindex="-1"></a>&amp; = \int_{\mathcal X}f(\mathbf t) \log f(\mathbf t)\ d\mathbf t - \lambda \left(\int_{\mathcal X}f(\mathbf t)\ d\mathbf t-1\right) - \sum_{j=1}^k\eta_j\left(\int_{\mathcal X} T_j(\mathbf t)f(\mathbf t)\ d\mathbf t-c_j\right)<span class="sc">\\</span></span>
<span id="cb13-583"><a href="#cb13-583" aria-hidden="true" tabindex="-1"></a>&amp; = \int_{\mathcal X}\left<span class="co">[</span><span class="ot">f(\mathbf t) \log f(\mathbf t)- \lambda(\mathbf t)- \sum_{j=1}^k\eta_jT_j(\mathbf t) \ d\mathbf t \right</span><span class="co">]</span>- \lambda- \boldsymbol \eta \cdot \thet</span>
<span id="cb13-584"><a href="#cb13-584" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-585"><a href="#cb13-585" aria-hidden="true" tabindex="-1"></a>where the multiplier $\lambda$ corresponds to the first constraint ($f$ is a valid density), and the multipliers $\et$ correspond to the second constraint (the moment conditions are satisfied). The corresponding first order conditions are:</span>
<span id="cb13-586"><a href="#cb13-586" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-587"><a href="#cb13-587" aria-hidden="true" tabindex="-1"></a>&amp;\frac{\delta \mathcal L}{\delta f}  = \log f(\x) + 1 - \lambda - \boldsymbol \eta \cdot \mathbf T(\x) = 0<span class="sc">\\</span></span>
<span id="cb13-588"><a href="#cb13-588" aria-hidden="true" tabindex="-1"></a>&amp;\int_{\mathcal X}f(\mathbf t)\ d\mathbf t  = 1<span class="sc">\\</span></span>
<span id="cb13-589"><a href="#cb13-589" aria-hidden="true" tabindex="-1"></a>&amp;\int_{\mathcal X}\mathbf T(\mathbf t)f(\mathbf t) \ d\mathbf t = \thet</span>
<span id="cb13-590"><a href="#cb13-590" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-591"><a href="#cb13-591" aria-hidden="true" tabindex="-1"></a>Solving the first equation for $f(\x)$ gives</span>
<span id="cb13-592"><a href="#cb13-592" aria-hidden="true" tabindex="-1"></a>$$ f(\x) = \exp(\lambda - 1)\exp(\et \cdot \mathbf T(\x)).$$ </span>
<span id="cb13-593"><a href="#cb13-593" aria-hidden="true" tabindex="-1"></a>Substituting this into the second condition gives:</span>
<span id="cb13-594"><a href="#cb13-594" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-595"><a href="#cb13-595" aria-hidden="true" tabindex="-1"></a>&amp;\int_{\mathcal X}\exp(\lambda- 1)\exp(\et \cdot \mathbf T(\x))\ d\x = 1<span class="sc">\\</span></span>
<span id="cb13-596"><a href="#cb13-596" aria-hidden="true" tabindex="-1"></a>\implies &amp; \exp(\lambda - 1)\int_{\mathcal X}\exp(\et \cdot \mathbf T(\x))\ d\x = 1<span class="sc">\\</span></span>
<span id="cb13-597"><a href="#cb13-597" aria-hidden="true" tabindex="-1"></a>\implies &amp; \int_{\mathcal X}\exp(\et \cdot \mathbf T(\x))\ d\x = \exp(1-\lambda)</span>
<span id="cb13-598"><a href="#cb13-598" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-599"><a href="#cb13-599" aria-hidden="true" tabindex="-1"></a>We integrate over $\x$, but $\exp(1-\lambda)$ is still a function of $\et$. Define $A(\et)$ as </span>
<span id="cb13-600"><a href="#cb13-600" aria-hidden="true" tabindex="-1"></a>$$ A(\et) =  \log\left<span class="co">[</span><span class="ot">\int_{\mathcal X}\exp(\et \cdot \mathbf T(\x))\ d\x\right</span><span class="co">]</span>$$ such that $\exp(\lambda - 1) = \exp (-A(\et))$.</span>
<span id="cb13-601"><a href="#cb13-601" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-602"><a href="#cb13-602" aria-hidden="true" tabindex="-1"></a> f(\x) &amp;= \exp(\lambda - 1)\exp(\et \cdot \mathbf T(\x)) <span class="sc">\\</span> &amp;= \exp(-A(\et))\exp(\et \cdot \mathbf T(\x))<span class="sc">\\</span> &amp; = \exp\left<span class="co">[</span><span class="ot">\et \cdot \mathbf T(\x) - A(\et)\right</span><span class="co">]</span>.</span>
<span id="cb13-603"><a href="#cb13-603" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-604"><a href="#cb13-604" aria-hidden="true" tabindex="-1"></a>It turns out that $f(\x)$ is an exponential family where $h(\x) = 1$. The reason $h(\x)$ is normalized in this instance has to do with a change of probability measure, but in general exponential families are those with maximum entropy.</span>
<span id="cb13-605"><a href="#cb13-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-606"><a href="#cb13-606" aria-hidden="true" tabindex="-1"></a>:::{#thm-exmax}</span>
<span id="cb13-607"><a href="#cb13-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-608"><a href="#cb13-608" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exponential Families Maximize Entropy</span></span>
<span id="cb13-609"><a href="#cb13-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-610"><a href="#cb13-610" aria-hidden="true" tabindex="-1"></a>For all probability densities $g(\x)$ satisfying $\E{\mathbf T(\x)} = \thet$, </span>
<span id="cb13-611"><a href="#cb13-611" aria-hidden="true" tabindex="-1"></a>$$ H(f) \ge H(g)$$ where $f(\x)= \exp\left<span class="co">[</span><span class="ot">\et \cdot \mathbf T(\x) - A(\et)\right</span><span class="co">]</span>$ is define as above.</span>
<span id="cb13-612"><a href="#cb13-612" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-613"><a href="#cb13-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-614"><a href="#cb13-614" aria-hidden="true" tabindex="-1"></a>::: {#exm-}</span>
<span id="cb13-615"><a href="#cb13-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-616"><a href="#cb13-616" aria-hidden="true" tabindex="-1"></a><span class="fu">## Numerical Optimization</span></span>
<span id="cb13-617"><a href="#cb13-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-618"><a href="#cb13-618" aria-hidden="true" tabindex="-1"></a>In a perfect world we could confirm the fact that exponential families maximize entropy by telling our computer "solve this constrained optimization problem" and confirming the result is an exponential family. Unfortunately, this is only feasible for discrete random variable. For continuous random variables, the solution to the maximum-entropy problem is a continuous function, so it's not clear how to solve the problem numerically. Fortunately, we can approximate the optimization problem arbitrarily well via "discretization", just like how we can approximate integrals with finite Riemann sums. Suppose our sample space $\mathcal X$ is an interval of $\R$. Instead of calculating the entropy by integrating over all of $\mathcal X$, we can approximate it by calculating the sum of the entropy at a set of discrete points in $\X$. If these points are $<span class="sc">\{</span>x_i<span class="sc">\}</span>_{i=1}^n$, and the $p_i=f(x_i)$ for a density function $f$, then we have </span>
<span id="cb13-619"><a href="#cb13-619" aria-hidden="true" tabindex="-1"></a>$$ H(t) = -\int_{\mathcal X}f(t)\log f(t)\ dt \approx - \sum_{i=1}^n p_i\log p_i \cdot\underbrace{(x_{i-1}-x_i)}_{\Delta x_i}.$$ Similarly, our approximate constraints are </span>
<span id="cb13-620"><a href="#cb13-620" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-621"><a href="#cb13-621" aria-hidden="true" tabindex="-1"></a>\int_{\mathcal X}f(t)\ dt &amp;\approx \sum_{i=1}^n p_i\cdot\Delta x_i = 1,<span class="sc">\\</span></span>
<span id="cb13-622"><a href="#cb13-622" aria-hidden="true" tabindex="-1"></a>\int_{\mathcal X}\mathbf T(\mathbf t)f(\mathbf t) \ d\mathbf t &amp;\approx \sum_{i=1}^n\mathbf T(x_i)p_i\cdot\Delta x_i = \thet.</span>
<span id="cb13-623"><a href="#cb13-623" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-624"><a href="#cb13-624" aria-hidden="true" tabindex="-1"></a>Our discretized problem is </span>
<span id="cb13-625"><a href="#cb13-625" aria-hidden="true" tabindex="-1"></a>$$ \max_{\mathbf p} - \sum_{i=1}^n p_i\log p_i \cdot\Delta x_i \text{ such that }\sum_{i=1}^n p_i\cdot\Delta x_i = 1 \text{ and } \sum_{i=1}^n\mathbf T(x_i)p_i\cdot\Delta x_i = \thet,$$ where the vector $\x$ is the finite set of points which we approximate the sample space $\mathcal X$ with, and $\mathbf p = f(\x)$ is the probability assigned to each of these points. For a concrete example, consider the problem of maximizing the entropy of a distribution over the interval $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ with no other constraints. We've already shown that the resulting distribution is the uniform distribution using the calculus of variations, but let's arrive at the same conclusion by solving the discretized version of the problem. We'll divide the interval $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ using 100 equally spaced points $<span class="sc">\{</span>0.01,0.02,\ldots,0.99,1<span class="sc">\}</span>$ ($\Delta x_i = 1/100$ for all $i$). We will find the vector $\mathbf p\in\R^{100}$ which solves</span>
<span id="cb13-626"><a href="#cb13-626" aria-hidden="true" tabindex="-1"></a>$$ \max_{\mathbf p} - \sum_{i=1}^n \frac{p_i\log p_i}{100} \text{ such that }\sum_{i=1}^n \frac{p_i}{100} = 1.$$ We could solve this problem using R's ```optim()```, but instead  we'll use the ```CVXR``` package due to @fu2017cvxr based on the work of @grant2006disciplined. This package is made specifically for convex optimization problems (a category which our problem falls into), and is very user-friendly.</span>
<span id="cb13-627"><a href="#cb13-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-630"><a href="#cb13-630" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb13-631"><a href="#cb13-631" aria-hidden="true" tabindex="-1"></a><span class="co">#set the dimension of the problem</span></span>
<span id="cb13-632"><a href="#cb13-632" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb13-633"><a href="#cb13-633" aria-hidden="true" tabindex="-1"></a>delta_x <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>n</span>
<span id="cb13-634"><a href="#cb13-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-635"><a href="#cb13-635" aria-hidden="true" tabindex="-1"></a><span class="co">#define variable, objective, constraints, and problem</span></span>
<span id="cb13-636"><a href="#cb13-636" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">Variable</span>(n)</span>
<span id="cb13-637"><a href="#cb13-637" aria-hidden="true" tabindex="-1"></a><span class="co">#make sure to use CVXR's entr() function</span></span>
<span id="cb13-638"><a href="#cb13-638" aria-hidden="true" tabindex="-1"></a>objective <span class="ot">&lt;-</span> <span class="fu">Maximize</span>(<span class="fu">sum</span>(<span class="fu">entr</span>(p)<span class="sc">*</span>delta_x))</span>
<span id="cb13-639"><a href="#cb13-639" aria-hidden="true" tabindex="-1"></a>constraints <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="fu">sum</span>(p<span class="sc">*</span>delta_x) <span class="sc">==</span> <span class="dv">1</span>)</span>
<span id="cb13-640"><a href="#cb13-640" aria-hidden="true" tabindex="-1"></a>problem <span class="ot">&lt;-</span> <span class="fu">Problem</span>(objective, constraints)</span>
<span id="cb13-641"><a href="#cb13-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-642"><a href="#cb13-642" aria-hidden="true" tabindex="-1"></a><span class="co">#solve problem</span></span>
<span id="cb13-643"><a href="#cb13-643" aria-hidden="true" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">solve</span>(problem)</span>
<span id="cb13-644"><a href="#cb13-644" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-645"><a href="#cb13-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-646"><a href="#cb13-646" aria-hidden="true" tabindex="-1"></a>If we plot our solution, we see that it corresponds perfectly to the uniform distribution.</span>
<span id="cb13-647"><a href="#cb13-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-650"><a href="#cb13-650" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb13-651"><a href="#cb13-651" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb13-652"><a href="#cb13-652" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot44</span></span>
<span id="cb13-653"><a href="#cb13-653" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb13-654"><a href="#cb13-654" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb13-655"><a href="#cb13-655" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb13-656"><a href="#cb13-656" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Analytic and numerical solution to maximum entropy problem."</span></span>
<span id="cb13-657"><a href="#cb13-657" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb13-658"><a href="#cb13-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-659"><a href="#cb13-659" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb13-660"><a href="#cb13-660" aria-hidden="true" tabindex="-1"></a> <span class="at">x =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length =</span> n),</span>
<span id="cb13-661"><a href="#cb13-661" aria-hidden="true" tabindex="-1"></a> <span class="at">p =</span> result<span class="sc">$</span><span class="fu">getValue</span>(p)</span>
<span id="cb13-662"><a href="#cb13-662" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb13-663"><a href="#cb13-663" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, p)) <span class="sc">+</span> </span>
<span id="cb13-664"><a href="#cb13-664" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_function</span>(<span class="at">fun =</span> dunif, <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">"Uniform Distribution"</span>)) <span class="sc">+</span> </span>
<span id="cb13-665"><a href="#cb13-665" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.5</span>, <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">"Numerical Solution"</span>)) <span class="sc">+</span></span>
<span id="cb13-666"><a href="#cb13-666" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="dv">0</span>,<span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb13-667"><a href="#cb13-667" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb13-668"><a href="#cb13-668" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="st">""</span>, <span class="at">values =</span> <span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"red"</span>)) <span class="sc">+</span></span>
<span id="cb13-669"><a href="#cb13-669" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb13-670"><a href="#cb13-670" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span>
<span id="cb13-671"><a href="#cb13-671" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-672"><a href="#cb13-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-673"><a href="#cb13-673" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-674"><a href="#cb13-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-675"><a href="#cb13-675" aria-hidden="true" tabindex="-1"></a>We'll demonstrate @thm-exmax with two more examples where we'll derive exponential families by</span>
<span id="cb13-676"><a href="#cb13-676" aria-hidden="true" tabindex="-1"></a>maximizing entropy. In each case we'll confirm our work by solving the corresponding discretized</span>
<span id="cb13-677"><a href="#cb13-677" aria-hidden="true" tabindex="-1"></a>optimization problem numerically.</span>
<span id="cb13-678"><a href="#cb13-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-679"><a href="#cb13-679" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb13-680"><a href="#cb13-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-681"><a href="#cb13-681" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exponential Distribution</span></span>
<span id="cb13-682"><a href="#cb13-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-683"><a href="#cb13-683" aria-hidden="true" tabindex="-1"></a>Suppose we want to model a random variable $X$ with a sample space $\mathcal X =[0,\infty)$ according to the principle of maximum entropy such that $\E{X}= \theta$. The Lagrangian is </span>
<span id="cb13-684"><a href="#cb13-684" aria-hidden="true" tabindex="-1"></a>$$ \mathcal L(f) = \int_0^\infty f(t)\log f(t)\ dt - \lambda\left(\int_0^\infty f(t) \ dt - 1\right) - \eta\left(\int_0^\infty t\cdot f(t) \ dt - \theta\right),$$ </span>
<span id="cb13-685"><a href="#cb13-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-686"><a href="#cb13-686" aria-hidden="true" tabindex="-1"></a>which gives first order conditions:</span>
<span id="cb13-687"><a href="#cb13-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-688"><a href="#cb13-688" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-689"><a href="#cb13-689" aria-hidden="true" tabindex="-1"></a>&amp;\frac{\delta \mathcal L}{\delta f} = \log f(x) + 1 - \lambda - \eta x = 0<span class="sc">\\</span></span>
<span id="cb13-690"><a href="#cb13-690" aria-hidden="true" tabindex="-1"></a>&amp;\int_0^\infty f(t) \ dt = 1<span class="sc">\\</span></span>
<span id="cb13-691"><a href="#cb13-691" aria-hidden="true" tabindex="-1"></a>&amp;\int_0^\infty t\cdot f(t) = \theta</span>
<span id="cb13-692"><a href="#cb13-692" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-693"><a href="#cb13-693" aria-hidden="true" tabindex="-1"></a>Solving the first equation for $f(x)$ gives $f(x) = \exp(1-\lambda)\exp(\eta x)$. If we plug this into the second equation (the first constraint) we have:</span>
<span id="cb13-694"><a href="#cb13-694" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-695"><a href="#cb13-695" aria-hidden="true" tabindex="-1"></a>&amp;  \int_0^\infty \exp(\lambda - 1)\exp(\eta x) = 1<span class="sc">\\</span></span>
<span id="cb13-696"><a href="#cb13-696" aria-hidden="true" tabindex="-1"></a>\implies &amp; \exp(\lambda - 1)\int_0^\infty \exp(\eta x) = 1<span class="sc">\\</span></span>
<span id="cb13-697"><a href="#cb13-697" aria-hidden="true" tabindex="-1"></a>\implies &amp; \exp(\lambda - 1)\left<span class="co">[</span><span class="ot">\frac{1}{\eta}\exp(\eta x)\right</span><span class="co">]</span>_0^\infty = 1<span class="sc">\\</span></span>
<span id="cb13-698"><a href="#cb13-698" aria-hidden="true" tabindex="-1"></a>\implies &amp; \exp(\lambda - 1)(-1/\eta) = 1 &amp; (-1/\eta &lt; 0)</span>
<span id="cb13-699"><a href="#cb13-699" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-700"><a href="#cb13-700" aria-hidden="true" tabindex="-1"></a>If $-1/\eta \ge 0$, then the improper integral will not converge. Let's repeat this step with the second integral:</span>
<span id="cb13-701"><a href="#cb13-701" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-702"><a href="#cb13-702" aria-hidden="true" tabindex="-1"></a>&amp; \int_0^\infty x\exp(\lambda - 1)\exp(\eta x) = \theta<span class="sc">\\</span></span>
<span id="cb13-703"><a href="#cb13-703" aria-hidden="true" tabindex="-1"></a>\implies &amp; \exp(\lambda - 1)\int_0^\infty x\exp(\eta x) = \theta<span class="sc">\\</span></span>
<span id="cb13-704"><a href="#cb13-704" aria-hidden="true" tabindex="-1"></a>\implies &amp; \exp(\lambda - 1)\left<span class="co">[</span><span class="ot">\frac{x\exp(\eta x)}{\eta} - \frac{\exp(\eta x)}{\eta}\right</span><span class="co">]</span>_0^\infty = 1 &amp; (\text{integration by parts})<span class="sc">\\</span></span>
<span id="cb13-705"><a href="#cb13-705" aria-hidden="true" tabindex="-1"></a>\implies &amp; \exp(\lambda - 1)(1/\eta^2) = \theta</span>
<span id="cb13-706"><a href="#cb13-706" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-707"><a href="#cb13-707" aria-hidden="true" tabindex="-1"></a>If we divide the two constraints by each other, we have $-\eta = 1/\theta$:</span>
<span id="cb13-708"><a href="#cb13-708" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-709"><a href="#cb13-709" aria-hidden="true" tabindex="-1"></a>&amp; \frac{\exp(\lambda - 1)(-1/\eta)}{\exp(\lambda - 1)(1/\eta^2)} = \frac{1}{\theta}<span class="sc">\\</span></span>
<span id="cb13-710"><a href="#cb13-710" aria-hidden="true" tabindex="-1"></a>\implies &amp; -\eta = 1/\theta<span class="sc">\\</span></span>
<span id="cb13-711"><a href="#cb13-711" aria-hidden="true" tabindex="-1"></a>\implies &amp; -1/\eta = \theta</span>
<span id="cb13-712"><a href="#cb13-712" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-713"><a href="#cb13-713" aria-hidden="true" tabindex="-1"></a>Therefore,</span>
<span id="cb13-714"><a href="#cb13-714" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-715"><a href="#cb13-715" aria-hidden="true" tabindex="-1"></a>&amp;\exp(\lambda - 1)(-1/\eta) = 1<span class="sc">\\</span></span>
<span id="cb13-716"><a href="#cb13-716" aria-hidden="true" tabindex="-1"></a>\implies &amp;\exp(\lambda - 1)\theta = 1<span class="sc">\\</span></span>
<span id="cb13-717"><a href="#cb13-717" aria-hidden="true" tabindex="-1"></a>\implies &amp; \exp(\lambda - 1) = 1/\theta</span>
<span id="cb13-718"><a href="#cb13-718" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-719"><a href="#cb13-719" aria-hidden="true" tabindex="-1"></a>so $$f(x) = \exp(1-\lambda)\exp(\eta x) = \frac{1}{\theta}\exp(-x/\theta).$$ This is the exponential distribution which is parameterized by $\theta$, where $\theta$ comes from the constraint $\E{X} = \theta$.</span>
<span id="cb13-720"><a href="#cb13-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-721"><a href="#cb13-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-722"><a href="#cb13-722" aria-hidden="true" tabindex="-1"></a>To discretize the problem, we'll approximate the sample space $\mathcal X =(0,\infty)$ with the $n=250$ points $<span class="sc">\{</span>0.04, 0.08, \ldots, 10<span class="sc">\}</span>$ ($\Delta x_i = 1/100$ for $i=1,\ldots,250$). The exponential distribution has negligible density on the interval $(10,\infty)$, so our approximation of $<span class="co">[</span><span class="ot">0,\infty)$ is still valid despite the points being a subset of $[0,10</span><span class="co">]</span>$. The approximated problem is </span>
<span id="cb13-723"><a href="#cb13-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-724"><a href="#cb13-724" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-725"><a href="#cb13-725" aria-hidden="true" tabindex="-1"></a>&amp; \max_{\mathbf p} - \sum_{i=1}^n p_i\log p_i \cdot \frac{1}{250},<span class="sc">\\</span></span>
<span id="cb13-726"><a href="#cb13-726" aria-hidden="true" tabindex="-1"></a>&amp;\text{such that} \sum_{i=1}^n p_i\cdot  \frac{1}{250} = 1, <span class="sc">\\</span></span>
<span id="cb13-727"><a href="#cb13-727" aria-hidden="true" tabindex="-1"></a>&amp;\text{and} \sum_{i=1}^n p_i x_i  \frac{1}{250} = \theta .<span class="sc">\\</span></span>
<span id="cb13-728"><a href="#cb13-728" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-729"><a href="#cb13-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-730"><a href="#cb13-730" aria-hidden="true" tabindex="-1"></a>We'll take $\theta = 1$ for this problem.</span>
<span id="cb13-731"><a href="#cb13-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-734"><a href="#cb13-734" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb13-735"><a href="#cb13-735" aria-hidden="true" tabindex="-1"></a><span class="co">#set the dimension of the problem</span></span>
<span id="cb13-736"><a href="#cb13-736" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb13-737"><a href="#cb13-737" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">250</span></span>
<span id="cb13-738"><a href="#cb13-738" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.04</span>, <span class="dv">10</span>, <span class="at">length =</span> n)</span>
<span id="cb13-739"><a href="#cb13-739" aria-hidden="true" tabindex="-1"></a>delta_x <span class="ot">&lt;-</span> x[<span class="dv">2</span>] <span class="sc">-</span> x[<span class="dv">1</span>]</span>
<span id="cb13-740"><a href="#cb13-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-741"><a href="#cb13-741" aria-hidden="true" tabindex="-1"></a><span class="co">#define variable, objective, constraints, and problem</span></span>
<span id="cb13-742"><a href="#cb13-742" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">Variable</span>(n)</span>
<span id="cb13-743"><a href="#cb13-743" aria-hidden="true" tabindex="-1"></a>objective <span class="ot">&lt;-</span> <span class="fu">Maximize</span>(<span class="fu">sum</span>(<span class="fu">entr</span>(p)<span class="sc">*</span>delta_x))</span>
<span id="cb13-744"><a href="#cb13-744" aria-hidden="true" tabindex="-1"></a>constraints <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="fu">sum</span>(p<span class="sc">*</span>delta_x) <span class="sc">==</span> <span class="dv">1</span>, </span>
<span id="cb13-745"><a href="#cb13-745" aria-hidden="true" tabindex="-1"></a>                    <span class="fu">sum</span>(p<span class="sc">*</span>x<span class="sc">*</span>delta_x) <span class="sc">==</span> theta)</span>
<span id="cb13-746"><a href="#cb13-746" aria-hidden="true" tabindex="-1"></a>problem <span class="ot">&lt;-</span> <span class="fu">Problem</span>(objective, constraints)</span>
<span id="cb13-747"><a href="#cb13-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-748"><a href="#cb13-748" aria-hidden="true" tabindex="-1"></a><span class="co">#solve problem</span></span>
<span id="cb13-749"><a href="#cb13-749" aria-hidden="true" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">solve</span>(problem)</span>
<span id="cb13-750"><a href="#cb13-750" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-751"><a href="#cb13-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-752"><a href="#cb13-752" aria-hidden="true" tabindex="-1"></a>If we plot our solution, we find that it is in almost perfect alignment with the exponential distribution.   </span>
<span id="cb13-753"><a href="#cb13-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-756"><a href="#cb13-756" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb13-757"><a href="#cb13-757" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb13-758"><a href="#cb13-758" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot45</span></span>
<span id="cb13-759"><a href="#cb13-759" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb13-760"><a href="#cb13-760" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb13-761"><a href="#cb13-761" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb13-762"><a href="#cb13-762" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Analytic and numerical solution to maximum entropy problem."</span></span>
<span id="cb13-763"><a href="#cb13-763" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb13-764"><a href="#cb13-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-765"><a href="#cb13-765" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb13-766"><a href="#cb13-766" aria-hidden="true" tabindex="-1"></a> <span class="at">x =</span> x,</span>
<span id="cb13-767"><a href="#cb13-767" aria-hidden="true" tabindex="-1"></a> <span class="at">p =</span> result<span class="sc">$</span><span class="fu">getValue</span>(p)</span>
<span id="cb13-768"><a href="#cb13-768" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb13-769"><a href="#cb13-769" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, p)) <span class="sc">+</span> </span>
<span id="cb13-770"><a href="#cb13-770" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_function</span>(<span class="at">fun =</span> dexp, <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">"Exponential Distribution, θ = 1"</span>)) <span class="sc">+</span> </span>
<span id="cb13-771"><a href="#cb13-771" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.1</span>, <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">"Numerical Solution"</span>)) <span class="sc">+</span> </span>
<span id="cb13-772"><a href="#cb13-772" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb13-773"><a href="#cb13-773" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="st">""</span>, <span class="at">values =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"black"</span>)) <span class="sc">+</span></span>
<span id="cb13-774"><a href="#cb13-774" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb13-775"><a href="#cb13-775" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span>
<span id="cb13-776"><a href="#cb13-776" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-777"><a href="#cb13-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-778"><a href="#cb13-778" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-779"><a href="#cb13-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-780"><a href="#cb13-780" aria-hidden="true" tabindex="-1"></a>:::{#exm-}</span>
<span id="cb13-781"><a href="#cb13-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-782"><a href="#cb13-782" aria-hidden="true" tabindex="-1"></a><span class="fu">## Normal Distribution</span></span>
<span id="cb13-783"><a href="#cb13-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-784"><a href="#cb13-784" aria-hidden="true" tabindex="-1"></a>Suppose we want to model a random variable $X$ with a sample space $\mathcal X =\mathbb R$ according to the principle of maximum entropy such that $\E{X}= \mu$, and $\var{X} = \sigma^2$. We can combine these into a single constraint $\E{(x-\mu)^2}=\sigma^2$. The Lagrangian is </span>
<span id="cb13-785"><a href="#cb13-785" aria-hidden="true" tabindex="-1"></a>$$ \mathcal L(f) = \int_{-\infty}^\infty f(t)\log f(t)\ dt - \lambda\left(\int_{-\infty}^\infty f(t) \ dt - 1\right) - \eta\left(\int_{-\infty}^\infty (t-\mu)^2\cdot f(t) \ dt - \sigma^2\right),$$ which gives the first order conditions:</span>
<span id="cb13-786"><a href="#cb13-786" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-787"><a href="#cb13-787" aria-hidden="true" tabindex="-1"></a>&amp;\frac{\delta \mathcal L}{\delta f} = \log f(x) + 1 - \lambda - \eta (x-\mu)^2  = 0<span class="sc">\\</span></span>
<span id="cb13-788"><a href="#cb13-788" aria-hidden="true" tabindex="-1"></a>&amp;\int_{-\infty}^\infty f(t) \ dt  = 1<span class="sc">\\</span></span>
<span id="cb13-789"><a href="#cb13-789" aria-hidden="true" tabindex="-1"></a>&amp;\int_{-\infty}^\infty  (t-\mu)^2\cdot f(t) \ dt = \sigma^2</span>
<span id="cb13-790"><a href="#cb13-790" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-791"><a href="#cb13-791" aria-hidden="true" tabindex="-1"></a>Solving the first equation gives </span>
<span id="cb13-792"><a href="#cb13-792" aria-hidden="true" tabindex="-1"></a>$$ f(x) = \exp(\lambda - 1)\exp(\eta(x-\mu)^2),$$</span>
<span id="cb13-793"><a href="#cb13-793" aria-hidden="true" tabindex="-1"></a>which we can substitute into the first constraint. </span>
<span id="cb13-794"><a href="#cb13-794" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-795"><a href="#cb13-795" aria-hidden="true" tabindex="-1"></a>&amp; \int_{-\infty}^\infty \exp(\lambda - 1)\exp(\eta(t-\mu)^2) \ dt = 1<span class="sc">\\</span></span>
<span id="cb13-796"><a href="#cb13-796" aria-hidden="true" tabindex="-1"></a>\implies &amp; \exp(\lambda - 1)\int_{-\infty}^\infty \exp(\eta(t-\mu)^2) \ dt = 1<span class="sc">\\</span></span>
<span id="cb13-797"><a href="#cb13-797" aria-hidden="true" tabindex="-1"></a>\implies &amp; \exp(\lambda - 1)(-\pi/\eta)^{1/2} = 1 &amp; \left(\int_{-\infty}^\infty \exp(a(t+b)^2)\ dt = \sqrt{\pi/a}\right)<span class="sc">\\</span></span>
<span id="cb13-798"><a href="#cb13-798" aria-hidden="true" tabindex="-1"></a>\implies &amp; \exp(\lambda - 1) = (-\eta/\pi)^{1/2}</span>
<span id="cb13-799"><a href="#cb13-799" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-800"><a href="#cb13-800" aria-hidden="true" tabindex="-1"></a>The key step was recognizing the <span class="co">[</span><span class="ot">integral of the Gaussian function</span><span class="co">](https://mathworld.wolfram.com/GaussianIntegral.html)</span>. Now we can turn to the second constraint. </span>
<span id="cb13-801"><a href="#cb13-801" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-802"><a href="#cb13-802" aria-hidden="true" tabindex="-1"></a>&amp; \int_{-\infty}^\infty  (t-\mu)^2\cdot f(t) \ dt = \sigma^2<span class="sc">\\</span></span>
<span id="cb13-803"><a href="#cb13-803" aria-hidden="true" tabindex="-1"></a>\implies &amp; \int_{-\infty}^\infty  (t-\mu)^2\cdot \exp(\lambda - 1)\exp(\eta(x-\mu)^2) \ dt = \sigma^2<span class="sc">\\</span></span>
<span id="cb13-804"><a href="#cb13-804" aria-hidden="true" tabindex="-1"></a>\implies &amp; (-\eta/\pi)^{1/2}\int_{-\infty}^\infty  (t-\mu)^2\cdot \exp(\eta(x-\mu)^2) \ dt = \sigma^2 &amp; (\exp(\lambda - 1) = (-\eta/\pi)^{1/2})<span class="sc">\\</span></span>
<span id="cb13-805"><a href="#cb13-805" aria-hidden="true" tabindex="-1"></a>\implies &amp; (-\eta/\pi)^{1/2}\cdot \frac{1}{2}(-\pi/\eta^3)^{1/2} = \sigma^2<span class="sc">\\</span></span>
<span id="cb13-806"><a href="#cb13-806" aria-hidden="true" tabindex="-1"></a>\implies &amp; \eta = -\frac{1}{2\sigma^2}</span>
<span id="cb13-807"><a href="#cb13-807" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-808"><a href="#cb13-808" aria-hidden="true" tabindex="-1"></a>The integral of $(t-\mu)^2\cdot \exp(\eta(x-\mu)^2)$ follows from a generalization of the integral of the Gaussian function. Therefore,</span>
<span id="cb13-809"><a href="#cb13-809" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-810"><a href="#cb13-810" aria-hidden="true" tabindex="-1"></a>f(x) &amp;= \exp(\lambda - 1)\exp(\eta(x-\mu)^2)<span class="sc">\\</span></span>
<span id="cb13-811"><a href="#cb13-811" aria-hidden="true" tabindex="-1"></a>&amp; = (-\eta/\pi)^{1/2}\exp\left(-\frac{1}{2\sigma^2}(x-\mu)^2\right)<span class="sc">\\</span></span>
<span id="cb13-812"><a href="#cb13-812" aria-hidden="true" tabindex="-1"></a>&amp; = (-(-1/2\sigma^2)/\pi)^{1/2}\exp\left(-\frac{1}{2\sigma^2}(x-\mu)^2\right)<span class="sc">\\</span></span>
<span id="cb13-813"><a href="#cb13-813" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left<span class="co">[</span><span class="ot">-\frac{(x-\mu^2)}{2\sigma^2}\right</span><span class="co">]</span></span>
<span id="cb13-814"><a href="#cb13-814" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-815"><a href="#cb13-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-816"><a href="#cb13-816" aria-hidden="true" tabindex="-1"></a>The normal distribution has negligible density outside the interval $<span class="co">[</span><span class="ot">-4,4</span><span class="co">]</span>\subset\mathcal X=\R$, so we can approximate it with $n=250$ equally spaced points on $<span class="co">[</span><span class="ot">-4,4</span><span class="co">]</span>$ ($\Delta$). The discretized problem is </span>
<span id="cb13-817"><a href="#cb13-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-818"><a href="#cb13-818" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb13-819"><a href="#cb13-819" aria-hidden="true" tabindex="-1"></a>&amp; \max_{\mathbf p} - \sum_{i=1}^n p_i\log p_i \cdot \frac{8}{250},<span class="sc">\\</span></span>
<span id="cb13-820"><a href="#cb13-820" aria-hidden="true" tabindex="-1"></a>&amp;\text{such that} \sum_{i=1}^n p_i\cdot  \frac{8}{250} = \theta_1, <span class="sc">\\</span></span>
<span id="cb13-821"><a href="#cb13-821" aria-hidden="true" tabindex="-1"></a>&amp;\text{and} \sum_{i=1}^n p_i x_i^2 \cdot \frac{8}{250} = \theta_2 .<span class="sc">\\</span></span>
<span id="cb13-822"><a href="#cb13-822" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb13-823"><a href="#cb13-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-824"><a href="#cb13-824" aria-hidden="true" tabindex="-1"></a>We'll let $\thet = (0,1)$, which should give the standard normal distribution.</span>
<span id="cb13-825"><a href="#cb13-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-828"><a href="#cb13-828" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb13-829"><a href="#cb13-829" aria-hidden="true" tabindex="-1"></a><span class="co">#set the dimension of the problem</span></span>
<span id="cb13-830"><a href="#cb13-830" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb13-831"><a href="#cb13-831" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">250</span></span>
<span id="cb13-832"><a href="#cb13-832" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span> <span class="sc">+</span> <span class="dv">8</span><span class="sc">/</span>n, <span class="dv">4</span>, <span class="at">length =</span> n)</span>
<span id="cb13-833"><a href="#cb13-833" aria-hidden="true" tabindex="-1"></a>delta_x <span class="ot">&lt;-</span> x[<span class="dv">2</span>] <span class="sc">-</span> x[<span class="dv">1</span>]</span>
<span id="cb13-834"><a href="#cb13-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-835"><a href="#cb13-835" aria-hidden="true" tabindex="-1"></a><span class="co">#define variable, objective, constraints, and problem</span></span>
<span id="cb13-836"><a href="#cb13-836" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">Variable</span>(n)</span>
<span id="cb13-837"><a href="#cb13-837" aria-hidden="true" tabindex="-1"></a>objective <span class="ot">&lt;-</span> <span class="fu">Maximize</span>(<span class="fu">sum</span>(<span class="fu">entr</span>(p)<span class="sc">*</span>delta_x))</span>
<span id="cb13-838"><a href="#cb13-838" aria-hidden="true" tabindex="-1"></a>constraints <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="fu">sum</span>(p<span class="sc">*</span>delta_x) <span class="sc">==</span> <span class="dv">1</span>, </span>
<span id="cb13-839"><a href="#cb13-839" aria-hidden="true" tabindex="-1"></a>                    <span class="fu">sum</span>(p<span class="sc">*</span>x<span class="sc">*</span>delta_x) <span class="sc">==</span> theta[<span class="dv">1</span>],</span>
<span id="cb13-840"><a href="#cb13-840" aria-hidden="true" tabindex="-1"></a>                    <span class="fu">sum</span>(p<span class="sc">*</span>x<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>delta_x) <span class="sc">==</span> theta[<span class="dv">2</span>])</span>
<span id="cb13-841"><a href="#cb13-841" aria-hidden="true" tabindex="-1"></a>problem <span class="ot">&lt;-</span> <span class="fu">Problem</span>(objective, constraints)</span>
<span id="cb13-842"><a href="#cb13-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-843"><a href="#cb13-843" aria-hidden="true" tabindex="-1"></a><span class="co">#solve problem</span></span>
<span id="cb13-844"><a href="#cb13-844" aria-hidden="true" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">solve</span>(problem)</span>
<span id="cb13-845"><a href="#cb13-845" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-846"><a href="#cb13-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-847"><a href="#cb13-847" aria-hidden="true" tabindex="-1"></a>Once again, we have a solution that looks nearly identitical to the distribution we derived analytically. </span>
<span id="cb13-848"><a href="#cb13-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-851"><a href="#cb13-851" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb13-852"><a href="#cb13-852" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb13-853"><a href="#cb13-853" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-plot46</span></span>
<span id="cb13-854"><a href="#cb13-854" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: center</span></span>
<span id="cb13-855"><a href="#cb13-855" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-asp: 0.7</span></span>
<span id="cb13-856"><a href="#cb13-856" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb13-857"><a href="#cb13-857" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Analytic and numerical solution to maximum entropy problem."</span></span>
<span id="cb13-858"><a href="#cb13-858" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: "Show code which generates figure"</span></span>
<span id="cb13-859"><a href="#cb13-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-860"><a href="#cb13-860" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb13-861"><a href="#cb13-861" aria-hidden="true" tabindex="-1"></a> <span class="at">x =</span> x,</span>
<span id="cb13-862"><a href="#cb13-862" aria-hidden="true" tabindex="-1"></a> <span class="at">p =</span> result<span class="sc">$</span><span class="fu">getValue</span>(p)</span>
<span id="cb13-863"><a href="#cb13-863" aria-hidden="true" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb13-864"><a href="#cb13-864" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(x, p)) <span class="sc">+</span> </span>
<span id="cb13-865"><a href="#cb13-865" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_function</span>(<span class="at">fun =</span> dnorm, <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">"Standard Normal Distribution"</span>)) <span class="sc">+</span> </span>
<span id="cb13-866"><a href="#cb13-866" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.1</span>, <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">"Numerical Solution"</span>)) <span class="sc">+</span> </span>
<span id="cb13-867"><a href="#cb13-867" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb13-868"><a href="#cb13-868" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="st">""</span>, <span class="at">values =</span> <span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"red"</span>)) <span class="sc">+</span></span>
<span id="cb13-869"><a href="#cb13-869" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb13-870"><a href="#cb13-870" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span>
<span id="cb13-871"><a href="#cb13-871" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb13-872"><a href="#cb13-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-873"><a href="#cb13-873" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb13-874"><a href="#cb13-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-875"><a href="#cb13-875" aria-hidden="true" tabindex="-1"></a><span class="fu">## Further reading</span></span>
<span id="cb13-876"><a href="#cb13-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-877"><a href="#cb13-877" aria-hidden="true" tabindex="-1"></a>**_Exponential Families_**: Chapter 18 of @dasgupta2011probability, Section 1.6 of @bickel2015mathematical, Section 1.5 of @lehmann2006theory, these <span class="co">[</span><span class="ot">notes</span><span class="co">](https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf)</span></span>
<span id="cb13-878"><a href="#cb13-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-879"><a href="#cb13-879" aria-hidden="true" tabindex="-1"></a>**_Information Theory_**: Chapter 6 of @pml1Book, Section 1.6 @bishop2006pattern</span>
<span id="cb13-880"><a href="#cb13-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-881"><a href="#cb13-881" aria-hidden="true" tabindex="-1"></a>**_Maximum Entropy Principle_**: <span class="co">[</span><span class="ot">Here</span><span class="co">](http://www.di.fc.ul.pt1/n/~jpn/r/maxent/maxent.html)</span>, <span class="co">[</span><span class="ot">here</span><span class="co">](https://mtlsites.mit.edu/Courses/6.050/2003/notes/chapter10.pdf)</span>, <span class="co">[</span><span class="ot">here</span><span class="co">](https://sgfin.github.io/2017/03/16/Deriving-probability-distributions-using-the-Principle-of-Maximum-Entropy/)</span>,</span>
<span id="cb13-882"><a href="#cb13-882" aria-hidden="true" tabindex="-1"></a>and <span class="co">[</span><span class="ot">here</span><span class="co">](https://bjlkeng.github.io/posts/maximum-entropy-distributions/)</span>. This <span class="co">[</span><span class="ot">course page</span><span class="co">](https://homes.cs.washington.edu/~jrl/teaching/cse599swi16/)</span></span>
<span id="cb13-883"><a href="#cb13-883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-884"><a href="#cb13-884" aria-hidden="true" tabindex="-1"></a>**_All of the Above_**: @jaynes2003probability</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>